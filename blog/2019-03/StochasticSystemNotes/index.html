<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/blog/uploads/assets/icon200.png"><link rel="icon" type="image/png" sizes="32x32" href="/blog/uploads/assets/icon32.png"><link rel="icon" type="image/png" sizes="16x16" href="/blog/uploads/assets/icon16.png"><link rel="mask-icon" href="/blog/images/logo.svg" color="#222"><link rel="stylesheet" href="/blog/css/main.css"><link rel="stylesheet" href="/blog/lib/font-awesome/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:new URL("http://zyxin.xyz/blog").hostname,root:"/blog/",scheme:"Muse",version:"7.6.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!0},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}}}</script><meta name="description" content="Combined notes for part of EECS 501 &amp;amp; MECHENG 549. Prerequisites: Knowledge of Elementary Calculus, Linear Algebra and ProbabilityDiscrete-Time Stochastic SystemStochastic SequencesDefinition: Giv"><meta name="keywords" content="Math,Probability,Stochastic"><meta property="og:type" content="article"><meta property="og:title" content="Notes for Stochastic System"><meta property="og:url" content="http://zyxin.xyz/blog/2019-03/StochasticSystemNotes/index.html"><meta property="og:site_name" content="Jacob Zhong"><meta property="og:description" content="Combined notes for part of EECS 501 &amp;amp; MECHENG 549. Prerequisites: Knowledge of Elementary Calculus, Linear Algebra and ProbabilityDiscrete-Time Stochastic SystemStochastic SequencesDefinition: Giv"><meta property="og:locale" content="en"><meta property="og:updated_time" content="2019-11-12T03:02:41.997Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Notes for Stochastic System"><meta name="twitter:description" content="Combined notes for part of EECS 501 &amp;amp; MECHENG 549. Prerequisites: Knowledge of Elementary Calculus, Linear Algebra and ProbabilityDiscrete-Time Stochastic SystemStochastic SequencesDefinition: Giv"><link rel="canonical" href="http://zyxin.xyz/blog/2019-03/StochasticSystemNotes/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0}</script><title>Notes for Stochastic System | Jacob Zhong</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/blog/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Jacob Zhong</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Blog</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="Toggle navigation bar"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/blog/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/blog/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/blog/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/blog/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a></li><li class="menu-item menu-item-sitemap"><a href="/blog/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>Sitemap</a></li></ul></nav></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en"><link itemprop="mainEntityOfPage" href="http://zyxin.xyz/blog/2019-03/StochasticSystemNotes/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/blog/uploads/assets/avatar.png"><meta itemprop="name" content="Jacob Zhong"><meta itemprop="description" content="Blog of Jacob Zhong"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Jacob Zhong"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Notes for Stochastic System</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2019-03-26 15:47:33" itemprop="dateCreated datePublished" datetime="2019-03-26T15:47:33-04:00">2019-03-26</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">Edited on</span> <time title="Modified: 2019-11-11 22:02:41" itemprop="dateModified" datetime="2019-11-11T22:02:41-05:00">2019-11-11</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/blog/categories/Notes/" itemprop="url" rel="index"><span itemprop="name">Notes</span> </a></span>, <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/blog/categories/Notes/Math/" itemprop="url" rel="index"><span itemprop="name">Math</span> </a></span></span><span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span id="busuanzi_value_page_pv"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">Disqus: </span><a title="disqus" href="/blog/2019-03/StochasticSystemNotes/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019-03/StochasticSystemNotes/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><blockquote><p>Combined notes for part of <code>EECS 501</code> &amp; <code>MECHENG 549</code>. Prerequisites: Knowledge of Elementary Calculus, Linear Algebra and Probability</p></blockquote><h1 id="discrete-time-stochastic-system">Discrete-Time Stochastic System</h1><h2 id="stochastic-sequences">Stochastic Sequences</h2><ul><li>Definition: Given <span class="math inline">k\in\mathbb{K}\subseteq\mathbb{Z}</span> a sequence of integers, <span class="math inline">\mathcal{X}(k,\omega): (\Omega,\mathcal{F},\mathbb{P})\rightarrow(\mathbb{R}^n,\mathcal{F}_ \mathcal{X},\mathbb{P}_ \mathcal{X})</span> is a random/stochastic sequence.</li><li>Uncertainties: Consider a casual system <span class="math inline">F</span> relates some scalar inputs <span class="math inline">u(k)</span> to output <span class="math inline">x(k)</span><ul><li><strong>Epistemic/Model uncertainty</strong>: <span class="math inline">\mathcal{X}(k,\omega)=F(k,u(k),u(k-1),\ldots,\omega)</span>. (system is stochastic and input is deterministic).</li><li><strong>Aleatoric/Input uncertainty</strong>: <span class="math inline">\mathcal{X}(k,\omega)=f(k,U(k,\omega),u(k-1,\omega),\ldots)</span> (system is deterministic and input is stochastic).</li></ul></li><li><strong>Realization</strong>: An outcome <span class="math inline">\mathcal{X}(k,\omega)=x(k)</span> given <span class="math inline">\omega</span> is called a realization of stochastic sequence <span class="math inline">\mathcal{X}</span></li><li>Terminology and Convention<ul><li><span class="math inline">\mathcal{X}(k,\omega)</span> is often written as <span class="math inline">\mathcal{X}(k)</span> when there's no ambiguity.</li><li><span class="math inline">\mathbb{K}=\mathbb{Z}</span> if not specified.</li><li>Sequence over a set <span class="math inline">\mathcal{K}_1\subseteq\mathbb{K}</span> are denoted <span class="math inline">\mathcal{X}(\mathcal{K}_1)</span>.</li><li><span class="math inline">\mathcal{X}</span> denotes <span class="math inline">\mathcal{X}(\mathbb{K})</span> if not specified.</li><li>Consecutive subsequence: <span class="math display">\mathcal{X}(k:l)=\\{\mathcal{X}(k),\mathcal{X}(k+1),\ldots,\mathcal{X}(l)\\},\;x(k:l)=\\{x(k),x(k+1),\ldots,x(l)\\}</span></li><li>Abbreviations:<ul><li><strong><em>SS</em></strong> - stochastic sequence</li><li><strong><em>IID</em></strong> - independent indentically distributed</li></ul></li></ul></li></ul><a id="more"></a><h2 id="probabilistic-characterization">Probabilistic characterization</h2><ul><li>Distribution and density: <span class="math display">F_ \mathcal{X}\left(k:l;x(k:l)\right)\equiv\mathbb{P}((\mathcal{X}_i(k)\leqslant x_i(k))\cap\cdots\cap(\mathcal{X}_i(l)\leqslant x_i(l)),\;i=1\ldots n)</span> <span class="math display">f_ \mathcal{X}\left(k:l;x(k:l)\right)\equiv \frac{\partial^{n(l-k+1)}}{\partial x_1(k)\cdots\partial x_n(l)}F_ \mathcal{X}(k:l;x(k:l))</span> Here <span class="math inline">k:l</span> actually denotes a set of consecutive integers, it can be also changed to ordinary sets <span class="math inline">\\{k,l\\}</span> or single scalar <span class="math inline">k</span>.</li><li><strong>Ensemble Average</strong>: <span class="math inline">\mathbb{E}[\psi(\mathcal{X}(k))]</span>, doing summation over different realization at same time <span class="math inline">k</span><ul><li><strong>Mean</strong>: <span class="math inline">\mu_ \mathcal{X}(k)\equiv\mathbb{E}[\mathcal{X}(k)]=\int^\infty_{-\infty}x(k)f_ \mathcal{X}(k;x(k))\mathrm{d}x(k)</span></li><li><strong>Conditional Mean</strong>: <span class="math inline">\mu_ \mathcal{X}(l|k)\equiv\mathbb{E}[\mathcal{X}(l)|\mathcal{X}(k)=x(k)]</span></li></ul></li><li><strong>Time Average</strong>: <span class="math inline">\frac{1}{2N+1}\sum^N_{k=-N}\psi(\mathcal{X}(k))</span>, doing summation over different time k of same realization</li><li><strong>Autocorrelation</strong>:<ul><li>Scalar case: <span class="math inline">r_ \mathcal{X}(k,l)\equiv\mathbb{E}[\mathcal{X}(k)\mathcal{X}(l)]=\int^\infty_{-\infty}\int^\infty_{-\infty}x(k)x(l)f_ \mathcal{X}(k,l;x(k,l))\mathrm{d}x(k)\mathrm{d}x(l)</span></li><li>Vector case: <span class="math inline">R_ \mathcal{X}(k,l)\equiv\mathbb{E}[\mathcal{X}(k)\mathcal{X}^\top(l)]</span></li><li>Conditional autocorrelation: <span class="math inline">R_ \mathcal{X}(k,l|q)\equiv\mathbb{E}[\mathcal{X}(k)\mathcal{X}^\top(l)|\mathcal{X}(q)=x(q)]</span></li><li>Often we denote <span class="math inline">C_ \mathcal{X}(k)=R_ \mathcal{X}(k,k)</span></li></ul></li><li><strong>Autocovariance</strong>:<ul><li>Scalar case: <span class="math inline">\kappa_ \mathcal{X}(k,l)\equiv\mathbb{E}[(\mathcal{X}(k)-\mu_ \mathcal{X}(k))(\mathcal{X}(l)-\mu_ \mathcal{X}(l))]</span></li><li>Vector case: <span class="math inline">\mathrm{K}_ \mathcal{X}(k,l)\equiv\mathbb{E}[(\mathcal{X}(k)-\mu_ \mathcal{X}(k))(\mathcal{X}(l)-\mu_ \mathcal{X}(l))^\top]</span></li><li>Conditional autocovariance: <span class="math inline">\mathrm{K}_ \mathcal{X}(k,l|q)\equiv\mathbb{E}[(\mathcal{X}(k)-\mu_ \mathcal{X}(k|q))(\mathcal{X}(l)-\mu_ \mathcal{X}(l|q))^\top]</span></li><li>Often we denote <span class="math inline">S_ \mathcal{X}(k|q)=\mathrm{K}_ \mathcal{X}(k,k|q)</span></li><li>Useful conclusion: <span class="math inline">\mathrm{K}(a,b)=\mathrm{K}(b,a)^T</span></li><li>Normalized (<strong>autocorrelation coefficient</strong>): <span class="math inline">\rho_ \mathcal{X}(k,l)\equiv\mathrm{K}_ \mathcal{X}(k,l)/\sigma^2_{\mathcal{X}(k)}\sigma^2_{\mathcal{X}(l)}</span></li></ul></li><li><strong>Strong Stationarity</strong>(aka. strict sense): (necessarily identically distributed over time) <span class="math display">\forall x(k:l)\in\mathbb{R}^{n(l-k+1)},\;\forall s\in\mathbb{Z},\;f_ \mathcal{X}(k:l;x(k:l))=f_ \mathcal{X}(k+s:l+s;x(k:l))</span></li><li><strong>Weak Stationarity</strong>(aka. wide sense): <span class="math inline">\forall k,l</span> if <span class="math display">\mu_ \mathcal{X}(k)=\mu_ \mathcal{X}(l)\;\text{and}\;\mathrm{K}_ \mathcal{X}(k,l)=\mathrm{K}_ \mathcal{X}(k+s,l+s)\equiv\bar{\mathrm{K}}_ \mathcal{X}(s)</span> Weak stationarity is necessary condition for stationarity. (Equal when Gaussian distributed)</li><li><strong>Ergodicity</strong>: <span class="math inline">\mathcal{X}</span> is called ergodic in <span class="math inline">\psi</span> if<ol type="1"><li><span class="math inline">\mathbb{E}[\psi(\mathcal{X})]</span> is stationary</li><li>Ensemble average is equal to Time average, that is <span class="math display">\frac{1}{2N+1}\sum^N_{k=-N}\psi(\mathcal{X}(k))\rightarrow\mathbb{E}[\psi(\mathcal{X})]\;\text{as}\;l\rightarrow \infty</span></li></ol></li></ul><h2 id="markov-sequence">Markov Sequence</h2><ul><li><strong>Markov Sequence</strong>: A ss. <span class="math inline">\mathcal{X}(k)</span> is called a (discrete-time) Markov sequence if <span class="math display">f_ \mathcal{X}\left(k;x(k)\middle|\mathcal{X}(k-1)=x(k-1),\mathcal{X}(k-2)=x(k-2),\ldots\right)=f_ \mathcal{X}(k;x(k)|\mathcal{X}(k-1)=x(k-1))</span><ul><li>We often make some assumption on the initial condition <span class="math inline">\mathcal{X}(0)</span>, such as known, deteministic or uniformly distributed within certain domain.</li></ul></li><li><strong>Markov Chains</strong>: Markov sequence with discrete set of values(states) <span class="math inline">{x_1\ldots x_m}</span></li><li><strong>Hidden Markov model</strong>: Sequence <span class="math inline">\mathcal{Y}</span> is called a Hidden Markov Model if it's modeled by a system of the form <span class="math display">\begin{align}\mathcal{X}(k+1)&amp;=g(k,\mathcal{X}(k),\mathcal{W}(k)) \\\\ \mathcal{Y}(k)&amp;=h(k,\mathcal{X}(k),\mathcal{W}(k))\end{align}</span> We also say that <span class="math inline">\mathcal{Y}</span> has a <strong>(discrete-time) stochastic state space</strong>.</li><li><strong>Guassian-Markov Sequence</strong> (GMS): <span class="math inline">\mathcal{X}(k+1)=g(k,\mathcal{X}(k),\mathcal{W}(k))</span> where <span class="math inline">\mathcal{W}(k)</span> is iid. Guassian</li></ul><h2 id="linear-stochastic-sequence">Linear Stochastic Sequence</h2><p><span class="math display">\begin{align}\mathcal{X}(k+1)&amp;=A(k)\mathcal{X}(k)+B(k)\mathcal{W}(k) \\\\ \mathcal{Y}(k)&amp;=C(k)\mathcal{X}(k)+D(k)\mathcal{W}(k)\end{align}</span> &gt; For linear Markov sequences, the deterministic mean sequence and centered uncertain sequence completely decouple. So we often assume that <span class="math inline">\mathcal{X}(k)</span> and <span class="math inline">\mathcal{Y}(k)</span> are <strong>centered</strong> in this case, with regard to deterministic inputs. The equation with deterministic inputs is often written as <span class="math display">\mathcal{X}(k+1)=A(k)\mathcal{X}(k)+B_u(k)u(k)+B_\mathcal{W}(k)\mathcal{W}(k)</span></p><ul><li><p>Recursive-form expectations:</p><ul><li><strong>Mean</strong>: <span class="math inline">\mu_ \mathcal{X}(k+1|q)=A(k)\mu_ \mathcal{X}(k|q)+B(k)\mu_\mathcal{W}(k)</span></li><li><strong>Covariance (Discrete-time algebraic Lyapunov/Stein difference equation)</strong>: <span class="math display">S_ \mathcal{X}(k+1|q)=A(k)S_ \mathcal{X}(k|q)A^\top(k)+B(k)S_\mathcal{W}(k)B^\top(k)</span> &gt; Can be solved with <code>dlyap</code> in MATLAB</li></ul></li><li><p>Convergence when <span class="math inline">k\rightarrow\infty</span>:</p><ul><li><strong>Mean convergence</strong>: <span class="math inline">\mu_ \mathcal{X}(k|q)</span> converges requires <span class="math inline">\max_i|\lambda_i(A)|&lt;1</span> (<span class="math inline">\lambda</span> denotes eigenvalue)</li><li><strong>Covariance convergence</strong>: <span class="math inline">S_ \mathcal{X}(k|q)</span> converges requires <span class="math inline">\max_i|\lambda_i(A)|&lt;1</span></li><li><strong>(Discrete-time) Lyapunov equation</strong> (Stein equation): <span class="math inline">\bar{S}_ \mathcal{X}=A\bar{S}_ \mathcal{X}A^\top+B\bar{S}_\mathcal{W}(k)B^\top</span>. Solution for this equation exists iff. <span class="math inline">A</span> is asymptotically stable (characterizing sequence is stationary).</li></ul></li><li><p>Explicit state transition: By recursive substitution, <span class="math display">\mathcal{X}(k)=\Psi(k,q)\mathcal{X}(q)+\sum^{k-1}_ {i=q}\Gamma(k,i)\mathcal{W}(i)</span> where state transition matrix <span class="math inline">\Psi(k,q)=\begin{cases}I, &amp;k=q \\\\ \prod^{k-1}_ {i=q}A(i),&amp; k&gt;q\end{cases}</span> and <span class="math inline">\Gamma(k,i)=\Psi(k,i+1)B(i)</span>.</p><ul><li><strong>Conditioned Mean sequence</strong>: <span class="math inline">\mu_ \mathcal{X}(k|q)=\Psi(k,q)\mu_ \mathcal{X}(q)+\sum^{k-1}_ {i=q}\Gamma(k,i)\mu_\mathcal{W}(i)</span></li><li><strong>Conditioned Autocovariance Matrix</strong>: <span class="math display">\mathrm{K}_ \mathcal{X}(k,l|q)=\Psi(k,q)S_ \mathcal{X}(q)\Psi^\top(l,q)+\sum^{min\\{k,l\\}-1}_ {i=q}\Gamma(k,i)S_\mathcal{W}(i)\Gamma^\top(l,i)</span><ul><li>A special case: <span class="math inline">S_ \mathcal{X}(k|q)=\mathrm{K}_ \mathcal{X}(k,k|q)=\sum^{k-1}_ {i=q}\Gamma(k,i)S_\mathcal{W}(i)\Gamma^\top(k,i)</span>, <span class="math inline">S_ \mathcal{X}(k|k)=0</span></li><li>Useful equation (stationary centered case): <span class="math display">\mathrm{K}_ \mathcal{X}(k,l)=\begin{cases} S_ \mathcal{X}(k)\cdot(A^\top)^{(l-k)}&amp;,l&gt;k\\\\ S_ \mathcal{X}(k)&amp;,l=k\\\\ A^{(k-l)}S_ \mathcal{X}(k)&amp;,l&lt; k\end{cases}</span></li></ul></li></ul></li><li><p><strong>Conditional Autocorrelation Matrix</strong>: <span class="math inline">R_ \mathcal{X}(k,l|q)=\mathrm{K}_ \mathcal{X}(k,l|q)+\mu_ \mathcal{X}(k|q)\mu_ \mathcal{X}^\top(l|q)</span></p></li><li><p>Observation <span class="math inline">\mathcal{Y}</span> property:</p><ul><li>Mean: <span class="math inline">\mu_ \mathcal{Y}(k|q)=C(k)\mu_ \mathcal{X}(k|q)+D(k)\mu_\mathcal{W}(k)</span></li><li>Covariance: <span class="math display">\mathrm{K}_ \mathcal{Y}(k,l|q)=\begin{cases}C(k)\mathrm{K}_ \mathcal{X}(k,l|q)C^\top(l)+C(k)\Gamma(k,l)S_W(l)D^\top(l)&amp;:k&gt;l \\\\C(k)S_ \mathcal{X}C^\top(k)+D(k)S_\mathcal{W}(k)D^\top(k)&amp;:k=l\\\\ C(k)\mathrm{K}_ \mathcal{X}(k,l|q)C^\top(l)+D(k)S_\mathcal{W}(k)\Gamma^\top(l,k)C^\top(l)&amp;:k&lt; l\end{cases}</span></li><li>Stationary time-invariant covariance: <span class="math display">\mathrm{K}_ \mathcal{Y}(s)=\begin{cases}CA^{|s|}\bar{S}_ \mathcal{X}C^\top+CA^{|s|-1}B\bar{S}_ WD^\top&amp;:s\neq 0 \\\\C\bar{S}_ \mathcal{X}C^\top+D\bar{S}_ WD^\top&amp;:s=0\end{cases}</span></li></ul></li></ul><h2 id="gaussian-stochastic-sequence">Gaussian Stochastic Sequence</h2><ul><li>Jointly Gaussian <span class="math inline">\Rightarrow, \nLeftarrow</span> Marginally Gaussian</li><li><span class="math inline">c^\top X</span> is Gaussian <span class="math inline">\Leftrightarrow X</span> is Gaussian</li><li>Conditional Gaussian: if <span class="math inline">X</span> and <span class="math inline">Y</span> are Gaussian, then <span class="math inline">X|Y \sim \mathcal{N}(\mu_{X|Y},S_{X|Y})</span> where <span class="math inline">\mu_{X|Y}=\mu_X+S_{XY}S_Y^{-1}(Y-\mu_Y)</span>, <span class="math inline">S_{X|Y}=S_X-S_{XY}S_Y^{-1}S_{YX}</span></li><li>A linear controllable GMS <span class="math inline">\mathcal{X}(k)</span> is stationary iff. <span class="math inline">A(k)=A, B(k)=B</span> (time-invariant) and <span class="math inline">A</span> is asymptotically stable.</li><li>All LTI stationary GMS are also ergodic in all finite momoents</li><li>Solve <span class="math inline">\mathcal{X}(k+1)=A\mathcal{X}(k)+B\mathcal{W}(k)</span> when <span class="math inline">\mathcal{X}</span> is stationary (<span class="math inline">\max_i|\lambda_i(A)|&lt;1</span>)<ol type="1"><li>solve <span class="math inline">\bar{\mu}_ \mathcal{X}=A\bar{\mu}_ \mathcal{X}+B\bar{\mu}_\mathcal{W}</span> for <span class="math inline">\bar{\mu}</span></li><li>solve <span class="math inline">\bar{S}_ \mathcal{X}=A\bar{S}_ \mathcal{X}A^\top+B\bar{S}_ \mathcal{W}B^\top</span> for <span class="math inline">\bar{S}_ \mathcal{X}</span></li><li>calculate <span class="math inline">\Sigma_ \mathcal{X}(k:l|q)=\begin{bmatrix}\mathrm{K}_ \mathcal{X}(k,k|q)&amp;\cdots&amp;\mathrm{K}_ \mathcal{X}(k,l|q)\\\\ \vdots&amp;\ddots&amp;\vdots\\\\ \mathrm{K}_ \mathcal{X}(l,k|q)&amp;\cdots&amp;\mathrm{K}_ \mathcal{X}(l,l|q)\end{bmatrix}</span> using <span class="math inline">\bar{S}_ \mathcal{X}</span></li><li>Then <span class="math inline">f_ \mathcal{X}(k:l;x(k:l))</span> is determined with <span class="math inline">\mu_ \mathcal{X}(k:l)=\\{\bar{\mu}_ \mathcal{X},\bar{\mu}_ \mathcal{X}\ldots\bar{\mu}_ \mathcal{X}\\}</span> and <span class="math inline">\Sigma_ \mathcal{X}(k:l)</span> above.</li></ol></li></ul><h2 id="observation-filtering">Observation &amp; Filtering</h2><ul><li>(LTI) <strong>Luenberger observer</strong>: <span class="math display">\begin{align}\hat{\mathcal{X}}(k+1)&amp;=A\hat{\mathcal{X}}(k)+L(\hat{\mathcal{Y}}(k)-\mathcal{Y}(k))+B\bar{\mu}_ \mathcal{W} \\\\ \hat{\mathcal{Y}}(k)&amp;=C\hat{\mathcal{X}}(k)+D\bar{\mu}_\mathcal{W}\end{align}</span> where <span class="math inline">L</span> is the observer gain.<ul><li>Note: We often assume that the process &amp; measurement noise are decoupled and independent</li><li>Combined form: <span class="math inline">\hat{\mathcal{X}}(k+1)=[A+LC]\hat{\mathcal{X}}(k)-L\mathcal{Y}(k)+B\mu_\mathcal{W}(k)</span></li></ul></li><li><strong>State estimation residual</strong> <span class="math inline">r(k)=\mathcal{X}(k)-\hat{\mathcal{X}}(k)</span><ul><li>Combined form: <span class="math inline">r(k+1)=[A+LC]r(k)+[B+LD]\tilde{\mathcal{W}}(k)</span></li><li>Stationary covariance can be solved by a Lyapunov equation <span class="math display">\bar{S}_ r=[A+LC]\bar{S}_ r[A+LC]^T+[LD+B]\bar{S}_ \mathcal{W}[LD+B]^T</span></li></ul></li><li>A common objective: minimize <span class="math inline">\bar{S}_r=\mathbb{E}(rr^\top)</span><ul><li>Solutions: <span class="math inline">L=-A\bar{S}_ rC^\top[C\bar{S}_ rC^\top+D\bar{S}_ \mathcal{W}D^\top]^{-1}</span> (<strong>Kalman observer gain</strong>)</li><li>Or solve <strong>(Discrete-time) Algebraic Riccati equation</strong> <span class="math display">\bar{S}_ r=A\bar{S}_ rA^\top+B\bar{S}_ \mathcal{W}B^\top-A\bar{S}_ rC^\top[C\bar{S}_ rC^\top+D\bar{S}_ \mathcal{W}D^\top]^{-1}C\bar{S}_ rA^\top</span></li></ul></li><li><strong>Innovation sequence</strong> <span class="math inline">e(k)=\hat{\mathcal{Y}}(k)-\mathcal{Y}(k)</span> with <span class="math inline">L</span> is optimal<ul><li>We can find that <span class="math inline">\mu_e=0</span> and <span class="math inline">\mathrm{K}_e(k+s,k)=\begin{cases}C\bar{S}_rC^\top+D\bar{S}_WD^T&amp;:s=0 \\\\0&amp;:s\neq0\end{cases}</span>. So the innovation sequence is iid. (only in Kalman observer)</li></ul></li><li><strong>(Output) Probabilistically-equivalent model</strong>: <span class="math display">\begin{align}\mathcal{X}(k+1)&amp;=A\mathcal{X}(k)+Le(k) \\\\ \mathcal{Y}(k)&amp;=C\mathcal{X}(k)-e(k)\end{align}</span></li></ul><h1 id="markov-chains">Markov Chains</h1><blockquote><p>Content in this section comes from EECS 501 In this specific field, we often use stand-alone analysis methods.</p></blockquote><h2 id="basic-definitions">Basic definitions</h2><ul><li><strong>State distribution</strong>: We denote row vector <span class="math inline">\pi_t</span> as <span class="math inline">\pi_t(x)=\mathbb{P}(\mathcal{X}_t=x),\; x\in S</span> (<span class="math inline">S</span> is the set of states). Directly we have <span class="math inline">\sum_x\pi(x)=1</span></li><li><strong>Time homogeneous</strong>: <span class="math inline">\mathbb{P}(\mathcal{X}_ {t+1}=y|\mathcal{X}_ t=x)=\mathbb{P}(\mathcal{X}_{s+1}=y|\mathcal{X}_s=x)\;\forall s,t</span></li><li><strong>One-step transition probability matrix</strong>: <span class="math inline">P_t=[P_{xy,t}]</span> where <span class="math inline">P_{xy,t}=\mathbb{P}(\mathcal{X}_ {t+1}=y|\mathcal{X}_ t=x)</span>.<ul><li>Time-homo case: <span class="math inline">P=[P_{xy}]</span> where <span class="math inline">P_{xy}=p(y|x)</span></li><li>Rows of the matrix sum up to 1.</li><li>This matrix is also called <strong>stochastic matrix</strong>.</li><li>Extend this matrix to continuous states, then we use <strong>transition kernel</strong> <span class="math inline">T(x,y)</span> to describe the transition probability,</li></ul></li><li><strong>m-step transition probability matrix</strong>: <span class="math inline">P_{xy,t}^{(m)}=\mathbb{P}(\mathcal{X}_ {t+m}=y|\mathcal{X}_ t=x)</span><ul><li><strong>Chapman-Kolmogorov Equation</strong>: <span class="math inline">P^{(n+m)}_t=P^{(n)}_t P^{(m)}_t</span></li></ul></li></ul><h2 id="state-variables">State Variables</h2><ul><li><strong>Hitting time</strong>: <span class="math inline">T_1(y)=\min\\{n\geq 0:\mathcal{X}_n=y\\},\;T_k(y)=\min\\{n&gt;T_{k-1}(y):\mathcal{X}_n=y\\}</span> where <span class="math inline">n\in\mathbb{N}</span><ul><li><strong>Period</strong>: For state <span class="math inline">i\in x</span>, its period is the greatest common divisor of <span class="math inline">\\{n&gt;1|T_n(i)&gt;0\\}</span>. A Markov Chain is <strong>aperiodic</strong> If all states have period 1.</li></ul></li><li><strong>Return probability</strong>: <span class="math inline">f_{xy}=\mathbb{P}(T_1(y)&lt;\infty|\mathcal{X}_0=x)</span></li><li><strong>Occupation time</strong>: <span class="math inline">V(y)=\sum^\infty_{n=1}\unicode{x1D7D9}_{\mathcal{X_n}}(y)</span></li><li>Some properties<ul><li><span class="math inline">f_{xy}=\mathbb{P}(V(y)\geqslant 1|\mathcal{X}_0=x)</span></li><li><span class="math inline">\mathbb{P}(V(y)=m|\mathcal{X}_ 0=x)=\begin{cases} 1-f_{xy}&amp;:m=0\\\\f_{xy}f_{yy}^{m-1}(1-f_{yy})&amp;:m\geqslant 1\end{cases}</span></li><li><span class="math inline">\mathbb{E}[ V(y)|\mathcal{X}_ 0=x]=\begin{cases} 0&amp;:f_{xy}=0\\\\\infty&amp;:f_{xy}&gt;0,\ f_{yy}=1\\\\f_{xy}/(1-f_{yy})&amp;:f_{xy}&gt;0,\ f_{yy}&lt;1\end{cases}</span></li><li><span class="math inline">\mathbb{E}[ V(y)|\mathcal{X}_ 0=x]=\sum^\infty_{n=1}P^{(n)}_{xy}</span></li></ul></li></ul><h2 id="state-classification">State Classification</h2><blockquote><p>Here we usually consider only time-homogeneous Markov Chains</p></blockquote><ul><li><strong>Accessible</strong> (<span class="math inline">x\rightarrow y</span>): <span class="math inline">\exists n\;\text{s.t.}\ P_{xy}^{(n)}&gt;0</span><ul><li><span class="math inline">x\rightarrow y \Leftrightarrow f_{xy}&gt;0</span></li></ul></li><li><strong>Communicate</strong> (<span class="math inline">x\leftrightarrow y</span>): <span class="math inline">x\rightarrow y\;\text{and}\;y\rightarrow x</span>. This is a <a href="https://en.wikipedia.org/wiki/Equivalence_relation" target="_blank" rel="noopener">equivalence relation</a>.</li><li><strong>Equivalent class</strong>: set of states that communicate with each other</li><li><strong>Irreducible</strong>: a Markov chain with only one communicating class</li><li><strong>Absorbing/Closed state</strong>: <span class="math inline">P_{xx}=1</span><ul><li><strong>Absorbing class</strong>: set <span class="math inline">C</span> is absorbing iff <span class="math inline">\forall x \in C, \sum_{y\in C}P_{xy}=1</span></li></ul></li><li><strong>Transient state</strong>: <span class="math inline">f_{xx}&lt;1 \Leftrightarrow \mathbb{E}[V_i|\mathcal{X}_0=i]&lt;\infty</span></li><li><strong>Recurrent state</strong>: <span class="math inline">f_{xx}=1 \Leftrightarrow \mathbb{E}[V_i|\mathcal{X}_0=i]=\infty</span><ul><li><strong>Positive recurrent</strong>: <span class="math inline">\mathbb{E}[T_1(x)|\mathcal{X}_0=x]&lt;0</span></li><li><strong>Null recurrent</strong>: <span class="math inline">\mathbb{E}[T_1(x)|\mathcal{X}_0=x]=0</span></li><li>These two kind of recurrent states also make up communicating classes</li></ul></li><li>Some properties<ul><li>If <span class="math inline">x</span> is (positive) recurrent and <span class="math inline">x\rightarrow y</span>, then <span class="math inline">y</span> is also (positive) recurrent and <span class="math inline">f_{xy}=f_{yx}=1</span></li><li>Every closed and finite subset of <span class="math inline">X</span> contains at least one (positive) recurrent state.</li><li>All states of a communicating class are either positive recurrent, null recurrent or transient.</li><li>Method to determine whether class <span class="math inline">C</span> is recurrent/transient<ol type="1"><li>If <span class="math inline">C</span> is non-closed, it's transient</li><li>If <span class="math inline">C</span> is closed and finite, then <span class="math inline">C</span> is positive recurrent</li><li>If <span class="math inline">C</span> is closed and infinite, then <span class="math inline">C</span> can be either positive/null recurrent or transient &gt; A typical example is the <a href="https://en.wikipedia.org/wiki/Birth%E2%80%93death_process" target="_blank" rel="noopener">birth-death chain</a></li></ol></li></ul></li><li><strong>Stationary distribution</strong>: <span class="math inline">\bar{\pi}\;\text{s.t.}\;\bar{\pi}=\bar{\pi} P</span><ul><li>Stationary in limit form: <span class="math inline">\bar{\pi}=\lim_{n\rightarrow \infty} \frac{1}{n}\sum^{n-1}_{t=0} \pi_t</span><ul><li>This is a <strong>Cesaro</strong> limit, we use this to deal with the problem that <span class="math inline">\lim_{n\rightarrow \infty} \pi_t</span> might not exist if the chain is periodic.</li></ul></li><li>Reversibility criterion for stationary: <span class="math inline">\forall i,j \in S, \pi_i P_ij = \pi_j P_ji</span> (a.k.a detailed balance condition), then the process is reversible and therefore stationary.</li><li>Existence for stationary distribution satisfying <span class="math inline">\bar{\pi}=\bar{\pi} P</span><ol type="1"><li>If the chain has single positive recurrent class, then exists unique solution: <span class="math inline">\bar{\pi}(x)=0</span> for all transient or null recurrent <span class="math inline">x</span></li><li>If the chain has multiple positive recurrent class, then there are multiple solutions, for each positive recurrent <span class="math inline">x</span> we have a <span class="math inline">\bar{\pi}^i</span>.</li><li>If the chain has only transient and null recurrent states, there is no solution$</li></ol></li><li>Convergence of stationary distribution<ul><li>If the chain has positive recurrent class, then <span class="math inline">\frac{1}{n}\sum^n_{t=1}\mathbb{P}(\mathcal{X}_t=j)\xrightarrow[n\rightarrow \infty]{}\bar{\pi}_j,\;\forall \mu_0</span></li><li>(Ergodic) If the chain is positive recurrent, then <span class="math inline">\frac{1}{n}\sum^n_{t=1}\unicode{x1D7D9}_{\mathcal{X}_t}(j)\xrightarrow[n\rightarrow \infty]{\text{a.s.}}\bar{\pi}_j</span></li><li>If the chain is positive recurrent and aperiodic, then <span class="math inline">\mathbb{P}(\mathcal{X}_n=j)\xrightarrow[n\rightarrow \infty]{}\bar{\pi}_j</span></li></ul></li></ul></li></ul><h1 id="continuous-time-stochastic-system">Continuous-Time Stochastic System</h1><h2 id="stochastic-sequences-1">Stochastic Sequences</h2><ul><li>Definition: Given <span class="math inline">t\in\mathbb{T}\subset\mathbb{R}</span> a sequence of time, <span class="math inline">\mathcal{X}(t,\omega): (\Omega,\mathcal{F},\mathbb{P})\rightarrow(\mathbb{R}^n,\mathcal{F}_ \mathcal{X},\mathbb{P}_ \mathcal{X})</span> is a continuous stochastic sequence.</li><li>Most definitions are similar to discrete sequence (including <strong>stationarity</strong>), while the sequence is often defined as <span class="math inline">\mathcal{X}(\mathcal{G}),\;\mathcal{G}={t_1,t_2,\ldots,t_N}\subset\mathbb{T}, t_i&lt; t_{i+1}</span></li><li><strong>Stochastic Differential Equation (SDE)</strong>: <span class="math display">\mathrm{d}\mathcal{X}(t)=F(\mathcal{X}(t),t)\mathrm{d}t+\sum^r_{i=1}G_i(\mathcal{X}(t),t)\mathrm{d}\mathcal{W}_i(t)</span> Here <span class="math inline">\mathcal{W}</span> is often a certain kind of <em>noise</em> random process.</li></ul><h2 id="markov-sequence-1">Markov Sequence</h2><ul><li><strong>Markov Sequence</strong>: A ss. <span class="math inline">\mathcal{X}(k)</span> is called a (continuous-time) Markov sequence if for the set of times <span class="math inline">\mathcal{G}={t_1,t_2,\ldots,t_N}</span> with <span class="math inline">t_i &lt; t_{i+1}</span>, we have <span class="math display">f_ \mathcal{X}\left(t_N;x(t_N)\middle|\mathcal{X}(t_{N-1})=x(t_{N-1}),\mathcal{X}(t_{N-2})=x(t_{N-2}),\ldots\right)=f_ \mathcal{X}(t_N;x(t_N)|\mathcal{X}(t_{N-1})=x(t_{N-1}))</span></li><li><strong>Hidden Markov Model</strong>: Definition similar to discrete case. A <strong>continuous-time stochastic state space</strong> is of the form <span class="math display">\begin{align}\dot{\mathcal{X}}(t)&amp;=F(\mathcal{X},t)+G(\mathcal{X},t)\mathrm{d}\mathcal{W}(t)/\mathrm{d}t \\\\ \mathcal{Y}(t)&amp;=H(\mathcal{X},t)+J(\mathcal{X},t)\mathrm{d}\mathcal{W}(t)/\mathrm{d}t\end{align}</span> where <span class="math inline">\mathcal{W}</span> is usually <a href="#Wiener-Process">Wiener process</a> and white noise <span class="math inline">\mathrm{d}\mathcal{W}(t)/\mathrm{d}t</span> is often written as <span class="math inline">\mathcal{U}(t)</span>.<ul><li>The state space is <strong>affine</strong> if <span class="math inline">F(\mathcal{X}(t),t)=A(t)\mathcal{X}(t)+u(t),\;N(\mathcal{X},t)=C(t)\mathcal{X}(t)+v(t)</span></li><li>The state space is <strong>bilinear</strong> if <span class="math inline">G(\mathcal{X},t)\mathrm{d}\mathcal{W}(t)=\sum^r_{i=1}B_i(t)\mathcal{X}\mathrm{d}\mathcal{W}_i(t)</span></li></ul></li></ul><h2 id="poisson-counters">Poisson Counters</h2><ul><li>Definition: It's a stochastic Markow process <span class="math inline">\mathcal{N}(t)\in\\{0,1,2,\ldots\\}</span> with the characteristic that it jumps up by only one integer at a time.<ul><li>Characteristics: transition times are random, transition step is always 1.</li></ul></li><li>Transition rule: <span class="math inline">\frac{\partial}{\partial t}p_\mathcal{N}=\begin{cases}-\lambda p_\mathcal{N}(t;n)&amp;:n=0\\\\-\lambda p_\mathcal{N}(t;n)+\lambda p_\mathcal{N}(t;n-1)&amp;:n&gt;0\end{cases}</span><ul><li>From the rule we can conclude that <span class="math inline">p_\mathcal{N}(t;n)=\frac{1}{n!}(\lambda t)^n e^{-\lambda t}</span></li></ul></li><li>Bidirectional counter: <span class="math inline">\mathcal{N}(t)</span> is defined as one-directional. <span class="math inline">\mathcal{N}_1(t)-\mathcal{N}_2(t)</span> is called <strong>bidirectional poisson counter</strong>.</li><li>Expectation: <span class="math inline">\mathbb{E}[\mathcal{N}(t)]=\lambda t,\;\mathbb{E}[\mathrm{d}\mathcal{N}(t)]=\lambda\mathrm{d}t</span></li><li><strong>Ito calculus</strong> for Poisson counter:<ul><li><strong>Ito sense</strong>: <span class="math inline">n(t)</span> is a realization of poisson counter <span class="math inline">\mathcal{N}</span>. <span class="math inline">x(t)</span> is a solution in Ito sense to <span class="math inline">\mathrm{d}x(t)=F(x(t),t)\mathrm{d}t+G(x(t),t)\mathrm{d}n(t)</span> if<ol type="1"><li>On all intervals where <span class="math inline">n(t)</span> is constant, <span class="math inline">\dot{x}(t)=F(x(t),t)</span></li><li>If <span class="math inline">n(t)</span> jumps at time <span class="math inline">t_1</span>, <span class="math inline">\lim_{t\rightarrow t_1^+}x(t)=\lim_{t\rightarrow t_1^-}x(t)+G\left(\lim_{t\rightarrow t_1^+}x(t),t_1\right)</span></li></ol></li><li><strong>Ito rule</strong>: Given a fuction <span class="math inline">\psi(\mathcal{X}(t),t)</span>, taking Taylor expansion we have <span class="math display">\mathrm{d}\psi=\left\\{\frac{\partial\psi}{\partial t}+(\nabla_\mathcal{X}\psi)F(\mathcal{X},t)\right\\}\mathrm{d}t+\sum^r_{i=1}\left\\{\psi\left(\mathcal{X}+G_i(\mathcal{X},t),t\right)-\psi(\mathcal{X},t)\right\\}\mathrm{d}\mathcal{N}_i(t)</span></li></ul></li></ul><h2 id="wiener-process">Wiener-Process</h2><ul><li>Definition (<strong>Brownian Motion</strong>): It's a bidirectional poisson counter with infinite rate, i.e. <span class="math inline">\mathcal{W}=\lim_{\lambda\rightarrow\infty}\frac{1}{\sqrt(\lambda)}(\mathcal{N}_1-\mathcal{N}_2)</span> where <span class="math inline">\mathcal{N}_1,\;\mathcal{N}_2</span> have rate <span class="math inline">\lambda/2</span><ul><li>Characteristic: It's a Gaussian with zero mean and variance <span class="math inline">t</span></li><li>Note: Actual Wiener Process has stronger continuity property than Brownian motion.</li></ul></li><li>Expectation: <span class="math inline">\mathbb{E}[\mathcal{W}]=\mathbb{E}[d\mathcal{W}]=0,\;\mathbb{E}[\mathcal{W}(\tau)\mathcal{W}(t)]=\mathbb{E}[\mathcal{W}^2(\min\\{t,\tau\\})]=\min\\{t,\tau\\}</span></li><li>Principle of independent increments: If the interval <span class="math inline">[r,t), [\sigma,s)</span> don't overlap, then <span class="math inline">\mathcal{W}(t)-\mathcal{W}(\tau)</span> and <span class="math inline">\mathcal{W}(s)-\mathcal{W}(\sigma)</span> are uncorrelated.</li><li><strong>Ito calculus</strong> for Wiener Process<ul><li><strong>Ito rule</strong>: Given a fuction <span class="math inline">\psi(\mathcal{X}(t),t)</span>, taking Taylor expansion we have <span class="math display">\mathrm{d}\psi=\frac{\partial\psi}{\partial t}\mathrm{d}t+(\nabla_\mathcal{X}\psi)F(\mathcal{X},t)\mathrm{d}t+\sum^r_{i=1}\left(\nabla_\mathcal{X}\psi(\mathcal{X},t)\right)G_i(\mathcal{X},t)\mathrm{d}\mathcal{W}_ i-\sum^r_{i=1}\frac{1}{2}G_i^\top(\mathcal{X},t)\left(\mathrm{H}_\mathcal{X}\psi(\mathcal{X},t)\right)G_i(\mathcal{X},t)\mathrm{d}t</span> &gt; Note that gradient <span class="math inline">\nabla_{\mathcal{X}}=\begin{bmatrix}\frac{\partial\psi}{\partial x_1}&amp;\cdots&amp;\frac{\partial\psi}{\partial x_n}\end{bmatrix}</span>, Hessian <span class="math inline">\mathrm{H}_{\mathcal{X}}=\begin{bmatrix}\frac{\partial^2\psi}{\partial x_1^2}&amp;\cdots&amp;\frac{\partial^2\psi}{\partial x_1\partial x_n}\\\\ \vdots&amp;\ddots&amp;\vdots \\\\ \frac{\partial^2\psi}{\partial x_n\partial x_1}&amp;\cdots&amp;\frac{\partial^2\psi}{\partial x_n^2}\end{bmatrix}</span>. By default, the operator target is <span class="math inline">\mathcal{X}</span> is not noted.</li></ul></li><li><strong>White noise</strong>: It is Guassian distributed stationary stochastic process with <span class="math inline">\mu_\mathcal{U}(t)=0,\;\mathrm{K}_ \mathcal{U}(t,\tau)=\Phi_ \mathcal{U}\delta(t-\tau)</span>, where <span class="math inline">\Phi_\mathcal{U}</span> is <strong>spectral intensity</strong>.<ul><li>We often consider white noise as derivative of Wiener process: <span class="math inline">\mathcal{U}\sim\mathrm{d}\mathcal{W}/\mathrm{d}t</span>.</li></ul></li></ul><h2 id="linear-stochastic-sequence-1">Linear Stochastic Sequence</h2><p><span class="math display">\begin{align}\mathrm{d}\mathcal{X}(t)&amp;=\\{A(t)\mathcal{X}(t)+u(t)\\}\mathrm{d}t+B(k)\mathrm{d}\mathcal{W}(t) \\\\ \mathcal{Y}(t)&amp;=C(k)\mathcal{X}(k)+D(k)\mathcal{W}(k)\end{align}</span></p><ul><li>Differential equation of expectations<ul><li><strong>Mean</strong>: <span class="math inline">\frac{\mathrm{d}}{\mathrm{d}t}\mu_\mathcal{X}(t)=A(t)\mu_\mathcal{X}(t)</span></li><li><strong>Autocovariance</strong>: <span class="math inline">\frac{\mathrm{d}}{\mathrm{d}t}S_\mathcal{X}(t)=A(t)S_\mathcal{X}(t)+S_\mathcal{X}A^\top(t)+B(t)B^\top(t)</span> (called <strong>Lyapunov differential equation</strong>)</li></ul></li><li>Stationary LTI case<ul><li>Useful equation: <span class="math inline">\mathrm{R}_ \mathcal{X}(t,\tau)=\begin{cases} S_ \mathcal{X}\exp\\{A^\top(\tau-t)\\}&amp;,\tau&gt;t \\\\ \exp\\{A(t-\tau)\\}S_ \mathcal{X}&amp;,\tau&lt; t\end{cases}</span></li><li><strong>(Continuous-time) algerbraic Lyapunov equation</strong>: <span class="math inline">A\bar{S}_ \mathcal{X}+\bar{S}_\mathcal{X}A^\top+BB^\top=0</span></li></ul></li></ul><h2 id="nonlinear-stochastic-sequence">Nonlinear Stochastic Sequence</h2><ul><li><strong>The Fokker-Planck Equation (FPE)</strong>: Consider the Wiener-process excited general SDE, we have <span class="math display">\frac{\partial f_ \mathcal{X}(x;t)}{\partial t}=-\nabla\left(F(\mathcal{X})f_ \mathcal{X}(x;t)\right)+\frac{1}{2}\text{tr}\left[\mathrm{H}\left(G(\mathcal{X})G^\top(\mathcal{X})f_ \mathcal{X}(x;t)\right)\right]</span><ul><li><span class="math inline">\mathcal{X}</span> is stationary means <span class="math inline">\frac{\partial f_\mathcal{X}(x;t)}{\partial t}=0</span></li></ul></li><li><strong>Gaussian closure</strong>: An approximate solution for FPE is supposing <span class="math inline">f_\mathcal{X}</span> as multivariate Gaussian with some <span class="math inline">S_\mathcal{X}</span> and <span class="math inline">{\mu_\mathcal{X}}</span>. That is we focus on 1st-order and 2nd-order estimation. Denote the estimated distribution as <span class="math inline">\hat{f}_\mathcal{X}(x;t)</span><ul><li><strong>Estimated expectation</strong>: <span class="math inline">\hat{\mathbb{E}}\phi(\mathcal{X})=\int\cdots\int\phi(x)\hat{f}_\mathcal{X}(x;t)\mathrm{d}x</span></li><li>Solution:<span class="math inline">h(x,t)=-\nabla F+\tilde{x}^\top S^{-1}F-\nabla(GG^\top)S^{-1}\tilde{x}+\frac{1}{2}\text{tr}\left[\mathrm{H}(GG^\top) \right]+\frac{1}{2}\text{tr}\left[(-S^{-1}+S^{-1}\tilde{x}\tilde{x}^\top S^{-1})GG^\top\right]</span>, <span class="math display">\begin{align}\dot{\mu}_\mathcal{X}(t)&amp;=S(t)\hat{\mathbb{E}}[\nabla^\top h(x)] \\\\ \dot{S} _\mathcal{X}(t)&amp;=S(t)\hat{\mathbb{E}}[\mathrm{H} h(x)]S(t)\end{align}</span></li><li>Useful simplification: If <span class="math inline">G</span> is constant (independent of <span class="math inline">\mathcal{X}</span>), then the ODE of <span class="math inline">\dot{\mu}</span> and <span class="math inline">\dot{S}</span> become <span class="math display">\begin{align}\dot{\mu}_ \mathcal{X}(t)&amp;=\hat{\mathbb{E}}[F(\mathcal{X})] \\\\ \dot{S} _\mathcal{X}(t)&amp;=\hat{A}^\top S+S\hat{A}+GG^\top\end{align}</span> where <span class="math inline">\hat{A}=\hat{\mathbb{E}}\left[\frac{\partial F}{\partial\mathcal{X}}\right]</span><ul><li><strong>Equivalent linearization (Quasi-linearization)</strong>: In the estimation, <span class="math inline">\mathcal{X}</span> evolves equivalently to <span class="math inline">\mathrm{d}\mathcal{X}(t)=\hat{A}(t)\mathcal{X}(t)\mathrm{d}t+G\mathrm{d}\mathcal{W}(t)</span></li></ul></li><li>No guaranteed bounds generally exist for the estimation error <span class="math display">e(\mathcal{X},t)=\left(\frac{\partial\hat{f}_ \mathcal{X}}{\partial t}\right)-\left(-\nabla\left(F\hat{f}_ \mathcal{X}\right)+\frac{1}{2}\text{tr}\left[\mathrm{H}\left(GG^\top\hat{f}_ \mathcal{X}\right)\right]\right)</span></li><li>Estimation of <span class="math inline">\mu(t),\;S(t)</span> could also have error. And stationarity may not be the consistent between original system and estimated system</li></ul></li></ul><h1 id="spectral-analysis">Spectral Analysis</h1><blockquote><p>Content in this section comes from MECHENG 549</p></blockquote><h2 id="power-spectral-density">Power Spectral Density</h2><ul><li>Definition: The <strong>power spectral density (PSD)</strong> of stochastic process <span class="math inline">\mathcal{Y}</span> is <span class="math display">\Phi_\mathcal{Y}(\omega)\equiv\mathbb{E}\left[ \lim_{T\rightarrow\infty}\frac{1}{2T}Y_T(\omega)Y_T^\top(\omega)\right]</span> where <span class="math inline">Y_T(\omega)</span> is the Fourier transform of centered process <span class="math inline">\mathcal{Y}(t)</span>.<ul><li>White noise has the same PSD for whatever <span class="math inline">\omega</span></li></ul></li><li><strong>Wiener-Khinchin</strong> Theorem: <span class="math inline">\Phi_\mathcal{Y}(\omega)=\int^\infty_{-\infty}e^{-i\omega\theta}\bar{\mathrm{K}}_\mathcal{Y}(\theta)\mathrm{d}\theta</span></li><li><strong>Signal propagation</strong>: If the system <span class="math inline">P</span> has input <span class="math inline">\mathcal{Y}</span> and output <span class="math inline">\mathcal{Z}</span>, then <span class="math inline">\Phi_\mathcal{Z}=|P(i\omega)\Phi_\mathcal{Y}(\omega)P^\top(i\omega)|</span> where <span class="math inline">P(i\omega)</span> is the Fourier transform of the system.</li><li><strong>Cross spectrum</strong>: If the system <span class="math inline">G</span> has input <span class="math inline">\mathcal{Y}</span> and output <span class="math inline">\mathcal{Z}</span>, then the cross-spectrum is <span class="math inline">\Phi_{\mathcal{ZY}}(\omega)=G(i\omega)\Phi_\mathcal{Y}(\omega)</span></li></ul><h2 id="periodograms">Periodograms</h2><ul><li>Definition: We want to estimate <span class="math inline">\Phi_{\mathcal{Y}}(\omega)</span> without the expectation, then we use <span class="math display">Q_T(\omega,y)=\frac{1}{2T}\left|\int^T_{-T}e^{-i\omega t}y(t)\mathrm{d}t\right|^2</span> where <span class="math inline">y</span> is a sample realization of <span class="math inline">\mathcal{Y}</span>.<ul><li>We also use window functions to calculate the spectrum over a finite time interval (using <strong>Bartlett's procedure</strong>)</li></ul></li></ul><h2 id="stochastic-realizations">Stochastic realizations</h2><p>We want to find a stochastic model which gives the same spectrum given. This is called <strong>stochastic realization problem</strong>. We focus on scalar LTI case.</p><p>If we know <span class="math inline">P(s)=C[sI-A]^{-1}B+D</span> (the Laplace transform of LTI system) and <span class="math inline">P(s)=c\frac{\prod^m_{k=1}(s-z_k)}{\prod^n_{k=1}(s-p_k)}</span> for some <span class="math inline">m\leq n</span> and real constant <span class="math inline">c</span>. Then we have <span class="math display">\Phi_\mathcal{Y}(\omega)=P(i\omega)P(-i\omega)=c^2\frac{\prod^m_{k=1}(\omega^2+z_k^2)}{\prod^n_{k=1}(\omega^2+p_k^2)}</span>.</p><p>Lemma: For any valid PSD, there exists a spectral factorization <span class="math inline">\Phi_\mathcal{Y}(\omega)=\frac{\sum^m_{k=0}a_k(\omega^2)^k}{\sum^n_{k=0}b_k(\omega^2)^k}=P(i\omega)P(-i\omega)</span> where <span class="math inline">P(s)</span> is an asymptotically stable n-th order transfer function, iff 1. <span class="math inline">\Phi_\mathcal{Y}(\omega)</span> is a ratio of polynomials of <span class="math inline">\omega^2</span> 2. All coefficients <span class="math inline">a_k</span> and <span class="math inline">b_k</span> are real 3. The denominator has no positive real roots in <span class="math inline">\omega^2</span> 4. Any positive real roots in the numerator have even multiplicity</p><h1 id="notes-for-math-typing-in-hexo">Notes for math typing in Hexo</h1><blockquote><ol type="1"><li>Escape <code>\</code> by <code>\\</code>. Especially escape <code>{</code> by <code>\\{</code> instead of <code>\{</code>, and escape <code>\\</code> by <code>\\\\</code>.</li><li>Be careful about <code>_</code>, it's used in markdown as italic indicator. Add space after <code>_</code> is a useful solution.</li><li><a href="https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference" target="_blank" rel="noopener">Some useful Mathjax tricks at StackExchange</a></li><li><a href="https://www.overleaf.com/learn/latex/List_of_Greek_letters_and_math_symbols" target="_blank" rel="noopener">Several capital Greek characters should directly use its related Latin alphabet with <code>\mathrm</code> command</a>.</li></ol></blockquote></div><div class="reward-container"><div>Treat me some coffee XD</div><button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">Donate</button><div id="qr" style="display:none"><div style="display:inline-block"><img src="/blog/uploads/assets/wechatpay.jpg" alt="Jacob Zhong WeChat Pay"><p>WeChat Pay</p></div><div style="display:inline-block"><img src="/blog/uploads/assets/alipay.jpg" alt="Jacob Zhong Alipay"><p>Alipay</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>Post author: </strong>Jacob Zhong</li><li class="post-copyright-link"><strong>Post link: </strong><a href="http://zyxin.xyz/blog/2019-03/StochasticSystemNotes/" title="Notes for Stochastic System">http://zyxin.xyz/blog/2019-03/StochasticSystemNotes/</a></li><li class="post-copyright-license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/blog/tags/Math/" rel="tag"># Math</a> <a href="/blog/tags/Probability/" rel="tag"># Probability</a> <a href="/blog/tags/Stochastic/" rel="tag"># Stochastic</a></div><div class="post-nav"><div class="post-nav-item"><a href="/blog/2019-02/ProbabilityNotes/" rel="prev" title="Notes for Probability Theory (Basics)"><i class="fa fa-chevron-left"></i> Notes for Probability Theory (Basics)</a></div><div class="post-nav-item"><a href="/blog/2019-04/LinuxRemoteSetup/" rel="next" title="LinuxUbuntu 16.04/18.04">LinuxUbuntu 16.04/18.04 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript></div></div><script>window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">Table of Contents</li><li class="sidebar-nav-overview">Overview</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#discrete-time-stochastic-system"><span class="nav-number">1.</span> <span class="nav-text">Discrete-Time Stochastic System</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#stochastic-sequences"><span class="nav-number">1.1.</span> <span class="nav-text">Stochastic Sequences</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#probabilistic-characterization"><span class="nav-number">1.2.</span> <span class="nav-text">Probabilistic characterization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#markov-sequence"><span class="nav-number">1.3.</span> <span class="nav-text">Markov Sequence</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#linear-stochastic-sequence"><span class="nav-number">1.4.</span> <span class="nav-text">Linear Stochastic Sequence</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gaussian-stochastic-sequence"><span class="nav-number">1.5.</span> <span class="nav-text">Gaussian Stochastic Sequence</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#observation-filtering"><span class="nav-number">1.6.</span> <span class="nav-text">Observation &amp; Filtering</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#markov-chains"><span class="nav-number">2.</span> <span class="nav-text">Markov Chains</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#basic-definitions"><span class="nav-number">2.1.</span> <span class="nav-text">Basic definitions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#state-variables"><span class="nav-number">2.2.</span> <span class="nav-text">State Variables</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#state-classification"><span class="nav-number">2.3.</span> <span class="nav-text">State Classification</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#continuous-time-stochastic-system"><span class="nav-number">3.</span> <span class="nav-text">Continuous-Time Stochastic System</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#stochastic-sequences-1"><span class="nav-number">3.1.</span> <span class="nav-text">Stochastic Sequences</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#markov-sequence-1"><span class="nav-number">3.2.</span> <span class="nav-text">Markov Sequence</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#poisson-counters"><span class="nav-number">3.3.</span> <span class="nav-text">Poisson Counters</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#wiener-process"><span class="nav-number">3.4.</span> <span class="nav-text">Wiener-Process</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#linear-stochastic-sequence-1"><span class="nav-number">3.5.</span> <span class="nav-text">Linear Stochastic Sequence</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nonlinear-stochastic-sequence"><span class="nav-number">3.6.</span> <span class="nav-text">Nonlinear Stochastic Sequence</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#spectral-analysis"><span class="nav-number">4.</span> <span class="nav-text">Spectral Analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#power-spectral-density"><span class="nav-number">4.1.</span> <span class="nav-text">Power Spectral Density</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#periodograms"><span class="nav-number">4.2.</span> <span class="nav-text">Periodograms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#stochastic-realizations"><span class="nav-number">4.3.</span> <span class="nav-text">Stochastic realizations</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#notes-for-math-typing-in-hexo"><span class="nav-number">5.</span> <span class="nav-text">Notes for math typing in Hexo</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Jacob Zhong" src="/blog/uploads/assets/avatar.png"><p class="site-author-name" itemprop="name">Jacob Zhong</p><div class="site-description" itemprop="description">Blog of Jacob Zhong</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/blog/archives/"><span class="site-state-item-count">47</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/blog/categories/"><span class="site-state-item-count">13</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/blog/tags/"><span class="site-state-item-count">42</span> <span class="site-state-item-name">tags</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/cmpute" title="GitHub  https://github.com/cmpute" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://www.linkedin.com/in/jacobzhong/" title="LinkedIn  https://www.linkedin.com/in/jacobzhong/" rel="noopener" target="_blank"><i class="fa fa-fw fa-linkedin-square"></i>LinkedIn</a> </span><span class="links-of-author-item"><a href="mailto:cmpute@foxmail.com" title="E-Mail  mailto:cmpute@foxmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="https://www.last.fm/user/cmpute" title="Last.fm  https://www.last.fm/user/cmpute" rel="noopener" target="_blank"><i class="fa fa-fw fa-lastfm-square"></i>Last.fm</a></span></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Friend Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="https://hanyuz1996.github.io/" title="https://hanyuz1996.github.io/" rel="noopener" target="_blank">Hanyuz</a></li><li class="links-of-blogroll-item"><a href="https://xingminw.github.io/" title="https://xingminw.github.io/" rel="noopener" target="_blank">Xingminw</a></li></ul></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2017  <span itemprop="copyrightYear">2019</span> <span class="with-love"><i class="fa fa-fire"></i> </span><span class="author" itemprop="copyrightHolder">Jacob Zhong</span></div><div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div><span class="post-meta-divider">|</span><div class="theme-info">Theme  <a href="https://muse.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.6.0</div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="Total Visitors"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="Total Views"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/blog/lib/anime.min.js"></script><script src="/blog/lib/velocity/velocity.min.js"></script><script src="/blog/lib/velocity/velocity.ui.min.js"></script><script src="/blog/js/utils.js"></script><script src="/blog/js/motion.js"></script><script src="/blog/js/schemes/muse.js"></script><script src="/blog/js/next-boot.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });</script><script>NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);</script><script>function loadCount(){var o=document,d=o.createElement("script");d.src="https://jacob-zhongs-blog.disqus.com/count.js",d.id="dsq-count-scr",(o.head||o.body).appendChild(d)}window.addEventListener("load",loadCount,!1)</script><script>NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: {page: {
            url: "http://zyxin.xyz/blog/2019-03/StochasticSystemNotes/",
            identifier: "2019-03/StochasticSystemNotes/",
            title: "Notes for Stochastic System"
          }
        }
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://jacob-zhongs-blog.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });</script></body></html>