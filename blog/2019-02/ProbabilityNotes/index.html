<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/blog/uploads/assets/icon200.png"><link rel="icon" type="image/png" sizes="32x32" href="/blog/uploads/assets/icon32.png"><link rel="icon" type="image/png" sizes="16x16" href="/blog/uploads/assets/icon16.png"><link rel="mask-icon" href="/blog/images/logo.svg" color="#222"><link rel="stylesheet" href="/blog/css/main.css"><link rel="stylesheet" href="/blog/lib/font-awesome/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:new URL("http://zyxin.xyz/blog").hostname,root:"/blog/",scheme:"Muse",version:"7.6.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!0},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}}}</script><meta name="description" content="Notes for EECS 501.Probability SpaceNotation: (\Omega,\mathcal{F},\mathbb{P})\Omega: Sample space\mathcal{F}: Event space. Required to be σ-algebra. We often use Borel σ-algebra for continuous \Omega)"><meta name="keywords" content="Math,Probability"><meta property="og:type" content="article"><meta property="og:title" content="Notes for Probability Theory (Basics)"><meta property="og:url" content="http://zyxin.xyz/blog/2019-02/ProbabilityNotes/index.html"><meta property="og:site_name" content="Jacob Zhong"><meta property="og:description" content="Notes for EECS 501.Probability SpaceNotation: (\Omega,\mathcal{F},\mathbb{P})\Omega: Sample space\mathcal{F}: Event space. Required to be σ-algebra. We often use Borel σ-algebra for continuous \Omega)"><meta property="og:locale" content="en"><meta property="og:updated_time" content="2019-12-14T18:44:04.051Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Notes for Probability Theory (Basics)"><meta name="twitter:description" content="Notes for EECS 501.Probability SpaceNotation: (\Omega,\mathcal{F},\mathbb{P})\Omega: Sample space\mathcal{F}: Event space. Required to be σ-algebra. We often use Borel σ-algebra for continuous \Omega)"><link rel="canonical" href="http://zyxin.xyz/blog/2019-02/ProbabilityNotes/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0}</script><title>Notes for Probability Theory (Basics) | Jacob Zhong</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/blog/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Jacob Zhong</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Blog</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="Toggle navigation bar"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/blog/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/blog/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/blog/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/blog/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a></li><li class="menu-item menu-item-sitemap"><a href="/blog/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>Sitemap</a></li></ul></nav></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en"><link itemprop="mainEntityOfPage" href="http://zyxin.xyz/blog/2019-02/ProbabilityNotes/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/blog/uploads/assets/avatar.png"><meta itemprop="name" content="Jacob Zhong"><meta itemprop="description" content="Blog of Jacob Zhong"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Jacob Zhong"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Notes for Probability Theory (Basics)</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2019-02-23 16:38:32" itemprop="dateCreated datePublished" datetime="2019-02-23T16:38:32-05:00">2019-02-23</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">Edited on</span> <time title="Modified: 2019-12-14 13:44:04" itemprop="dateModified" datetime="2019-12-14T13:44:04-05:00">2019-12-14</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/blog/categories/Notes/" itemprop="url" rel="index"><span itemprop="name">Notes</span> </a></span>, <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/blog/categories/Notes/Math/" itemprop="url" rel="index"><span itemprop="name">Math</span> </a></span></span><span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span id="busuanzi_value_page_pv"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">Disqus: </span><a title="disqus" href="/blog/2019-02/ProbabilityNotes/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019-02/ProbabilityNotes/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><blockquote><p>Notes for <code>EECS 501</code>.</p></blockquote><h1 id="probability-space">Probability Space</h1><ul><li>Notation: <span class="math inline">(\Omega,\mathcal{F},\mathbb{P})</span><ul><li><span class="math inline">\Omega</span>: <strong>Sample space</strong></li><li><span class="math inline">\mathcal{F}</span>: <strong>Event space</strong>. Required to be <a href="http://mathworld.wolfram.com/Sigma-Algebra.html" target="_blank" rel="noopener"><strong>σ-algebra</strong></a>. We often use <a href="http://mathworld.wolfram.com/BorelSigma-Algebra.html" target="_blank" rel="noopener"><strong>Borel σ-algebra</strong></a> for continuous <span class="math inline">\Omega</span>).<ul><li>Axioms for <strong>σ-algebra</strong><ol type="1"><li><span class="math inline">\mathcal{F}</span> is non-empty</li><li><span class="math inline">A\in\mathcal{F} \Rightarrow A^C\in\mathcal{F}</span> (closed under complement)</li><li><span class="math inline">A_i \in\mathcal{F} \Rightarrow \bigcup^\infty_{k=1} A_k \in \mathcal{F}</span> (closed under countable union)</li></ol></li><li>For continuous case considering interval <span class="math inline">\Omega=[a,b]</span>, <span class="math inline">\mathcal{F}_0</span> is the set of all subintervals of <span class="math inline">\Omega</span>. Then its <strong>Borel σ-algebra</strong> is the smallest σ-algebra that contains <span class="math inline">\mathcal{F}_0</span>. Here the <span class="math inline">\mathcal{F}_0</span> is a <a href="https://en.wikibooks.org/wiki/Measure_Theory/Basic_Structures_And_Definitions/Semialgebras,_Algebras_and_%CF%83-algebras" target="_blank" rel="noopener"><strong>semialgebra</strong></a>. We can find a containing σ-algebra for every semialgebra.</li><li>Axioms for <strong>semialgebra</strong><ol type="1"><li><span class="math inline">\emptyset, \Omega \in \mathcal{F}</span></li><li><span class="math inline">A_i \in\mathcal{F} \Rightarrow \bigcap^n_{k=1} A_k \in \mathcal{F}</span> (closed under finite intersections)</li><li><span class="math inline">\forall B \in\mathcal{F}, B^C=\bigcup^n_{k=1} A_k</span> where <span class="math inline">A_i \in \mathcal{F}</span></li></ol></li></ul></li><li><span class="math inline">\mathbb{P}</span>: <strong>Probability measure</strong>.<ul><li>Axioms for probability measure<ol type="1"><li><span class="math inline">\mathbb{P}(\Omega) = 1</span></li><li><span class="math inline">\forall A\in\mathcal{F}, \mathbb{P}(A) \geqslant 0</span></li><li><span class="math inline">A_i, A_j \in\mathcal{F}</span> are pairwise disjoint, then <span class="math inline">\mathbb{P}(\bigcup^\infty_{k=1}A_k)=\sum^\infty_{k=1}\mathbb{P}(A_k)</span></li></ol></li></ul></li></ul></li><li><strong>Product Space</strong>: Probability spaces can be combined using Cartesian product.</li><li><strong>Independence</strong>: <span class="math inline">\mathbb{P}(A_{k_1}\cap A_{k_2}\cap ...\cap A_{k_l})=\prod_{i=1}^l \mathbb{P}(A_i),\;\forall \\{k_i\\}_1^l\subset\\{ 1..n\\}</span></li><li><strong>Conditional probability</strong>: <span class="math inline">\mathbb{P}\left(A_i \middle| A_j\right)=\mathbb{P}(A_i\cap A_j)/\mathbb{P}(A_j)</span></li><li><strong>Total probability</strong>: <span class="math inline">\mathbb{P}(B)=\sum_{i=1}^n \mathbb{P}(B\cap A_i)=\sum_{i=1}^n \mathbb{P}\left(B\middle| A_i\right)\mathbb{P}(A_i)</span>, where <span class="math inline">\\{A_1,\cdots,A_n\\}</span> are disjoint and partition of <span class="math inline">\Omega</span>.</li><li><strong>Bayes' Rule</strong>: <span class="math display">\mathbb{P}(A_j|B)=\frac{\mathbb{P}(B|A_j)\mathbb{P}(A_j)}{\sum^n_{i=1} \mathbb{P}(B|A_i)\mathbb{P}(A_i)}</span><ul><li>Priori: <span class="math inline">\mathbb{P}(B|A_j)</span></li><li>Posteriori: <span class="math inline">\mathbb{P}(A_j|B)</span></li></ul></li></ul><a id="more"></a><h1 id="random-variables">Random Variables</h1><blockquote><p>Note: The equations are written in continuous case by default, one can get the equation for discrete case by changing integration into summation and changing differential into difference.</p></blockquote><ul><li>Random Variable <span class="math inline">\mathcal{X}</span> is a mapping <span class="math inline">\mathcal{X}: (\Omega,\mathcal{F},\mathbb{P})\rightarrow(\Omega_ \mathcal{X},\mathcal{F}_ \mathcal{X},\mathbb{P}_ \mathcal{X})</span></li><li><strong>Continuous &amp; Discrete &amp; Mixed Random Variable</strong>:<ul><li>Can be defined upon whether <span class="math inline">\Omega_\mathcal{X}</span> is continuous</li><li>Can be defined upon whether we can find continuous density function <span class="math inline">f_\mathcal{X}</span></li><li><span class="math inline">\mathcal{F}_\mathcal{X}</span> for continuous <span class="math inline">\mathcal{X}</span> is a <strong>Borel σ-field</strong></li><li>All three kinds of random variables can be expressed by CDF or "extended" PDF with Dirac function and Lebesgue integration.</li></ul></li><li><strong>Scalar Random Variable</strong>: <span class="math inline">\mathcal{X}: \Omega\rightarrow\mathbb{F}</span><ul><li>Formal definition: <span class="math inline">\mathcal{X}: (\Omega,\mathcal{F},\mathbb{P})\rightarrow(\Omega_ \mathcal{X}\subset\mathbb{F},\mathcal{F}_ \mathcal{X}\subset\left\\{\omega\middle| \omega\subset\Omega_ \mathcal{X}\right\\},\mathbb{P}_ \mathcal{X}:\mathcal{F}_ \mathcal{X}\rightarrow[0,1])</span></li><li><strong>Cumulative Distribution Function</strong> (CDF): <span class="math inline">F_ \mathcal{X}(x)=\mathbb{P}_\mathcal{X}(\mathcal{X}(\omega)\leqslant x)</span></li><li><strong>Probability Mass Function</strong> (PMF): <span class="math inline">p_ \mathcal{X}(x)=\mathbb{P}_\mathcal{X}(\mathcal{X}(\omega)=x)</span></li><li><strong>Probability Density Function</strong> (PDF): <span class="math display">f_ \mathcal{X}(x)=\mathbb{P}_ \mathcal{X}(x&lt; \mathcal{X}(\omega)\leqslant x+ \mathrm{d}x)=\mathrm{d}F_ \mathcal{X}(x)/ \mathrm{d}x</span></li></ul></li><li><strong>Vector Random Variable</strong> (Multiple Random Variables): <span class="math inline">\mathcal{X}: \Omega\rightarrow\mathbb{F}^n</span><ul><li>Formal definition <span class="math inline">\mathcal{X}: (\Omega,\mathcal{F},\mathbb{P})\rightarrow(\Omega_ \mathcal{X}\subset\mathbb{F}^n,\mathcal{F}_ \mathcal{X}\subset\left\\{\omega\middle| \omega\subset\Omega_ \mathcal{X}\right\\},\mathbb{P}_ \mathcal{X}:\mathcal{F}_ \mathcal{X}\rightarrow[0,1])</span></li><li><strong>Cumulative Distribution Function</strong> (CDF): <span class="math inline">F_ \mathcal{X}(x)=\mathbb{P}_\mathcal{X}(\\{\mathcal{X}(\omega)\\}_i\leqslant x_i), i=1\ldots n</span></li><li><strong>Probability Mass Function</strong> (PMF): <span class="math inline">p_ \mathcal{X}(x)=\mathbb{P}_\mathcal{X}(\\{\mathcal{X}(\omega)\\}_i=x)</span></li><li><strong>Probability Density Function</strong> (PDF): <span class="math display">f_ \mathcal{X}(x)=\mathbb{P}_\mathcal{X}(x_i&lt; \\{\mathcal{X}(\omega)\\}_i\leqslant x_i+ \mathrm{d}x_i)=\frac{\partial^n}{\partial x_1\partial x_2\ldots\partial x_n}F_ \mathcal{X}(x)</span></li></ul></li></ul><blockquote><p>Afterwards, we don't distinguish <span class="math inline">\mathbb{P}_\mathcal{X}</span> with <span class="math inline">\mathbb{P}</span> if there's no ambiguity.</p></blockquote><ul><li><strong>Independence</strong>(<span class="math inline">\perp \\!\\!\\! \perp</span>): <span class="math inline">\mathbb{P}(\mathcal{X}\in A\cap \mathcal{Y}\in B)=\mathbb{P}(\mathcal{X}\in A)\mathbb{P}(\mathcal{Y}\in B)</span><ul><li><strong>Independent CDF</strong>:<span class="math inline">F_{\mathcal{XY}}(x,y)=F_\mathcal{X}(x)F_\mathcal{Y}(y)</span> iff <span class="math inline">\mathcal{X}\perp \\!\\!\\! \perp\mathcal{Y}</span></li><li><strong>Independent PMF</strong>:<span class="math inline">p_{\mathcal{XY}}(x,y)=p_\mathcal{X}(x)p_\mathcal{Y}(y)</span> iff <span class="math inline">\mathcal{X}\perp \\!\\!\\! \perp\mathcal{Y}</span></li><li><strong>Independent PDF</strong>:<span class="math inline">f_{\mathcal{XY}}(x,y)=f_\mathcal{X}(x)f_\mathcal{Y}(y)</span> iff <span class="math inline">\mathcal{X}\perp \\!\\!\\! \perp\mathcal{Y}</span></li></ul></li><li><strong>Marginalization</strong><ul><li><strong>Marginal distribution function</strong>: <span class="math inline">F_{1:m}(x_{1:m})\equiv F\left(\begin{bmatrix}x_1&amp;x_2&amp;\ldots&amp;x_m&amp;\infty&amp;\ldots&amp;\infty\end{bmatrix}^\top\right)</span></li><li><strong>Marginal density function</strong>: <span class="math inline">f_{1:m}(x_{1:m})=\int^\infty_{-\infty}\cdots\int^\infty_{-\infty} f(x) \mathrm{d}x_{m+1}\cdots \mathrm{d}x_n</span></li></ul></li><li><strong>Conditional</strong> (on event <span class="math inline">\mathcal{E}</span>)<ul><li><strong>Conditional probability</strong>: <span class="math inline">\mathbb{P}(\mathcal{X}\in\mathcal{D}|\mathcal{E})=\mathbb{P}(\\{\omega|\mathcal{X}(\omega)\in\mathcal{D}\\}\cap \mathcal{E})/\mathbb{P}(\mathcal{E})</span> on a event <span class="math inline">\mathcal{E}\in\mathcal{F}</span> and a set <span class="math inline">\mathcal{D}\subset\mathcal{F}</span>.</li><li><strong>Conditional CDF</strong>: <span class="math inline">F_ \mathcal{X}(x|\mathcal{E})=\mathbb{P}(\mathcal{X}_i\leqslant x_i|\mathcal{E})</span></li><li><strong>Conditional PMF</strong>: <span class="math inline">p_ \mathcal{X}(x|\mathcal{E})=\mathbb{P}(\mathcal{X}_i=x_i|\mathcal{E})</span></li><li><strong>Conditional PDF</strong>: <span class="math inline">f_ \mathcal{X}(x|\mathcal{E})=\frac{\partial^n}{\partial x_1\partial x_2\ldots\partial x_n}F_ \mathcal{X}(x|\mathcal{E})</span></li></ul></li><li><strong>Conditional</strong> (on variable <span class="math inline">\mathcal{Y}</span>)<ul><li><strong>Conditional probability</strong>: <span class="math inline">\mathbb{P}(\mathcal{X}\in\mathcal{D}_1|\mathcal{Y}\in\mathcal{D}_2)=\mathbb{P}(\\{\omega|\mathcal{X}(\omega)\in\mathcal{D}_1,\mathcal{Y}(\omega)\in\mathcal{D}_2\\})/\mathbb{P}(\mathcal{Y}(\omega)\in\mathcal{D}_2)</span>.</li><li><strong>Conditional PDF</strong> (similar for PMF): <span class="math display">f_{\mathcal{X}|\mathcal{Y}\in\mathcal{D}}(x)=\frac{\int_{\mathcal{Y}\in\mathcal{D}}f_{\mathcal{XY}}(x,y)}{\int_{\mathcal{Y}\in\mathcal{D}}f_{\mathcal{Y}}(y)},\;f_{\mathcal{X}|\mathcal{Y}}(x|y)=f_{\mathcal{X}|\mathcal{Y}=y}(x)=\frac{f_{\mathcal{XY}}(x,y)}{f_{\mathcal{Y}}(y)}</span><ul><li>Using total probability we have <span class="math inline">f_ \mathcal{X}(x)=\int f_{\mathcal{X}|\mathcal{Y}}(x|y)f_ \mathcal{Y}(y)\mathrm{d}y</span>. This can be further integrated into Bayes' rule.</li></ul></li><li><strong>Conditional CDF</strong>: Can be defined similarly, of defined as <span class="math inline">F_{\mathcal{X}|\mathcal{Y}\in\mathcal{D}}(x)\equiv\int f_{\mathcal{X}|\mathcal{Y}\in\mathcal{D}}(x)\mathrm{d}x</span><ul><li>Similarly we have <span class="math inline">F_ \mathcal{X}(x)=\int F_{\mathcal{X}|\mathcal{Y}}(x|y)f_ \mathcal{Y}(y)\mathrm{d}y</span></li></ul></li><li><strong>Substitution law</strong>: <span class="math inline">\mathbb{P}((\mathcal{X},\mathcal{Y})\in \mathcal{D}|\mathcal{X}=x)=\mathbb{P}((x,\mathcal{Y})\in \mathcal{D})</span><ul><li>Common usage: Suppose <span class="math inline">\mathcal{Z}=\psi(\mathcal{X},\mathcal{Y})</span>, then <span class="math inline">p_\mathcal{Z}(z)=\int_x \mathbb{P}\left(\psi(x,\mathcal{Y})=z\right)p_\mathcal{X}(x)</span></li></ul></li></ul></li></ul><h1 id="uncertainty-propagation">Uncertainty Propagation</h1><p>Suppose <span class="math inline">\mathcal{Y}=\psi(\mathcal{X})</span> or specifically <span class="math inline">y=\psi(x_1)=\psi(x_2)=\cdots=\psi(x_K)</span> - Scalar case: <span class="math display">f_ \mathcal{Y}(y)=\sum^K_{k=1} f_ \mathcal{X}(\psi^{-1}_ k(y))\left| \frac{\partial\psi}{\partial x}\biggr|_{x=\psi^{-1}_k(y)} \right|^{-1}</span> - Vector case: <span class="math display">f_ \mathcal{Y}(y)=\sum^K_{k=1} f_ \mathcal{X}(\psi^{-1}_ k(y))\left|\det(J)\right|^{-1},\text{where Jacobian }J=\frac{\partial\psi}{\partial x}\biggr|_{x=\psi^{-1}_k(y)}</span> - Trivial Case — Summation: <span class="math inline">\mathcal{Y}=\mathcal{X}_ 1+\mathcal{X}_ 2</span>, then <span class="math inline">f_ \mathcal{Y}=\int^\infty_{-\infty}f_{\mathcal{X}_1\mathcal{X}_2}(x_1,x_2-x_1) \mathrm{d}x_1</span> Another way is to use the method of choice: <span class="math inline">F_\mathcal{Y}(y)=\int_{\psi(x)\leq y}f_\mathcal{X}(x)\mathrm{d}x</span></p><h1 id="expectation-moments">Expectation &amp; Moments</h1><ul><li><p><strong>Expectation</strong>: <span class="math inline">\mathbb{E}_ \mathcal{X}[\psi(\mathcal{X})]=\int^\infty_\infty\cdots\int^\infty_\infty\psi(x)f_ \mathcal{X}(x) \mathrm{d}x_1\ldots \mathrm{d}x_n</span> &gt; A more rigorous way to define the expectation is <span class="math inline">\mathbb{E}_ \mathcal{X}[\psi(\mathcal{X})]=\int \psi(\mathcal{X})dF _\mathcal{X}(\mathcal{X})</span>. This definition uses Lebesgue Integral and works on both discrete and continuous (or even mixed) variables. See <a href="http://www.randomservices.org/random/dist/Integral.html" target="_blank" rel="noopener">this post</a> and <a href>this discussion</a> for more information. &gt; We write <span class="math inline">\mathbb{E}_\mathcal{X}</span> as <span class="math inline">\mathbb{E}</span> if there's no ambiguity (when only one random variable is included). And <span class="math inline">\mathbb{E}[\mathcal{X}]</span> will be abbreviated as <span class="math inline">\mathbb{E}\mathcal{X}</span> afterwards.</p><ul><li><strong>Linearity of expectation</strong>: <span class="math inline">\mathbb{E}[A\mathcal{\mathcal{X}}]=A\mathbb{E}[\mathcal{X}] (\forall \mathcal{X},\mathcal{Y},\forall A\in\mathbb{R}^{m\times n})</span></li><li><strong>Independent expectation</strong>: <span class="math inline">\mathbb{E}[\prod^n_i\mathcal{X}_i]=\prod^n_i\left(\mathbb{E}\mathcal{X}_i\right)(\forall\;\text{indep.}\;\mathcal{X}_i)</span></li><li><strong>Conditional expectation</strong>: <span class="math inline">\mathbb{E}[\mathcal{Y}|\mathcal{X}=x]=\int^\infty_{-\infty}yf_{\mathcal{Y}|\mathcal{X}}(y|x)\mathrm{d}y</span><ul><li><strong>Total expectation/Smoothing</strong>: <span class="math inline">\mathbb{E}_ \mathcal{X}[\mathbb{E}_ {\mathcal{Y}|\mathcal{X}}(\mathcal{Y}|\mathcal{X})]=\mathbb{E}\mathcal{Y}</span></li><li><strong>Substitution law</strong>: <span class="math inline">\mathbb{E}[g(\mathcal{X},\mathcal{Y})|\mathcal{X}=x]=\mathbb{E}[\psi(x,\mathcal{Y})|\mathcal{X}=x]</span></li><li><span class="math inline">\mathbb{E}[\psi(\mathcal{X})|\mathcal{X}]=\psi(\mathcal{X})</span></li><li><span class="math inline">\mathbb{E}[\psi(\mathcal{X})\mathcal{Y}|\mathcal{X}]=\psi(\mathcal{X})\mathbb{E}(\mathcal{Y}|\mathcal{X})</span></li><li><strong>Towering</strong>: <span class="math inline">\mathbb{E}_ {\mathcal{Y}|\mathcal{Z}}[\mathbb{E}_ \mathcal{X}(\mathcal{X}|\mathcal{Y},\mathcal{Z})|\mathcal{Z}]=\mathbb{E}_ {\mathcal{X}|\mathcal{Z}}[\mathcal{X}|\mathcal{Z}]</span></li></ul></li></ul></li><li><p><strong>Moment</strong> (p-th order): <span class="math inline">\mu_p(\mathcal{X})=\mathbb{E}[\mathcal{X}^p]</span></p><ul><li><strong>Mean</strong>: <span class="math inline">\mu_ \mathcal{X}=\mathbb{E}[\mathcal{X}]</span></li></ul></li><li><p><strong>Central moment</strong> (p-th order): <span class="math inline">\mathbb{E}[(\mathcal{X}-\mu_ \mathcal{X})^p]</span></p><ul><li><strong>Variance</strong>: <span class="math inline">\sigma_ \mathcal{X}^2=\mathbb{E}[(\mathcal{X}-\mu_ \mathcal{X})^2]</span>, sometimes written as <span class="math inline">\sigma_ \mathcal{X}^2=\mathbb{V}(\mathcal{X})</span></li><li><strong>Skewness</strong>: <span class="math inline">\gamma=\mathbb{E}[(\mathcal{X}-\mu_ \mathcal{X})^3]/\sigma_\mathcal{X}^3</span></li></ul></li><li><p><strong>Correlation</strong>: <span class="math inline">\text{corr}(\mathcal{X}_i,\mathcal{X}_j)=\mathbb{E}[\mathcal{X}_i\mathcal{X}_j]</span></p><ul><li><strong>Correlation matrix</strong>: <span class="math inline">C=\mathbb{E}[\mathcal{X}_i\mathcal{X}_j^\top]</span></li><li><strong>Correlation coefficient</strong>: <span class="math inline">\rho(\mathcal{X}_ i,\mathcal{X}_ j)=\frac{\text{corr}(\mathcal{X}_ i,\mathcal{X}_ j)}{\sigma_{\mathcal{X}_ i}^2\sigma_{\mathcal{X}_ j}^2}</span></li></ul></li><li><p><strong>Covariance</strong>: <span class="math inline">\text{cov}(\mathcal{X}_i,\mathcal{X}_j)=\mathbb{E}[(\mathcal{X}_i-\mu_i)(\mathcal{X}_j-\mu_j)]</span></p><ul><li><strong>Covariance matrix</strong>: <span class="math inline">S=\mathbb{E}\left[(\mathcal{X}-\mu_ \mathcal{X})(\mathcal{X}-\mu_ \mathcal{X})^\top\right]</span></li><li>Properties: <span class="math inline">\text{cov}(\mathcal{X}+c)=\text{cov}(\mathcal{X}), \text{cov}(A\mathcal{X},\mathcal{X}B)=A\text{cov}(\mathcal{X})+\text{cov}(\mathcal{X})B^\top</span></li><li><strong>Uncorrelated</strong>: <span class="math inline">\rho(\mathcal{X}_ i,\mathcal{X}_ j)=0 \Leftrightarrow\text{cov}(\mathcal{X}_i,\mathcal{X}_j)=0\Leftrightarrow\text{corr}(\mathcal{X}_i,\mathcal{X}_j)=\mathbb{E}[\mathcal{X}_i]\mathbb{E}[\mathcal{X}_j]</span> (uncorrelated is necessary for independent)</li><li>Cases where uncorrelated implies independence: (1) Jointly Gaussian (2) Bernoulli</li></ul></li><li><p><strong>Centered variable</strong>: <span class="math inline">\tilde{\mathcal{X}}=\mathcal{X}-\mu_ \mathcal{X}</span></p></li></ul><h1 id="transform-methods">Transform methods</h1><ul><li><strong>Probability generating function</strong>(PGF): similiar to Z-tranform <span class="math display">G_ \mathcal{X}(z)\equiv\mathbb{E}[z^\mathcal{X}]=\sum_{x_i}z^{x_i}p_ \mathcal{X}(x_i)</span><ul><li>For <span class="math inline">\mathcal{F}_ \mathcal{X}=\mathbb{N}</span>, we have <span class="math display">\frac{\mathrm{d}^k}{\mathrm{d}z^k}G_ \mathcal{X}(z)\Biggr|_{z=1}=\mathbb{E}[\mathcal{X}(\mathcal{X}-1)\cdots(\mathcal{X}-(k-1))]</span></li></ul></li><li><strong>Moment generating function</strong>(MGF): similar to Laplace transform <span class="math display">M_ \mathcal{X}(z)\equiv\mathbb{E}[e^{s\mathcal{X}}]=\int^\infty_{-\infty}e^{sx}f_ \mathcal{X}(x)\mathrm{d}x</span><ul><li>Generally we have <span class="math display">\frac{\mathrm{d}^k}{\mathrm{d}s^k}M_ \mathcal{X}(s)\Biggr|_{s=0}=\mathbb{E}[\mathcal{X}^k]</span></li></ul></li><li><strong>Characteristic function</strong>(CF): similar to Fourier transform <span class="math display">\phi_ \mathcal{X}(\omega)\equiv\mathbb{E}[e^{j\omega \mathcal{X}}]=\int^\infty_{-\infty}e^{j\omega x}f_ \mathcal{X}(x)\mathrm{d}x</span><ul><li>Generally we have <span class="math display">\frac{\mathrm{d}^k}{\mathrm{d}\omega^k}\phi_ \mathcal{X}(\omega)\Biggr|_{\omega=0}=j^k\mathbb{E}[\mathcal{X}^k]</span></li><li><strong>Independent</strong>: <span class="math inline">\mathcal{X}\perp \\!\\!\\! \perp\mathcal{Y}</span> iff. <span class="math inline">\phi_ \mathcal{XY}(\omega)=\phi_ \mathcal{X}(\omega)\phi_ \mathcal{Y}(\omega)</span></li></ul></li><li><strong>Joint characteristic function</strong>: for vector case <span class="math inline">\mathcal{X}\in\mathbb{R}^n</span>, we define vector <span class="math inline">u</span> and <span class="math display">\phi_ \mathcal{X}(u)\equiv\mathbb{E}[e^{ju^\top \mathcal{X}}]</span><ul><li>Trivial usage: if <span class="math inline">\mathcal{Y}=\mathcal{X}_ 1+\mathcal{X}_ 2</span>, then <span class="math inline">\phi_ \mathcal{Y}(u)=\phi_{\mathcal{X}_ 1}(u)\phi_{\mathcal{X}_ 2}(u)</span></li></ul></li></ul><h1 id="common-distributions"><a href="http://www.math.wm.edu/~leemis/chart/UDR/UDR.html" target="_blank" rel="noopener">Common distributions</a></h1><ul><li><strong><em>Bernoulli</em></strong>: <span class="math inline">\Omega_\mathcal{X}=\\{0,1\\}</span> <span class="math display">p_\mathcal{X}(1)=p,\;p_\mathcal{X}(0)=q=1-p</span><ul><li><span class="math inline">\mu_\mathcal{X}=p,\;\sigma^2_\mathcal{X}=pq,\;G_\mathcal{X}(z)=q+pz</span></li></ul></li><li><strong><em>Binomial</em></strong> - <span class="math inline">\mathcal{B}(n,p)</span>: <span class="math inline">\Omega_\mathcal{X}=\\{0,1,\ldots,n\\}</span> <span class="math display">p_\mathcal{X}(k)=\binom{n}{k}p^k(1-p)^{n-k}</span><ul><li><span class="math inline">\mu_\mathcal{X}=np,\;\sigma^2_\mathcal{X}=np(1-p),\;G_\mathcal{X}(z)=(1-p+pe^z)^n</span></li></ul></li><li><strong><em>Multinomial</em></strong>: <span class="math inline">\Omega_\mathcal{X}=\\{0,1,\ldots,n\\}^k</span> <span class="math display">p_\mathcal{X}(x)=\binom{n}{x_1!\cdots x_k!}p_1^{x_1}\cdots p_k^{x_k}</span></li><li><strong><em>Geometric</em></strong>: <span class="math inline">\Omega_\mathcal{X}=\mathbb{N}</span> <span class="math display">p_\mathcal{X}(k)=(1-p)^{k-1}p</span><ul><li><span class="math inline">\mu_\mathcal{X}=\frac{1}{p},\;\sigma^2_\mathcal{X}=\frac{1-p}{p^2},\;G_\mathcal{X}(z)=\frac{pz}{1-(1-p)z}</span></li></ul></li><li><strong><em>Poisson</em></strong>: <span class="math inline">\Omega_\mathcal{X}=\mathbb{N}</span> <span class="math display">p_\mathcal{X}(k)=\frac{\lambda^k}{k!}\exp\\{-\lambda\\}</span><ul><li><span class="math inline">\mu_\mathcal{X}=\lambda,\;\sigma^2_\mathcal{X}=\lambda,\;G_\mathcal{X}(z)=\exp\\{\lambda(z-1)\\}</span></li></ul></li><li><strong><em>Uniform</em></strong> - <span class="math inline">\mathcal{U}(a,b)</span>: <span class="math inline">\Omega_\mathcal{X}=[a,b]</span> <span class="math display">f(x)= \begin{cases} 1/(b-a), &amp; x \in [a,b] \\\\ 0, &amp; \text{otherwise} \end{cases}</span><ul><li><span class="math inline">\mu_\mathcal{X}=\frac{1}{2}(a+b),\;\sigma^2_\mathcal{X}=\frac{1}{12}(b-a)^2,\;M_\mathcal{X}(s)=\frac{e^{sb}-e^{sa}}{s(b-a)}\;(s\neq 0)</span></li></ul></li><li><strong><em>Normal</em></strong> - <span class="math inline">\mathcal{N}(\mu,\sigma)</span>: <span class="math inline">\Omega_\mathcal{X}=\mathbb{R}</span> <span class="math display">f(x)=\frac{1}{\sqrt{2\pi\sigma}}\exp\left\\{-\frac{(x-\mu)^2}{2\sigma^2}\right\\}</span><ul><li><span class="math inline">M_\mathcal{X}(s)=\exp\left\\{\mu s+\frac{1}{2}\sigma^2s^2\right\\}</span></li></ul></li><li><strong><em>Joint Normal</em></strong>: <span class="math inline">\Omega_\mathcal{X}=\mathbb{R}^n</span> <span class="math display">f(x)=\frac{1}{\sqrt{(2\pi)^n \det(S)}}\exp\left\\{-\frac{1}{2}(x-\mu)^\top S^{-1}(x-\mu)\right\\}</span><ul><li><span class="math inline">\phi_\mathcal{X}(u)=\exp\left\\{ju^\top \mu-\frac{1}{2}u^\top Su\right\\}</span></li></ul></li><li><strong><em>Rayleigh</em></strong>: <span class="math inline">\Omega_\mathcal{X}=[0,\infty]</span> <span class="math display">f(x)=\frac{x}{\sigma^2}\exp\left\\{-\frac{x^2}{2\sigma^2}\right\\}H(x)</span> where <span class="math inline">H(x)</span> is Heaviside step function</li><li><strong><em>Exponential</em></strong> - <span class="math inline">\mathcal{E}(\lambda)</span>: <span class="math inline">\Omega_\mathcal{X}=[0,\infty]</span> <span class="math display">f(x)=\frac{1}{\mu}\exp\left\\{-\frac{x}{\mu}\right\\}H(x)</span><ul><li><span class="math inline">\mu_\mathcal{X}=1/\lambda,\;\sigma^2_\mathcal{X}=1/\lambda^2,\;M_\mathcal{X}(s)=\lambda/(\lambda-s)</span></li></ul></li><li><strong><em>Laplacian</em></strong>: <span class="math display">f(x)=\frac{1}{2b}\exp\left\\{-\frac{|x-\mu|}{b}\right\\}</span></li></ul><h1 id="derivation-of-the-distributions">Derivation of the distributions</h1><ul><li><strong>Bernoulli → Binomial</strong>: <span class="math inline">\mathcal{X}_ i\sim\text{Bernoulli}(p) \Rightarrow \mathcal{Y}=\sum_{i=1}^n \mathcal{X}_ i\sim\mathcal{B}(n,p)</span></li><li><strong>Bernoulli → Geometric</strong>: <span class="math inline">\mathcal{X}_i\sim\text{Bernoulli}(p) \Rightarrow \mathcal{Y}\sim\text{Geometric}(p)</span> denoting the first <span class="math inline">\mathcal{X}_i=1</span></li><li><strong>Binomial → Poisson</strong>: <span class="math inline">\mathcal{X}_i\sim\mathcal{B}(n,p,k=1) \Rightarrow \mathcal{Y}\sim\text{Poisson}(\lambda)</span> when <span class="math inline">n\rightarrow \infty</span> with <span class="math inline">p=\frac{\lambda\tau}{n}</span></li><li><strong>Binomial → Exponential</strong>: <span class="math inline">\mathcal{X}_i\sim\mathcal{B}(n,p,k\neq0) \Rightarrow \mathcal{Y}\sim\mathcal{E}(\lambda)</span> when <span class="math inline">n\rightarrow \infty</span> with <span class="math inline">p=\frac{\lambda\tau}{n}</span> &gt; Actually <span class="math inline">\mathcal{B}(n,p,k) \Rightarrow e^{-\lambda \tau}\frac{(\lambda \tau)^k}{k!}</span></li><li><a href="https://en.wikipedia.org/wiki/Laplace_distribution#Related_distributions" target="_blank" rel="noopener"><strong>Exponential → Laplacian</strong></a>: <span class="math inline">\mathcal{X}_1, \mathcal{X}_2 \sim\mathcal{E}(\lambda) \Rightarrow \mathcal{X}_1-\mathcal{X}_2\sim\text{Laplacian}(\lambda^{-1})</span></li></ul><h1 id="concentration-inequalities">Concentration Inequalities</h1><ul><li><strong>Cauchy-Schwarz Inequality</strong>: <span class="math inline">S=\left(\mathbb{E}[\mathcal{X}\mathcal{X}_j]\right)^2\leqslant\left(\mathbb{E}[\mathcal{X}_i]\right)^2\left(\mathbb{E}[\mathcal{X}_j]\right)^2</span></li><li><strong>Markov Inequality</strong>: <span class="math inline">\mathbb{P}(\mathcal{X}\geqslant a)\leqslant \mathbb{E}\mathcal{X}/a,\;a&gt;0</span></li><li><strong>Chebychev Inequality</strong>: <span class="math inline">\mathbb{P}(|\mathcal{X}-\mu|\geqslant\delta)\leqslant\sigma^2/\delta^2</span></li><li><strong>Jenson Inequality</strong>: <span class="math inline">\psi(\mathbb{E} \mathcal{X}) \leqslant \mathbb{E}\psi(\mathcal{X})</span> for any convex function <span class="math inline">\psi</span></li><li><strong>Chernoff bound</strong>: <span class="math inline">\mathbb{P}(\mathcal{X}\geqslant a)\leqslant \underset{s\geqslant 0}{\min}\,e^{-as}M_ \mathcal{X}(s)</span></li><li><strong>Law of Large Numbers</strong>: let <span class="math inline">\mathcal{X}_i</span> be samples drawn from <span class="math inline">(\mathbb{R}^n,\mathcal{F}^n,\mathbb{P})</span>, and <span class="math inline">\mathbb{P}</span> is such that <span class="math inline">\mathcal{X}_k</span> has mean <span class="math inline">\mu</span> and covariance <span class="math inline">S</span><ul><li>Weak version: if <span class="math inline">\mathcal{Y}_ k=\frac{1}{k}\sum^k_{j=1}\mathcal{X}_j</span> then <span class="math display">\underset{k\rightarrow\infty}{\lim} \mathbb{P}\left(\left\lVert \mathcal{Y}_k-\mu\right\rVert&gt;\epsilon\right)=0</span></li><li>Strong version: if <span class="math inline">\mathcal{Y}_ k=\frac{1}{k}\sum^k_{j=1}\mathcal{X}_j</span> then <span class="math display">\underset{k\rightarrow\infty}{\lim} \mathcal{Y}_k=\mu</span></li><li>Central Limit Theorem: if <span class="math inline">\mathcal{Y}_ k=\frac{1}{\sqrt{k}}\sum^k_{j=1}S^{-1/2}\left(\mathcal{X}_ j-\mu\right)</span> then <span class="math display">\underset{k\rightarrow\infty}{\lim} f_{\mathcal{Y}_k}(y_k)=\mathcal{N}(0,I)</span></li></ul></li></ul><h1 id="estimation-theory">Estimation Theory</h1><ul><li><strong>Hilbert Space Projection Theorem</strong>: A Hilbert space is a complete inner product space. Let <span class="math inline">\mathcal{H}</span> be a Hilbert space, <span class="math inline">\mathcal{M}</span> be a closed subspace of <span class="math inline">\mathcal{H}</span> and <span class="math inline">z\in\mathcal{H}</span>. Then there is a unique <span class="math inline">\hat{z}\in\mathcal{M}</span> which is closest to <span class="math inline">z</span>: <span class="math display">\lVert z-\hat{z}\rVert &lt; \lVert z-y\rVert, \forall y\in\mathcal{M}, y\neq\hat{z}</span><ul><li><strong>Orthogonality Principle</strong>: <span class="math inline">\hat{z}</span> is the closest point iff. <span class="math inline">\langle z-\hat{z},y\rangle=0, \forall y\in\mathcal{M}</span>. In estimation we formulate inner product as <span class="math inline">\langle \mathcal{X}, \mathcal{Y}\rangle=\mathbb{E}[\mathcal{X}\mathcal{Y}^T]</span>, it's <span class="math inline">\mathbb{E}[(\mathcal{Y}-\mathbb{E}[\mathcal{Y}|\mathcal{X}])h(\mathcal{X})]=0, \forall h(\cdot)</span>.</li></ul></li><li><strong>Estimation Problem</strong>: Given random vector and random variable <span class="math inline">\mathcal{Y}</span> with joint PDF <span class="math inline">f_{\mathcal{XY}}(\cdot)</span>, we observe <span class="math inline">\mathcal{X}=x</span> and we want to form an estimate of <span class="math inline">\mathcal{Y}</span> as <span class="math inline">\hat{\mathcal{Y}}=g(x)</span></li><li><strong>Minimum Mean Square Error(MMSE) Estimation</strong>: <span class="math inline">\hat{\mathcal{Y}}=\mathbb{E}(\mathcal{Y}|\mathcal{X})</span><ul><li>Target: minimize <span class="math inline">\mathbb{E}[(\mathcal{Y}-\hat{\mathcal{Y}})(\mathcal{Y}-\hat{\mathcal{Y}})^\top]</span> with <span class="math inline">\hat{\mathcal{Y}}=g(\mathcal{X})</span> (arbitary <span class="math inline">g(\cdot)</span>)</li></ul></li><li><strong>Linear Minimum MSE(LMMSE) Estimation</strong>: <span class="math inline">\hat{A}</span> satisfies <span class="math inline">\mathbb{E}[\mathcal{YX^\top}]=A\mathbb{E}[\mathcal{XX^\top}]</span><ul><li>Target: minimize <span class="math inline">\mathbb{E}[(\mathcal{Y}-\hat{\mathcal{Y}})(\mathcal{Y}-\hat{\mathcal{Y}})^\top]</span> with <span class="math inline">\hat{\mathcal{Y}}=A\mathcal{X}</span></li></ul></li><li><strong>Affine Minimum MSE(AMMSE) Estimation</strong>: <span class="math inline">\hat{G}</span> satisfies <span class="math inline">\mathbb{E}[\mathcal{YX^\top}]=G\mathbb{E}[\mathcal{XX^\top}]</span>, <span class="math inline">\hat{c}=\mu_{\mathcal{Y}}-\hat{G}\mu_{\mathcal{X}}</span><ul><li>Target: minimize <span class="math inline">\mathbb{E}[(\mathcal{Y}-\hat{\mathcal{Y}})(\mathcal{Y}-\hat{\mathcal{Y}})^\top]</span> with <span class="math inline">\hat{\mathcal{Y}}=G\mathcal{X}+c</span></li></ul></li></ul><h1 id="convergence">Convergence</h1><ul><li>Categories: Generally we assume <span class="math inline">n\rightarrow\infty</span> and giving <span class="math inline">\mathcal{X}_n</span> are random variables.<ul><li><strong>Sure Convergence</strong> (<span class="math inline">\mathcal{X}_n\xrightarrow{\text{surely}}\mathcal{X}</span>): <span class="math display">\forall \omega\in\Omega, \mathcal{X}_n(\omega)\xrightarrow{n\rightarrow\infty}\mathcal{X}</span></li><li><strong>Almost Sure Convergence</strong> (<span class="math inline">\mathcal{X}_ n\xrightarrow{\text{a.s./w.p.1}}\mathcal{X}</span>, <code>w.p.1</code>: with probability 1): <span class="math display">\mathbb{P}(\\{\omega\in\Omega:\lim_ {n\rightarrow\infty}\mathcal{X} _n(\omega)=\mathcal{X}\\})=1</span><ul><li>Equivalent definition: <span class="math display">\mathbb{P}(\bigcup_{\epsilon&gt;0}A(\epsilon))=0\;\text{where}\;A(\epsilon)=\bigcap_{N=1}\bigcup_{n=N}\\{\omega:|\mathcal{X}_n(\omega)-\mathcal{X}(\omega)|\geqslant\epsilon\\}</span></li></ul></li><li><strong>Convergence in Probability</strong> (<span class="math inline">\mathcal{X}_ n\xrightarrow{\text{in prob.}}\mathcal{X}</span>): <span class="math display">\mathbb{P}(\\{\omega\in\Omega:\lim_ {nn\rightarrow\infty}|\mathcal{X}_n(\omega)-\mathcal{X}(\omega)|\geqslant\epsilon\\})=0,\;\forall\epsilon&gt;0</span></li><li><strong>Convergence in Distribution</strong> (<span class="math inline">\mathcal{X}_ n\xrightarrow{\text{D}}\mathcal{X}</span>): <span class="math display">\lim_ {n\rightarrow\infty} F_{\mathcal{X}_ n}(x)=F_{\mathcal{X}}(x)</span></li><li><strong>Convergence in mean of order</strong>: (<span class="math inline">\mathcal{X}_ n\xrightarrow{\text{mean r}}\mathcal{X}</span>, abbr. <code>m.s.</code> for <span class="math inline">r=2</span>): <span class="math display">\mathbb{E}[|\mathcal{X}_n-\mathcal{X}|^r]\rightarrow 0</span></li><li><span class="math inline">\mathcal{X}_n\xrightarrow{\text{a.s./mean r}}\mathcal{X}\Rightarrow\mathcal{X}_n\xrightarrow{\text{in prob.}}\mathcal{X}\Rightarrow\mathcal{X}_n\xrightarrow{\text{D}}\mathcal{X}</span></li></ul></li><li><strong>Strong Law of Large Numbers</strong> (requires iid.): <span class="math display">S_n=\frac{1}{n}\sum\mathcal{X}_ i\xrightarrow{\text{a.s./m.s.}}\mu_ \mathcal{X}</span></li><li><strong>Weak Law of Large Numbers</strong> (requires indentical uncorrelated distributed): <span class="math inline">S_n\xrightarrow{\text{in prob.}}\mu_\mathcal{X}</span></li><li><strong>Central Limit Theorem</strong> (requires independent): <span class="math display">\mathcal{Y}_n=\frac{1}{\sqrt{n}}\sum\frac{\mathcal{X}_i-m}{\sigma}\xrightarrow{\text{D}}\mathcal{N}(0,1)</span> &gt; Law of Large Numbers and Central Limit Theorem together characterized the limiting behavior of the average of samples. See <a href="https://en.wikipedia.org/wiki/Central_limit_theorem#Relation_to_the_law_of_large_numbers" target="_blank" rel="noopener">Wikipedia</a> and <a href="http://www.cs.toronto.edu/~yuvalf/CLT.pdf" target="_blank" rel="noopener">a proof</a> to see their relationships.</li></ul><h1 id="miscellaneous-corollaries">Miscellaneous Corollaries</h1><ol type="1"><li>For random valuable that takes positive values, <span class="math inline">\mathbb{E}(X)=\int^\infty_0 \mathbb(\mathcal{X}&gt;x)dx</span></li><li>If <span class="math inline">\mathcal{X}_1,...\mathcal{X}_n</span> are IID continuous random variables, then <span class="math inline">\mathbb{P}(\mathcal{X}_1&lt;\mathcal{X}_2&lt;\ldots&lt;\mathcal{X}_n)=1/n!</span></li><li>Define <span class="math inline">\mathcal{X}_ {(j)}</span> to be the j-th smallest among <span class="math inline">\mathcal{X}_ 1,...\mathcal{X}_ n</span>. Suppose <span class="math inline">\mathcal{X}_ 1,...\mathcal{X}_ n</span> are IID random variables with PDF <span class="math inline">f</span> and CDF <span class="math inline">F</span>, then <span class="math display">f_ {\mathcal{X}_ {(j)}}(x)=\frac{n!}{(n-j)!(j-1)!}[F(x)]^{j-1}[1-F(x)]^{n-j}f_ \mathcal{X}(x)</span></li></ol></div><div class="reward-container"><div>Treat me some coffee XD</div><button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">Donate</button><div id="qr" style="display:none"><div style="display:inline-block"><img src="/blog/uploads/assets/wechatpay.jpg" alt="Jacob Zhong WeChat Pay"><p>WeChat Pay</p></div><div style="display:inline-block"><img src="/blog/uploads/assets/alipay.jpg" alt="Jacob Zhong Alipay"><p>Alipay</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>Post author: </strong>Jacob Zhong</li><li class="post-copyright-link"><strong>Post link: </strong><a href="http://zyxin.xyz/blog/2019-02/ProbabilityNotes/" title="Notes for Probability Theory (Basics)">http://zyxin.xyz/blog/2019-02/ProbabilityNotes/</a></li><li class="post-copyright-license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/blog/tags/Math/" rel="tag"># Math</a> <a href="/blog/tags/Probability/" rel="tag"># Probability</a></div><div class="post-nav"><div class="post-nav-item"><a href="/blog/2019-01/WindowsRemoteDesktop/" rel="prev" title="Windows远程桌面设置"><i class="fa fa-chevron-left"></i> Windows远程桌面设置</a></div><div class="post-nav-item"><a href="/blog/2019-03/StochasticSystemNotes/" rel="next" title="Notes for Stochastic System">Notes for Stochastic System <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript></div></div><script>window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">Table of Contents</li><li class="sidebar-nav-overview">Overview</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#probability-space"><span class="nav-number">1.</span> <span class="nav-text">Probability Space</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#random-variables"><span class="nav-number">2.</span> <span class="nav-text">Random Variables</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#uncertainty-propagation"><span class="nav-number">3.</span> <span class="nav-text">Uncertainty Propagation</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#expectation-moments"><span class="nav-number">4.</span> <span class="nav-text">Expectation &amp; Moments</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#transform-methods"><span class="nav-number">5.</span> <span class="nav-text">Transform methods</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#common-distributions"><span class="nav-number">6.</span> <span class="nav-text">Common distributions</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#derivation-of-the-distributions"><span class="nav-number">7.</span> <span class="nav-text">Derivation of the distributions</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#concentration-inequalities"><span class="nav-number">8.</span> <span class="nav-text">Concentration Inequalities</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#estimation-theory"><span class="nav-number">9.</span> <span class="nav-text">Estimation Theory</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#convergence"><span class="nav-number">10.</span> <span class="nav-text">Convergence</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#miscellaneous-corollaries"><span class="nav-number">11.</span> <span class="nav-text">Miscellaneous Corollaries</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Jacob Zhong" src="/blog/uploads/assets/avatar.png"><p class="site-author-name" itemprop="name">Jacob Zhong</p><div class="site-description" itemprop="description">Blog of Jacob Zhong</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/blog/archives/"><span class="site-state-item-count">47</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/blog/categories/"><span class="site-state-item-count">13</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/blog/tags/"><span class="site-state-item-count">42</span> <span class="site-state-item-name">tags</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/cmpute" title="GitHub → https://github.com/cmpute" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://www.linkedin.com/in/jacobzhong/" title="LinkedIn → https://www.linkedin.com/in/jacobzhong/" rel="noopener" target="_blank"><i class="fa fa-fw fa-linkedin-square"></i>LinkedIn</a> </span><span class="links-of-author-item"><a href="mailto:cmpute@foxmail.com" title="E-Mail → mailto:cmpute@foxmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="https://www.last.fm/user/cmpute" title="Last.fm → https://www.last.fm/user/cmpute" rel="noopener" target="_blank"><i class="fa fa-fw fa-lastfm-square"></i>Last.fm</a></span></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Friend Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="https://hanyuz1996.github.io/" title="https://hanyuz1996.github.io/" rel="noopener" target="_blank">Hanyuz</a></li><li class="links-of-blogroll-item"><a href="https://xingminw.github.io/" title="https://xingminw.github.io/" rel="noopener" target="_blank">Xingminw</a></li></ul></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2017 – <span itemprop="copyrightYear">2019</span> <span class="with-love"><i class="fa fa-fire"></i> </span><span class="author" itemprop="copyrightHolder">Jacob Zhong</span></div><div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div><span class="post-meta-divider">|</span><div class="theme-info">Theme – <a href="https://muse.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.6.0</div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="Total Visitors"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="Total Views"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/blog/lib/anime.min.js"></script><script src="/blog/lib/velocity/velocity.min.js"></script><script src="/blog/lib/velocity/velocity.ui.min.js"></script><script src="/blog/js/utils.js"></script><script src="/blog/js/motion.js"></script><script src="/blog/js/schemes/muse.js"></script><script src="/blog/js/next-boot.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });</script><script>NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);</script><script>function loadCount(){var o=document,d=o.createElement("script");d.src="https://jacob-zhongs-blog.disqus.com/count.js",d.id="dsq-count-scr",(o.head||o.body).appendChild(d)}window.addEventListener("load",loadCount,!1)</script><script>NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: {page: {
            url: "http://zyxin.xyz/blog/2019-02/ProbabilityNotes/",
            identifier: "2019-02/ProbabilityNotes/",
            title: "Notes for Probability Theory (Basics)"
          }
        }
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://jacob-zhongs-blog.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });</script></body></html>