<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Math on JacobZ</title><link>https://zyxin.xyz/blog/en/categories/Math/</link><description>Recent content in Math on JacobZ</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sat, 27 Jun 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://zyxin.xyz/blog/en/categories/Math/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes for Algebra Basics</title><link>https://zyxin.xyz/blog/en/2020-06/AlgebraBasicsNotes/</link><pubDate>Sat, 27 Jun 2020 00:00:00 +0000</pubDate><guid>https://zyxin.xyz/blog/en/2020-06/AlgebraBasicsNotes/</guid><description>&lt;blockquote>
&lt;ul>
&lt;li>In this note, $\{x_i\}^b_a$ denotes set $\{x_a, x_{a+1}, \ldots, x_b\}$&lt;/li>
&lt;li>&lt;em>TODO: add Jordan Form&lt;/em>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h2 id="algebraic-structures">Algebraic Structures&lt;/h2>
&lt;h3 id="operation">Operation&lt;/h3>
&lt;ul>
&lt;li>Definition: an (binary, closed) &lt;strong>operation&lt;/strong> $\ast$ on a set $S$ is a mapping of $S\times S\to S$&lt;/li>
&lt;li>&lt;strong>Commutative&lt;/strong>: $x\ast y=y\ast x,\;\forall x,y\in S$&lt;/li>
&lt;li>&lt;strong>Associative&lt;/strong>: $(x\ast y)\ast z=x\ast (y\ast z),\;\forall x,y,z\in S$&lt;/li>
&lt;/ul>
&lt;h3 id="group">Group&lt;/h3>
&lt;ul>
&lt;li>Definition: a &lt;strong>group&lt;/strong> is a pair $(\mathcal{S},\ast)$ with following axioms
&lt;ol>
&lt;li>$\ast$ is associative on $\mathcal{S}$&lt;/li>
&lt;li>(Identity element) $\exists e\in \mathcal{S}\text{ s.t. }x\ast e=e\ast x=x,\;\forall x\in \mathcal{S}$&lt;/li>
&lt;li>(Inverse element) $\forall x\in \mathcal{S}, \exists x&amp;rsquo; \in \mathcal{S}\text{ s.t. }x\ast x&amp;rsquo;=x&amp;rsquo;\ast x=e$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>Abelian&lt;/strong>: a group is called &lt;strong>abelian group&lt;/strong> if $\ast$ is also commutative&lt;/li>
&lt;/ul>
&lt;h3 id="ring">Ring&lt;/h3>
&lt;ul>
&lt;li>Definition: a &lt;strong>ring&lt;/strong> is a triplet $(\mathcal{R},+,\ast)$ consisting of a set of &lt;code>scalars&lt;/code> $\mathcal{R}$ and two operators + and $\ast$ with following axioms
&lt;ol>
&lt;li>$(\mathcal{R},+)$ is an abelian group with identity denoted $0$&lt;/li>
&lt;li>$\forall a,b,c \in \mathcal{R}\text{ s.t. }a\ast(b\ast c) = (a\ast b)\ast c$&lt;/li>
&lt;li>$\exists 1\in\mathcal{R}, \forall a\in\mathcal{R}\text{ s.t. }a\cdot 1=a$&lt;/li>
&lt;li>$\ast$ is distributive over $+$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;h3 id="field">Field&lt;/h3>
&lt;ul>
&lt;li>Definition: a &lt;strong>field&lt;/strong> $(\mathcal{F},+,\ast)$ is a ring where $(\mathcal{F}\backslash\{0\},\ast)$ is also an abelian group.
&lt;blockquote>
&lt;p>Difference from ring to field is that $\ast$ need to be commutative and have a multiplicative inverse&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;h3 id="vector-space">Vector Space&lt;/h3>
&lt;ul>
&lt;li>Definition: a &lt;strong>vector space&lt;/strong> (aka. &lt;strong>linear space&lt;/strong>) is a triplet $(\mathcal{U},\oplus,\cdot)$ defined over a field $(\mathcal{F},+,\ast)$ with following axioms, where set $\mathcal{U}$ is called &lt;code>vectors&lt;/code>, operator $\oplus$ is called &lt;code>vector addition&lt;/code> and mapping $\cdot$ is called &lt;code>scalar multiplication&lt;/code>:
&lt;ol>
&lt;li>(&lt;strong>Null vector&lt;/strong>) $(\mathcal{U},+)$ is an abelian group with identity element $\emptyset$&lt;/li>
&lt;li>Scalar multiplication is a mapping of $\mathcal{F}\times\mathcal{U}\to\mathcal{U}$&lt;/li>
&lt;li>$\alpha\cdot(x\oplus y) = \alpha\cdot x \oplus \alpha\cdot y,\;\forall x,y\in\mathcal{U};\alpha\in\mathcal{F}$&lt;/li>
&lt;li>$(\alpha+\beta)\cdot x = \alpha\cdot x\oplus\beta\cdot x,\;\forall x\in\mathcal{U};\alpha,\beta\in\mathcal{F}$&lt;/li>
&lt;li>$(\alpha\ast\beta)\cdot x=\alpha\cdot(\beta\cdot x),\;\forall x\in\mathcal{U};\alpha,\beta\in\mathcal{F}$&lt;/li>
&lt;li>$1_\mathcal{F}\cdot x=x$&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>Usually we don&amp;rsquo;t distinguish vector addition $\oplus$ and addition of scalar $+$. Juxtaposition is also commonly used for &lt;em>both&lt;/em> scalar multiplication $\cdot$ and multiplication of scalars $\ast$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Subspace&lt;/strong>: a subspace $\mathcal{V}$ of a linear space $\mathcal{U}$ over field $\mathcal{F}$ is a subset of $\mathcal{U}$ which is itself a linear space over $\mathcal{F}$ under same vector addition and scalar multiplication.&lt;/li>
&lt;/ul>
&lt;h4 id="basis--coordinate">Basis &amp;amp; Coordinate&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Linear Independence&lt;/strong>: Let $\mathcal{V}$ be a vector space over $\mathcal{F}$ and let $X=\{x_i\}^n_1\subset \mathcal{V}$
&lt;ul>
&lt;li>X is &lt;strong>linearly dependent&lt;/strong> if $\exists \alpha_1,\ldots,\alpha_n\in\mathcal{F}$ not all 0 s.t. $\sum^n_{i=1} \alpha_i x_i=0$.&lt;/li>
&lt;li>X is &lt;strong>linearly independent&lt;/strong> if $\sum^n_{i=1} \alpha_i x_i=0 \Rightarrow \alpha_1=\alpha_2=\ldots=\alpha_n=0$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Span&lt;/strong>: Given a set of vectors $V$, the set of linear combinations of vectors in $V$ is called the &lt;strong>span&lt;/strong> of it, denoted $\mathrm{span}\{V\}$&lt;/li>
&lt;li>&lt;strong>Basis&lt;/strong>: A set of linearly independent vectors in a linear space $\mathcal{V}$ is a &lt;strong>basis&lt;/strong> if every vector in $\mathcal{V}$ can be expressed as a &lt;em>unique linear combination&lt;/em> of these vectors. (see below &amp;ldquo;Coordinate&amp;rdquo;)
&lt;ul>
&lt;li>Basis Expansion: Let $(X,\mathcal{F})$ be a vector space of dimension n. If $\{v_i\}^k_1,\;1\leqslant k&amp;lt; n$ is linearly independent, then $\exists \{v_i\}^n_{k+1}$ such that $\{v_i\}_1^n$ is a basis.&lt;/li>
&lt;li>&lt;strong>Reciprocal Basis&lt;/strong>: Given basis $\{v_i\}^n_1$, a set ${r_i}^1_n$ that satifies $\langle r_i,v_j \rangle=\delta_i(j)$ is a reciprocal basis. It can be generated by Gram-Schmidt Process and $\forall x\in\mathcal{X}, x=\sum^n_{i=1}\langle r_i,x\rangle v_i$.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Dimension&lt;/strong>: &lt;em>Cardinality&lt;/em> of the basis is called the &lt;strong>dimension&lt;/strong> of that vector space, which is equal to &lt;em>the maximum number of linearly independent vectors&lt;/em> in the space. Denoted as $dim(\mathcal{V})$.
&lt;ul>
&lt;li>In an $n$-dimensional vector space, any set of $n$ linearly independent vectors is a basis.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Coordinate&lt;/strong>: For a vector $x$ in vector space $\mathcal{V}$, given a basis $\{e_1, \ldots, e_n\}$ we can write $x$ as $x=\sum^n_{i=1}\beta_i e_i=E\beta$ where $E=\begin{bmatrix}e_1&amp;amp;e_2&amp;amp;\ldots&amp;amp;e_n\end{bmatrix}$ and $\beta=\begin{bmatrix}\beta_1&amp;amp;\beta_2&amp;amp;\ldots&amp;amp;\beta_n\end{bmatrix}^\top$. Here $\beta$ is called the &lt;strong>representation&lt;/strong> (or &lt;strong>coordinate&lt;/strong>) of $x$ given the basis $E$.&lt;/li>
&lt;/ul>
&lt;h4 id="norm--inner-product">Norm &amp;amp; Inner product&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Inner Product&lt;/strong>: an operator on two vectors that produces a scalar result (i.e. $\langle\cdot,\cdot\rangle:\mathcal{V}\to\mathbb{R}\;or\;\mathbb{C}$) with following axioms:
&lt;ol>
&lt;li>(Symmetry) $\langle x,y \rangle=\overline{\langle y,x\rangle},\;\forall x,y\in\mathcal{V}$&lt;/li>
&lt;li>(Bilinearity) $\langle \alpha x+\beta y,z\rangle=\alpha\langle x,z\rangle+\beta\langle y,z\rangle,\;\forall x,y,z\in\mathcal{V};\alpha,\beta\in\mathbb{C}$&lt;/li>
&lt;li>(Pos. definiteness) $\langle x,x\rangle\geqslant 0,\;\forall x\in\mathcal{V}$ and $\langle x,x\rangle=0\Rightarrow x=0_\mathcal{V}$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>Inner Product Space&lt;/strong>: A linear space with a defined inner product&lt;/li>
&lt;li>&lt;strong>Orthogonality&lt;/strong>:
&lt;ul>
&lt;li>Perpedicularity of vectors ($x\perp y$): $\langle x,y\rangle=0$&lt;/li>
&lt;li>Perpedicularity of a vector to a set ($y\perp\mathcal{S},\mathcal{S}\subset\mathcal{V}$): $y\perp x,\;\forall x\in\mathcal{S}$&lt;/li>
&lt;li>&lt;strong>Orthogonal Set&lt;/strong>: set $\mathcal{S}\subset(\mathcal{U},\langle\cdot,\cdot\rangle)$ is orthogonal $\Leftrightarrow x\perp y,\;\forall x,y\in\mathcal{S},x\neq y$&lt;/li>
&lt;li>&lt;strong>Orthonormal Set&lt;/strong>: set $\mathcal{S}$ is orthonormal iff $\mathcal{S}$ is orthogonal and $\Vert x\Vert=1,\;\forall x\in\mathcal{S}$&lt;/li>
&lt;li>Orthogonality of sets ($\mathcal{X}\perp\mathcal{Y}$): $\langle x,y\rangle=0,\;\forall x\in\mathcal{X};y\in\mathcal{Y}$&lt;/li>
&lt;li>&lt;strong>Orthogonal Complement&lt;/strong>: Let $(\mathcal{V},\langle\cdot,\cdot\rangle)$ be an inner product space and let $\mathcal{U}\subset\mathcal{V}$ be a subspace of $\mathcal{V}$, the orthogonal complement of $\mathcal{U}$ is $\mathcal{U}^\perp=\left\{v\in\mathcal{V}\middle|\langle v,u\rangle=0,\;\forall u\in\mathcal{U}\right\}$.
&lt;ul>
&lt;li>$\mathcal{U}^\perp\subset\mathcal{V}$ is a subspace&lt;/li>
&lt;li>$\mathcal{V}=\mathcal{U}\overset{\perp}{\oplus}\mathcal{U}^\perp$ ($\oplus$: direct sum, $\overset{\perp}{\oplus}$: orthogonal sum)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Norm&lt;/strong>: A &lt;strong>norm&lt;/strong> on a linear space $\mathcal{V}$ is mapping $\Vert\cdot\Vert:\;\mathcal{V}\to\mathbb{R}$ such that:
&lt;ol>
&lt;li>(Positive definiteness) $\Vert x\Vert\geqslant 0\;\forall x\in \mathcal{V}$ and $\Vert x\Vert =0\Rightarrow x=0_\mathcal{V}$&lt;/li>
&lt;li>(Homogeneous) $\Vert \alpha x\Vert=|\alpha|\cdot\Vert x\Vert,\;\forall x\in\mathcal{V},\alpha\in\mathbb{R}$&lt;/li>
&lt;li>(Triangle inequality) $\Vert x+y\Vert\leqslant\Vert x\Vert+\Vert y\Vert$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>Distance&lt;/strong>: Norm can be used to measure distance between two vectors. Meanwhile, distance from a vector to a (sub)space is defined as $d(x,\mathcal{S})=\inf_{y\in\mathcal{S}} d(x,y)=\inf_{y\in\mathcal{S}} \Vert x-y\Vert$
&lt;ul>
&lt;li>&lt;strong>Projection Point&lt;/strong>: $x^* =\arg\min_{y\in\mathcal{S}}\Vert x-y\Vert$ is the projection point of $x$ on linear space $\mathcal{S}$.&lt;/li>
&lt;li>&lt;strong>Projection Theorem&lt;/strong>: $\exists !x^* \in\mathcal{S}$ s.t. $\Vert x-x^* \Vert=d(x,\mathcal{S})$ and we have $(x-x^*) \perp\mathcal{S}$&lt;/li>
&lt;li>&lt;strong>Orthogonal Projection&lt;/strong>: $P(x)=x^*:\mathcal{X}\to\mathcal{M}$ is called the orthogonal projection of $\mathcal{X}$ onto $\mathcal{M}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Normed Space&lt;/strong>: A linear space with a defined norm $\Vert\cdot\Vert$, denoted $(\mathcal{V},\mathcal{F},\Vert\cdot\Vert)$
&lt;blockquote>
&lt;p>A inner product space is always a normed space because we can define $\Vert x\Vert=\sqrt{\langle x,x\rangle}$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>Common $\mathbb{R}^n$ Norms:
&lt;ul>
&lt;li>Euclidean norm (2-norm): $\Vert x\Vert_2=\left(\sum^n_{i=1}|x_i|^2\right)^{1/2}=\left\langle x,x\right\rangle^{1/2}=\left(x^\top x\right)^{1/2}$&lt;/li>
&lt;li>$l_p$ norm (p-norm): $\Vert x\Vert_p=\left(\sum^n_{i=1}|x_i|^p\right)^{1/p}$&lt;/li>
&lt;li>$l_1$ norm: $\Vert x\Vert_1=\sum^n_{i=1}|x_i|$&lt;/li>
&lt;li>$l_\infty$ norm: $\Vert x\Vert_\infty=\max_{i}\{x_i\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Common matrix norms:
&lt;blockquote>
&lt;p>Matrix norms are also called &lt;strong>operator norms&lt;/strong>, can measure how much a linear operator &amp;ldquo;magnifies&amp;rdquo; what it operates on.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>A general form induced from $\mathbb{R}^n$ norm: $$\Vert A\Vert=\sup_{x\neq 0}\frac{\Vert Ax\Vert}{\Vert x\Vert}=\sup_{\Vert x\Vert=1}\Vert Ax\Vert$$&lt;/li>
&lt;li>$\Vert A\Vert_1=\max_j\left(\sum^n_{i=1}|a_{ij}|\right)$&lt;/li>
&lt;li>$\Vert A\Vert_2=\left[ \max_{\Vert x\Vert=1}\left\{(Ax)^* (Ax)\right\}\right]^{1/2}=\left[ \lambda_{max}(A^ *A)\right]^{1/2}$ ($\lambda_{max}$: largest eigenvalue)&lt;/li>
&lt;li>$\Vert A\Vert_\infty=\max_i\left(\sum^n_{j=1}|a_{ij}|\right)$&lt;/li>
&lt;li>(Frobenius Norm) $\Vert A\Vert_F=\left[ \sum^m_{i=1}\sum^n_{j=1}\left|a_{ij}\right|^2\right]^{1/2}=\left[ tr(A^*A)\right]^{1/2}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Useful inequations:
&lt;ul>
&lt;li>&lt;strong>Cauchy-Schwarz&lt;/strong>: $|\langle x,y\rangle|\leqslant\left\langle x,x\right\rangle^{1/2}\cdot\left\langle y,y\right\rangle^{1/2}$&lt;/li>
&lt;li>&lt;strong>Triangle&lt;/strong> (aka. $\Delta$): $\Vert x+y\Vert\leqslant\Vert x\Vert+\Vert y\Vert$
&lt;blockquote>
&lt;p>Lemma: $\Vert x-y\Vert \geqslant \left| \Vert x\Vert-\Vert y\Vert \right|$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Pythagorean&lt;/strong>: $x\perp y \Leftrightarrow \Vert x+y\Vert=\Vert x\Vert+\Vert y\Vert$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="gramian">Gramian&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Gram-Schmidt Process&lt;/strong>: A method to find orthogonal basis $\{v_i\}^n_1$ given an ordinary basis $\{y_i\}^n_1$. It&amp;rsquo;s done by perform $v_k=y_k-\sum^{k-1}_{j=1}\frac{\langle y_k,v_j\rangle}{\langle v_j,v_j \rangle}\cdot v_j$ iteratively from 1 to $n$. To get an orthonormal basis, just normalize these vectors.&lt;/li>
&lt;li>&lt;a class="link" href="https://en.wikipedia.org/wiki/Gramian_matrix" target="_blank" rel="noopener"
>&lt;strong>Gram Matrix&lt;/strong>&lt;/a>: The Gram matrix generated from vectors $\{y_i\}_ 1^k$ is denoted $G(y_ 1,y_ 2,\ldots,y_ k)$. Its element $G_{ij}=\langle y_i,y_j\rangle$
&lt;ul>
&lt;li>&lt;strong>Gram Determinant&lt;/strong>: $g(y_1,y_2,\ldots,y_n)=\det G$&lt;/li>
&lt;li>&lt;strong>Normal Equations&lt;/strong>: Given subspace $\mathcal{M}$ and its basis $\{y_i\}^n_1$, the projection point of $\forall x\in\mathcal{M}$ can be represented by $$x^*=\alpha y=\begin{bmatrix}\alpha_1&amp;amp;\alpha_2&amp;amp;\ldots&amp;amp;\alpha_n\end{bmatrix}\begin{bmatrix}y_1\\y_2\\ \vdots \\y_n\end{bmatrix},\;\beta=\begin{bmatrix}\langle x,y_1\rangle\\ \langle x,y_2\rangle\\ \vdots\\ \langle x,y_n\rangle\end{bmatrix} where\;G^\top\alpha=\beta$$
&lt;blockquote>
&lt;p>For least-squares problem $Ax=b$, consider $\mathcal{M}$ to be the column space of $A$, then $G=A^\top A,\;\beta=A^\top b,\;G^\top\alpha=\beta\Rightarrow\alpha=(A^\top A)^{-1}A^\top b$. Similarly for weighted least-squares problem ($\Vert x\Vert=x^\top Mx$), let $G=A^\top MA, \beta=A^\top Mb$, we can get $\alpha=(A^\top MA)^{-1}A^\top Mb$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="linear-algebra">Linear Algebra&lt;/h2>
&lt;h3 id="linear-operator">Linear Operator&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>Definition: a linear operator $\mathcal{A}$ (aka. linear transformation, linear mapping) is a function $f: V\to U$ that operate on a linear space $(\mathcal{V},\mathcal{F})$ to produce elements in another linear space $(\mathcal{U},\mathcal{F})$ and obey $$\mathcal{A}(\alpha_1 x_1+\alpha_2 x_2) = \alpha_1\mathcal{A}(x_1) + \alpha_2\mathcal{A}(x_2),\;\forall x_1,x_2\in V;\alpha_1, \alpha_2\in\mathcal{F}$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Range (Space)&lt;/strong>: $\mathcal{R}(\mathcal{A})=\left\{u\in U\middle|\mathcal{A}(v)=u,\;\forall v\in V\right\}$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Null Space&lt;/strong> (aka. &lt;strong>kernel&lt;/strong>): $\mathcal{N}(\mathcal{A})=\left\{v\in V\middle|\mathcal{A}(v)=\emptyset_U\right\}$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>$\mathcal{A}$-invariant subspace&lt;/strong>: Given vector space $(\mathcal{V},\mathcal{F})$ and linear operator $\mathcal{A}:\mathcal{V}\rightarrow \mathcal{V}$, $\mathcal{W}\subseteq\mathcal{V}$ is $A$-invariant if $\forall x\in\mathcal{W}$, $\mathcal{A}x\in\mathcal{W}$.&lt;/p>
&lt;ul>
&lt;li>Both $\mathcal{R}(\mathcal{A})$ and $\mathcal{N}(\mathcal{A})$ are $\mathcal{A}$-invariant&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Matrix Representation: Given bases for both $V$ and $U$ (respectively $\{v_i\}^n_1$ and $\{u_j\}^m_1$), matrix representation $A$ satisfies $\mathcal{A}(v_i)=\sum^m_{j=0}A_{ji}u_j$ so that $\beta=A\alpha$ where $\alpha$ and $\beta$ is the representation of a vector under $\{v_i\}$ and $\{u_j\}$ respectively.
{% asset_img linear_map_relations.png Relation between a linear map and its matrix
representations %}&lt;/p>
&lt;ul>
&lt;li>$P$ and $Q$ are change of basis matrices, $A=Q^{-1}\tilde{A}P,\;\tilde{A}=QAP^{-1}$&lt;/li>
&lt;li>The i-th column of $A$ is the coordinates of $\mathcal{A}(v_i)$ represented by the basis $\{u_j\}$, similarly i-th column of $\tilde{A}$ is $\mathcal{A}(\tilde{v}_i)$ represented in $\{\tilde{u}_j\}$&lt;/li>
&lt;li>The i-th column of $P$ is the coordinates of $v_i$ represented by the basis $\{\tilde{v}\}$, similarly i-th column of $Q$ is $u_j$ represented in $\{\tilde{u}\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Matrix Similarity ($A\sim B$): Two (square) matrix representations ($A,B$) of the same linear operator are called &lt;strong>similar&lt;/strong> (or &lt;strong>conjugate&lt;/strong>) and they satisfies $\exists P$ s.t. $B=PAP^{-1}$.&lt;/p>
&lt;blockquote>
&lt;p>From now on we don&amp;rsquo;t distinguish between linear operator $\mathcal{A}$ and its matrix representation where choice of basis doesn&amp;rsquo;t matter.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Rank&lt;/strong>: $rank(A)=\rho(A)\equiv dim(\mathcal{R}(A))$&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Sylvester&amp;rsquo;s Inequality&lt;/strong>: $\rho(A)+\rho(B)-n\leqslant \rho(AB)\leqslant \min\{\rho(A), \rho(B)\}$&lt;/li>
&lt;li>&lt;strong>Singularity&lt;/strong>: $\rho(A)&amp;lt; n$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Nullity&lt;/strong>: $null(A)=\nu(A)\equiv dim(\mathcal{N}(A))$&lt;/p>
&lt;ul>
&lt;li>$\rho(A)+\nu(A)=n$ ($n$ is the dimensionality of domain space)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Adjoint&lt;/strong>: The adjoint of the linear map $\mathcal{A}: \mathcal{V}\to\mathcal{W}$ is the linear map $\mathcal{A}^*: \mathcal{W}\to\mathcal{V}$ such that $\langle y,\mathcal{A}(x)\rangle_\mathcal{W}=\langle \mathcal{A}^ *(y),x\rangle_\mathcal{V}$&lt;/p>
&lt;blockquote>
&lt;p>For its matrix representation, adjoint of $A$ is $A^ *$, which is $A^\top$ for real numbers.&lt;br>
Properties of $\mathcal{A}^ *$ is similar to matrix $A^ *$&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>$\mathcal{U}=\mathcal{R}(A)\overset{\perp}{\oplus}\mathcal{N}(A^ *),\;\mathcal{V}=\mathcal{R}(A^ *)\overset{\perp}{\oplus}\mathcal{N}(A)$&lt;/li>
&lt;li>$\mathcal{N}(A^* )=\mathcal{N}(AA^* )\subseteq\mathcal{U},\;\mathcal{R}(A)=\mathcal{R}(AA^*)\subseteq\mathcal{U}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Self-adjoint&lt;/strong>: $\mathcal{A}$ is self-adjoint iff $\mathcal{A}^*=\mathcal{A}$.&lt;/p>
&lt;ul>
&lt;li>For self-adjoint $\mathcal{A}$, if $\mathcal{V}=\mathbb{C}^{n\times n}$ then $A$ is &lt;strong>hermitian&lt;/strong>; if $\mathcal{V}=\mathbb{R}^{n\times n}$ then $A$ is &lt;strong>symmetric&lt;/strong>.&lt;/li>
&lt;li>Self-adjoint matrices have real eigenvalues and orthogonal eigenvectors&lt;/li>
&lt;li>&lt;strong>Skew symmetric&lt;/strong>: $A^*=-A$
&lt;blockquote>
&lt;p>For quadratic form $x^\top Ax=x^\top(\frac{A+A^\top}{2}+\frac{A-A^\top}{2})x$, since $A-A^\top$ is skew symmetric, scalar $x^\top (A-A^\top) x=-x^\top (A-A^\top)x$, so the skew-symmetric part is zero. Therefore for quadratic form $x^\top Ax$ we can always assume $A$ is symmetric.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Definiteness&lt;/strong>: (for symmetric matrix $P$)&lt;/p>
&lt;ul>
&lt;li>Positive definite ($P\succ 0$): $\forall x\in\mathbb{R}^n\neq 0,\; x^\top Px&amp;gt;0 \Leftrightarrow$ all eigenvalues of $P$ are positive.&lt;/li>
&lt;li>Semi-positive definite ($P\succcurlyeq 0$): $x^\top Px\geqslant 0 \Leftrightarrow$ all eigenvalues of $P$ are non-negative.&lt;/li>
&lt;li>Negative definite ($P\prec 0$): $x^\top Px &amp;lt; 0 \Leftrightarrow$ all eigenvalues of $P$ are negative.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Orthogonal Matrix&lt;/strong>: $Q$ is orthogonal iff $Q^\top Q=I$, iff columns of $Q$ are orthonormal.&lt;/p>
&lt;ul>
&lt;li>If $A\in\mathbb{R}^{n\times b}$ is symmetric, then $\exists$ orthogonal $Q$ s.t. $Q^\top AQ=\Lambda=\mathrm{diag}\{\lambda_1,\ldots,\lambda_n\}$ (see &lt;a class="link" href="#Eigendecomposition-and-Jordan-Form" >Eigen-decomposition&lt;/a> section below)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Orthogonal Projection&lt;/strong>: Given linear space $\mathcal{X}$ and subspace $\mathcal{M}$, $P(x)=x^*:\mathcal{X}\to\mathcal{M}$ ($x^ *$ is the projection point) is called orthogonal projection. If $\{v_i\}$ is a orthonormal basis of $\mathcal{M}$, then $P(x)=\sum_i \langle x,v_i\rangle v_i$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="eigenvalue-and-canonical-forms">Eigenvalue and Canonical Forms&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Eigenvalue&lt;/strong> and &lt;strong>Eigenvector&lt;/strong>: Given mapping $\mathcal{A}:\mathcal{V}\rightarrow\mathcal{V}$, if $\exists \lambda\in\mathcal{F}, v\neq \emptyset_{\mathcal{V}}\in\mathcal{V}$ s.t. $\mathcal{A}(v) = \lambda v$, then $\lambda$ is the &lt;strong>eigenvalue&lt;/strong>, $v$ is the &lt;strong>eigenvector&lt;/strong> (aka. &lt;strong>spectrum&lt;/strong>).
&lt;ul>
&lt;li>If eigenvalues are all distinct, then the associated eigenvectors form a basis.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Eigenspace&lt;/strong>: $\mathcal{N}_\lambda = \mathcal{N}(\mathcal{A}-\lambda \mathcal{I})$.
&lt;ul>
&lt;li>$q=dim(\mathcal{N}_\lambda)$ is called the &lt;strong>geometric multiplicity&lt;/strong> (几何重度)&lt;/li>
&lt;li>$\mathcal{N}_\lambda$ is an $\mathcal{A}$-invariant subspace.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Characteristic Polynomial&lt;/strong>: $\phi(s)\equiv\mathcal{det}(A-s I)$ is a polynomial of degree $n$ in $s$
&lt;ul>
&lt;li>Its solutions are the eigenvalues of $A$.&lt;/li>
&lt;li>The multiplicity $m_i$ of root term $(s-\lambda_i)$ here is called &lt;strong>algebraic multiplicity&lt;/strong> (代数重度) of $\lambda_i$.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Cayley-Hamilton Theorem&lt;/strong>: $\phi(A)=\mathbf{0}$
&lt;blockquote>
&lt;p>Proof needs the eigendecomposition or Jordan decomposition descibed below&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Minimal Polynomial&lt;/strong>: $\psi(s)$ is the minimal polynomial of $A$ iff $\psi(s)$ is the polynomial of least degree for which $\psi(A)=0$ and $\psi$ is monic (coefficient of highest order term is 1)
&lt;ul>
&lt;li>The multiplicity $\eta_i$ of root term $(s-\lambda_i)$ here is called the &lt;strong>index&lt;/strong> of $\lambda_i$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Eigendecomposition&lt;/strong> (aka. &lt;strong>Spectral Decomposition&lt;/strong>) is directly derived from the definition of eigenvalues: $$A=Q\Lambda Q^{-1}, \Lambda=\mathrm{diag}\left\{\lambda_1,\lambda_2,\ldots,\lambda_n\right\}$$
where $Q$ is a square matrix whose $i$-th column is the eigenvector $q_i$ corresponding to eigenvalue $\lambda_i$.
&lt;ul>
&lt;li>Feasibility: $A$ can be diagonalized (using eigendecomposition) iff. $q_i=m_i$ for all $\lambda_i$.&lt;/li>
&lt;li>If $A$ has $n$ distinct eigenvalues, then $A$ can be diagonalized.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Generalized eigenvector&lt;/strong>: A vector $v$ is a generalized eigenvector of rank $k$ associated with eigenvalue $\lambda$ iff $v\in\mathcal{N}\left((A-\lambda I)^k\right)$ but $v\notin\mathcal{N}\left((A-\lambda I)^{k-1}\right)$
&lt;ul>
&lt;li>If $v$ is a generalized eigenvector of rank $k$, $(A-\lambda I)v$ is a generalized eigenvector of rank $k-1$. This creates a chain of generalized eigenvectors (called &lt;strong>Jordan Chain&lt;/strong>) from rank $k$ to $1$, and they are linearly independent.&lt;/li>
&lt;li>$\eta$ (index, 幂零指数) of $\lambda$ is the smallest integer s.t. $dim\left(\mathcal{N}\left((A-\lambda I)^\eta\right)\right)$&lt;/li>
&lt;li>The space spanned by the chain of generalized eigenvectors from rank $\eta$ is called the &lt;strong>generalized eigenspace&lt;/strong> (with dimension $\eta$).&lt;/li>
&lt;li>Different generalized eigenspaces associated with the same and with different eigenvalues are orthogonal.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Jordan Decomposition&lt;/strong>: Similar to eigendecomposition, but works for all square matrices. $A=PJP^{-1}$ where $J=\mathrm{diag}\{J_1,J_2,\ldots,J_p\}$ is the &lt;strong>Jordan Form&lt;/strong> of A consisting of Jordan Blocks.
&lt;ul>
&lt;li>&lt;strong>Jordan Block&lt;/strong>: $J_i=\begin{bmatrix} \lambda &amp;amp; 1 &amp;amp;&amp;amp;&amp;amp; \\&amp;amp;\lambda&amp;amp;1&amp;amp;&amp;amp;\\&amp;amp;&amp;amp;\lambda&amp;amp;\ddots&amp;amp;\\&amp;amp;&amp;amp;&amp;amp;\ddots&amp;amp;1\\&amp;amp;&amp;amp;&amp;amp;&amp;amp;\lambda\end{bmatrix}$&lt;/li>
&lt;li>Each Jordan block corresponds to a generalized eigenspace&lt;/li>
&lt;li>$q_i$ = the count of Jordan blocks associated with $\lambda_i$&lt;/li>
&lt;li>$m_i$ = the count of $\lambda_i$ on diagonal of $J$&lt;/li>
&lt;li>$\eta_i$ = the dimension of the largest Jordan block associated with $\lambda_i$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>$\Lambda$ in eigendecomposition, $J$ in Jordan Form and $\Sigma$ in SVD (see below) are three kinds of &lt;strong>&lt;a class="link" href="https://en.wikipedia.org/wiki/Canonical_form#Linear_algebra" target="_blank" rel="noopener"
>Canonical Forms&lt;/a>&lt;/strong> of a matrix $A$&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Function of matrics&lt;/strong>: Let $f(\cdot)$ be an analytic function and $\lambda_i$ be an eigenvalue of $A$. If $p(\cdot)$ is a polynomial that satisfies $p(\lambda_i)=f(\lambda_i)$ and $\frac{\mathrm{d}^k}{\mathrm{d}s^k} p(\lambda_i)=\frac{\mathrm{d}^k}{\mathrm{d}s^k} f(\lambda_i)$ for $k=1,\ldots,\eta_i-1$, then $f(A)\equiv p(A)$.
&lt;blockquote>
&lt;ul>
&lt;li>This extends the functions applicable to matrics from polynomials (trivial) to any analytical functions&lt;/li>
&lt;li>By Cayley-Hamilton, we can always choose $p$ to be order $n-1$&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Sylvester&amp;rsquo;s Formula&lt;/strong>: $f(A)=\sum^k_{i=1}f(\lambda_i)A_i$ ($f$ being analytic)&lt;/li>
&lt;/ul>
&lt;h3 id="svd-and-linear-equations">SVD and Linear Equations&lt;/h3>
&lt;p>SVD Decomposition is useful in various fields and teached by a lot of courses, its complete version is formulated as $$A=U\Sigma V^*, \Sigma=\begin{bmatrix}\mathbf{\sigma}&amp;amp;\mathbf{0}\\ \mathbf{0}&amp;amp;\mathbf{0}\end{bmatrix}, \mathbf{\sigma}=\mathrm{diag}\left\{\sqrt{\lambda_1},\sqrt{\lambda_2},\ldots,\sqrt{\lambda_r}\right\},V=\begin{bmatrix}V_1&amp;amp;V_2\end{bmatrix},U=\begin{bmatrix}U_1&amp;amp;U_2\end{bmatrix}$$
where&lt;/p>
&lt;ul>
&lt;li>$r=\rho(A)$ is the rank of matrix $A$&lt;/li>
&lt;li>$\sigma_i$ are called &lt;strong>sigular values&lt;/strong>, $\lambda_i$ are eigenvalues of $A^* A$&lt;/li>
&lt;li>Columns of $V_1$ span $\mathcal{R}(A^ *A)=\mathcal{R}(A^ *)$, columns of $V_2$ span $\mathcal{N}(A^ *A)=\mathcal{N}(A)$&lt;/li>
&lt;li>Columns of $U_1=AV_1\sigma^{-1}$ span $\mathcal{R}(A)$, columns of $U_2$ span $\mathcal{N}(A^*)$&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>SVD can be derived by doing eigenvalue decomposition on $A^* A$&lt;/p>
&lt;/blockquote>
&lt;p>With SVD introduced, we can efficiently solve general linear equation $Ax=b$ as $x=x_r+x_n$ where $x_r\in\mathcal{R}(A^\top)$ and $x_n\in\mathcal{N}(A)$.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>$Ax=b$&lt;/th>
&lt;th>tall $A$ ($m&amp;gt;n$)&lt;/th>
&lt;th>fat $A$ ($m&amp;lt; n$)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Overdetermined, &lt;br> Least Squares, &lt;br> use Normal Equations&lt;/td>
&lt;td>Underdetermined, &lt;br> Quadratic Programming, &lt;br> use Lagrange Multiplies&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>I.$b\in\mathcal{R}(A)$&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1.$\mathcal{N}(A)={0}$&lt;/td>
&lt;td>$x$ exist &amp;amp; is unique&lt;/td>
&lt;td>$x=(A^\top A)^{-1}A^\top b=A^+b$&lt;/td>
&lt;td>$x=A^\top(AA^\top)^{-1}b=A^+b$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2.$\mathcal{N}(A)\neq{0}$&lt;/td>
&lt;td>$x$ exist &amp;amp; not unique&lt;/td>
&lt;td>$x_r=(A^\top A)^{-1}A^\top b=A^+b$&lt;/td>
&lt;td>$x_r=A^\top(AA^\top)^{-1}b=A^+b$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>II.$b\notin\mathcal{R}(A)$&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1.$\mathcal{N}(A)={0}$&lt;/td>
&lt;td>$x$ not exists, $x_r$ exist &amp;amp; is unique&lt;/td>
&lt;td>$x_r=(A^\top A)^{-1}A^\top b=A^+b$&lt;/td>
&lt;td>$x_r=A^\top(AA^\top)^{-1}b=A^+b$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2.$\mathcal{N}(A)\neq{0}$&lt;/td>
&lt;td>$x$ not exists, $x_r$ not exist&lt;/td>
&lt;td>$(A^\top A)^{-1}$ invertible&lt;/td>
&lt;td>$(AA^\top)^{-1}$ invertible&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>$A^+=(A^\top A)^{-1}A^\top$ is left pseudo-inverse, $A^+=A^\top (AA^\top)^{-1}$ is right pseudo-inverse.&lt;/li>
&lt;li>$A^+$ can be unified by the name &lt;strong>Moore-Penrose Inverse&lt;/strong> and calculated using SVD by $A^+=V\Sigma^+ U^\top$ where $\Sigma^+$ take inverse of non-zeros.&lt;/li>
&lt;/ul>
&lt;h3 id="miscellaneous">Miscellaneous&lt;/h3>
&lt;blockquote>
&lt;p>Selected theorems and lemmas useful in Linear Algebra. For more matrix properties see &lt;a class="link" href="https://zyxin.xyz/blog/en/2019-06/MatrixAlgebra/" >my post about Matrix Algebra&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>Matrix Square Root: $N^\top N=P$, then $N$ is the square root of $P$
&lt;blockquote>
&lt;p>Square root is not unique. Cholesky decomposition is often used as square root.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Schur Complement&lt;/strong>: Given matrices $A_{n\times n}, B_{n\times m}, C_{m\times m}$, the matrix $M=\begin{bmatrix}A&amp;amp;B\\ B^\top&amp;amp;C\end{bmatrix}$ is symmetric. Then the following are equivalent (TFAE)
&lt;ol>
&lt;li>$M\succ 0$&lt;/li>
&lt;li>$A\succ 0$ and $C-B^\top A^{-1}B\succ 0$ (LHS called Schur complement of $A$ in $M$)&lt;/li>
&lt;li>$C\succ 0$ and $A-B C^{-1}B^\top\succ 0$ (LHS called Schur complement of $C$ in $M$)&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Matrix Inverse Lemma: $(A+BCD)^{-1}=A^{-1}-A^{-1}B\left(C^{-1}+DA^{-1}B\right)^{-1}DA$&lt;/li>
&lt;li>Properties of $A^\top A$
&lt;ul>
&lt;li>$A^\top A \succeq 0$ and $A^\top A \succ 0 \Leftrightarrow A$ has full rank.&lt;/li>
&lt;li>$A^\top A$ and $AA^\top$ have same non-zero eigenvalues, but different eigenvectors.&lt;/li>
&lt;li>If $v$ is eigenvector of $A^\top A$ about $\lambda$, then $Av$ is eigenvector of $AA^\top$ about $\lambda$.&lt;/li>
&lt;li>If $v$ is eigenvector of $AA^\top$ about $\lambda$, then $A^\top v$ is eigenvector of $A^\top A$ about $\lambda$.&lt;/li>
&lt;li>$tr(A^\top A)=tr(AA^\top)=\sum_i\sum_j\left|A_{ij}\right|^2$&lt;/li>
&lt;li>$det(A)=\prod_i\lambda_i, tr(A)=\sum_i\lambda_i$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="real-analysis">Real Analysis&lt;/h2>
&lt;h3 id="set-theory">Set theory&lt;/h3>
&lt;blockquote>
&lt;p>$\text{~}S$ stands for complement of set $S$ in following contents. These concepts are discussed under normed space $(\mathcal{X}, \Vert\cdot\Vert)$&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Open Ball&lt;/strong>: Let $x_0\in\mathcal{X}$ and let $a\in\mathbb{R}, a&amp;gt;0$, then the open ball of radius $a$ about $x_0$ is $B_a(x_0)=\left\{x\in\mathcal{X}\middle| \Vert x-x_0\Vert &amp;lt; a\right\}$
&lt;ul>
&lt;li>Given subset $S\subset \mathcal{X}$, $d(x,S)=0\Leftrightarrow \forall\epsilon &amp;gt;0, B_\epsilon(x)\cap S\neq\emptyset$&lt;/li>
&lt;li>Given subset $S\subset \mathcal{X}$, $d(x,S)&amp;gt;0\Leftrightarrow \exists\epsilon &amp;gt;0, B_\epsilon(x)\cap S=\emptyset$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Interior Point&lt;/strong>: Given subset $S\subset\mathcal{X}$, $x\in S$ is an interior point of $S$ iff $\exists\epsilon &amp;gt;0, B_\epsilon(x)\subset S$
&lt;ul>
&lt;li>&lt;strong>Interior&lt;/strong>: $\mathring{S}=\{x\in \mathcal{X}|x\text{ is an interior point of }S\}=\{x\in\mathcal{X}|d(x,\text{~}S)&amp;gt;0\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Open Set&lt;/strong>: $S$ is open if $\mathring{S}=S$&lt;/li>
&lt;li>&lt;strong>Closure Point&lt;/strong>: Given subset $S\subset\mathcal{X}$, $x\in S$ is a closure point of $S$ iff $\forall\epsilon &amp;gt;0, B_\epsilon(x)\cap S\neq\emptyset$.
&lt;ul>
&lt;li>&lt;strong>Closure&lt;/strong>: $\bar{S}=\{x\in\mathcal{X}|x\text{ is a closure point of }S\}=\{x\in\mathcal{X}|d(x,S)=0\}$
&lt;blockquote>
&lt;p>Note that $\partial\mathcal{X}=\emptyset$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Closed Set&lt;/strong>: $S$ is closed if $\bar{S}=S$
&lt;blockquote>
&lt;p>$S$ is open $\Leftrightarrow$ $\text{~}S$ is closed, $S$ is closed $\Leftrightarrow$ $\text{~}S$ is open. Set being both open and closed is called &lt;strong>clopen&lt;/strong>(e.g. the whole set $\mathcal{X}$), empty set is clopen by convention.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Set Boundary&lt;/strong>: $\partial S=\bar{S}\cap\overline{\text{~}S}=\bar{S}\backslash\mathring{S}$&lt;/li>
&lt;/ul>
&lt;h3 id="sequences">Sequences&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Sequence&lt;/strong>($\{x_n\}$): a set of vectors indexed by the counting numbers
&lt;ul>
&lt;li>&lt;strong>Subsequence&lt;/strong>: Let $1\leqslant n_1&amp;lt; n_2&amp;lt;\ldots$ be an infinite set of increasing integers, then $\{x_{n_i}\}$ is a subsequence of $\{x_n\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Convergence&lt;/strong>($\{x_n\}\to x\in\mathcal{X}$): $\forall \epsilon&amp;gt;0,\exists N(\epsilon)&amp;lt;\infty\text{ s.t. }\forall n\geqslant N, \Vert x_n-x\Vert &amp;lt;\epsilon$
&lt;ul>
&lt;li>If $x_n \to x$ and $x_n \to y$, then $x=y$&lt;/li>
&lt;li>If $x_n \to x_0$ and $\{x_{n_i}\}$ is a subsequence of $\{x_n\}$, then $\{x_{n_i}\} \to x_0$&lt;/li>
&lt;li>&lt;strong>Cauchy Convergence&lt;/strong> (necessary condition for convergence): $\{x_n\}$ is cauchy if $\forall \epsilon&amp;gt;0,\exists N(\epsilon)&amp;lt;\infty$ s.t. $\forall n,m\geqslant N, \Vert x_n-x_m\Vert &amp;lt;\epsilon$&lt;/li>
&lt;li>If $\mathcal{X}$ is finite dimensional, $\{x_n\}$ is cauchy $\Rightarrow$ $\{x_n\}$ has a limit in $\mathcal{X}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Limit Point&lt;/strong>: Given subset $S\subset\mathcal{X}$, $x$ is a limit point of $S$ if $\exists \{x_n\}$ s.t. $\forall n\geqslant 1, x_n\in S$ and $x_n\to x$
&lt;ul>
&lt;li>$x$ is a limit point of $S$ iff $x\in\bar{S}$&lt;/li>
&lt;li>$S$ is closed iff $S$ contains its limit points&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Complete Space&lt;/strong>: a normed space is &lt;strong>complete&lt;/strong> if every Cauchy sequence has a limit. A complete normed space $(\mathcal{X}, \Vert\cdot\Vert)$ is called a &lt;strong>Banach space&lt;/strong>.
&lt;ul>
&lt;li>$S\subset \mathcal{X}$ is complete if every Cauchy sequence with elements from $S$ has a limit in $S$&lt;/li>
&lt;li>$S\subset \mathcal{X}$ is complete $\Rightarrow S$ is closed&lt;/li>
&lt;li>$\mathcal{X}$ is complete and $S\subset\mathcal{X} \Rightarrow S$ is complete&lt;/li>
&lt;li>All finite dimensional subspaces of $X$ are complete&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Completion of Normed Space&lt;/strong>: $\mathcal{Y}=\bar{\mathcal{X}}=\mathcal{X}+\{$all limit points of Cauchy sequences in $\mathcal{X}\}$
&lt;blockquote>
&lt;p>E.g. $C[a,b]$ contains continuous functions over $[a,b]$. $(C[a,b], \Vert\cdot\Vert_1)$ is not complete, $(C[a,b], \Vert\cdot\Vert_\infty)$ is complete. Completion of $(C[a,b], \Vert\cdot\Vert_1)$ requires Lebesque integration.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Contraction Mapping&lt;/strong>: Let $S\subset\mathcal{X}$ be a subset and $T:S\to S$ is a contraction mapping if $\exists 0\leqslant c\leqslant 1$ such that, $\forall x,y \in S, \Vert T(x)-T(y)\Vert\leqslant c\Vert x-y\Vert$
&lt;ul>
&lt;li>&lt;strong>Fixed Point&lt;/strong>: $x^* \in\mathcal{X}$ is a fixed point of $T$ if $T(x^ *)=x^ *$&lt;/li>
&lt;li>&lt;a class="link" href="https://en.wikipedia.org/wiki/Banach_fixed-point_theorem" target="_blank" rel="noopener"
>&lt;strong>Contraction Mapping Theorem&lt;/strong> (不动点定理)&lt;/a>: If $T:S\to S$ is a contraction mapping in a complete subset $S$, then $\exists! x^ *\in\mathcal{X}\text{ s.t. }T(x^ *)=x^ *$. Moreover, $\forall x_0\in S$, the sequence $x_{k+1}=T(x_k),k\geqslant 0$ is Cauchy and converges to $x^ *$.
&lt;blockquote>
&lt;p>E.g. Newton Method: $x_{k+1}=x_k-\epsilon\left[\frac{\partial h}{\partial x}(x_k)\right]^{-1}\left(h(x_k)-y\right)$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="continuity-and-compactness">Continuity and Compactness&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Continuous&lt;/strong>: Let $(\mathcal{X},\Vert\cdot\Vert_\mathcal{X})$ and $(\mathcal{Y},\Vert\cdot\Vert_\mathcal{Y})$ be two normed spaces. A function $f:\mathcal{X}\to\mathcal{Y}$ is continuous at $x_0\in\mathcal{X}$ if $\forall\epsilon &amp;gt;0,\exists \delta(\epsilon,x_0)&amp;gt;0\text{ s.t. }\Vert x-x_0\Vert_\mathcal{X}&amp;lt;\delta \Rightarrow\Vert f(x)-f(x_0)\Vert_\mathcal{Y} &amp;lt;\epsilon$
&lt;ul>
&lt;li>$f$ is continuous on $S\subset\mathcal{X}$ if $f$ is continuous at $\forall x_0\in S$&lt;/li>
&lt;li>If $f$ in continuous at $x_0$ and $\{x_n\}$ is a sequence s.t. $x_n\to x_0$, then the sequence $\{f(x_n)\}$ in $\mathcal{Y}$ converges to $f(x_0)$&lt;/li>
&lt;li>If $f$ is discontinuous at $x_0$, then $\exists \{x_n\}\in\mathcal{X}$ s.t. $x_n\to x_0$ but $f(x_n)\nrightarrow f(x_0)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Compact&lt;/strong>: $S\subset\mathcal{X}$ is (sequentially) compact if every sequence in $S$ has a convergent subsequence with limit in $S$&lt;/li>
&lt;li>&lt;strong>Bounded&lt;/strong>: $S\subset\mathcal{S}$ is bounded if $\exists r&amp;lt;\infty$ such that $S\subset B_r(0)$
&lt;ul>
&lt;li>$S$ is compact $\Rightarrow$ $S$ is closed and bounded&lt;/li>
&lt;li>&lt;strong>Bolzano-Weierstrass Theorem&lt;/strong>: In a finite-dimensional normed space, $C$ is closed and bounded $\Leftrightarrow$ for $C$ is compact&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Weierstrass Theorem&lt;/strong>: If $C\subset\mathcal{X}$ is a compact subset and $f:C\to\mathbb{R}$ is continuous at each point of $C$, then $f$ achieves its extreme values, i.e. $\exists \bar{x}\in C\text{ s.t. }f(\bar{x})=\sup_{x\in C} f(x)$ and $\exists \underline{x}\in C\text{ s.t. }f(\underline{x})=\inf_{x\in C} f(x)$
&lt;ul>
&lt;li>$f:C\to\mathbb{R}$ continuous and $C$ compact $\Rightarrow$ $\sup_{x\in C}f(x)&amp;lt;\infty$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Matrix Algebra</title><link>https://zyxin.xyz/blog/en/2019-06/MatrixAlgebra/</link><pubDate>Thu, 06 Jun 2019 00:00:00 +0000</pubDate><guid>https://zyxin.xyz/blog/en/2019-06/MatrixAlgebra/</guid><description>&lt;p>I encountered a lot of calculation of derivates and integration on matrices when I was learning linear systems and SLAM, but I haven&amp;rsquo;t learned much knowledge about them. Recently I know about the book &lt;em>Matrix Cookbook&lt;/em>, which thoroughly discussed the arithmetics of matrices. Therefore, I post it here for future references.&lt;/p>
&lt;h2 id="contents">Contents&lt;/h2>
&lt;ol>
&lt;li>基础内容&lt;/li>
&lt;li>求导&lt;/li>
&lt;li>逆&lt;/li>
&lt;li>复矩阵&lt;/li>
&lt;li>求解与分解&lt;/li>
&lt;li>统计与概率&lt;/li>
&lt;li>多元概率分布&lt;/li>
&lt;li>高斯&lt;/li>
&lt;li>特殊矩阵&lt;/li>
&lt;li>函数与运算符&lt;/li>
&lt;/ol>
&lt;h2 id="source">Source&lt;/h2>
&lt;p>The book can be downloaded from a website from &lt;a class="link" href="http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=3274" target="_blank" rel="noopener"
>Technical University of Denmark&lt;/a>. If the link failed, you can &lt;a class="link" href="https://zyxin.xyz/blog/blog/static/doc/matrixcookbook.pdf" >downloaded from here&lt;/a> directly.&lt;/p></description></item><item><title>Notes for Stochastic System</title><link>https://zyxin.xyz/blog/en/2019-03/StochasticSystemNotes/</link><pubDate>Tue, 26 Mar 2019 00:00:00 +0000</pubDate><guid>https://zyxin.xyz/blog/en/2019-03/StochasticSystemNotes/</guid><description>&lt;blockquote>
&lt;p>Prerequisites: Knowledge of Elementary Calculus, Linear Algebra and Probability&lt;/p>
&lt;/blockquote>
&lt;h2 id="discrete-time-stochastic-system">Discrete-Time Stochastic System&lt;/h2>
&lt;h3 id="stochastic-sequences">Stochastic Sequences&lt;/h3>
&lt;ul>
&lt;li>Definition: Given $k\in\mathbb{K}\subseteq\mathbb{Z}$ a sequence of integers, $\mathcal{X}(k,\omega): (\Omega,\mathcal{F},\mathbb{P})\to(\mathbb{R}^n,\mathcal{F}_ \mathcal{X},\mathbb{P}_ \mathcal{X})$ is a random/stochastic sequence.&lt;/li>
&lt;li>Uncertainties: Consider a casual system $F$ relates some scalar inputs $u(k)$ to output $x(k)$
&lt;ul>
&lt;li>&lt;strong>Epistemic/Model uncertainty&lt;/strong>: $\mathcal{X}(k,\omega)=F(k,u(k),u(k-1),\ldots,\omega)$. (system is stochastic and input is deterministic).&lt;/li>
&lt;li>&lt;strong>Aleatoric/Input uncertainty&lt;/strong>: $\mathcal{X}(k,\omega)=f(k,U(k,\omega),u(k-1,\omega),\ldots)$ (system is deterministic and input is stochastic).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Realization&lt;/strong>: An outcome $\mathcal{X}(k,\omega)=x(k)$ given $\omega$ is called a realization of stochastic sequence $\mathcal{X}$&lt;/li>
&lt;li>Terminology and Convention
&lt;ul>
&lt;li>$\mathcal{X}(k,\omega)$ is often written as $\mathcal{X}(k)$ when there&amp;rsquo;s no ambiguity.&lt;/li>
&lt;li>$\mathbb{K}=\mathbb{Z}$ if not specified.&lt;/li>
&lt;li>Sequence over a set $\mathcal{K}_1\subseteq\mathbb{K}$ are denoted $\mathcal{X}(\mathcal{K}_1)$.&lt;/li>
&lt;li>$\mathcal{X}$ denotes $\mathcal{X}(\mathbb{K})$ if not specified.&lt;/li>
&lt;li>Consecutive subsequence: $$\mathcal{X}(k:l)=\{\mathcal{X}(k),\mathcal{X}(k+1),\ldots,\mathcal{X}(l)\},\;x(k:l)=\{x(k),x(k+1),\ldots,x(l)\}$$&lt;/li>
&lt;li>Abbreviations:
&lt;ul>
&lt;li>&lt;em>&lt;strong>SS&lt;/strong>&lt;/em> - stochastic sequence&lt;/li>
&lt;li>&lt;em>&lt;strong>IID&lt;/strong>&lt;/em> - independent indentically distributed&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="probabilistic-characterization">Probabilistic characterization&lt;/h3>
&lt;ul>
&lt;li>Distribution and density: $$F_ \mathcal{X}\left(k:l;x(k:l)\right)\equiv\mathbb{P}((\mathcal{X}_ i(k)\leqslant x_i(k))\cap\cdots\cap(\mathcal{X}_ i(l)\leqslant x_ i(l)),\;i=1\ldots n)$$ $$f_ \mathcal{X}\left(k:l;x(k:l)\right)\equiv \frac{\partial^{n(l-k+1)}}{\partial x_ 1(k)\cdots\partial x_ n(l)}F_ \mathcal{X}(k:l;x(k:l))$$
Here $k:l$ actually denotes a set of consecutive integers, it can be also changed to ordinary sets $\{k,l\}$ or single scalar $k$.&lt;/li>
&lt;li>&lt;strong>Ensemble Average&lt;/strong>: $\mathbb{E}[\psi(\mathcal{X}(k))]$, doing summation over different realization at same time $k$
&lt;ul>
&lt;li>&lt;strong>Mean&lt;/strong>: $\mu_ \mathcal{X}(k)\equiv\mathbb{E}[\mathcal{X}(k)]=\int^\infty_{-\infty}x(k)f_ \mathcal{X}(k;x(k))\mathrm{d}x(k)$&lt;/li>
&lt;li>&lt;strong>Conditional Mean&lt;/strong>: $\mu_ \mathcal{X}(l|k)\equiv\mathbb{E}[\mathcal{X}(l)|\mathcal{X}(k)=x(k)]$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Time Average&lt;/strong>: $\frac{1}{2N+1}\sum^N_{k=-N}\psi(\mathcal{X}(k))$, doing summation over different time k of same realization&lt;/li>
&lt;li>&lt;strong>Autocorrelation&lt;/strong>:
&lt;ul>
&lt;li>Scalar case: $r_ \mathcal{X}(k,l)\equiv\mathbb{E}[\mathcal{X}(k)\mathcal{X}(l)]=\int^\infty_{-\infty}\int^\infty_{-\infty}x(k)x(l)f_ \mathcal{X}(k,l;x(k,l))\mathrm{d}x(k)\mathrm{d}x(l)$&lt;/li>
&lt;li>Vector case: $R_ \mathcal{X}(k,l)\equiv\mathbb{E}[\mathcal{X}(k)\mathcal{X}^\top(l)]$&lt;/li>
&lt;li>Conditional autocorrelation: $R_ \mathcal{X}(k,l|q)\equiv\mathbb{E}[\mathcal{X}(k)\mathcal{X}^\top(l)|\mathcal{X}(q)=x(q)]$&lt;/li>
&lt;li>Often we denote $C_ \mathcal{X}(k)=R_ \mathcal{X}(k,k)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Autocovariance&lt;/strong>:
&lt;ul>
&lt;li>Scalar case: $\kappa_ \mathcal{X}(k,l)\equiv\mathbb{E}[(\mathcal{X}(k)-\mu_ \mathcal{X}(k))(\mathcal{X}(l)-\mu_ \mathcal{X}(l))]$&lt;/li>
&lt;li>Vector case: $\mathrm{K}_ \mathcal{X}(k,l)\equiv\mathbb{E}[(\mathcal{X}(k)-\mu_ \mathcal{X}(k))(\mathcal{X}(l)-\mu_ \mathcal{X}(l))^\top]$&lt;/li>
&lt;li>Conditional autocovariance: $\mathrm{K}_ \mathcal{X}(k,l|q)\equiv\mathbb{E}[(\mathcal{X}(k)-\mu_ \mathcal{X}(k|q))(\mathcal{X}(l)-\mu_ \mathcal{X}(l|q))^\top]$&lt;/li>
&lt;li>Often we denote $S_ \mathcal{X}(k|q)=\mathrm{K}_ \mathcal{X}(k,k|q)$&lt;/li>
&lt;li>Useful conclusion: $\mathrm{K}(a,b)=\mathrm{K}(b,a)^T$&lt;/li>
&lt;li>Normalized (&lt;strong>autocorrelation coefficient&lt;/strong>): $\rho_ \mathcal{X}(k,l)\equiv\mathrm{K}_ \mathcal{X}(k,l)/\sigma^2_{\mathcal{X}(k)}\sigma^2_{\mathcal{X}(l)}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Strong Stationarity&lt;/strong>(aka. strict sense): (necessarily identically distributed over time) $$\forall x(k:l)\in\mathbb{R}^{n(l-k+1)},\;\forall s\in\mathbb{Z},\;f_ \mathcal{X}(k:l;x(k:l))=f_ \mathcal{X}(k+s:l+s;x(k:l))$$&lt;/li>
&lt;li>&lt;strong>Weak Stationarity&lt;/strong>(aka. wide sense): $\forall k,l$ if $$\mu_ \mathcal{X}(k)=\mu_ \mathcal{X}(l)\;\text{and}\;\mathrm{K}_ \mathcal{X}(k,l)=\mathrm{K}_ \mathcal{X}(k+s,l+s)\equiv\bar{\mathrm{K}}_ \mathcal{X}(s)$$ Weak stationarity is necessary condition for stationarity. (Equal when Gaussian distributed)&lt;/li>
&lt;li>&lt;strong>Ergodicity&lt;/strong>: $\mathcal{X}$ is called ergodic in $\psi$ if
&lt;ol>
&lt;li>$\mathbb{E}[\psi(\mathcal{X})]$ is stationary&lt;/li>
&lt;li>Ensemble average is equal to Time average, that is $$ \frac{1}{2N+1}\sum^N_{k=-N}\psi(\mathcal{X}(k))\to\mathbb{E}[\psi(\mathcal{X})];\text{as};l\to \infty$$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;h3 id="markov-sequence">Markov Sequence&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Markov Sequence&lt;/strong>: A ss. $\mathcal{X}(k)$ is called a (discrete-time) Markov sequence if $$\begin{split}f_ \mathcal{X}\left(k;x(k)\middle|\mathcal{X}(k-1)=x(k-1),\mathcal{X}(k-2)=x(k-2),\ldots\right) \\=f_ \mathcal{X}(k;x(k)|\mathcal{X}(k-1)=x(k-1))\end{split}$$
&lt;ul>
&lt;li>We often make some assumption on the initial condition $\mathcal{X}(0)$, such as known, deteministic or uniformly distributed within certain domain.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Markov Chains&lt;/strong>: Markov sequence with discrete set of values(states) ${x_1\ldots x_m}$&lt;/li>
&lt;li>&lt;strong>Hidden Markov model&lt;/strong>: Sequence $\mathcal{Y}$ is called a Hidden Markov Model if it&amp;rsquo;s modeled by a system of the form $$\begin{align}\mathcal{X}(k+1)&amp;amp;=g(k,\mathcal{X}(k),\mathcal{W}(k)) \\ \mathcal{Y}(k)&amp;amp;=h(k,\mathcal{X}(k),\mathcal{W}(k))\end{align}$$
We also say that $\mathcal{Y}$ has a &lt;strong>(discrete-time) stochastic state space&lt;/strong>.&lt;/li>
&lt;li>&lt;strong>Guassian-Markov Sequence&lt;/strong> (GMS): $\mathcal{X}(k+1)=g(k,\mathcal{X}(k),\mathcal{W}(k))$ where $\mathcal{W}(k)$ is iid. Guassian&lt;/li>
&lt;/ul>
&lt;h3 id="linear-stochastic-sequence">Linear Stochastic Sequence&lt;/h3>
&lt;p>$$\begin{align}\mathcal{X}(k+1)&amp;amp;=A(k)\mathcal{X}(k)+B(k)\mathcal{W}(k) \\ \mathcal{Y}(k)&amp;amp;=C(k)\mathcal{X}(k)+D(k)\mathcal{W}(k)\end{align}$$&lt;/p>
&lt;blockquote>
&lt;p>For linear Markov sequences, the deterministic mean sequence and centered uncertain sequence completely decouple. So we often assume that $\mathcal{X}(k)$ and $\mathcal{Y}(k)$ are &lt;strong>centered&lt;/strong> in this case, with regard to deterministic inputs. The equation with deterministic inputs is often written as $$\mathcal{X}(k+1)=A(k)\mathcal{X}(k)+B_u(k)u(k)+B_\mathcal{W}(k)\mathcal{W}(k)$$&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>
&lt;p>Recursive-form expectations:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Mean&lt;/strong>: $\mu_ \mathcal{X}(k+1|q)=A(k)\mu_ \mathcal{X}(k|q)+B(k)\mu_\mathcal{W}(k)$&lt;/li>
&lt;li>&lt;strong>Covariance (Discrete-time algebraic Lyapunov/Stein difference equation)&lt;/strong>: $$S_ \mathcal{X}(k+1|q)=A(k)S_ \mathcal{X}(k|q)A^\top(k)+B(k)S_\mathcal{W}(k)B^\top(k)$$
&lt;blockquote>
&lt;p>Can be solved with &lt;code>dlyap&lt;/code> in MATLAB&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Convergence when $k\to\infty$:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Mean convergence&lt;/strong>: $\mu_ \mathcal{X}(k|q)$ converges requires $\max_i|\lambda_i(A)|&amp;lt;1$ ($\lambda$ denotes eigenvalue)&lt;/li>
&lt;li>&lt;strong>Covariance convergence&lt;/strong>: $S_ \mathcal{X}(k|q)$ converges requires $\max_i|\lambda_i(A)|&amp;lt;1$&lt;/li>
&lt;li>&lt;strong>(Discrete-time) Lyapunov equation&lt;/strong> (Stein equation): $\bar{S}_ \mathcal{X}=A\bar{S}_ \mathcal{X}A^\top+B\bar{S}_\mathcal{W}(k)B^\top$. Solution for this equation exists iff. $A$ is asymptotically stable (characterizing sequence is stationary).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Explicit state transition: By recursive substitution, $$\mathcal{X}(k)=\Psi(k,q)\mathcal{X}(q)+\sum^{k-1}_ {i=q}\Gamma(k,i)\mathcal{W}(i)$$ where state transition matrix $\Psi(k,q)=\begin{cases}I, &amp;amp;k=q \\ \prod^{k-1}_ {i=q}A(i),&amp;amp; k&amp;gt;q\end{cases}$ and $\Gamma(k,i)=\Psi(k,i+1)B(i)$.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Conditioned Mean sequence&lt;/strong>: $\mu_ \mathcal{X}(k|q)=\Psi(k,q)\mu_ \mathcal{X}(q)+\sum^{k-1}_ {i=q}\Gamma(k,i)\mu_\mathcal{W}(i)$&lt;/li>
&lt;li>&lt;strong>Conditioned Autocovariance Matrix&lt;/strong>: $$\mathrm{K}_ \mathcal{X}(k,l|q)=\Psi(k,q)S_ \mathcal{X}(q)\Psi^\top(l,q)+\sum^{min\{k,l\}-1}_ {i=q}\Gamma(k,i)S_\mathcal{W}(i)\Gamma^\top(l,i)$$
&lt;ul>
&lt;li>A special case: $S_ \mathcal{X}(k|q)=\mathrm{K}_ \mathcal{X}(k,k|q)=\sum^{k-1}_ {i=q}\Gamma(k,i)S_\mathcal{W}(i)\Gamma^\top(k,i)$, $S_ \mathcal{X}(k|k)=0$&lt;/li>
&lt;li>Useful equation (stationary centered case): $$\mathrm{K}_ \mathcal{X}(k,l)=\begin{cases} S_ \mathcal{X}(k)\cdot(A^\top)^{(l-k)}&amp;amp;,l&amp;gt;k\\ S_ \mathcal{X}(k)&amp;amp;,l=k\\ A^{(k-l)}S_ \mathcal{X}(k)&amp;amp;,l&amp;lt; k\end{cases}$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Conditional Autocorrelation Matrix&lt;/strong>: $R_ \mathcal{X}(k,l|q)=\mathrm{K}_ \mathcal{X}(k,l|q)+\mu_ \mathcal{X}(k|q)\mu_ \mathcal{X}^\top(l|q)$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Observation $\mathcal{Y}$ property:&lt;/p>
&lt;ul>
&lt;li>Mean: $\mu_ \mathcal{Y}(k|q)=C(k)\mu_ \mathcal{X}(k|q)+D(k)\mu_\mathcal{W}(k)$&lt;/li>
&lt;li>Covariance: $$\mathrm{K}_ \mathcal{Y}(k,l|q)=\begin{cases}C(k)\mathrm{K}_ \mathcal{X}(k,l|q)C^\top(l)+C(k)\Gamma(k,l)S_W(l)D^\top(l)&amp;amp;:k&amp;gt;l \\C(k)S_ \mathcal{X}C^\top(k)+D(k)S_\mathcal{W}(k)D^\top(k)&amp;amp;:k=l\\ C(k)\mathrm{K}_ \mathcal{X}(k,l|q)C^\top(l)+D(k)S_\mathcal{W}(k)\Gamma^\top(l,k)C^\top(l)&amp;amp;:k&amp;lt; l\end{cases}$$&lt;/li>
&lt;li>Stationary time-invariant covariance: $$\mathrm{K}_ \mathcal{Y}(s)=\begin{cases}CA^{|s|}\bar{S}_ \mathcal{X}C^\top+CA^{|s|-1}B\bar{S}_ WD^\top&amp;amp;:s\neq 0 \\C\bar{S}_ \mathcal{X}C^\top+D\bar{S}_ WD^\top&amp;amp;:s=0\end{cases}$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="gaussian-stochastic-sequence">Gaussian Stochastic Sequence&lt;/h3>
&lt;ul>
&lt;li>Jointly Gaussian $\Rightarrow, \nLeftarrow$ Marginally Gaussian&lt;/li>
&lt;li>$c^\top X$ is Gaussian $\Leftrightarrow X$ is Gaussian&lt;/li>
&lt;li>Conditional Gaussian: if $X$ and $Y$ are Gaussian, then $X|Y \sim \mathcal{N}(\mu_{X|Y},S_{X|Y})$ where $\mu_{X|Y}=\mu_X+S_{XY}S_Y^{-1}(Y-\mu_Y)$, $S_{X|Y}=S_X-S_{XY}S_Y^{-1}S_{YX}$&lt;/li>
&lt;li>A linear controllable GMS $\mathcal{X}(k)$ is stationary iff. $A(k)=A, B(k)=B$ (time-invariant) and $A$ is asymptotically stable.&lt;/li>
&lt;li>All LTI stationary GMS are also ergodic in all finite momoents&lt;/li>
&lt;li>Solve $\mathcal{X}(k+1)=A\mathcal{X}(k)+B\mathcal{W}(k)$ when $\mathcal{X}$ is stationary ($\max_i|\lambda_i(A)|&amp;lt;1$)
&lt;ol>
&lt;li>solve $\bar{\mu}_ \mathcal{X}=A\bar{\mu}_ \mathcal{X}+B\bar{\mu}_\mathcal{W}$ for $\bar{\mu}$&lt;/li>
&lt;li>solve $\bar{S}_ \mathcal{X}=A\bar{S}_ \mathcal{X}A^\top+B\bar{S}_ \mathcal{W}B^\top$ for $\bar{S}_ \mathcal{X}$&lt;/li>
&lt;li>calculate $\Sigma_ \mathcal{X}(k:l|q)=\begin{bmatrix}\mathrm{K}_ \mathcal{X}(k,k|q)&amp;amp;\cdots&amp;amp;\mathrm{K}_ \mathcal{X}(k,l|q)\\ \vdots&amp;amp;\ddots&amp;amp;\vdots\\ \mathrm{K}_ \mathcal{X}(l,k|q)&amp;amp;\cdots&amp;amp;\mathrm{K}_ \mathcal{X}(l,l|q)\end{bmatrix}$ using $\bar{S}_ \mathcal{X}$&lt;/li>
&lt;li>Then $f_ \mathcal{X}(k:l;x(k:l))$ is determined with $\mu_ \mathcal{X}(k:l)=\{\bar{\mu}_ \mathcal{X},\bar{\mu}_ \mathcal{X}\ldots\bar{\mu}_ \mathcal{X}\}$ and $\Sigma_ \mathcal{X}(k:l)$ above.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;h3 id="observation--filtering">Observation &amp;amp; Filtering&lt;/h3>
&lt;blockquote>
&lt;p>For deterministic version, please check &lt;a href="{% post_path ControlSystemNotes %}#State-Estimation-Observer-Design">my notes for control system&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>(LTI) &lt;strong>Luenberger observer&lt;/strong>: $$\begin{align}\hat{\mathcal{X}}(k+1)&amp;amp;=A\hat{\mathcal{X}}(k)+L(\hat{\mathcal{Y}}(k)-\mathcal{Y}(k))+B\bar{\mu}_ \mathcal{W} \\ \hat{\mathcal{Y}}(k)&amp;amp;=C\hat{\mathcal{X}}(k)+D\bar{\mu}_\mathcal{W}\end{align}$$ where $L$ is the observer gain.
&lt;ul>
&lt;li>Note: We often assume that the process &amp;amp; measurement noise are decoupled and independent&lt;/li>
&lt;li>Combined form: $\hat{\mathcal{X}}(k+1)=[A+LC]\hat{\mathcal{X}}(k)-L\mathcal{Y}(k)+B\mu_\mathcal{W}(k)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>State estimation residual&lt;/strong> $r(k)=\mathcal{X}(k)-\hat{\mathcal{X}}(k)$
&lt;ul>
&lt;li>Combined form: $r(k+1)=[A+LC]r(k)+[B+LD]\tilde{\mathcal{W}}(k)$&lt;/li>
&lt;li>Stationary covariance can be solved by a Lyapunov equation $$\bar{S}_ r=[A+LC]\bar{S}_ r[A+LC]^T+[LD+B]\bar{S}_ \mathcal{W}[LD+B]^T$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>A common objective: minimize $\bar{S}_r=\mathbb{E}(rr^\top)$
&lt;ul>
&lt;li>Solutions: $L=-A\bar{S}_ rC^\top[C\bar{S}_ rC^\top+D\bar{S}_ \mathcal{W}D^\top]^{-1}$ (&lt;strong>Kalman observer gain&lt;/strong>)&lt;/li>
&lt;li>Or solve &lt;strong>(Discrete-time) Algebraic Riccati equation&lt;/strong> $$\bar{S}_ r=A\bar{S}_ rA^\top+B\bar{S}_ \mathcal{W}B^\top-A\bar{S}_ rC^\top[C\bar{S}_ rC^\top+D\bar{S}_ \mathcal{W}D^\top]^{-1}C\bar{S}_ rA^\top$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Innovation sequence&lt;/strong> $e(k)=\hat{\mathcal{Y}}(k)-\mathcal{Y}(k)$ with $L$ is optimal
&lt;ul>
&lt;li>We can find that $\mu_e=0$ and $\mathrm{K}_e(k+s,k)=\begin{cases}C\bar{S}_rC^\top+D\bar{S}_WD^T&amp;amp;:s=0 \\0&amp;amp;:s\neq0\end{cases}$. So the innovation sequence is iid. (only in Kalman observer)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>(Output) Probabilistically-equivalent model&lt;/strong>: $$\begin{align}\mathcal{X}(k+1)&amp;amp;=A\mathcal{X}(k)+Le(k) \\ \mathcal{Y}(k)&amp;amp;=C\mathcal{X}(k)-e(k)\end{align}$$&lt;/li>
&lt;/ul>
&lt;h2 id="markov-chains">Markov Chains&lt;/h2>
&lt;blockquote>
&lt;p>Content in this section comes from EECS 501
In this specific field, we often use stand-alone analysis methods.&lt;/p>
&lt;/blockquote>
&lt;h3 id="basic-definitions">Basic definitions&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>State distribution&lt;/strong>: We denote row vector $\pi_t$ as $\pi_t(x)=\mathbb{P}(\mathcal{X}_t=x),\; x\in S$ ($S$ is the set of states). Directly we have $\sum_x\pi(x)=1$&lt;/li>
&lt;li>&lt;strong>Time homogeneous&lt;/strong>: $\mathbb{P}(\mathcal{X}_ {t+1}=y|\mathcal{X}_ t=x)=\mathbb{P}(\mathcal{X}_{s+1}=y|\mathcal{X}_s=x)\;\forall s,t$&lt;/li>
&lt;li>&lt;strong>One-step transition probability matrix&lt;/strong>: $P_t=[P_{xy,t}]$ where $P_{xy,t}=\mathbb{P}(\mathcal{X}_ {t+1}=y|\mathcal{X}_ t=x)$.
&lt;ul>
&lt;li>Time-homo case: $P=[P_{xy}]$ where $P_{xy}=p(y|x)$&lt;/li>
&lt;li>Rows of the matrix sum up to 1.&lt;/li>
&lt;li>This matrix is also called &lt;strong>stochastic matrix&lt;/strong>.&lt;/li>
&lt;li>Extend this matrix to continuous states, then we use &lt;strong>transition kernel&lt;/strong> $T(x,y)$ to describe the transition probability,&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>m-step transition probability matrix&lt;/strong>: $P_{xy,t}^{(m)}=\mathbb{P}(\mathcal{X}_ {t+m}=y|\mathcal{X}_ t=x)$
&lt;ul>
&lt;li>&lt;strong>Chapman-Kolmogorov Equation&lt;/strong>: $P^{(n+m)}_t=P^{(n)}_t P^{(m)}_t$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="state-variables">State Variables&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Hitting time&lt;/strong>: $T_1(y)=\min\{n\geq 0:\mathcal{X}_ n=y\},\;T_k(y)=\min\{n&amp;gt;T_{k-1}(y):\mathcal{X}_ n=y\}$ where $n\in\mathbb{N}$
&lt;ul>
&lt;li>&lt;strong>Period&lt;/strong>: For state $i\in x$, its period is the greatest common divisor of $\{n&amp;gt;1|T_n(i)&amp;gt;0\}$. A Markov Chain is &lt;strong>aperiodic&lt;/strong> If all states have period 1.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Return probability&lt;/strong>: $f_{xy}=\mathbb{P}(T_1(y)&amp;lt;\infty|\mathcal{X}_0=x)$&lt;/li>
&lt;li>&lt;strong>Occupation time&lt;/strong>: $V(y)=\sum^\infty_{n=1}\unicode{x1D7D9}_{\mathcal{X_n}}(y)$&lt;/li>
&lt;li>Some properties
&lt;ul>
&lt;li>$f_{xy}=\mathbb{P}(V(y)\geqslant 1|\mathcal{X}_0=x)$&lt;/li>
&lt;li>$\mathbb{P}(V(y)=m|\mathcal{X}_ 0=x)=\begin{cases} 1-f_{xy}&amp;amp;:m=0\\f_{xy}f_{yy}^{m-1}(1-f_{yy})&amp;amp;:m\geqslant 1\end{cases}$&lt;/li>
&lt;li>$\mathbb{E}[ V(y)|\mathcal{X}_ 0=x]=\begin{cases} 0&amp;amp;:f_{xy}=0\\\infty&amp;amp;:f_{xy}&amp;gt;0,\ f_{yy}=1\\f_{xy}/(1-f_{yy})&amp;amp;:f_{xy}&amp;gt;0,\ f_{yy}&amp;lt;1\end{cases}$&lt;/li>
&lt;li>$\mathbb{E}[ V(y)|\mathcal{X}_ 0=x]=\sum^\infty_{n=1}P^{(n)}_{xy}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="state-classification">State Classification&lt;/h3>
&lt;blockquote>
&lt;p>Here we usually consider only time-homogeneous Markov Chains&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Accessible&lt;/strong> ($x\to y$): $\exists n\;\text{s.t.}\ P_{xy}^{(n)}&amp;gt;0$
&lt;ul>
&lt;li>$x\to y \Leftrightarrow f_{xy}&amp;gt;0$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Communicate&lt;/strong> ($x\leftrightarrow y$): $x\to y\;\text{and}\;y\to x$. This is a &lt;a class="link" href="https://en.wikipedia.org/wiki/Equivalence_relation" target="_blank" rel="noopener"
>equivalence relation&lt;/a>.&lt;/li>
&lt;li>&lt;strong>Equivalent class&lt;/strong>: set of states that communicate with each other&lt;/li>
&lt;li>&lt;strong>Irreducible&lt;/strong>: a Markov chain with only one communicating class&lt;/li>
&lt;li>&lt;strong>Absorbing/Closed state&lt;/strong>: $P_{xx}=1$
&lt;ul>
&lt;li>&lt;strong>Absorbing class&lt;/strong>: set $C$ is absorbing iff $\forall x \in C, \sum_{y\in C}P_{xy}=1$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Transient state&lt;/strong>: $f_{xx}&amp;lt;1 \Leftrightarrow \mathbb{E}[V_i|\mathcal{X}_0=i]&amp;lt;\infty$&lt;/li>
&lt;li>&lt;strong>Recurrent state&lt;/strong>: $f_{xx}=1 \Leftrightarrow \mathbb{E}[V_i|\mathcal{X}_0=i]=\infty$
&lt;ul>
&lt;li>&lt;strong>Positive recurrent&lt;/strong>: $\mathbb{E}[T_1(x)|\mathcal{X}_0=x]&amp;lt;0$&lt;/li>
&lt;li>&lt;strong>Null recurrent&lt;/strong>: $\mathbb{E}[T_1(x)|\mathcal{X}_0=x]=0$&lt;/li>
&lt;li>These two kind of recurrent states also make up communicating classes&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Some properties
&lt;ul>
&lt;li>If $x$ is (positive) recurrent and $x\to y$, then $y$ is also (positive) recurrent and $f_{xy}=f_{yx}=1$&lt;/li>
&lt;li>Every closed and finite subset of $X$ contains at least one (positive) recurrent state.&lt;/li>
&lt;li>All states of a communicating class are either positive recurrent, null recurrent or transient.&lt;/li>
&lt;li>Method to determine whether class $C$ is recurrent/transient
&lt;ol>
&lt;li>If $C$ is non-closed, it&amp;rsquo;s transient&lt;/li>
&lt;li>If $C$ is closed and finite, then $C$ is positive recurrent&lt;/li>
&lt;li>If $C$ is closed and infinite, then $C$ can be either positive/null recurrent or transient&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>A typical example is the &lt;a class="link" href="https://en.wikipedia.org/wiki/Birth%E2%80%93death_process" target="_blank" rel="noopener"
>birth-death chain&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Stationary distribution&lt;/strong>: $\bar{\pi}\;\text{s.t.}\;\bar{\pi}=\bar{\pi} P$
&lt;ul>
&lt;li>Stationary in limit form: $\bar{\pi}=\lim_{n\to \infty} \frac{1}{n}\sum^{n-1}_{t=0} \pi_t$
&lt;ul>
&lt;li>This is a &lt;strong>Cesaro&lt;/strong> limit, we use this to deal with the problem that $\lim_{n\to \infty} \pi_t$ might not exist if the chain is periodic.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Reversibility criterion for stationary: $\forall i,j \in S, \pi_i P_{ij} = \pi_j P_{ji}$ (a.k.a detailed balance condition), then the process is reversible and therefore stationary.&lt;/li>
&lt;li>Existence for stationary distribution satisfying $\bar{\pi}=\bar{\pi} P$
&lt;ol>
&lt;li>If the chain has single positive recurrent class, then exists unique solution: $\bar{\pi}(x)=0$ for all transient or null recurrent $x$&lt;/li>
&lt;li>If the chain has multiple positive recurrent class, then there are multiple solutions, for each positive recurrent $x$ we have a $\bar{\pi}^i$.&lt;/li>
&lt;li>If the chain has only transient and null recurrent states, there is no solution$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Convergence of stationary distribution
&lt;ul>
&lt;li>If the chain has positive recurrent class, then $\frac{1}{n}\sum^n_{t=1}\mathbb{P}(\mathcal{X}_t=j)\xrightarrow[n\to \infty]{}\bar{\pi}_j,\;\forall \mu_0$&lt;/li>
&lt;li>(Ergodic) If the chain is positive recurrent, then $\frac{1}{n}\sum^n_{t=1}\unicode{x1D7D9}_{\mathcal{X}_t}(j)\xrightarrow[n\to \infty]{\text{a.s.}}\bar{\pi}_j$&lt;/li>
&lt;li>If the chain is positive recurrent and aperiodic, then $\mathbb{P}(\mathcal{X}_n=j)\xrightarrow[n\to \infty]{}\bar{\pi}_j$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="continuous-time-stochastic-system">Continuous-Time Stochastic System&lt;/h2>
&lt;h3 id="stochastic-sequences-1">Stochastic Sequences&lt;/h3>
&lt;ul>
&lt;li>Definition: Given $t\in\mathbb{T}\subset\mathbb{R}$ a sequence of time, $\mathcal{X}(t,\omega): (\Omega,\mathcal{F},\mathbb{P})\to(\mathbb{R}^n,\mathcal{F}_ \mathcal{X},\mathbb{P}_ \mathcal{X})$ is a continuous stochastic sequence.&lt;/li>
&lt;li>Most definitions are similar to discrete sequence (including &lt;strong>stationarity&lt;/strong>), while the sequence is often defined as $\mathcal{X}(\mathcal{G}),\;\mathcal{G}={t_1,t_2,\ldots,t_N}\subset\mathbb{T}, t_i&amp;lt; t_{i+1}$&lt;/li>
&lt;li>&lt;strong>Stochastic Differential Equation (SDE)&lt;/strong>: $$\mathrm{d}\mathcal{X}(t)=F(\mathcal{X}(t),t)\mathrm{d}t+\sum^r_{i=1}G_i(\mathcal{X}(t),t)\mathrm{d}\mathcal{W}_i(t)$$ Here $\mathcal{W}$ is often a certain kind of &lt;em>noise&lt;/em> random process.&lt;/li>
&lt;/ul>
&lt;h3 id="markov-sequence-1">Markov Sequence&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Markov Sequence&lt;/strong>: A ss. $\mathcal{X}(k)$ is called a (continuous-time) Markov sequence if for the set of times $\mathcal{G}={t_1,t_2,\ldots,t_N}$ with $t_i &amp;lt; t_{i+1}$, we have $$\begin{split}f_ \mathcal{X}\left(t_N;x(t_N)\middle|\mathcal{X}(t_{N-1})=x(t_{N-1}),\mathcal{X}(t_{N-2})=x(t_{N-2}),\ldots\right)\qquad \\ =f_ \mathcal{X}(t_N;x(t_N)|\mathcal{X}(t_{N-1})=x(t_{N-1}))\end{split}$$&lt;/li>
&lt;li>&lt;strong>Hidden Markov Model&lt;/strong>: Definition similar to discrete case. A &lt;strong>continuous-time stochastic state space&lt;/strong> is of the form $$\begin{align}\dot{\mathcal{X}}(t)&amp;amp;=F(\mathcal{X},t)+G(\mathcal{X},t)\mathrm{d}\mathcal{W}(t)/\mathrm{d}t \\ \mathcal{Y}(t)&amp;amp;=H(\mathcal{X},t)+J(\mathcal{X},t)\mathrm{d}\mathcal{W}(t)/\mathrm{d}t\end{align}$$ where $\mathcal{W}$ is usually &lt;a class="link" href="#Wiener-Process" >Wiener process&lt;/a> and white noise $\mathrm{d}\mathcal{W}(t)/\mathrm{d}t$ is often written as $\mathcal{U}(t)$.
&lt;ul>
&lt;li>The state space is &lt;strong>affine&lt;/strong> if $F(\mathcal{X}(t),t)=A(t)\mathcal{X}(t)+u(t),\;N(\mathcal{X},t)=C(t)\mathcal{X}(t)+v(t)$&lt;/li>
&lt;li>The state space is &lt;strong>bilinear&lt;/strong> if $G(\mathcal{X},t)\mathrm{d}\mathcal{W}(t)=\sum^r_{i=1}B_i(t)\mathcal{X}\mathrm{d}\mathcal{W}_i(t)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="poisson-counters">Poisson Counters&lt;/h3>
&lt;ul>
&lt;li>Definition: It&amp;rsquo;s a stochastic Markow process $\mathcal{N}(t)\in\{0,1,2,\ldots\}$ with the characteristic that it jumps up by only one integer at a time.
&lt;ul>
&lt;li>Characteristics: transition times are random, transition step is always 1.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Transition rule: $\frac{\partial}{\partial t}p_\mathcal{N}=\begin{cases}-\lambda p_\mathcal{N}(t;n)&amp;amp;:n=0\\-\lambda p_\mathcal{N}(t;n)+\lambda p_\mathcal{N}(t;n-1)&amp;amp;:n&amp;gt;0\end{cases}$
&lt;ul>
&lt;li>From the rule we can conclude that $p_\mathcal{N}(t;n)=\frac{1}{n!}(\lambda t)^n e^{-\lambda t}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Bidirectional counter: $\mathcal{N}(t)$ is defined as one-directional. $\mathcal{N}_1(t)-\mathcal{N}_2(t)$ is called &lt;strong>bidirectional poisson counter&lt;/strong>.&lt;/li>
&lt;li>Expectation: $\mathbb{E}[\mathcal{N}(t)]=\lambda t,\;\mathbb{E}[\mathrm{d}\mathcal{N}(t)]=\lambda\mathrm{d}t$&lt;/li>
&lt;li>&lt;strong>Ito calculus&lt;/strong> for Poisson counter:
&lt;ul>
&lt;li>&lt;strong>Ito sense&lt;/strong>: $n(t)$ is a realization of poisson counter $\mathcal{N}$. $x(t)$ is a solution in Ito sense to $\mathrm{d}x(t)=F(x(t),t)\mathrm{d}t+G(x(t),t)\mathrm{d}n(t)$ if
&lt;ol>
&lt;li>On all intervals where $n(t)$ is constant, $\dot{x}(t)=F(x(t),t)$&lt;/li>
&lt;li>If $n(t)$ jumps at time $t_1$, $\lim_{t\to t_1^+}x(t)=\lim_{t\to t_1^-}x(t)+G\left(\lim_{t\to t_1^+}x(t),t_1\right)$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>Ito rule&lt;/strong>: Given a fuction $\psi(\mathcal{X}(t),t)$, taking Taylor expansion we have $$\mathrm{d}\psi=\left\{\frac{\partial\psi}{\partial t}+(\nabla_\mathcal{X}\psi)F(\mathcal{X},t)\right\}\mathrm{d}t+\sum^r_{i=1}\left\{\psi\left(\mathcal{X}+G_i(\mathcal{X},t),t\right)-\psi(\mathcal{X},t)\right\}\mathrm{d}\mathcal{N}_i(t)$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="wiener-process">Wiener-Process&lt;/h3>
&lt;ul>
&lt;li>Definition (&lt;strong>Brownian Motion&lt;/strong>): It&amp;rsquo;s a bidirectional poisson counter with infinite rate, i.e. $\mathcal{W}=\lim_{\lambda\to\infty}\frac{1}{\sqrt(\lambda)}(\mathcal{N}_1-\mathcal{N}_2)$ where $\mathcal{N}_1,\;\mathcal{N}_2$ have rate $\lambda/2$
&lt;ul>
&lt;li>Characteristic: It&amp;rsquo;s a Gaussian with zero mean and variance $t$&lt;/li>
&lt;li>Note: Actual Wiener Process has stronger continuity property than Brownian motion.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Expectation: $\mathbb{E}[\mathcal{W}]=\mathbb{E}[d\mathcal{W}]=0,\;\mathbb{E}[\mathcal{W}(\tau)\mathcal{W}(t)]=\mathbb{E}[\mathcal{W}^2(\min\{t,\tau\})]=\min\{t,\tau\}$&lt;/li>
&lt;li>Principle of independent increments: If the interval $[r,t), [\sigma,s)$ don&amp;rsquo;t overlap, then $\mathcal{W}(t)-\mathcal{W}(\tau)$ and $\mathcal{W}(s)-\mathcal{W}(\sigma)$ are uncorrelated.&lt;/li>
&lt;li>&lt;strong>Ito calculus&lt;/strong> for Wiener Process
&lt;ul>
&lt;li>&lt;strong>Ito rule&lt;/strong>: Given a fuction $\psi(\mathcal{X}(t),t)$, taking Taylor expansion we have $$\begin{split}\mathrm{d}\psi=\frac{\partial\psi}{\partial t}\mathrm{d}t+(\nabla_\mathcal{X}\psi)F(\mathcal{X},t)\mathrm{d}t+\sum^r_{i=1}\left(\nabla_\mathcal{X}\psi(\mathcal{X},t)\right)G_i(\mathcal{X},t)\mathrm{d}\mathcal{W}_ i \\ -\sum^r_{i=1}\frac{1}{2}G_i^\top(\mathcal{X},t)\left(\mathrm{H}_\mathcal{X}\psi(\mathcal{X},t)\right)G_i(\mathcal{X},t)\mathrm{d}t\end{split}$$
&lt;blockquote>
&lt;p>Note $\nabla_{\mathcal{X}}=\begin{bmatrix}\frac{\partial\psi}{\partial x_1}&amp;amp;\cdots&amp;amp;\frac{\partial\psi}{\partial x_n}\end{bmatrix}$ and $\mathrm{H}_{\mathcal{X}}=\begin{bmatrix}\frac{\partial^2\psi}{\partial x_1^2}&amp;amp;\cdots&amp;amp;\frac{\partial^2\psi}{\partial x_1\partial x_n}\\ \vdots&amp;amp;\ddots&amp;amp;\vdots \\ \frac{\partial^2\psi}{\partial x_n\partial x_1}&amp;amp;\cdots&amp;amp;\frac{\partial^2\psi}{\partial x_n^2}\end{bmatrix}$ are the gradient and Hessian operator respectively. By default, the operator target is $\mathcal{X}$ is not noted.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>White noise&lt;/strong>: It is Guassian distributed stationary stochastic process with $\mu_\mathcal{U}(t)=0,\;\mathrm{K}_ \mathcal{U}(t,\tau)=\Phi_ \mathcal{U}\delta(t-\tau)$, where $\Phi_\mathcal{U}$ is &lt;strong>spectral intensity&lt;/strong>.
&lt;ul>
&lt;li>We often consider white noise as derivative of Wiener process: $\mathcal{U}\sim\mathrm{d}\mathcal{W}/\mathrm{d}t$.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="linear-stochastic-sequence-1">Linear Stochastic Sequence&lt;/h3>
&lt;p>$$\begin{align}\mathrm{d}\mathcal{X}(t)&amp;amp;=\{A(t)\mathcal{X}(t)+u(t)\}\mathrm{d}t+B(k)\mathrm{d}\mathcal{W}(t) \\ \mathcal{Y}(t)&amp;amp;=C(k)\mathcal{X}(k)+D(k)\mathcal{W}(k)\end{align}$$&lt;/p>
&lt;ul>
&lt;li>Differential equation of expectations
&lt;ul>
&lt;li>&lt;strong>Mean&lt;/strong>: $\frac{\mathrm{d}}{\mathrm{d}t}\mu_\mathcal{X}(t)=A(t)\mu_\mathcal{X}(t)$&lt;/li>
&lt;li>&lt;strong>Autocovariance&lt;/strong>: $\frac{\mathrm{d}}{\mathrm{d}t}S_\mathcal{X}(t)=A(t)S_\mathcal{X}(t)+S_\mathcal{X}A^\top(t)+B(t)B^\top(t)$ (called &lt;strong>Lyapunov differential equation&lt;/strong>)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Stationary LTI case
&lt;ul>
&lt;li>Useful equation: $\mathrm{R}_ \mathcal{X}(t,\tau)=\begin{cases} S_ \mathcal{X}\exp\{A^\top(\tau-t)\}&amp;amp;,\tau&amp;gt;t \\ \exp\{A(t-\tau)\}S_ \mathcal{X}&amp;amp;,\tau&amp;lt; t\end{cases}$&lt;/li>
&lt;li>&lt;strong>(Continuous-time) algerbraic Lyapunov equation&lt;/strong>: $A\bar{S}_ \mathcal{X}+\bar{S}_\mathcal{X}A^\top+BB^\top=0$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="nonlinear-stochastic-sequence">Nonlinear Stochastic Sequence&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>The Fokker-Planck Equation (FPE)&lt;/strong>: Consider the Wiener-process excited general SDE, we have $$\frac{\partial f_ \mathcal{X}(x;t)}{\partial t}=-\nabla\left(F(\mathcal{X})f_ \mathcal{X}(x;t)\right)+\frac{1}{2}\mathrm{tr}\left[\mathrm{H}\left(\Gamma(\mathcal{X})f_ \mathcal{X}(x;t)\right)\right]$$
here $\Gamma(\mathcal{X})=G(\mathcal{X})G^\top(\mathcal{X})$&lt;/p>
&lt;ul>
&lt;li>$\mathcal{X}$ is stationary means $\frac{\partial f_\mathcal{X}(x;t)}{\partial t}=0$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Gaussian closure&lt;/strong>: An approximate solution for FPE is supposing $f_\mathcal{X}$ as multivariate Gaussian with some $S_\mathcal{X}$ and ${\mu_\mathcal{X}}$. That is we focus on 1st-order and 2nd-order estimation. Denote the estimated distribution as $\hat{f}_\mathcal{X}(x;t)$&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Estimated expectation&lt;/strong>: $\hat{\mathbb{E}}\phi(\mathcal{X})=\int\cdots\int\phi(x)\hat{f}_\mathcal{X}(x;t)\mathrm{d}x$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Solution:$$\begin{split}h(x,t)=-\nabla F+\tilde{x}^\top S^{-1}F-(\nabla\Gamma)S^{-1}\tilde{x}+\frac{1}{2}\mathrm{tr}\left[\mathrm{H}\Gamma \right] \\ +\frac{1}{2}\mathrm{tr}\left[\left(-S^{-1}+S^{-1}\tilde{x}\tilde{x}^\top S^{-1}\right)\Gamma\right]\end{split}$$ and $$\begin{align}\dot{\mu}_\mathcal{X}(t)&amp;amp;=S(t)\hat{\mathbb{E}}[\nabla^\top h(x)] \\ \dot{S} _\mathcal{X}(t)&amp;amp;=S(t)\hat{\mathbb{E}}[\mathrm{H} h(x)]S(t)\end{align}$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Useful simplification: If $G$ is constant (independent of $\mathcal{X}$), then the ODE of $\dot{\mu}$ and $\dot{S}$ become $$\begin{align}\dot{\mu}_ \mathcal{X}(t)&amp;amp;=\hat{\mathbb{E}}[F(\mathcal{X})] \\ \dot{S} _\mathcal{X}(t)&amp;amp;=\hat{A}^\top S+S\hat{A}+\Gamma\end{align}$$ where $\hat{A}=\hat{\mathbb{E}}\left[\frac{\partial F}{\partial\mathcal{X}}\right]$&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Equivalent linearization (Quasi-linearization)&lt;/strong>: In the estimation, $\mathcal{X}$ evolves equivalently to $\mathrm{d}\mathcal{X}(t)=\hat{A}(t)\mathcal{X}(t)\mathrm{d}t+G\mathrm{d}\mathcal{W}(t)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>No guaranteed bounds generally exist for the estimation error $$e(\mathcal{X},t)=\left(\frac{\partial\hat{f}_ \mathcal{X}}{\partial t}\right)-\left(-\nabla\left(F\hat{f}_ \mathcal{X}\right)+\frac{1}{2}\mathrm{tr}\left[\mathrm{H}\left(\Gamma\hat{f}_ \mathcal{X}\right)\right]\right)$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Estimation of $\mu(t),\;S(t)$ could also have error. And stationarity may not be the consistent between original system and estimated system&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="spectral-analysis">Spectral Analysis&lt;/h2>
&lt;blockquote>
&lt;p>Content in this section comes from MECHENG 549&lt;/p>
&lt;/blockquote>
&lt;h3 id="power-spectral-density">Power Spectral Density&lt;/h3>
&lt;ul>
&lt;li>Definition: The &lt;strong>power spectral density (PSD)&lt;/strong> of stochastic process $\mathcal{Y}$ is $$\Phi_\mathcal{Y}(\omega)\equiv\mathbb{E}\left[ \lim_{T\to\infty}\frac{1}{2T}Y_T(\omega)Y_T^\top(\omega)\right]$$ where $Y_T(\omega)$ is the Fourier transform of centered process $\mathcal{Y}(t)$.
&lt;ul>
&lt;li>White noise has the same PSD for whatever $\omega$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Wiener-Khinchin&lt;/strong> Theorem: $\Phi_\mathcal{Y}(\omega)=\int^\infty_{-\infty}e^{-i\omega\theta}\bar{\mathrm{K}}_\mathcal{Y}(\theta)\mathrm{d}\theta$&lt;/li>
&lt;li>&lt;strong>Signal propagation&lt;/strong>: If the system $P$ has input $\mathcal{Y}$ and output $\mathcal{Z}$, then $\Phi_\mathcal{Z}=|P(i\omega)\Phi_\mathcal{Y}(\omega)P^\top(i\omega)|$ where $P(i\omega)$ is the Fourier transform of the system.&lt;/li>
&lt;li>&lt;strong>Cross spectrum&lt;/strong>: If the system $G$ has input $\mathcal{Y}$ and output $\mathcal{Z}$, then the cross-spectrum is $\Phi_{\mathcal{ZY}}(\omega)=G(i\omega)\Phi_\mathcal{Y}(\omega)$&lt;/li>
&lt;/ul>
&lt;h3 id="periodograms">Periodograms&lt;/h3>
&lt;ul>
&lt;li>Definition: We want to estimate $\Phi_{\mathcal{Y}}(\omega)$ without the expectation, then we use $$Q_T(\omega,y)=\frac{1}{2T}\left|\int^T_{-T}e^{-i\omega t}y(t)\mathrm{d}t\right|^2$$ where $y$ is a sample realization of $\mathcal{Y}$.
&lt;ul>
&lt;li>We also use window functions to calculate the spectrum over a finite time interval (using &lt;strong>Bartlett&amp;rsquo;s procedure&lt;/strong>)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="stochastic-realizations">Stochastic realizations&lt;/h3>
&lt;p>We want to find a stochastic model which gives the same spectrum given. This is called &lt;strong>stochastic realization problem&lt;/strong>. We focus on scalar LTI case.&lt;/p>
&lt;p>If we know $P(s)=C[sI-A]^{-1}B+D$ (the Laplace transform of LTI system) and $P(s)=c\frac{\prod^m_{k=1}(s-z_k)}{\prod^n_{k=1}(s-p_k)}$ for some $m\leq n$ and real constant $c$. Then we have $$\Phi_\mathcal{Y}(\omega)=P(i\omega)P(-i\omega)=c^2\frac{\prod^m_{k=1}(\omega^2+z_k^2)}{\prod^n_{k=1}(\omega^2+p_k^2)}$$&lt;/p>
&lt;p>Lemma: For any valid PSD, there exists a spectral factorization $\Phi_\mathcal{Y}(\omega)=\frac{\sum^m_{k=0}a_k(\omega^2)^k}{\sum^n_{k=0}b_k(\omega^2)^k}=P(i\omega)P(-i\omega)$ where $P(s)$ is an asymptotically stable n-th order transfer function, iff&lt;/p>
&lt;ol>
&lt;li>$\Phi_\mathcal{Y}(\omega)$ is a ratio of polynomials of $\omega^2$&lt;/li>
&lt;li>All coefficients $a_k$ and $b_k$ are real&lt;/li>
&lt;li>The denominator has no positive real roots in $\omega^2$&lt;/li>
&lt;li>Any positive real roots in the numerator have even multiplicity&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;h2 id="notes-for-math-typing-in-hexo">Notes for math typing in Hexo&lt;/h2>
&lt;ol>
&lt;li>Escape &lt;code>\&lt;/code> by &lt;code>\\&lt;/code>. Especially escape &lt;code>{&lt;/code> by &lt;code>\\{&lt;/code> instead of &lt;code>\{&lt;/code>, and escape &lt;code>\\&lt;/code> by &lt;code>\\\\&lt;/code>.&lt;/li>
&lt;li>Be careful about &lt;code>_&lt;/code>, it&amp;rsquo;s used in markdown as italic indicator. Add space after &lt;code>_&lt;/code> is a useful solution.&lt;/li>
&lt;li>&lt;a class="link" href="https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference" target="_blank" rel="noopener"
>Some useful Mathjax tricks at StackExchange&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.overleaf.com/learn/latex/List_of_Greek_letters_and_math_symbols" target="_blank" rel="noopener"
>Several capital Greek characters should directly use its related Latin alphabet with &lt;code>\mathrm&lt;/code> command&lt;/a>.&lt;/li>
&lt;/ol>
&lt;p>Although I have migrated to Hugo, some tricks might still be relevant.&lt;/p>
&lt;/blockquote></description></item><item><title>Notes for Probability Theory (Basics)</title><link>https://zyxin.xyz/blog/en/2019-02/ProbabilityNotes/</link><pubDate>Sat, 23 Feb 2019 00:00:00 +0000</pubDate><guid>https://zyxin.xyz/blog/en/2019-02/ProbabilityNotes/</guid><description>&lt;h2 id="probability-space">Probability Space&lt;/h2>
&lt;ul>
&lt;li>Notation: $(\Omega,\mathcal{F},\mathbb{P})$
&lt;ul>
&lt;li>$\Omega$: &lt;strong>Sample space&lt;/strong>&lt;/li>
&lt;li>$\mathcal{F}$: &lt;strong>Event space&lt;/strong>. Required to be &lt;a class="link" href="http://mathworld.wolfram.com/Sigma-Algebra.html" target="_blank" rel="noopener"
>&lt;strong>σ-algebra&lt;/strong>&lt;/a>. We often use &lt;a class="link" href="http://mathworld.wolfram.com/BorelSigma-Algebra.html" target="_blank" rel="noopener"
>&lt;strong>Borel σ-algebra&lt;/strong>&lt;/a> for continuous $\Omega$).
&lt;ul>
&lt;li>Axioms for &lt;strong>σ-algebra&lt;/strong>
&lt;ol>
&lt;li>$\mathcal{F}$ is non-empty&lt;/li>
&lt;li>$A\in\mathcal{F} \Rightarrow A^C\in\mathcal{F}$ (closed under complement)&lt;/li>
&lt;li>$A_i \in\mathcal{F} \Rightarrow \bigcup^\infty_{k=1} A_k \in \mathcal{F}$ (closed under countable union)&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>For continuous case considering interval $\Omega=[a,b]$, $\mathcal{F}_0$ is the set of all subintervals of $\Omega$. Then its &lt;strong>Borel σ-algebra&lt;/strong> is the smallest σ-algebra that contains $\mathcal{F}_0$. Here the $\mathcal{F}_0$ is a &lt;a class="link" href="https://en.wikibooks.org/wiki/Measure_Theory/Basic_Structures_And_Definitions/Semialgebras,_Algebras_and_%CF%83-algebras" target="_blank" rel="noopener"
>&lt;strong>semialgebra&lt;/strong>&lt;/a>. We can find a containing σ-algebra for every semialgebra.&lt;/li>
&lt;li>Axioms for &lt;strong>semialgebra&lt;/strong>
&lt;ol>
&lt;li>$\emptyset, \Omega \in \mathcal{F}$&lt;/li>
&lt;li>$A_i \in\mathcal{F} \Rightarrow \bigcap^n_{k=1} A_k \in \mathcal{F}$ (closed under finite intersections)&lt;/li>
&lt;li>$\forall B \in\mathcal{F}, B^C=\bigcup^n_{k=1} A_k$ where $A_i \in \mathcal{F}$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>$\mathbb{P}$: &lt;strong>Probability measure&lt;/strong>.
&lt;ul>
&lt;li>Axioms for probability measure
&lt;ol>
&lt;li>$\mathbb{P}(\Omega) = 1$&lt;/li>
&lt;li>$\forall A\in\mathcal{F}, \mathbb{P}(A) \geqslant 0$&lt;/li>
&lt;li>$A_i, A_j \in\mathcal{F}$ are pairwise disjoint, then $\mathbb{P}(\bigcup^\infty_{k=1}A_k)=\sum^\infty_{k=1}\mathbb{P}(A_k)$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Product Space&lt;/strong>: Probability spaces can be combined using Cartesian product.&lt;/li>
&lt;li>&lt;strong>Independence&lt;/strong>: $\mathbb{P}(A_{k_1}\cap A_{k_2}\cap &amp;hellip;\cap A_{k_l})=\prod_{i=1}^l \mathbb{P}(A_i),\;\forall \{k_i\}_1^l\subset\{ 1..n\}$&lt;/li>
&lt;li>&lt;strong>Conditional probability&lt;/strong>: $\mathbb{P}\left(A_i \middle| A_j\right)=\mathbb{P}(A_i\cap A_j)/\mathbb{P}(A_j)$&lt;/li>
&lt;li>&lt;strong>Total probability&lt;/strong>: $\mathbb{P}(B)=\sum_{i=1}^n \mathbb{P}(B\cap A_i)=\sum_{i=1}^n \mathbb{P}\left(B\middle| A_i\right)\mathbb{P}(A_i)$, where $\{A_1,\cdots,A_n\}$ are disjoint and partition of $\Omega$.&lt;/li>
&lt;li>&lt;strong>Bayes&amp;rsquo; Rule&lt;/strong>: $$\mathbb{P}(A_j|B)=\frac{\mathbb{P}(B|A_j)\mathbb{P}(A_j)}{\sum^n_{i=1} \mathbb{P}(B|A_i)\mathbb{P}(A_i)}$$
&lt;ul>
&lt;li>Priori: $\mathbb{P}(B|A_j)$&lt;/li>
&lt;li>Posteriori: $\mathbb{P}(A_j|B)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="random-variables">Random Variables&lt;/h2>
&lt;blockquote>
&lt;p>Note: The equations are written in continuous case by default, one can get the equation for discrete case by changing integration into summation and changing differential into difference.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>Random Variable $\mathcal{X}$ is a mapping $\mathcal{X}: (\Omega,\mathcal{F},\mathbb{P})\to(\Omega_ \mathcal{X},\mathcal{F}_ \mathcal{X},\mathbb{P}_ \mathcal{X})$&lt;/li>
&lt;li>&lt;strong>Continuous &amp;amp; Discrete &amp;amp; Mixed Random Variable&lt;/strong>:
&lt;ul>
&lt;li>Can be defined upon whether $\Omega_\mathcal{X}$ is continuous&lt;/li>
&lt;li>Can be defined upon whether we can find continuous density function $f_\mathcal{X}$&lt;/li>
&lt;li>$\mathcal{F}_\mathcal{X}$ for continuous $\mathcal{X}$ is a &lt;strong>Borel σ-field&lt;/strong>&lt;/li>
&lt;li>All three kinds of random variables can be expressed by CDF or &amp;ldquo;extended&amp;rdquo; PDF with Dirac function and Lebesgue integration.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Scalar Random Variable&lt;/strong>: $\mathcal{X}: \Omega\to\mathbb{F}$
&lt;ul>
&lt;li>Formal definition: $\mathcal{X}: (\Omega,\mathcal{F},\mathbb{P})\to(\Omega_ \mathcal{X}\subset\mathbb{F},\mathcal{F}_ \mathcal{X}\subset\left\{\omega\middle| \omega\subset\Omega_ \mathcal{X}\right\},\mathbb{P}_ \mathcal{X}:\mathcal{F}_ \mathcal{X}\to[0,1])$&lt;/li>
&lt;li>&lt;strong>Cumulative Distribution Function&lt;/strong> (CDF): $F_ \mathcal{X}(x)=\mathbb{P}_\mathcal{X}(\mathcal{X}(\omega)\leqslant x)$&lt;/li>
&lt;li>&lt;strong>Probability Mass Function&lt;/strong> (PMF): $p_ \mathcal{X}(x)=\mathbb{P}_\mathcal{X}(\mathcal{X}(\omega)=x)$&lt;/li>
&lt;li>&lt;strong>Probability Density Function&lt;/strong> (PDF): $$f_ \mathcal{X}(x)=\mathbb{P}_ \mathcal{X}(x&amp;lt; \mathcal{X}(\omega)\leqslant x+ \mathrm{d}x)=\mathrm{d}F_ \mathcal{X}(x)/ \mathrm{d}x$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Vector Random Variable&lt;/strong> (Multiple Random Variables): $\mathcal{X}: \Omega\to\mathbb{F}^n$
&lt;ul>
&lt;li>Formal definition $\mathcal{X}: (\Omega,\mathcal{F},\mathbb{P})\to(\Omega_ \mathcal{X}\subset\mathbb{F}^n,\mathcal{F}_ \mathcal{X}\subset\left\{\omega\middle| \omega\subset\Omega_ \mathcal{X}\right\},\mathbb{P}_ \mathcal{X}:\mathcal{F}_ \mathcal{X}\to[0,1])$&lt;/li>
&lt;li>&lt;strong>Cumulative Distribution Function&lt;/strong> (CDF): $F_ \mathcal{X}(x)=\mathbb{P}_\mathcal{X}(\{\mathcal{X}(\omega)\}_i\leqslant x_i), i=1\ldots n$&lt;/li>
&lt;li>&lt;strong>Probability Mass Function&lt;/strong> (PMF): $p_ \mathcal{X}(x)=\mathbb{P}_\mathcal{X}(\{\mathcal{X}(\omega)\}_i=x)$&lt;/li>
&lt;li>&lt;strong>Probability Density Function&lt;/strong> (PDF): $$f_ \mathcal{X}(x)=\mathbb{P}_ \mathcal{X}(x_i&amp;lt; \{\mathcal{X}(\omega)\}_ i\leqslant x_i+ \mathrm{d}x_i)=\frac{\partial^n}{\partial x_1\partial x_2\ldots\partial x_n}F_ \mathcal{X}(x)$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Afterwards, we don&amp;rsquo;t distinguish $\mathbb{P}_\mathcal{X}$ with $\mathbb{P}$ if there&amp;rsquo;s no ambiguity.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Independence&lt;/strong>($\perp$ or $\perp$ with double vertical lines): $\mathbb{P}(\mathcal{X}\in A\cap \mathcal{Y}\in B)=\mathbb{P}(\mathcal{X}\in A)\mathbb{P}(\mathcal{Y}\in B)$
&lt;ul>
&lt;li>&lt;strong>Independent CDF&lt;/strong>:$F_{\mathcal{XY}}(x,y)=F_\mathcal{X}(x)F_\mathcal{Y}(y)$ iff $\mathcal{X}\perp\mathcal{Y}$&lt;/li>
&lt;li>&lt;strong>Independent PMF&lt;/strong>:$p_{\mathcal{XY}}(x,y)=p_\mathcal{X}(x)p_\mathcal{Y}(y)$ iff $\mathcal{X}\perp\mathcal{Y}$&lt;/li>
&lt;li>&lt;strong>Independent PDF&lt;/strong>:$f_{\mathcal{XY}}(x,y)=f_\mathcal{X}(x)f_\mathcal{Y}(y)$ iff $\mathcal{X}\perp\mathcal{Y}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Marginalization&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>Marginal distribution function&lt;/strong>: $F_{1:m}(x_{1:m})\equiv F\left(\begin{bmatrix}x_1&amp;amp;x_2&amp;amp;\ldots&amp;amp;x_m&amp;amp;\infty&amp;amp;\ldots&amp;amp;\infty\end{bmatrix}^\top\right)$&lt;/li>
&lt;li>&lt;strong>Marginal density function&lt;/strong>: $f_{1:m}(x_{1:m})=\int^\infty_{-\infty}\cdots\int^\infty_{-\infty} f(x) \mathrm{d}x_{m+1}\cdots \mathrm{d}x_n$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Conditional&lt;/strong> (on event $\mathcal{E}$)
&lt;ul>
&lt;li>&lt;strong>Conditional probability&lt;/strong>: $\mathbb{P}(\mathcal{X}\in\mathcal{D}|\mathcal{E})=\mathbb{P}(\{\omega|\mathcal{X}(\omega)\in\mathcal{D}\}\cap \mathcal{E})/\mathbb{P}(\mathcal{E})$ on a event $\mathcal{E}\in\mathcal{F}$ and a set $\mathcal{D}\subset\mathcal{F}$.&lt;/li>
&lt;li>&lt;strong>Conditional CDF&lt;/strong>: $F_ \mathcal{X}(x|\mathcal{E})=\mathbb{P}(\mathcal{X}_i\leqslant x_i|\mathcal{E})$&lt;/li>
&lt;li>&lt;strong>Conditional PMF&lt;/strong>: $p_ \mathcal{X}(x|\mathcal{E})=\mathbb{P}(\mathcal{X}_i=x_i|\mathcal{E})$&lt;/li>
&lt;li>&lt;strong>Conditional PDF&lt;/strong>: $f_ \mathcal{X}(x|\mathcal{E})=\frac{\partial^n}{\partial x_1\partial x_2\ldots\partial x_n}F_ \mathcal{X}(x|\mathcal{E})$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Conditional&lt;/strong> (on variable $\mathcal{Y}$)
&lt;ul>
&lt;li>&lt;strong>Conditional probability&lt;/strong>: $$\mathbb{P}(\mathcal{X}\in\mathcal{D}_1|\mathcal{Y}\in\mathcal{D}_2)=\mathbb{P}(\{\omega|\mathcal{X}(\omega)\in\mathcal{D}_1,\mathcal{Y}(\omega)\in\mathcal{D}_2\})/\mathbb{P}(\mathcal{Y}(\omega)\in\mathcal{D}_2)$$&lt;/li>
&lt;li>&lt;strong>Conditional PDF&lt;/strong> (similar for PMF): $$f_{\mathcal{X}|\mathcal{Y}\in\mathcal{D}}(x)=\frac{\int_{\mathcal{Y}\in\mathcal{D}}f_{\mathcal{XY}}(x,y)}{\int_{\mathcal{Y}\in\mathcal{D}}f_{\mathcal{Y}}(y)},\;f_{\mathcal{X}|\mathcal{Y}}(x|y)=f_{\mathcal{X}|\mathcal{Y}=y}(x)=\frac{f_{\mathcal{XY}}(x,y)}{f_{\mathcal{Y}}(y)}$$
&lt;ul>
&lt;li>Using total probability we have $f_ \mathcal{X}(x)=\int f_{\mathcal{X}|\mathcal{Y}}(x|y)f_ \mathcal{Y}(y)\mathrm{d}y$. This can be further integrated into Bayes&amp;rsquo; rule.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Conditional CDF&lt;/strong>: Can be defined similarly, of defined as $F_{\mathcal{X}|\mathcal{Y}\in\mathcal{D}}(x)\equiv\int f_{\mathcal{X}|\mathcal{Y}\in\mathcal{D}}(x)\mathrm{d}x$
&lt;ul>
&lt;li>Similarly we have $F_ \mathcal{X}(x)=\int F_{\mathcal{X}|\mathcal{Y}}(x|y)f_ \mathcal{Y}(y)\mathrm{d}y$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Substitution law&lt;/strong>: $\mathbb{P}((\mathcal{X},\mathcal{Y})\in \mathcal{D}|\mathcal{X}=x)=\mathbb{P}((x,\mathcal{Y})\in \mathcal{D})$
&lt;ul>
&lt;li>Common usage: Suppose $\mathcal{Z}=\psi(\mathcal{X},\mathcal{Y})$, then $p_\mathcal{Z}(z)=\int_x \mathbb{P}\left(\psi(x,\mathcal{Y})=z\right)p_\mathcal{X}(x)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="uncertainty-propagation">Uncertainty Propagation&lt;/h2>
&lt;p>Suppose $\mathcal{Y}=\psi(\mathcal{X})$ or specifically $y=\psi(x_1)=\psi(x_2)=\cdots=\psi(x_K)$&lt;/p>
&lt;ul>
&lt;li>Scalar case: $$f_ \mathcal{Y}(y)=\sum^K_{k=1} f_ \mathcal{X}(\psi^{-1}_ k(y))\left| \frac{\partial\psi}{\partial x}\biggr|_{x=\psi^{-1}_k(y)} \right|^{-1}$$&lt;/li>
&lt;li>Vector case: $$f_ \mathcal{Y}(y)=\sum^K_{k=1} f_ \mathcal{X}(\psi^{-1}_ k(y))\left|\det(J)\right|^{-1},\text{where Jacobian }J=\frac{\partial\psi}{\partial x}\biggr|_{x=\psi^{-1}_k(y)}$$&lt;/li>
&lt;li>Trivial Case — Summation: $\mathcal{Y}=\mathcal{X}_ 1+\mathcal{X}_ 2$, then $f_ \mathcal{Y}=\int^\infty_{-\infty}f_ {\mathcal{X}_ 1\mathcal{X}_ 2}(x_1,x_2-x_1) \mathrm{d}x_1$
Another way is to use the method of choice: $F_ \mathcal{Y}(y)=\int_ {\psi(x)\leq y}f_\mathcal{X}(x)\mathrm{d}x$&lt;/li>
&lt;/ul>
&lt;h2 id="expectation--moments">Expectation &amp;amp; Moments&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Expectation&lt;/strong>: $\mathbb{E}_ \mathcal{X}[\psi(\mathcal{X})]=\int^\infty_\infty\cdots\int^\infty_\infty\psi(x)f_ \mathcal{X}(x) \mathrm{d}x_1\ldots \mathrm{d}x_n$&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>A more rigorous way to define the expectation is $\mathbb{E}_ \mathcal{X}[\psi(\mathcal{X})]=\int \psi(\mathcal{X})dF_ \mathcal{X}(\mathcal{X})$. This definition uses Lebesgue Integral and works on both discrete and continuous (or even mixed) variables. See &lt;a class="link" href="http://www.randomservices.org/random/dist/Integral.html" target="_blank" rel="noopener"
>this post&lt;/a> and &lt;a class="link" href="" >this discussion&lt;/a> for more information.
We write $\mathbb{E}_ \mathcal{X}$ as $\mathbb{E}$ if there&amp;rsquo;s no ambiguity (when only one random variable is included). And $\mathbb{E}[\mathcal{X}]$ will be abbreviated as $\mathbb{E}\mathcal{X}$ afterwards.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Linearity of expectation&lt;/strong>: $\mathbb{E}[A\mathcal{\mathcal{X}}]=A\mathbb{E}[\mathcal{X}] (\forall \mathcal{X},\mathcal{Y},\forall A\in\mathbb{R}^{m\times n})$&lt;/li>
&lt;li>&lt;strong>Independent expectation&lt;/strong>: $\mathbb{E}[\prod^n_i\mathcal{X}_i]=\prod^n_i\left(\mathbb{E}\mathcal{X}_i\right)(\forall\;\text{indep.}\;\mathcal{X}_i)$&lt;/li>
&lt;li>&lt;strong>Conditional expectation&lt;/strong>: $\mathbb{E}[\mathcal{Y}|\mathcal{X}=x]=\int^\infty_{-\infty}yf_{\mathcal{Y}|\mathcal{X}}(y|x)\mathrm{d}y$
&lt;ul>
&lt;li>&lt;strong>Total expectation/Smoothing&lt;/strong>: $\mathbb{E}_ \mathcal{X}[\mathbb{E}_ {\mathcal{Y}|\mathcal{X}}(\mathcal{Y}|\mathcal{X})]=\mathbb{E}\mathcal{Y}$&lt;/li>
&lt;li>&lt;strong>Substitution law&lt;/strong>: $\mathbb{E}[g(\mathcal{X},\mathcal{Y})|\mathcal{X}=x]=\mathbb{E}[\psi(x,\mathcal{Y})|\mathcal{X}=x]$&lt;/li>
&lt;li>$\mathbb{E}[\psi(\mathcal{X})|\mathcal{X}]=\psi(\mathcal{X})$&lt;/li>
&lt;li>$\mathbb{E}[\psi(\mathcal{X})\mathcal{Y}|\mathcal{X}]=\psi(\mathcal{X})\mathbb{E}(\mathcal{Y}|\mathcal{X})$&lt;/li>
&lt;li>&lt;strong>Towering&lt;/strong>: $\mathbb{E}_ {\mathcal{Y}|\mathcal{Z}}[\mathbb{E}_ \mathcal{X}(\mathcal{X}|\mathcal{Y},\mathcal{Z})|\mathcal{Z}]=\mathbb{E}_ {\mathcal{X}|\mathcal{Z}}[\mathcal{X}|\mathcal{Z}]$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Moment&lt;/strong> (p-th order): $\mu_p(\mathcal{X})=\mathbb{E}[\mathcal{X}^p]$
&lt;ul>
&lt;li>&lt;strong>Mean&lt;/strong>: $\mu_ \mathcal{X}=\mathbb{E}[\mathcal{X}]$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Central moment&lt;/strong> (p-th order): $\mathbb{E}[(\mathcal{X}-\mu_ \mathcal{X})^p]$
&lt;ul>
&lt;li>&lt;strong>Variance&lt;/strong>: $\sigma_ \mathcal{X}^2=\mathbb{E}[(\mathcal{X}-\mu_ \mathcal{X})^2]$, sometimes written as $\sigma_ \mathcal{X}^2=\mathbb{V}(\mathcal{X})$&lt;/li>
&lt;li>&lt;strong>Skewness&lt;/strong>: $\tilde{\mu}_ 3=\mathbb{E}[(\mathcal{X}-\mu_ \mathcal{X})^3]/\sigma_\mathcal{X}^3$ ($\tilde{\mu}_ 3&amp;gt;0$ right-skewed, $\tilde{\mu}_ 3&amp;lt;0$ left-skewed)&lt;/li>
&lt;li>&lt;strong>Kurtosis&lt;/strong>: $\tilde{\mu}_ 4=\mathbb{E}[(\mathcal{X}-\mu_ \mathcal{X})^4]/\sigma_\mathcal{X}^4$&lt;/li>
&lt;li>&lt;strong>Excessive Kurtosis&lt;/strong>: $\gamma = \mu_ 4-3$
&lt;img src="https://zyxin.xyz/blog/blog/en/2019-02/ProbabilityNotes/skewness_and_kurtosis.jpg"
width="1266"
height="237"
loading="lazy"
alt="Skewness and Kurtosis"
class="gallery-image"
data-flex-grow="534"
data-flex-basis="1282px"
>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Correlation&lt;/strong>: $\text{corr}(\mathcal{X}_i,\mathcal{X}_j)=\mathbb{E}[\mathcal{X}_i\mathcal{X}_j]$
&lt;ul>
&lt;li>&lt;strong>Correlation matrix&lt;/strong>: $C=\mathbb{E}[\mathcal{X}_i\mathcal{X}_j^\top]$&lt;/li>
&lt;li>&lt;strong>Correlation coefficient&lt;/strong>: $\rho(\mathcal{X}_ i,\mathcal{X}_ j)=\frac{\text{corr}(\mathcal{X}_ i,\mathcal{X}_ j)}{\sigma_{\mathcal{X}_ i}^2\sigma_{\mathcal{X}_ j}^2}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Covariance&lt;/strong>: $\text{cov}(\mathcal{X}_i,\mathcal{X}_j)=\mathbb{E}[(\mathcal{X}_i-\mu_i)(\mathcal{X}_j-\mu_j)]$
&lt;ul>
&lt;li>&lt;strong>Covariance matrix&lt;/strong>: $S=\mathbb{E}\left[(\mathcal{X}-\mu_ \mathcal{X})(\mathcal{X}-\mu_ \mathcal{X})^\top\right]$&lt;/li>
&lt;li>Properties: $\text{cov}(\mathcal{X}+c)=\text{cov}(\mathcal{X}), \text{cov}(A\mathcal{X},\mathcal{X}B)=A\text{cov}(\mathcal{X})+\text{cov}(\mathcal{X})B^\top$&lt;/li>
&lt;li>&lt;strong>Uncorrelated&lt;/strong>: $\rho(\mathcal{X}_ i,\mathcal{X}_ j)=0 \Leftrightarrow\text{cov}(\mathcal{X}_i,\mathcal{X}_j)=0\Leftrightarrow\text{corr}(\mathcal{X}_i,\mathcal{X}_j)=\mathbb{E}[\mathcal{X}_i]\mathbb{E}[\mathcal{X}_j]$ (uncorrelated is necessary for independent)&lt;/li>
&lt;li>Cases where uncorrelated implies independence: (1) Jointly Gaussian (2) Bernoulli&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Centered variable&lt;/strong>: $\tilde{\mathcal{X}}=\mathcal{X}-\mu_ \mathcal{X}$&lt;/li>
&lt;/ul>
&lt;h2 id="transform-methods">Transform methods&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Probability generating function&lt;/strong>(PGF): similiar to Z-tranform $$G_ \mathcal{X}(z)\equiv\mathbb{E}[z^\mathcal{X}]=\sum_{x_i}z^{x_i}p_ \mathcal{X}(x_i)$$
&lt;ul>
&lt;li>For $\mathcal{F}_ \mathcal{X}=\mathbb{N}$, we have $$\frac{\mathrm{d}^k}{\mathrm{d}z^k}G_ \mathcal{X}(z)\Biggr|_{z=1}=\mathbb{E}[\mathcal{X}(\mathcal{X}-1)\cdots(\mathcal{X}-(k-1))]$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Moment generating function&lt;/strong>(MGF): similar to Laplace transform $$M_ \mathcal{X}(z)\equiv\mathbb{E}[e^{s\mathcal{X}}]=\int^\infty_{-\infty}e^{sx}f_ \mathcal{X}(x)\mathrm{d}x$$
&lt;ul>
&lt;li>Generally we have $$\frac{\mathrm{d}^k}{\mathrm{d}s^k}M_ \mathcal{X}(s)\Biggr|_{s=0}=\mathbb{E}[\mathcal{X}^k]$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Characteristic function&lt;/strong>(CF): similar to Fourier transform $$\phi_ \mathcal{X}(\omega)\equiv\mathbb{E}[e^{j\omega \mathcal{X}}]=\int^\infty_{-\infty}e^{j\omega x}f_ \mathcal{X}(x)\mathrm{d}x$$
&lt;ul>
&lt;li>Generally we have $$\frac{\mathrm{d}^k}{\mathrm{d}\omega^k}\phi_ \mathcal{X}(\omega)\Biggr|_{\omega=0}=j^k\mathbb{E}[\mathcal{X}^k]$$&lt;/li>
&lt;li>&lt;strong>Independent&lt;/strong>: $\mathcal{X}\perp \!\!\! \perp\mathcal{Y}$ iff. $\phi_ \mathcal{XY}(\omega)=\phi_ \mathcal{X}(\omega)\phi_ \mathcal{Y}(\omega)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Joint characteristic function&lt;/strong>: for vector case $\mathcal{X}\in\mathbb{R}^n$, we define vector $u$ and $$\phi_ \mathcal{X}(u)\equiv\mathbb{E}[e^{ju^\top \mathcal{X}}]$$
&lt;ul>
&lt;li>Trivial usage: if $\mathcal{Y}=\mathcal{X}_ 1+\mathcal{X}_ 2$, then $\phi_ \mathcal{Y}(u)=\phi_{\mathcal{X}_ 1}(u)\phi_{\mathcal{X}_ 2}(u)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="common-distributions1">Common distributions&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/h2>
&lt;ul>
&lt;li>&lt;em>&lt;strong>Bernoulli&lt;/strong>&lt;/em>: $\Omega_\mathcal{X}=\{0,1\}$ $$p_\mathcal{X}(1)=p,\;p_\mathcal{X}(0)=q=1-p$$
&lt;ul>
&lt;li>$\mu_\mathcal{X}=p,\;\sigma^2_\mathcal{X}=pq,\;G_\mathcal{X}(z)=q+pz$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Binomial&lt;/strong>&lt;/em> - $\mathcal{B}(n,p)$: $\Omega_\mathcal{X}=\{0,1,\ldots,n\}$ $$p_\mathcal{X}(k)=\binom{n}{k}p^k(1-p)^{n-k}$$
&lt;ul>
&lt;li>$\mu_\mathcal{X}=np,\;\sigma^2_\mathcal{X}=np(1-p),\;G_\mathcal{X}(z)=(1-p+pe^z)^n$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Multinomial&lt;/strong>&lt;/em>: $\Omega_\mathcal{X}=\{0,1,\ldots,n\}^k$ $$p_\mathcal{X}(x)=\binom{n}{x_1!\cdots x_k!}p_1^{x_1}\cdots p_k^{x_k}$$&lt;/li>
&lt;li>&lt;em>&lt;strong>Geometric&lt;/strong>&lt;/em>: $\Omega_\mathcal{X}=\mathbb{N}$ $$p_\mathcal{X}(k)=(1-p)^{k-1}p$$
&lt;ul>
&lt;li>$\mu_\mathcal{X}=\frac{1}{p},\;\sigma^2_\mathcal{X}=\frac{1-p}{p^2},\;G_\mathcal{X}(z)=\frac{pz}{1-(1-p)z}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Poisson&lt;/strong>&lt;/em>: $\Omega_\mathcal{X}=\mathbb{N}$ $$p_\mathcal{X}(k)=\frac{\lambda^k}{k!}\exp\{-\lambda\}$$
&lt;ul>
&lt;li>$\mu_\mathcal{X}=\lambda,\;\sigma^2_\mathcal{X}=\lambda,\;G_\mathcal{X}(z)=\exp\{\lambda(z-1)\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Uniform&lt;/strong>&lt;/em> - $\mathcal{U}(a,b)$: $\Omega_\mathcal{X}=[a,b]$ $$f(x)=
\begin{cases}
1/(b-a)&amp;amp;,\;x \in [a,b] \\
0&amp;amp;,\;\text{otherwise}
\end{cases}$$
&lt;ul>
&lt;li>$\mu_\mathcal{X}=\frac{1}{2}(a+b),\;\sigma^2_\mathcal{X}=\frac{1}{12}(b-a)^2,\;M_\mathcal{X}(s)=\frac{e^{sb}-e^{sa}}{s(b-a)}\;(s\neq 0)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Normal&lt;/strong>&lt;/em> - $\mathcal{N}(\mu,\sigma)$: $\Omega_\mathcal{X}=\mathbb{R}$
$$f(x)=\frac{1}{\sqrt{2\pi\sigma}}\exp\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}$$
&lt;ul>
&lt;li>$M_\mathcal{X}(s)=\exp\left\{\mu s+\frac{1}{2}\sigma^2s^2\right\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Joint Normal&lt;/strong>&lt;/em>: $\Omega_\mathcal{X}=\mathbb{R}^n$
$$f(x)=\frac{1}{\sqrt{(2\pi)^n \det(S)}}\exp\left\{-\frac{1}{2}(x-\mu)^\top S^{-1}(x-\mu)\right\}$$
&lt;ul>
&lt;li>$\phi_\mathcal{X}(u)=\exp\left\{ju^\top \mu-\frac{1}{2}u^\top Su\right\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Rayleigh&lt;/strong>&lt;/em>: $\Omega_\mathcal{X}=[0,\infty]$ $$f(x)=\frac{x}{\sigma^2}\exp\left\{-\frac{x^2}{2\sigma^2}\right\}H(x)$$ where $H(x)$ is Heaviside step function&lt;/li>
&lt;li>&lt;em>&lt;strong>Exponential&lt;/strong>&lt;/em> - $\mathcal{E}(\lambda)$: $\Omega_\mathcal{X}=[0,\infty]$ $$f(x)=\frac{1}{\mu}\exp\left\{-\frac{x}{\mu}\right\}H(x)$$
&lt;ul>
&lt;li>$\mu_\mathcal{X}=1/\lambda,\;\sigma^2_\mathcal{X}=1/\lambda^2,\;M_\mathcal{X}(s)=\lambda/(\lambda-s)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Laplacian&lt;/strong>&lt;/em>: $$f(x)=\frac{1}{2b}\exp\left\{-\frac{|x-\mu|}{b}\right\}$$&lt;/li>
&lt;/ul>
&lt;h2 id="derivation-of-the-distributions">Derivation of the distributions&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Bernoulli → Binomial&lt;/strong>: $\mathcal{X}_ i\sim\text{Bernoulli}( p) \Rightarrow \mathcal{Y}=\sum_{i=1}^n \mathcal{X}_ i\sim\mathcal{B}(n,p)$&lt;/li>
&lt;li>&lt;strong>Bernoulli → Geometric&lt;/strong>: $\mathcal{X}_i\sim\text{Bernoulli}( p) \Rightarrow \mathcal{Y}\sim\text{Geometric}( p)$ denoting the first $\mathcal{X}_i=1$&lt;/li>
&lt;li>&lt;strong>Binomial → Poisson&lt;/strong>: $\mathcal{X}_i\sim\mathcal{B}(n,p,k=1) \Rightarrow \mathcal{Y}\sim\text{Poisson}(\lambda)$ when $n\to \infty$ with $p=\frac{\lambda\tau}{n}$&lt;/li>
&lt;li>&lt;strong>Binomial → Exponential&lt;/strong>: $\mathcal{X}_i\sim\mathcal{B}(n,p,k\neq0) \Rightarrow \mathcal{Y}\sim\mathcal{E}(\lambda)$ when $n\to \infty$ with $p=\frac{\lambda\tau}{n}$
&lt;blockquote>
&lt;p>Actually $\mathcal{B}(n,p,k) \Rightarrow e^{-\lambda \tau}\frac{(\lambda \tau)^k}{k!}$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;a class="link" href="https://en.wikipedia.org/wiki/Laplace_distribution#Related_distributions" target="_blank" rel="noopener"
>&lt;strong>Exponential → Laplacian&lt;/strong>&lt;/a>: $\mathcal{X}_1, \mathcal{X}_2 \sim\mathcal{E}(\lambda) \Rightarrow \mathcal{X}_1-\mathcal{X}_2\sim\text{Laplacian}(\lambda^{-1})$&lt;/li>
&lt;/ul>
&lt;h2 id="concentration-inequalities">Concentration Inequalities&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Cauchy-Schwarz Inequality&lt;/strong>: $S=\left(\mathbb{E}[\mathcal{X}\mathcal{X}_j]\right)^2\leqslant\left(\mathbb{E}[\mathcal{X}_i]\right)^2\left(\mathbb{E}[\mathcal{X}_j]\right)^2$&lt;/li>
&lt;li>&lt;strong>Markov Inequality&lt;/strong>: $\mathbb{P}(\mathcal{X}\geqslant a)\leqslant \mathbb{E}\mathcal{X}/a,\;a&amp;gt;0$&lt;/li>
&lt;li>&lt;strong>Chebychev Inequality&lt;/strong>: $\mathbb{P}(|\mathcal{X}-\mu|\geqslant\delta)\leqslant\sigma^2/\delta^2$&lt;/li>
&lt;li>&lt;strong>Jenson Inequality&lt;/strong>: $\psi(\mathbb{E} \mathcal{X}) \leqslant \mathbb{E}\psi(\mathcal{X})$ for any convex function $\psi$&lt;/li>
&lt;li>&lt;strong>Chernoff bound&lt;/strong>: $\mathbb{P}(\mathcal{X}\geqslant a)\leqslant \underset{s\geqslant 0}{\min},e^{-as}M_ \mathcal{X}(s)$&lt;/li>
&lt;li>&lt;strong>Law of Large Numbers&lt;/strong>: let $\mathcal{X}_i$ be samples drawn from $(\mathbb{R}^n,\mathcal{F}^n,\mathbb{P})$, and $\mathbb{P}$ is such that $\mathcal{X}_k$ has mean $\mu$ and covariance $S$
&lt;ul>
&lt;li>Weak version: if $\mathcal{Y}_ k=\frac{1}{k}\sum^k_{j=1}\mathcal{X}_j$ then $$\underset{k\to\infty}{\lim} \mathbb{P}\left(\left\Vert \mathcal{Y}_k-\mu\right\Vert&amp;gt;\epsilon\right)=0$$&lt;/li>
&lt;li>Strong version: if $\mathcal{Y}_ k=\frac{1}{k}\sum^k_{j=1}\mathcal{X}_j$ then $$\underset{k\to\infty}{\lim} \mathcal{Y}_k=\mu$$&lt;/li>
&lt;li>Central Limit Theorem: if $\mathcal{Y}_ k=\frac{1}{\sqrt{k}}\sum^k_{j=1}S^{-1/2}\left(\mathcal{X}_ j-\mu\right)$ then $$\underset{k\to\infty}{\lim} f_{\mathcal{Y}_k}(y_k)=\mathcal{N}(0,I)$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="estimation-theory">Estimation Theory&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Hilbert Space Projection Theorem&lt;/strong>: A Hilbert space is a complete inner product space. Let $\mathcal{H}$ be a Hilbert space, $\mathcal{M}$ be a closed subspace of $\mathcal{H}$ and $z\in\mathcal{H}$. Then there is a unique $\hat{z}\in\mathcal{M}$ which is closest to $z$: $$\Vert z-\hat{z}\Vert &amp;lt; \Vert z-y\Vert, \forall y\in\mathcal{M}, y\neq\hat{z}$$
&lt;ul>
&lt;li>&lt;strong>Orthogonality Principle&lt;/strong>: $\hat{z}$ is the closest point iff. $\langle z-\hat{z},y\rangle=0, \forall y\in\mathcal{M}$. In estimation we formulate inner product as $\langle \mathcal{X}, \mathcal{Y}\rangle=\mathbb{E}[\mathcal{X}\mathcal{Y}^T]$, it&amp;rsquo;s $\mathbb{E}[(\mathcal{Y}-\mathbb{E}[\mathcal{Y}|\mathcal{X}])h(\mathcal{X})]=0, \forall h(\cdot)$.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Estimation Problem&lt;/strong>: Given random vector \mathcal{X} and random variable $\mathcal{Y}$ with joint PDF $f_{\mathcal{XY}}(\cdot)$, we observe $\mathcal{X}=x$ and we want to form an estimate of $\mathcal{Y}$ as $\hat{\mathcal{Y}}=g(x)$&lt;/li>
&lt;li>&lt;strong>Minimum Mean Square Error(MMSE) Estimation&lt;/strong>: $\hat{\mathcal{Y}}=\mathbb{E}(\mathcal{Y}|\mathcal{X})$
&lt;ul>
&lt;li>Target: minimize $\mathbb{E}[(\mathcal{Y}-\hat{\mathcal{Y}})(\mathcal{Y}-\hat{\mathcal{Y}})^\top]$ with $\hat{\mathcal{Y}}=g(\mathcal{X})$ (arbitary $g(\cdot)$)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Linear Minimum MSE(LMMSE) Estimation&lt;/strong>: $\hat{A}$ satisfies $\mathbb{E}[\mathcal{YX^\top}]=A\mathbb{E}[\mathcal{XX^\top}]$
&lt;ul>
&lt;li>Target: minimize $\mathbb{E}[(\mathcal{Y}-\hat{\mathcal{Y}})(\mathcal{Y}-\hat{\mathcal{Y}})^\top]$ with $\hat{\mathcal{Y}}=A\mathcal{X}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Affine Minimum MSE(AMMSE) Estimation&lt;/strong>: $\hat{G}$ satisfies $\mathbb{E}[\mathcal{YX^\top}]=G\mathbb{E}[\mathcal{XX^\top}]$, $\hat{c}=\mu_{\mathcal{Y}}-\hat{G}\mu_{\mathcal{X}}$
&lt;ul>
&lt;li>Target: minimize $\mathbb{E}[(\mathcal{Y}-\hat{\mathcal{Y}})(\mathcal{Y}-\hat{\mathcal{Y}})^\top]$ with $\hat{\mathcal{Y}}=G\mathcal{X}+c$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="convergence">Convergence&lt;/h2>
&lt;ul>
&lt;li>Categories: Generally we assume $n\to\infty$ and giving $\mathcal{X}_n$ are random variables.
&lt;ul>
&lt;li>&lt;strong>Sure Convergence&lt;/strong> ($\mathcal{X}_n\xrightarrow{\text{surely}}\mathcal{X}$): $$\forall \omega\in\Omega, \mathcal{X}_n(\omega)\xrightarrow{n\to\infty}\mathcal{X}$$&lt;/li>
&lt;li>&lt;strong>Almost Sure Convergence&lt;/strong> ($\mathcal{X}_ n\xrightarrow{\text{a.s./w.p.1}}\mathcal{X}$, &lt;code>w.p.1&lt;/code>: with probability 1): $$\mathbb{P}(\{\omega\in\Omega:\lim_ {n\to\infty}\mathcal{X} _n(\omega)=\mathcal{X}\})=1$$
&lt;ul>
&lt;li>Equivalent definition: $$\mathbb{P}(\bigcup_{\epsilon&amp;gt;0}A(\epsilon))=0\;\text{where}\;A(\epsilon)=\bigcap_{N=1}\bigcup_{n=N}\{\omega:|\mathcal{X}_n(\omega)-\mathcal{X}(\omega)|\geqslant\epsilon\}$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Convergence in Probability&lt;/strong> ($\mathcal{X}_ n\xrightarrow{\text{in prob.}}\mathcal{X}$): $$\mathbb{P}(\{\omega\in\Omega:\lim_ {nn\to\infty}|\mathcal{X}_n(\omega)-\mathcal{X}(\omega)|\geqslant\epsilon\})=0,\;\forall\epsilon&amp;gt;0$$&lt;/li>
&lt;li>&lt;strong>Convergence in Distribution&lt;/strong> ($\mathcal{X}_ n\xrightarrow{\text{D}}\mathcal{X}$): $$\lim_ {n\to\infty} F_{\mathcal{X}_ n}(x)=F_{\mathcal{X}}(x)$$&lt;/li>
&lt;li>&lt;strong>Convergence in mean of order&lt;/strong>: ($\mathcal{X}_ n\xrightarrow{\text{mean r}}\mathcal{X}$, abbr. &lt;code>m.s.&lt;/code> for $r=2$): $$\mathbb{E}[|\mathcal{X}_n-\mathcal{X}|^r]\to 0$$&lt;/li>
&lt;li>$\mathcal{X}_n\xrightarrow{\text{a.s./mean r}}\mathcal{X}\Rightarrow\mathcal{X}_n\xrightarrow{\text{in prob.}}\mathcal{X}\Rightarrow\mathcal{X}_n\xrightarrow{\text{D}}\mathcal{X}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Strong Law of Large Numbers&lt;/strong> (requires iid.): $$S_n=\frac{1}{n}\sum\mathcal{X}_ i\xrightarrow{\text{a.s./m.s.}}\mu_ \mathcal{X}$$&lt;/li>
&lt;li>&lt;strong>Weak Law of Large Numbers&lt;/strong> (requires indentical uncorrelated distributed): $S_n\xrightarrow{\text{in prob.}}\mu_\mathcal{X}$&lt;/li>
&lt;li>&lt;strong>Central Limit Theorem&lt;/strong> (requires independent): $$\mathcal{Y}_n=\frac{1}{\sqrt{n}}\sum\frac{\mathcal{X}_i-m}{\sigma}\xrightarrow{\text{D}}\mathcal{N}(0,1)$$&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Law of Large Numbers and Central Limit Theorem together characterized the limiting behavior of the average of samples. See &lt;a class="link" href="https://en.wikipedia.org/wiki/Central_limit_theorem#Relation_to_the_law_of_large_numbers" target="_blank" rel="noopener"
>Wikipedia&lt;/a> and &lt;a class="link" href="http://www.cs.toronto.edu/~yuvalf/CLT.pdf" target="_blank" rel="noopener"
>a proof&lt;/a> to see their relationships.&lt;/p>
&lt;/blockquote>
&lt;h2 id="miscellaneous-corollaries">Miscellaneous Corollaries&lt;/h2>
&lt;ol>
&lt;li>For random valuable that takes positive values, $\mathbb{E}(X)=\int^\infty_0 \mathbb(\mathcal{X}&amp;gt;x)dx$&lt;/li>
&lt;li>If $\mathcal{X}_1,&amp;hellip;\mathcal{X}_n$ are IID continuous random variables, then $\mathbb{P}(\mathcal{X}_1&amp;lt;\mathcal{X}_2&amp;lt;\ldots&amp;lt;\mathcal{X}_n)=1/n!$&lt;/li>
&lt;li>Define $\mathcal{X}_ {(j)}$ to be the j-th smallest among $\mathcal{X}_ 1,&amp;hellip;\mathcal{X}_ n$. Suppose $\mathcal{X}_ 1,&amp;hellip;\mathcal{X}_ n$ are IID random variables with PDF $f$ and CDF $F$, then $$f_ {\mathcal{X}_ {(j)}}(x)=\frac{n!}{(n-j)!(j-1)!}[F(x)]^{j-1}[1-F(x)]^{n-j}f_ \mathcal{X}(x)$$&lt;/li>
&lt;/ol>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>There is a &lt;a class="link" href="http://www.math.wm.edu/~leemis/chart/UDR/UDR.html" target="_blank" rel="noopener"
>chart about &lt;em>Univariate Distribution Relationships&lt;/em>&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item></channel></rss>