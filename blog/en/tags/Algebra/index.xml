<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Algebra on JacobZ</title><link>https://zyxin.xyz/blog/en/tags/Algebra/</link><description>Recent content in Algebra on JacobZ</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sat, 27 Jun 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://zyxin.xyz/blog/en/tags/Algebra/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes for Algebra Basics</title><link>https://zyxin.xyz/blog/en/2020-06/AlgebraBasicsNotes/</link><pubDate>Sat, 27 Jun 2020 00:00:00 +0000</pubDate><guid>https://zyxin.xyz/blog/en/2020-06/AlgebraBasicsNotes/</guid><description>&lt;blockquote>
&lt;ul>
&lt;li>In this note, $\{x_i\}^b_a$ denotes set $\{x_a, x_{a+1}, \ldots, x_b\}$&lt;/li>
&lt;li>&lt;em>TODO: add Jordan Form&lt;/em>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h2 id="algebraic-structures">Algebraic Structures&lt;/h2>
&lt;h3 id="operation">Operation&lt;/h3>
&lt;ul>
&lt;li>Definition: an (binary, closed) &lt;strong>operation&lt;/strong> $\ast$ on a set $S$ is a mapping of $S\times S\to S$&lt;/li>
&lt;li>&lt;strong>Commutative&lt;/strong>: $x\ast y=y\ast x,\;\forall x,y\in S$&lt;/li>
&lt;li>&lt;strong>Associative&lt;/strong>: $(x\ast y)\ast z=x\ast (y\ast z),\;\forall x,y,z\in S$&lt;/li>
&lt;/ul>
&lt;h3 id="group">Group&lt;/h3>
&lt;ul>
&lt;li>Definition: a &lt;strong>group&lt;/strong> is a pair $(\mathcal{S},\ast)$ with following axioms
&lt;ol>
&lt;li>$\ast$ is associative on $\mathcal{S}$&lt;/li>
&lt;li>(Identity element) $\exists e\in \mathcal{S}\text{ s.t. }x\ast e=e\ast x=x,\;\forall x\in \mathcal{S}$&lt;/li>
&lt;li>(Inverse element) $\forall x\in \mathcal{S}, \exists x&amp;rsquo; \in \mathcal{S}\text{ s.t. }x\ast x&amp;rsquo;=x&amp;rsquo;\ast x=e$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>Abelian&lt;/strong>: a group is called &lt;strong>abelian group&lt;/strong> if $\ast$ is also commutative&lt;/li>
&lt;/ul>
&lt;h3 id="ring">Ring&lt;/h3>
&lt;ul>
&lt;li>Definition: a &lt;strong>ring&lt;/strong> is a triplet $(\mathcal{R},+,\ast)$ consisting of a set of &lt;code>scalars&lt;/code> $\mathcal{R}$ and two operators + and $\ast$ with following axioms
&lt;ol>
&lt;li>$(\mathcal{R},+)$ is an abelian group with identity denoted $0$&lt;/li>
&lt;li>$\forall a,b,c \in \mathcal{R}\text{ s.t. }a\ast(b\ast c) = (a\ast b)\ast c$&lt;/li>
&lt;li>$\exists 1\in\mathcal{R}, \forall a\in\mathcal{R}\text{ s.t. }a\cdot 1=a$&lt;/li>
&lt;li>$\ast$ is distributive over $+$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;h3 id="field">Field&lt;/h3>
&lt;ul>
&lt;li>Definition: a &lt;strong>field&lt;/strong> $(\mathcal{F},+,\ast)$ is a ring where $(\mathcal{F}\backslash\{0\},\ast)$ is also an abelian group.
&lt;blockquote>
&lt;p>Difference from ring to field is that $\ast$ need to be commutative and have a multiplicative inverse&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;h3 id="vector-space">Vector Space&lt;/h3>
&lt;ul>
&lt;li>Definition: a &lt;strong>vector space&lt;/strong> (aka. &lt;strong>linear space&lt;/strong>) is a triplet $(\mathcal{U},\oplus,\cdot)$ defined over a field $(\mathcal{F},+,\ast)$ with following axioms, where set $\mathcal{U}$ is called &lt;code>vectors&lt;/code>, operator $\oplus$ is called &lt;code>vector addition&lt;/code> and mapping $\cdot$ is called &lt;code>scalar multiplication&lt;/code>:
&lt;ol>
&lt;li>(&lt;strong>Null vector&lt;/strong>) $(\mathcal{U},+)$ is an abelian group with identity element $\emptyset$&lt;/li>
&lt;li>Scalar multiplication is a mapping of $\mathcal{F}\times\mathcal{U}\to\mathcal{U}$&lt;/li>
&lt;li>$\alpha\cdot(x\oplus y) = \alpha\cdot x \oplus \alpha\cdot y,\;\forall x,y\in\mathcal{U};\alpha\in\mathcal{F}$&lt;/li>
&lt;li>$(\alpha+\beta)\cdot x = \alpha\cdot x\oplus\beta\cdot x,\;\forall x\in\mathcal{U};\alpha,\beta\in\mathcal{F}$&lt;/li>
&lt;li>$(\alpha\ast\beta)\cdot x=\alpha\cdot(\beta\cdot x),\;\forall x\in\mathcal{U};\alpha,\beta\in\mathcal{F}$&lt;/li>
&lt;li>$1_\mathcal{F}\cdot x=x$&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>Usually we don&amp;rsquo;t distinguish vector addition $\oplus$ and addition of scalar $+$. Juxtaposition is also commonly used for &lt;em>both&lt;/em> scalar multiplication $\cdot$ and multiplication of scalars $\ast$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Subspace&lt;/strong>: a subspace $\mathcal{V}$ of a linear space $\mathcal{U}$ over field $\mathcal{F}$ is a subset of $\mathcal{U}$ which is itself a linear space over $\mathcal{F}$ under same vector addition and scalar multiplication.&lt;/li>
&lt;/ul>
&lt;h4 id="basis--coordinate">Basis &amp;amp; Coordinate&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Linear Independence&lt;/strong>: Let $\mathcal{V}$ be a vector space over $\mathcal{F}$ and let $X=\{x_i\}^n_1\subset \mathcal{V}$
&lt;ul>
&lt;li>X is &lt;strong>linearly dependent&lt;/strong> if $\exists \alpha_1,\ldots,\alpha_n\in\mathcal{F}$ not all 0 s.t. $\sum^n_{i=1} \alpha_i x_i=0$.&lt;/li>
&lt;li>X is &lt;strong>linearly independent&lt;/strong> if $\sum^n_{i=1} \alpha_i x_i=0 \Rightarrow \alpha_1=\alpha_2=\ldots=\alpha_n=0$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Span&lt;/strong>: Given a set of vectors $V$, the set of linear combinations of vectors in $V$ is called the &lt;strong>span&lt;/strong> of it, denoted $\mathrm{span}\{V\}$&lt;/li>
&lt;li>&lt;strong>Basis&lt;/strong>: A set of linearly independent vectors in a linear space $\mathcal{V}$ is a &lt;strong>basis&lt;/strong> if every vector in $\mathcal{V}$ can be expressed as a &lt;em>unique linear combination&lt;/em> of these vectors. (see below &amp;ldquo;Coordinate&amp;rdquo;)
&lt;ul>
&lt;li>Basis Expansion: Let $(X,\mathcal{F})$ be a vector space of dimension n. If $\{v_i\}^k_1,\;1\leqslant k&amp;lt; n$ is linearly independent, then $\exists \{v_i\}^n_{k+1}$ such that $\{v_i\}_1^n$ is a basis.&lt;/li>
&lt;li>&lt;strong>Reciprocal Basis&lt;/strong>: Given basis $\{v_i\}^n_1$, a set ${r_i}^1_n$ that satifies $\langle r_i,v_j \rangle=\delta_i(j)$ is a reciprocal basis. It can be generated by Gram-Schmidt Process and $\forall x\in\mathcal{X}, x=\sum^n_{i=1}\langle r_i,x\rangle v_i$.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Dimension&lt;/strong>: &lt;em>Cardinality&lt;/em> of the basis is called the &lt;strong>dimension&lt;/strong> of that vector space, which is equal to &lt;em>the maximum number of linearly independent vectors&lt;/em> in the space. Denoted as $dim(\mathcal{V})$.
&lt;ul>
&lt;li>In an $n$-dimensional vector space, any set of $n$ linearly independent vectors is a basis.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Coordinate&lt;/strong>: For a vector $x$ in vector space $\mathcal{V}$, given a basis $\{e_1, \ldots, e_n\}$ we can write $x$ as $x=\sum^n_{i=1}\beta_i e_i=E\beta$ where $E=\begin{bmatrix}e_1&amp;amp;e_2&amp;amp;\ldots&amp;amp;e_n\end{bmatrix}$ and $\beta=\begin{bmatrix}\beta_1&amp;amp;\beta_2&amp;amp;\ldots&amp;amp;\beta_n\end{bmatrix}^\top$. Here $\beta$ is called the &lt;strong>representation&lt;/strong> (or &lt;strong>coordinate&lt;/strong>) of $x$ given the basis $E$.&lt;/li>
&lt;/ul>
&lt;h4 id="norm--inner-product">Norm &amp;amp; Inner product&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Inner Product&lt;/strong>: an operator on two vectors that produces a scalar result (i.e. $\langle\cdot,\cdot\rangle:\mathcal{V}\to\mathbb{R}\;or\;\mathbb{C}$) with following axioms:
&lt;ol>
&lt;li>(Symmetry) $\langle x,y \rangle=\overline{\langle y,x\rangle},\;\forall x,y\in\mathcal{V}$&lt;/li>
&lt;li>(Bilinearity) $\langle \alpha x+\beta y,z\rangle=\alpha\langle x,z\rangle+\beta\langle y,z\rangle,\;\forall x,y,z\in\mathcal{V};\alpha,\beta\in\mathbb{C}$&lt;/li>
&lt;li>(Pos. definiteness) $\langle x,x\rangle\geqslant 0,\;\forall x\in\mathcal{V}$ and $\langle x,x\rangle=0\Rightarrow x=0_\mathcal{V}$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>Inner Product Space&lt;/strong>: A linear space with a defined inner product&lt;/li>
&lt;li>&lt;strong>Orthogonality&lt;/strong>:
&lt;ul>
&lt;li>Perpedicularity of vectors ($x\perp y$): $\langle x,y\rangle=0$&lt;/li>
&lt;li>Perpedicularity of a vector to a set ($y\perp\mathcal{S},\mathcal{S}\subset\mathcal{V}$): $y\perp x,\;\forall x\in\mathcal{S}$&lt;/li>
&lt;li>&lt;strong>Orthogonal Set&lt;/strong>: set $\mathcal{S}\subset(\mathcal{U},\langle\cdot,\cdot\rangle)$ is orthogonal $\Leftrightarrow x\perp y,\;\forall x,y\in\mathcal{S},x\neq y$&lt;/li>
&lt;li>&lt;strong>Orthonormal Set&lt;/strong>: set $\mathcal{S}$ is orthonormal iff $\mathcal{S}$ is orthogonal and $\Vert x\Vert=1,\;\forall x\in\mathcal{S}$&lt;/li>
&lt;li>Orthogonality of sets ($\mathcal{X}\perp\mathcal{Y}$): $\langle x,y\rangle=0,\;\forall x\in\mathcal{X};y\in\mathcal{Y}$&lt;/li>
&lt;li>&lt;strong>Orthogonal Complement&lt;/strong>: Let $(\mathcal{V},\langle\cdot,\cdot\rangle)$ be an inner product space and let $\mathcal{U}\subset\mathcal{V}$ be a subspace of $\mathcal{V}$, the orthogonal complement of $\mathcal{U}$ is $\mathcal{U}^\perp=\left\{v\in\mathcal{V}\middle|\langle v,u\rangle=0,\;\forall u\in\mathcal{U}\right\}$.
&lt;ul>
&lt;li>$\mathcal{U}^\perp\subset\mathcal{V}$ is a subspace&lt;/li>
&lt;li>$\mathcal{V}=\mathcal{U}\overset{\perp}{\oplus}\mathcal{U}^\perp$ ($\oplus$: direct sum, $\overset{\perp}{\oplus}$: orthogonal sum)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Norm&lt;/strong>: A &lt;strong>norm&lt;/strong> on a linear space $\mathcal{V}$ is mapping $\Vert\cdot\Vert:\;\mathcal{V}\to\mathbb{R}$ such that:
&lt;ol>
&lt;li>(Positive definiteness) $\Vert x\Vert\geqslant 0\;\forall x\in \mathcal{V}$ and $\Vert x\Vert =0\Rightarrow x=0_\mathcal{V}$&lt;/li>
&lt;li>(Homogeneous) $\Vert \alpha x\Vert=|\alpha|\cdot\Vert x\Vert,\;\forall x\in\mathcal{V},\alpha\in\mathbb{R}$&lt;/li>
&lt;li>(Triangle inequality) $\Vert x+y\Vert\leqslant\Vert x\Vert+\Vert y\Vert$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>Distance&lt;/strong>: Norm can be used to measure distance between two vectors. Meanwhile, distance from a vector to a (sub)space is defined as $d(x,\mathcal{S})=\inf_{y\in\mathcal{S}} d(x,y)=\inf_{y\in\mathcal{S}} \Vert x-y\Vert$
&lt;ul>
&lt;li>&lt;strong>Projection Point&lt;/strong>: $x^* =\arg\min_{y\in\mathcal{S}}\Vert x-y\Vert$ is the projection point of $x$ on linear space $\mathcal{S}$.&lt;/li>
&lt;li>&lt;strong>Projection Theorem&lt;/strong>: $\exists !x^* \in\mathcal{S}$ s.t. $\Vert x-x^* \Vert=d(x,\mathcal{S})$ and we have $(x-x^*) \perp\mathcal{S}$&lt;/li>
&lt;li>&lt;strong>Orthogonal Projection&lt;/strong>: $P(x)=x^*:\mathcal{X}\to\mathcal{M}$ is called the orthogonal projection of $\mathcal{X}$ onto $\mathcal{M}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Normed Space&lt;/strong>: A linear space with a defined norm $\Vert\cdot\Vert$, denoted $(\mathcal{V},\mathcal{F},\Vert\cdot\Vert)$
&lt;blockquote>
&lt;p>A inner product space is always a normed space because we can define $\Vert x\Vert=\sqrt{\langle x,x\rangle}$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>Common $\mathbb{R}^n$ Norms:
&lt;ul>
&lt;li>Euclidean norm (2-norm): $\Vert x\Vert_2=\left(\sum^n_{i=1}|x_i|^2\right)^{1/2}=\left\langle x,x\right\rangle^{1/2}=\left(x^\top x\right)^{1/2}$&lt;/li>
&lt;li>$l_p$ norm (p-norm): $\Vert x\Vert_p=\left(\sum^n_{i=1}|x_i|^p\right)^{1/p}$&lt;/li>
&lt;li>$l_1$ norm: $\Vert x\Vert_1=\sum^n_{i=1}|x_i|$&lt;/li>
&lt;li>$l_\infty$ norm: $\Vert x\Vert_\infty=\max_{i}\{x_i\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Common matrix norms:
&lt;blockquote>
&lt;p>Matrix norms are also called &lt;strong>operator norms&lt;/strong>, can measure how much a linear operator &amp;ldquo;magnifies&amp;rdquo; what it operates on.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>A general form induced from $\mathbb{R}^n$ norm: $$\Vert A\Vert=\sup_{x\neq 0}\frac{\Vert Ax\Vert}{\Vert x\Vert}=\sup_{\Vert x\Vert=1}\Vert Ax\Vert$$&lt;/li>
&lt;li>$\Vert A\Vert_1=\max_j\left(\sum^n_{i=1}|a_{ij}|\right)$&lt;/li>
&lt;li>$\Vert A\Vert_2=\left[ \max_{\Vert x\Vert=1}\left\{(Ax)^* (Ax)\right\}\right]^{1/2}=\left[ \lambda_{max}(A^ *A)\right]^{1/2}$ ($\lambda_{max}$: largest eigenvalue)&lt;/li>
&lt;li>$\Vert A\Vert_\infty=\max_i\left(\sum^n_{j=1}|a_{ij}|\right)$&lt;/li>
&lt;li>(Frobenius Norm) $\Vert A\Vert_F=\left[ \sum^m_{i=1}\sum^n_{j=1}\left|a_{ij}\right|^2\right]^{1/2}=\left[ tr(A^*A)\right]^{1/2}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Useful inequations:
&lt;ul>
&lt;li>&lt;strong>Cauchy-Schwarz&lt;/strong>: $|\langle x,y\rangle|\leqslant\left\langle x,x\right\rangle^{1/2}\cdot\left\langle y,y\right\rangle^{1/2}$&lt;/li>
&lt;li>&lt;strong>Triangle&lt;/strong> (aka. $\Delta$): $\Vert x+y\Vert\leqslant\Vert x\Vert+\Vert y\Vert$
&lt;blockquote>
&lt;p>Lemma: $\Vert x-y\Vert \geqslant \left| \Vert x\Vert-\Vert y\Vert \right|$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Pythagorean&lt;/strong>: $x\perp y \Leftrightarrow \Vert x+y\Vert=\Vert x\Vert+\Vert y\Vert$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="gramian">Gramian&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Gram-Schmidt Process&lt;/strong>: A method to find orthogonal basis $\{v_i\}^n_1$ given an ordinary basis $\{y_i\}^n_1$. It&amp;rsquo;s done by perform $v_k=y_k-\sum^{k-1}_{j=1}\frac{\langle y_k,v_j\rangle}{\langle v_j,v_j \rangle}\cdot v_j$ iteratively from 1 to $n$. To get an orthonormal basis, just normalize these vectors.&lt;/li>
&lt;li>&lt;a class="link" href="https://en.wikipedia.org/wiki/Gramian_matrix" target="_blank" rel="noopener"
>&lt;strong>Gram Matrix&lt;/strong>&lt;/a>: The Gram matrix generated from vectors $\{y_i\}_ 1^k$ is denoted $G(y_ 1,y_ 2,\ldots,y_ k)$. Its element $G_{ij}=\langle y_i,y_j\rangle$
&lt;ul>
&lt;li>&lt;strong>Gram Determinant&lt;/strong>: $g(y_1,y_2,\ldots,y_n)=\det G$&lt;/li>
&lt;li>&lt;strong>Normal Equations&lt;/strong>: Given subspace $\mathcal{M}$ and its basis $\{y_i\}^n_1$, the projection point of $\forall x\in\mathcal{M}$ can be represented by $$x^*=\alpha y=\begin{bmatrix}\alpha_1&amp;amp;\alpha_2&amp;amp;\ldots&amp;amp;\alpha_n\end{bmatrix}\begin{bmatrix}y_1\\y_2\\ \vdots \\y_n\end{bmatrix},\;\beta=\begin{bmatrix}\langle x,y_1\rangle\\ \langle x,y_2\rangle\\ \vdots\\ \langle x,y_n\rangle\end{bmatrix} where\;G^\top\alpha=\beta$$
&lt;blockquote>
&lt;p>For least-squares problem $Ax=b$, consider $\mathcal{M}$ to be the column space of $A$, then $G=A^\top A,\;\beta=A^\top b,\;G^\top\alpha=\beta\Rightarrow\alpha=(A^\top A)^{-1}A^\top b$. Similarly for weighted least-squares problem ($\Vert x\Vert=x^\top Mx$), let $G=A^\top MA, \beta=A^\top Mb$, we can get $\alpha=(A^\top MA)^{-1}A^\top Mb$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="linear-algebra">Linear Algebra&lt;/h2>
&lt;h3 id="linear-operator">Linear Operator&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>Definition: a linear operator $\mathcal{A}$ (aka. linear transformation, linear mapping) is a function $f: V\to U$ that operate on a linear space $(\mathcal{V},\mathcal{F})$ to produce elements in another linear space $(\mathcal{U},\mathcal{F})$ and obey $$\mathcal{A}(\alpha_1 x_1+\alpha_2 x_2) = \alpha_1\mathcal{A}(x_1) + \alpha_2\mathcal{A}(x_2),\;\forall x_1,x_2\in V;\alpha_1, \alpha_2\in\mathcal{F}$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Range (Space)&lt;/strong>: $\mathcal{R}(\mathcal{A})=\left\{u\in U\middle|\mathcal{A}(v)=u,\;\forall v\in V\right\}$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Null Space&lt;/strong> (aka. &lt;strong>kernel&lt;/strong>): $\mathcal{N}(\mathcal{A})=\left\{v\in V\middle|\mathcal{A}(v)=\emptyset_U\right\}$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>$\mathcal{A}$-invariant subspace&lt;/strong>: Given vector space $(\mathcal{V},\mathcal{F})$ and linear operator $\mathcal{A}:\mathcal{V}\rightarrow \mathcal{V}$, $\mathcal{W}\subseteq\mathcal{V}$ is $A$-invariant if $\forall x\in\mathcal{W}$, $\mathcal{A}x\in\mathcal{W}$.&lt;/p>
&lt;ul>
&lt;li>Both $\mathcal{R}(\mathcal{A})$ and $\mathcal{N}(\mathcal{A})$ are $\mathcal{A}$-invariant&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Matrix Representation: Given bases for both $V$ and $U$ (respectively $\{v_i\}^n_1$ and $\{u_j\}^m_1$), matrix representation $A$ satisfies $\mathcal{A}(v_i)=\sum^m_{j=0}A_{ji}u_j$ so that $\beta=A\alpha$ where $\alpha$ and $\beta$ is the representation of a vector under $\{v_i\}$ and $\{u_j\}$ respectively.
{% asset_img linear_map_relations.png Relation between a linear map and its matrix
representations %}&lt;/p>
&lt;ul>
&lt;li>$P$ and $Q$ are change of basis matrices, $A=Q^{-1}\tilde{A}P,\;\tilde{A}=QAP^{-1}$&lt;/li>
&lt;li>The i-th column of $A$ is the coordinates of $\mathcal{A}(v_i)$ represented by the basis $\{u_j\}$, similarly i-th column of $\tilde{A}$ is $\mathcal{A}(\tilde{v}_i)$ represented in $\{\tilde{u}_j\}$&lt;/li>
&lt;li>The i-th column of $P$ is the coordinates of $v_i$ represented by the basis $\{\tilde{v}\}$, similarly i-th column of $Q$ is $u_j$ represented in $\{\tilde{u}\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Matrix Similarity ($A\sim B$): Two (square) matrix representations ($A,B$) of the same linear operator are called &lt;strong>similar&lt;/strong> (or &lt;strong>conjugate&lt;/strong>) and they satisfies $\exists P$ s.t. $B=PAP^{-1}$.&lt;/p>
&lt;blockquote>
&lt;p>From now on we don&amp;rsquo;t distinguish between linear operator $\mathcal{A}$ and its matrix representation where choice of basis doesn&amp;rsquo;t matter.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Rank&lt;/strong>: $rank(A)=\rho(A)\equiv dim(\mathcal{R}(A))$&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Sylvester&amp;rsquo;s Inequality&lt;/strong>: $\rho(A)+\rho(B)-n\leqslant \rho(AB)\leqslant \min\{\rho(A), \rho(B)\}$&lt;/li>
&lt;li>&lt;strong>Singularity&lt;/strong>: $\rho(A)&amp;lt; n$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Nullity&lt;/strong>: $null(A)=\nu(A)\equiv dim(\mathcal{N}(A))$&lt;/p>
&lt;ul>
&lt;li>$\rho(A)+\nu(A)=n$ ($n$ is the dimensionality of domain space)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Adjoint&lt;/strong>: The adjoint of the linear map $\mathcal{A}: \mathcal{V}\to\mathcal{W}$ is the linear map $\mathcal{A}^*: \mathcal{W}\to\mathcal{V}$ such that $\langle y,\mathcal{A}(x)\rangle_\mathcal{W}=\langle \mathcal{A}^ *(y),x\rangle_\mathcal{V}$&lt;/p>
&lt;blockquote>
&lt;p>For its matrix representation, adjoint of $A$ is $A^ *$, which is $A^\top$ for real numbers.&lt;br>
Properties of $\mathcal{A}^ *$ is similar to matrix $A^ *$&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>$\mathcal{U}=\mathcal{R}(A)\overset{\perp}{\oplus}\mathcal{N}(A^ *),\;\mathcal{V}=\mathcal{R}(A^ *)\overset{\perp}{\oplus}\mathcal{N}(A)$&lt;/li>
&lt;li>$\mathcal{N}(A^* )=\mathcal{N}(AA^* )\subseteq\mathcal{U},\;\mathcal{R}(A)=\mathcal{R}(AA^*)\subseteq\mathcal{U}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Self-adjoint&lt;/strong>: $\mathcal{A}$ is self-adjoint iff $\mathcal{A}^*=\mathcal{A}$.&lt;/p>
&lt;ul>
&lt;li>For self-adjoint $\mathcal{A}$, if $\mathcal{V}=\mathbb{C}^{n\times n}$ then $A$ is &lt;strong>hermitian&lt;/strong>; if $\mathcal{V}=\mathbb{R}^{n\times n}$ then $A$ is &lt;strong>symmetric&lt;/strong>.&lt;/li>
&lt;li>Self-adjoint matrices have real eigenvalues and orthogonal eigenvectors&lt;/li>
&lt;li>&lt;strong>Skew symmetric&lt;/strong>: $A^*=-A$
&lt;blockquote>
&lt;p>For quadratic form $x^\top Ax=x^\top(\frac{A+A^\top}{2}+\frac{A-A^\top}{2})x$, since $A-A^\top$ is skew symmetric, scalar $x^\top (A-A^\top) x=-x^\top (A-A^\top)x$, so the skew-symmetric part is zero. Therefore for quadratic form $x^\top Ax$ we can always assume $A$ is symmetric.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Definiteness&lt;/strong>: (for symmetric matrix $P$)&lt;/p>
&lt;ul>
&lt;li>Positive definite ($P\succ 0$): $\forall x\in\mathbb{R}^n\neq 0,\; x^\top Px&amp;gt;0 \Leftrightarrow$ all eigenvalues of $P$ are positive.&lt;/li>
&lt;li>Semi-positive definite ($P\succcurlyeq 0$): $x^\top Px\geqslant 0 \Leftrightarrow$ all eigenvalues of $P$ are non-negative.&lt;/li>
&lt;li>Negative definite ($P\prec 0$): $x^\top Px &amp;lt; 0 \Leftrightarrow$ all eigenvalues of $P$ are negative.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Orthogonal Matrix&lt;/strong>: $Q$ is orthogonal iff $Q^\top Q=I$, iff columns of $Q$ are orthonormal.&lt;/p>
&lt;ul>
&lt;li>If $A\in\mathbb{R}^{n\times b}$ is symmetric, then $\exists$ orthogonal $Q$ s.t. $Q^\top AQ=\Lambda=\mathrm{diag}\{\lambda_1,\ldots,\lambda_n\}$ (see &lt;a class="link" href="#Eigendecomposition-and-Jordan-Form" >Eigen-decomposition&lt;/a> section below)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Orthogonal Projection&lt;/strong>: Given linear space $\mathcal{X}$ and subspace $\mathcal{M}$, $P(x)=x^*:\mathcal{X}\to\mathcal{M}$ ($x^ *$ is the projection point) is called orthogonal projection. If $\{v_i\}$ is a orthonormal basis of $\mathcal{M}$, then $P(x)=\sum_i \langle x,v_i\rangle v_i$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="eigenvalue-and-canonical-forms">Eigenvalue and Canonical Forms&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Eigenvalue&lt;/strong> and &lt;strong>Eigenvector&lt;/strong>: Given mapping $\mathcal{A}:\mathcal{V}\rightarrow\mathcal{V}$, if $\exists \lambda\in\mathcal{F}, v\neq \emptyset_{\mathcal{V}}\in\mathcal{V}$ s.t. $\mathcal{A}(v) = \lambda v$, then $\lambda$ is the &lt;strong>eigenvalue&lt;/strong>, $v$ is the &lt;strong>eigenvector&lt;/strong> (aka. &lt;strong>spectrum&lt;/strong>).
&lt;ul>
&lt;li>If eigenvalues are all distinct, then the associated eigenvectors form a basis.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Eigenspace&lt;/strong>: $\mathcal{N}_\lambda = \mathcal{N}(\mathcal{A}-\lambda \mathcal{I})$.
&lt;ul>
&lt;li>$q=dim(\mathcal{N}_\lambda)$ is called the &lt;strong>geometric multiplicity&lt;/strong> (几何重度)&lt;/li>
&lt;li>$\mathcal{N}_\lambda$ is an $\mathcal{A}$-invariant subspace.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Characteristic Polynomial&lt;/strong>: $\phi(s)\equiv\mathcal{det}(A-s I)$ is a polynomial of degree $n$ in $s$
&lt;ul>
&lt;li>Its solutions are the eigenvalues of $A$.&lt;/li>
&lt;li>The multiplicity $m_i$ of root term $(s-\lambda_i)$ here is called &lt;strong>algebraic multiplicity&lt;/strong> (代数重度) of $\lambda_i$.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Cayley-Hamilton Theorem&lt;/strong>: $\phi(A)=\mathbf{0}$
&lt;blockquote>
&lt;p>Proof needs the eigendecomposition or Jordan decomposition descibed below&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Minimal Polynomial&lt;/strong>: $\psi(s)$ is the minimal polynomial of $A$ iff $\psi(s)$ is the polynomial of least degree for which $\psi(A)=0$ and $\psi$ is monic (coefficient of highest order term is 1)
&lt;ul>
&lt;li>The multiplicity $\eta_i$ of root term $(s-\lambda_i)$ here is called the &lt;strong>index&lt;/strong> of $\lambda_i$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Eigendecomposition&lt;/strong> (aka. &lt;strong>Spectral Decomposition&lt;/strong>) is directly derived from the definition of eigenvalues: $$A=Q\Lambda Q^{-1}, \Lambda=\mathrm{diag}\left\{\lambda_1,\lambda_2,\ldots,\lambda_n\right\}$$
where $Q$ is a square matrix whose $i$-th column is the eigenvector $q_i$ corresponding to eigenvalue $\lambda_i$.
&lt;ul>
&lt;li>Feasibility: $A$ can be diagonalized (using eigendecomposition) iff. $q_i=m_i$ for all $\lambda_i$.&lt;/li>
&lt;li>If $A$ has $n$ distinct eigenvalues, then $A$ can be diagonalized.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Generalized eigenvector&lt;/strong>: A vector $v$ is a generalized eigenvector of rank $k$ associated with eigenvalue $\lambda$ iff $v\in\mathcal{N}\left((A-\lambda I)^k\right)$ but $v\notin\mathcal{N}\left((A-\lambda I)^{k-1}\right)$
&lt;ul>
&lt;li>If $v$ is a generalized eigenvector of rank $k$, $(A-\lambda I)v$ is a generalized eigenvector of rank $k-1$. This creates a chain of generalized eigenvectors (called &lt;strong>Jordan Chain&lt;/strong>) from rank $k$ to $1$, and they are linearly independent.&lt;/li>
&lt;li>$\eta$ (index, 幂零指数) of $\lambda$ is the smallest integer s.t. $dim\left(\mathcal{N}\left((A-\lambda I)^\eta\right)\right)$&lt;/li>
&lt;li>The space spanned by the chain of generalized eigenvectors from rank $\eta$ is called the &lt;strong>generalized eigenspace&lt;/strong> (with dimension $\eta$).&lt;/li>
&lt;li>Different generalized eigenspaces associated with the same and with different eigenvalues are orthogonal.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Jordan Decomposition&lt;/strong>: Similar to eigendecomposition, but works for all square matrices. $A=PJP^{-1}$ where $J=\mathrm{diag}\{J_1,J_2,\ldots,J_p\}$ is the &lt;strong>Jordan Form&lt;/strong> of A consisting of Jordan Blocks.
&lt;ul>
&lt;li>&lt;strong>Jordan Block&lt;/strong>: $J_i=\begin{bmatrix} \lambda &amp;amp; 1 &amp;amp;&amp;amp;&amp;amp; \\&amp;amp;\lambda&amp;amp;1&amp;amp;&amp;amp;\\&amp;amp;&amp;amp;\lambda&amp;amp;\ddots&amp;amp;\\&amp;amp;&amp;amp;&amp;amp;\ddots&amp;amp;1\\&amp;amp;&amp;amp;&amp;amp;&amp;amp;\lambda\end{bmatrix}$&lt;/li>
&lt;li>Each Jordan block corresponds to a generalized eigenspace&lt;/li>
&lt;li>$q_i$ = the count of Jordan blocks associated with $\lambda_i$&lt;/li>
&lt;li>$m_i$ = the count of $\lambda_i$ on diagonal of $J$&lt;/li>
&lt;li>$\eta_i$ = the dimension of the largest Jordan block associated with $\lambda_i$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>$\Lambda$ in eigendecomposition, $J$ in Jordan Form and $\Sigma$ in SVD (see below) are three kinds of &lt;strong>&lt;a class="link" href="https://en.wikipedia.org/wiki/Canonical_form#Linear_algebra" target="_blank" rel="noopener"
>Canonical Forms&lt;/a>&lt;/strong> of a matrix $A$&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Function of matrics&lt;/strong>: Let $f(\cdot)$ be an analytic function and $\lambda_i$ be an eigenvalue of $A$. If $p(\cdot)$ is a polynomial that satisfies $p(\lambda_i)=f(\lambda_i)$ and $\frac{\mathrm{d}^k}{\mathrm{d}s^k} p(\lambda_i)=\frac{\mathrm{d}^k}{\mathrm{d}s^k} f(\lambda_i)$ for $k=1,\ldots,\eta_i-1$, then $f(A)\equiv p(A)$.
&lt;blockquote>
&lt;ul>
&lt;li>This extends the functions applicable to matrics from polynomials (trivial) to any analytical functions&lt;/li>
&lt;li>By Cayley-Hamilton, we can always choose $p$ to be order $n-1$&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Sylvester&amp;rsquo;s Formula&lt;/strong>: $f(A)=\sum^k_{i=1}f(\lambda_i)A_i$ ($f$ being analytic)&lt;/li>
&lt;/ul>
&lt;h3 id="svd-and-linear-equations">SVD and Linear Equations&lt;/h3>
&lt;p>SVD Decomposition is useful in various fields and teached by a lot of courses, its complete version is formulated as $$A=U\Sigma V^*, \Sigma=\begin{bmatrix}\mathbf{\sigma}&amp;amp;\mathbf{0}\\ \mathbf{0}&amp;amp;\mathbf{0}\end{bmatrix}, \mathbf{\sigma}=\mathrm{diag}\left\{\sqrt{\lambda_1},\sqrt{\lambda_2},\ldots,\sqrt{\lambda_r}\right\},V=\begin{bmatrix}V_1&amp;amp;V_2\end{bmatrix},U=\begin{bmatrix}U_1&amp;amp;U_2\end{bmatrix}$$
where&lt;/p>
&lt;ul>
&lt;li>$r=\rho(A)$ is the rank of matrix $A$&lt;/li>
&lt;li>$\sigma_i$ are called &lt;strong>sigular values&lt;/strong>, $\lambda_i$ are eigenvalues of $A^* A$&lt;/li>
&lt;li>Columns of $V_1$ span $\mathcal{R}(A^ *A)=\mathcal{R}(A^ *)$, columns of $V_2$ span $\mathcal{N}(A^ *A)=\mathcal{N}(A)$&lt;/li>
&lt;li>Columns of $U_1=AV_1\sigma^{-1}$ span $\mathcal{R}(A)$, columns of $U_2$ span $\mathcal{N}(A^*)$&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>SVD can be derived by doing eigenvalue decomposition on $A^* A$&lt;/p>
&lt;/blockquote>
&lt;p>With SVD introduced, we can efficiently solve general linear equation $Ax=b$ as $x=x_r+x_n$ where $x_r\in\mathcal{R}(A^\top)$ and $x_n\in\mathcal{N}(A)$.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>$Ax=b$&lt;/th>
&lt;th>tall $A$ ($m&amp;gt;n$)&lt;/th>
&lt;th>fat $A$ ($m&amp;lt; n$)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Overdetermined, &lt;br> Least Squares, &lt;br> use Normal Equations&lt;/td>
&lt;td>Underdetermined, &lt;br> Quadratic Programming, &lt;br> use Lagrange Multiplies&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>I.$b\in\mathcal{R}(A)$&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1.$\mathcal{N}(A)={0}$&lt;/td>
&lt;td>$x$ exist &amp;amp; is unique&lt;/td>
&lt;td>$x=(A^\top A)^{-1}A^\top b=A^+b$&lt;/td>
&lt;td>$x=A^\top(AA^\top)^{-1}b=A^+b$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2.$\mathcal{N}(A)\neq{0}$&lt;/td>
&lt;td>$x$ exist &amp;amp; not unique&lt;/td>
&lt;td>$x_r=(A^\top A)^{-1}A^\top b=A^+b$&lt;/td>
&lt;td>$x_r=A^\top(AA^\top)^{-1}b=A^+b$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>II.$b\notin\mathcal{R}(A)$&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1.$\mathcal{N}(A)={0}$&lt;/td>
&lt;td>$x$ not exists, $x_r$ exist &amp;amp; is unique&lt;/td>
&lt;td>$x_r=(A^\top A)^{-1}A^\top b=A^+b$&lt;/td>
&lt;td>$x_r=A^\top(AA^\top)^{-1}b=A^+b$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2.$\mathcal{N}(A)\neq{0}$&lt;/td>
&lt;td>$x$ not exists, $x_r$ not exist&lt;/td>
&lt;td>$(A^\top A)^{-1}$ invertible&lt;/td>
&lt;td>$(AA^\top)^{-1}$ invertible&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>$A^+=(A^\top A)^{-1}A^\top$ is left pseudo-inverse, $A^+=A^\top (AA^\top)^{-1}$ is right pseudo-inverse.&lt;/li>
&lt;li>$A^+$ can be unified by the name &lt;strong>Moore-Penrose Inverse&lt;/strong> and calculated using SVD by $A^+=V\Sigma^+ U^\top$ where $\Sigma^+$ take inverse of non-zeros.&lt;/li>
&lt;/ul>
&lt;h3 id="miscellaneous">Miscellaneous&lt;/h3>
&lt;blockquote>
&lt;p>Selected theorems and lemmas useful in Linear Algebra. For more matrix properties see &lt;a class="link" href="https://zyxin.xyz/blog/en/2019-06/MatrixAlgebra/" >my post about Matrix Algebra&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>Matrix Square Root: $N^\top N=P$, then $N$ is the square root of $P$
&lt;blockquote>
&lt;p>Square root is not unique. Cholesky decomposition is often used as square root.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Schur Complement&lt;/strong>: Given matrices $A_{n\times n}, B_{n\times m}, C_{m\times m}$, the matrix $M=\begin{bmatrix}A&amp;amp;B\\ B^\top&amp;amp;C\end{bmatrix}$ is symmetric. Then the following are equivalent (TFAE)
&lt;ol>
&lt;li>$M\succ 0$&lt;/li>
&lt;li>$A\succ 0$ and $C-B^\top A^{-1}B\succ 0$ (LHS called Schur complement of $A$ in $M$)&lt;/li>
&lt;li>$C\succ 0$ and $A-B C^{-1}B^\top\succ 0$ (LHS called Schur complement of $C$ in $M$)&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Matrix Inverse Lemma: $(A+BCD)^{-1}=A^{-1}-A^{-1}B\left(C^{-1}+DA^{-1}B\right)^{-1}DA$&lt;/li>
&lt;li>Properties of $A^\top A$
&lt;ul>
&lt;li>$A^\top A \succeq 0$ and $A^\top A \succ 0 \Leftrightarrow A$ has full rank.&lt;/li>
&lt;li>$A^\top A$ and $AA^\top$ have same non-zero eigenvalues, but different eigenvectors.&lt;/li>
&lt;li>If $v$ is eigenvector of $A^\top A$ about $\lambda$, then $Av$ is eigenvector of $AA^\top$ about $\lambda$.&lt;/li>
&lt;li>If $v$ is eigenvector of $AA^\top$ about $\lambda$, then $A^\top v$ is eigenvector of $A^\top A$ about $\lambda$.&lt;/li>
&lt;li>$tr(A^\top A)=tr(AA^\top)=\sum_i\sum_j\left|A_{ij}\right|^2$&lt;/li>
&lt;li>$det(A)=\prod_i\lambda_i, tr(A)=\sum_i\lambda_i$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="real-analysis">Real Analysis&lt;/h2>
&lt;h3 id="set-theory">Set theory&lt;/h3>
&lt;blockquote>
&lt;p>$\text{~}S$ stands for complement of set $S$ in following contents. These concepts are discussed under normed space $(\mathcal{X}, \Vert\cdot\Vert)$&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Open Ball&lt;/strong>: Let $x_0\in\mathcal{X}$ and let $a\in\mathbb{R}, a&amp;gt;0$, then the open ball of radius $a$ about $x_0$ is $B_a(x_0)=\left\{x\in\mathcal{X}\middle| \Vert x-x_0\Vert &amp;lt; a\right\}$
&lt;ul>
&lt;li>Given subset $S\subset \mathcal{X}$, $d(x,S)=0\Leftrightarrow \forall\epsilon &amp;gt;0, B_\epsilon(x)\cap S\neq\emptyset$&lt;/li>
&lt;li>Given subset $S\subset \mathcal{X}$, $d(x,S)&amp;gt;0\Leftrightarrow \exists\epsilon &amp;gt;0, B_\epsilon(x)\cap S=\emptyset$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Interior Point&lt;/strong>: Given subset $S\subset\mathcal{X}$, $x\in S$ is an interior point of $S$ iff $\exists\epsilon &amp;gt;0, B_\epsilon(x)\subset S$
&lt;ul>
&lt;li>&lt;strong>Interior&lt;/strong>: $\mathring{S}=\{x\in \mathcal{X}|x\text{ is an interior point of }S\}=\{x\in\mathcal{X}|d(x,\text{~}S)&amp;gt;0\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Open Set&lt;/strong>: $S$ is open if $\mathring{S}=S$&lt;/li>
&lt;li>&lt;strong>Closure Point&lt;/strong>: Given subset $S\subset\mathcal{X}$, $x\in S$ is a closure point of $S$ iff $\forall\epsilon &amp;gt;0, B_\epsilon(x)\cap S\neq\emptyset$.
&lt;ul>
&lt;li>&lt;strong>Closure&lt;/strong>: $\bar{S}=\{x\in\mathcal{X}|x\text{ is a closure point of }S\}=\{x\in\mathcal{X}|d(x,S)=0\}$
&lt;blockquote>
&lt;p>Note that $\partial\mathcal{X}=\emptyset$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Closed Set&lt;/strong>: $S$ is closed if $\bar{S}=S$
&lt;blockquote>
&lt;p>$S$ is open $\Leftrightarrow$ $\text{~}S$ is closed, $S$ is closed $\Leftrightarrow$ $\text{~}S$ is open. Set being both open and closed is called &lt;strong>clopen&lt;/strong>(e.g. the whole set $\mathcal{X}$), empty set is clopen by convention.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Set Boundary&lt;/strong>: $\partial S=\bar{S}\cap\overline{\text{~}S}=\bar{S}\backslash\mathring{S}$&lt;/li>
&lt;/ul>
&lt;h3 id="sequences">Sequences&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Sequence&lt;/strong>($\{x_n\}$): a set of vectors indexed by the counting numbers
&lt;ul>
&lt;li>&lt;strong>Subsequence&lt;/strong>: Let $1\leqslant n_1&amp;lt; n_2&amp;lt;\ldots$ be an infinite set of increasing integers, then $\{x_{n_i}\}$ is a subsequence of $\{x_n\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Convergence&lt;/strong>($\{x_n\}\to x\in\mathcal{X}$): $\forall \epsilon&amp;gt;0,\exists N(\epsilon)&amp;lt;\infty\text{ s.t. }\forall n\geqslant N, \Vert x_n-x\Vert &amp;lt;\epsilon$
&lt;ul>
&lt;li>If $x_n \to x$ and $x_n \to y$, then $x=y$&lt;/li>
&lt;li>If $x_n \to x_0$ and $\{x_{n_i}\}$ is a subsequence of $\{x_n\}$, then $\{x_{n_i}\} \to x_0$&lt;/li>
&lt;li>&lt;strong>Cauchy Convergence&lt;/strong> (necessary condition for convergence): $\{x_n\}$ is cauchy if $\forall \epsilon&amp;gt;0,\exists N(\epsilon)&amp;lt;\infty$ s.t. $\forall n,m\geqslant N, \Vert x_n-x_m\Vert &amp;lt;\epsilon$&lt;/li>
&lt;li>If $\mathcal{X}$ is finite dimensional, $\{x_n\}$ is cauchy $\Rightarrow$ $\{x_n\}$ has a limit in $\mathcal{X}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Limit Point&lt;/strong>: Given subset $S\subset\mathcal{X}$, $x$ is a limit point of $S$ if $\exists \{x_n\}$ s.t. $\forall n\geqslant 1, x_n\in S$ and $x_n\to x$
&lt;ul>
&lt;li>$x$ is a limit point of $S$ iff $x\in\bar{S}$&lt;/li>
&lt;li>$S$ is closed iff $S$ contains its limit points&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Complete Space&lt;/strong>: a normed space is &lt;strong>complete&lt;/strong> if every Cauchy sequence has a limit. A complete normed space $(\mathcal{X}, \Vert\cdot\Vert)$ is called a &lt;strong>Banach space&lt;/strong>.
&lt;ul>
&lt;li>$S\subset \mathcal{X}$ is complete if every Cauchy sequence with elements from $S$ has a limit in $S$&lt;/li>
&lt;li>$S\subset \mathcal{X}$ is complete $\Rightarrow S$ is closed&lt;/li>
&lt;li>$\mathcal{X}$ is complete and $S\subset\mathcal{X} \Rightarrow S$ is complete&lt;/li>
&lt;li>All finite dimensional subspaces of $X$ are complete&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Completion of Normed Space&lt;/strong>: $\mathcal{Y}=\bar{\mathcal{X}}=\mathcal{X}+\{$all limit points of Cauchy sequences in $\mathcal{X}\}$
&lt;blockquote>
&lt;p>E.g. $C[a,b]$ contains continuous functions over $[a,b]$. $(C[a,b], \Vert\cdot\Vert_1)$ is not complete, $(C[a,b], \Vert\cdot\Vert_\infty)$ is complete. Completion of $(C[a,b], \Vert\cdot\Vert_1)$ requires Lebesque integration.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Contraction Mapping&lt;/strong>: Let $S\subset\mathcal{X}$ be a subset and $T:S\to S$ is a contraction mapping if $\exists 0\leqslant c\leqslant 1$ such that, $\forall x,y \in S, \Vert T(x)-T(y)\Vert\leqslant c\Vert x-y\Vert$
&lt;ul>
&lt;li>&lt;strong>Fixed Point&lt;/strong>: $x^* \in\mathcal{X}$ is a fixed point of $T$ if $T(x^ *)=x^ *$&lt;/li>
&lt;li>&lt;a class="link" href="https://en.wikipedia.org/wiki/Banach_fixed-point_theorem" target="_blank" rel="noopener"
>&lt;strong>Contraction Mapping Theorem&lt;/strong> (不动点定理)&lt;/a>: If $T:S\to S$ is a contraction mapping in a complete subset $S$, then $\exists! x^ *\in\mathcal{X}\text{ s.t. }T(x^ *)=x^ *$. Moreover, $\forall x_0\in S$, the sequence $x_{k+1}=T(x_k),k\geqslant 0$ is Cauchy and converges to $x^ *$.
&lt;blockquote>
&lt;p>E.g. Newton Method: $x_{k+1}=x_k-\epsilon\left[\frac{\partial h}{\partial x}(x_k)\right]^{-1}\left(h(x_k)-y\right)$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="continuity-and-compactness">Continuity and Compactness&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Continuous&lt;/strong>: Let $(\mathcal{X},\Vert\cdot\Vert_\mathcal{X})$ and $(\mathcal{Y},\Vert\cdot\Vert_\mathcal{Y})$ be two normed spaces. A function $f:\mathcal{X}\to\mathcal{Y}$ is continuous at $x_0\in\mathcal{X}$ if $\forall\epsilon &amp;gt;0,\exists \delta(\epsilon,x_0)&amp;gt;0\text{ s.t. }\Vert x-x_0\Vert_\mathcal{X}&amp;lt;\delta \Rightarrow\Vert f(x)-f(x_0)\Vert_\mathcal{Y} &amp;lt;\epsilon$
&lt;ul>
&lt;li>$f$ is continuous on $S\subset\mathcal{X}$ if $f$ is continuous at $\forall x_0\in S$&lt;/li>
&lt;li>If $f$ in continuous at $x_0$ and $\{x_n\}$ is a sequence s.t. $x_n\to x_0$, then the sequence $\{f(x_n)\}$ in $\mathcal{Y}$ converges to $f(x_0)$&lt;/li>
&lt;li>If $f$ is discontinuous at $x_0$, then $\exists \{x_n\}\in\mathcal{X}$ s.t. $x_n\to x_0$ but $f(x_n)\nrightarrow f(x_0)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Compact&lt;/strong>: $S\subset\mathcal{X}$ is (sequentially) compact if every sequence in $S$ has a convergent subsequence with limit in $S$&lt;/li>
&lt;li>&lt;strong>Bounded&lt;/strong>: $S\subset\mathcal{S}$ is bounded if $\exists r&amp;lt;\infty$ such that $S\subset B_r(0)$
&lt;ul>
&lt;li>$S$ is compact $\Rightarrow$ $S$ is closed and bounded&lt;/li>
&lt;li>&lt;strong>Bolzano-Weierstrass Theorem&lt;/strong>: In a finite-dimensional normed space, $C$ is closed and bounded $\Leftrightarrow$ for $C$ is compact&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Weierstrass Theorem&lt;/strong>: If $C\subset\mathcal{X}$ is a compact subset and $f:C\to\mathbb{R}$ is continuous at each point of $C$, then $f$ achieves its extreme values, i.e. $\exists \bar{x}\in C\text{ s.t. }f(\bar{x})=\sup_{x\in C} f(x)$ and $\exists \underline{x}\in C\text{ s.t. }f(\underline{x})=\inf_{x\in C} f(x)$
&lt;ul>
&lt;li>$f:C\to\mathbb{R}$ continuous and $C$ compact $\Rightarrow$ $\sup_{x\in C}f(x)&amp;lt;\infty$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item></channel></rss>