<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Stochastic on JacobZ</title><link>https://zyxin.xyz/blog/en/tags/Stochastic/</link><description>Recent content in Stochastic on JacobZ</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Tue, 26 Mar 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://zyxin.xyz/blog/en/tags/Stochastic/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes for Stochastic System</title><link>https://zyxin.xyz/blog/en/2019-03/StochasticSystemNotes/</link><pubDate>Tue, 26 Mar 2019 00:00:00 +0000</pubDate><guid>https://zyxin.xyz/blog/en/2019-03/StochasticSystemNotes/</guid><description>&lt;blockquote>
&lt;p>Prerequisites: Knowledge of Elementary Calculus, Linear Algebra and Probability&lt;/p>
&lt;/blockquote>
&lt;h2 id="discrete-time-stochastic-system">Discrete-Time Stochastic System&lt;/h2>
&lt;h3 id="stochastic-sequences">Stochastic Sequences&lt;/h3>
&lt;ul>
&lt;li>Definition: Given $k\in\mathbb{K}\subseteq\mathbb{Z}$ a sequence of integers, $\mathcal{X}(k,\omega): (\Omega,\mathcal{F},\mathbb{P})\to(\mathbb{R}^n,\mathcal{F}_ \mathcal{X},\mathbb{P}_ \mathcal{X})$ is a random/stochastic sequence.&lt;/li>
&lt;li>Uncertainties: Consider a casual system $F$ relates some scalar inputs $u(k)$ to output $x(k)$
&lt;ul>
&lt;li>&lt;strong>Epistemic/Model uncertainty&lt;/strong>: $\mathcal{X}(k,\omega)=F(k,u(k),u(k-1),\ldots,\omega)$. (system is stochastic and input is deterministic).&lt;/li>
&lt;li>&lt;strong>Aleatoric/Input uncertainty&lt;/strong>: $\mathcal{X}(k,\omega)=f(k,U(k,\omega),u(k-1,\omega),\ldots)$ (system is deterministic and input is stochastic).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Realization&lt;/strong>: An outcome $\mathcal{X}(k,\omega)=x(k)$ given $\omega$ is called a realization of stochastic sequence $\mathcal{X}$&lt;/li>
&lt;li>Terminology and Convention
&lt;ul>
&lt;li>$\mathcal{X}(k,\omega)$ is often written as $\mathcal{X}(k)$ when there&amp;rsquo;s no ambiguity.&lt;/li>
&lt;li>$\mathbb{K}=\mathbb{Z}$ if not specified.&lt;/li>
&lt;li>Sequence over a set $\mathcal{K}_1\subseteq\mathbb{K}$ are denoted $\mathcal{X}(\mathcal{K}_1)$.&lt;/li>
&lt;li>$\mathcal{X}$ denotes $\mathcal{X}(\mathbb{K})$ if not specified.&lt;/li>
&lt;li>Consecutive subsequence: $$\mathcal{X}(k:l)=\{\mathcal{X}(k),\mathcal{X}(k+1),\ldots,\mathcal{X}(l)\},\;x(k:l)=\{x(k),x(k+1),\ldots,x(l)\}$$&lt;/li>
&lt;li>Abbreviations:
&lt;ul>
&lt;li>&lt;em>&lt;strong>SS&lt;/strong>&lt;/em> - stochastic sequence&lt;/li>
&lt;li>&lt;em>&lt;strong>IID&lt;/strong>&lt;/em> - independent indentically distributed&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="probabilistic-characterization">Probabilistic characterization&lt;/h3>
&lt;ul>
&lt;li>Distribution and density: $$F_ \mathcal{X}\left(k:l;x(k:l)\right)\equiv\mathbb{P}((\mathcal{X}_ i(k)\leqslant x_i(k))\cap\cdots\cap(\mathcal{X}_ i(l)\leqslant x_ i(l)),\;i=1\ldots n)$$ $$f_ \mathcal{X}\left(k:l;x(k:l)\right)\equiv \frac{\partial^{n(l-k+1)}}{\partial x_ 1(k)\cdots\partial x_ n(l)}F_ \mathcal{X}(k:l;x(k:l))$$
Here $k:l$ actually denotes a set of consecutive integers, it can be also changed to ordinary sets $\{k,l\}$ or single scalar $k$.&lt;/li>
&lt;li>&lt;strong>Ensemble Average&lt;/strong>: $\mathbb{E}[\psi(\mathcal{X}(k))]$, doing summation over different realization at same time $k$
&lt;ul>
&lt;li>&lt;strong>Mean&lt;/strong>: $\mu_ \mathcal{X}(k)\equiv\mathbb{E}[\mathcal{X}(k)]=\int^\infty_{-\infty}x(k)f_ \mathcal{X}(k;x(k))\mathrm{d}x(k)$&lt;/li>
&lt;li>&lt;strong>Conditional Mean&lt;/strong>: $\mu_ \mathcal{X}(l|k)\equiv\mathbb{E}[\mathcal{X}(l)|\mathcal{X}(k)=x(k)]$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Time Average&lt;/strong>: $\frac{1}{2N+1}\sum^N_{k=-N}\psi(\mathcal{X}(k))$, doing summation over different time k of same realization&lt;/li>
&lt;li>&lt;strong>Autocorrelation&lt;/strong>:
&lt;ul>
&lt;li>Scalar case: $r_ \mathcal{X}(k,l)\equiv\mathbb{E}[\mathcal{X}(k)\mathcal{X}(l)]=\int^\infty_{-\infty}\int^\infty_{-\infty}x(k)x(l)f_ \mathcal{X}(k,l;x(k,l))\mathrm{d}x(k)\mathrm{d}x(l)$&lt;/li>
&lt;li>Vector case: $R_ \mathcal{X}(k,l)\equiv\mathbb{E}[\mathcal{X}(k)\mathcal{X}^\top(l)]$&lt;/li>
&lt;li>Conditional autocorrelation: $R_ \mathcal{X}(k,l|q)\equiv\mathbb{E}[\mathcal{X}(k)\mathcal{X}^\top(l)|\mathcal{X}(q)=x(q)]$&lt;/li>
&lt;li>Often we denote $C_ \mathcal{X}(k)=R_ \mathcal{X}(k,k)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Autocovariance&lt;/strong>:
&lt;ul>
&lt;li>Scalar case: $\kappa_ \mathcal{X}(k,l)\equiv\mathbb{E}[(\mathcal{X}(k)-\mu_ \mathcal{X}(k))(\mathcal{X}(l)-\mu_ \mathcal{X}(l))]$&lt;/li>
&lt;li>Vector case: $\mathrm{K}_ \mathcal{X}(k,l)\equiv\mathbb{E}[(\mathcal{X}(k)-\mu_ \mathcal{X}(k))(\mathcal{X}(l)-\mu_ \mathcal{X}(l))^\top]$&lt;/li>
&lt;li>Conditional autocovariance: $\mathrm{K}_ \mathcal{X}(k,l|q)\equiv\mathbb{E}[(\mathcal{X}(k)-\mu_ \mathcal{X}(k|q))(\mathcal{X}(l)-\mu_ \mathcal{X}(l|q))^\top]$&lt;/li>
&lt;li>Often we denote $S_ \mathcal{X}(k|q)=\mathrm{K}_ \mathcal{X}(k,k|q)$&lt;/li>
&lt;li>Useful conclusion: $\mathrm{K}(a,b)=\mathrm{K}(b,a)^T$&lt;/li>
&lt;li>Normalized (&lt;strong>autocorrelation coefficient&lt;/strong>): $\rho_ \mathcal{X}(k,l)\equiv\mathrm{K}_ \mathcal{X}(k,l)/\sigma^2_{\mathcal{X}(k)}\sigma^2_{\mathcal{X}(l)}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Strong Stationarity&lt;/strong>(aka. strict sense): (necessarily identically distributed over time) $$\forall x(k:l)\in\mathbb{R}^{n(l-k+1)},\;\forall s\in\mathbb{Z},\;f_ \mathcal{X}(k:l;x(k:l))=f_ \mathcal{X}(k+s:l+s;x(k:l))$$&lt;/li>
&lt;li>&lt;strong>Weak Stationarity&lt;/strong>(aka. wide sense): $\forall k,l$ if $$\mu_ \mathcal{X}(k)=\mu_ \mathcal{X}(l)\;\text{and}\;\mathrm{K}_ \mathcal{X}(k,l)=\mathrm{K}_ \mathcal{X}(k+s,l+s)\equiv\bar{\mathrm{K}}_ \mathcal{X}(s)$$ Weak stationarity is necessary condition for stationarity. (Equal when Gaussian distributed)&lt;/li>
&lt;li>&lt;strong>Ergodicity&lt;/strong>: $\mathcal{X}$ is called ergodic in $\psi$ if
&lt;ol>
&lt;li>$\mathbb{E}[\psi(\mathcal{X})]$ is stationary&lt;/li>
&lt;li>Ensemble average is equal to Time average, that is $$ \frac{1}{2N+1}\sum^N_{k=-N}\psi(\mathcal{X}(k))\to\mathbb{E}[\psi(\mathcal{X})];\text{as};l\to \infty$$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;h3 id="markov-sequence">Markov Sequence&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Markov Sequence&lt;/strong>: A ss. $\mathcal{X}(k)$ is called a (discrete-time) Markov sequence if $$\begin{split}f_ \mathcal{X}\left(k;x(k)\middle|\mathcal{X}(k-1)=x(k-1),\mathcal{X}(k-2)=x(k-2),\ldots\right) \\=f_ \mathcal{X}(k;x(k)|\mathcal{X}(k-1)=x(k-1))\end{split}$$
&lt;ul>
&lt;li>We often make some assumption on the initial condition $\mathcal{X}(0)$, such as known, deteministic or uniformly distributed within certain domain.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Markov Chains&lt;/strong>: Markov sequence with discrete set of values(states) ${x_1\ldots x_m}$&lt;/li>
&lt;li>&lt;strong>Hidden Markov model&lt;/strong>: Sequence $\mathcal{Y}$ is called a Hidden Markov Model if it&amp;rsquo;s modeled by a system of the form $$\begin{align}\mathcal{X}(k+1)&amp;amp;=g(k,\mathcal{X}(k),\mathcal{W}(k)) \\ \mathcal{Y}(k)&amp;amp;=h(k,\mathcal{X}(k),\mathcal{W}(k))\end{align}$$
We also say that $\mathcal{Y}$ has a &lt;strong>(discrete-time) stochastic state space&lt;/strong>.&lt;/li>
&lt;li>&lt;strong>Guassian-Markov Sequence&lt;/strong> (GMS): $\mathcal{X}(k+1)=g(k,\mathcal{X}(k),\mathcal{W}(k))$ where $\mathcal{W}(k)$ is iid. Guassian&lt;/li>
&lt;/ul>
&lt;h3 id="linear-stochastic-sequence">Linear Stochastic Sequence&lt;/h3>
&lt;p>$$\begin{align}\mathcal{X}(k+1)&amp;amp;=A(k)\mathcal{X}(k)+B(k)\mathcal{W}(k) \\ \mathcal{Y}(k)&amp;amp;=C(k)\mathcal{X}(k)+D(k)\mathcal{W}(k)\end{align}$$&lt;/p>
&lt;blockquote>
&lt;p>For linear Markov sequences, the deterministic mean sequence and centered uncertain sequence completely decouple. So we often assume that $\mathcal{X}(k)$ and $\mathcal{Y}(k)$ are &lt;strong>centered&lt;/strong> in this case, with regard to deterministic inputs. The equation with deterministic inputs is often written as $$\mathcal{X}(k+1)=A(k)\mathcal{X}(k)+B_u(k)u(k)+B_\mathcal{W}(k)\mathcal{W}(k)$$&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>
&lt;p>Recursive-form expectations:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Mean&lt;/strong>: $\mu_ \mathcal{X}(k+1|q)=A(k)\mu_ \mathcal{X}(k|q)+B(k)\mu_\mathcal{W}(k)$&lt;/li>
&lt;li>&lt;strong>Covariance (Discrete-time algebraic Lyapunov/Stein difference equation)&lt;/strong>: $$S_ \mathcal{X}(k+1|q)=A(k)S_ \mathcal{X}(k|q)A^\top(k)+B(k)S_\mathcal{W}(k)B^\top(k)$$
&lt;blockquote>
&lt;p>Can be solved with &lt;code>dlyap&lt;/code> in MATLAB&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Convergence when $k\to\infty$:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Mean convergence&lt;/strong>: $\mu_ \mathcal{X}(k|q)$ converges requires $\max_i|\lambda_i(A)|&amp;lt;1$ ($\lambda$ denotes eigenvalue)&lt;/li>
&lt;li>&lt;strong>Covariance convergence&lt;/strong>: $S_ \mathcal{X}(k|q)$ converges requires $\max_i|\lambda_i(A)|&amp;lt;1$&lt;/li>
&lt;li>&lt;strong>(Discrete-time) Lyapunov equation&lt;/strong> (Stein equation): $\bar{S}_ \mathcal{X}=A\bar{S}_ \mathcal{X}A^\top+B\bar{S}_\mathcal{W}(k)B^\top$. Solution for this equation exists iff. $A$ is asymptotically stable (characterizing sequence is stationary).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Explicit state transition: By recursive substitution, $$\mathcal{X}(k)=\Psi(k,q)\mathcal{X}(q)+\sum^{k-1}_ {i=q}\Gamma(k,i)\mathcal{W}(i)$$ where state transition matrix $\Psi(k,q)=\begin{cases}I, &amp;amp;k=q \\ \prod^{k-1}_ {i=q}A(i),&amp;amp; k&amp;gt;q\end{cases}$ and $\Gamma(k,i)=\Psi(k,i+1)B(i)$.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Conditioned Mean sequence&lt;/strong>: $\mu_ \mathcal{X}(k|q)=\Psi(k,q)\mu_ \mathcal{X}(q)+\sum^{k-1}_ {i=q}\Gamma(k,i)\mu_\mathcal{W}(i)$&lt;/li>
&lt;li>&lt;strong>Conditioned Autocovariance Matrix&lt;/strong>: $$\mathrm{K}_ \mathcal{X}(k,l|q)=\Psi(k,q)S_ \mathcal{X}(q)\Psi^\top(l,q)+\sum^{min\{k,l\}-1}_ {i=q}\Gamma(k,i)S_\mathcal{W}(i)\Gamma^\top(l,i)$$
&lt;ul>
&lt;li>A special case: $S_ \mathcal{X}(k|q)=\mathrm{K}_ \mathcal{X}(k,k|q)=\sum^{k-1}_ {i=q}\Gamma(k,i)S_\mathcal{W}(i)\Gamma^\top(k,i)$, $S_ \mathcal{X}(k|k)=0$&lt;/li>
&lt;li>Useful equation (stationary centered case): $$\mathrm{K}_ \mathcal{X}(k,l)=\begin{cases} S_ \mathcal{X}(k)\cdot(A^\top)^{(l-k)}&amp;amp;,l&amp;gt;k\\ S_ \mathcal{X}(k)&amp;amp;,l=k\\ A^{(k-l)}S_ \mathcal{X}(k)&amp;amp;,l&amp;lt; k\end{cases}$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Conditional Autocorrelation Matrix&lt;/strong>: $R_ \mathcal{X}(k,l|q)=\mathrm{K}_ \mathcal{X}(k,l|q)+\mu_ \mathcal{X}(k|q)\mu_ \mathcal{X}^\top(l|q)$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Observation $\mathcal{Y}$ property:&lt;/p>
&lt;ul>
&lt;li>Mean: $\mu_ \mathcal{Y}(k|q)=C(k)\mu_ \mathcal{X}(k|q)+D(k)\mu_\mathcal{W}(k)$&lt;/li>
&lt;li>Covariance: $$\mathrm{K}_ \mathcal{Y}(k,l|q)=\begin{cases}C(k)\mathrm{K}_ \mathcal{X}(k,l|q)C^\top(l)+C(k)\Gamma(k,l)S_W(l)D^\top(l)&amp;amp;:k&amp;gt;l \\C(k)S_ \mathcal{X}C^\top(k)+D(k)S_\mathcal{W}(k)D^\top(k)&amp;amp;:k=l\\ C(k)\mathrm{K}_ \mathcal{X}(k,l|q)C^\top(l)+D(k)S_\mathcal{W}(k)\Gamma^\top(l,k)C^\top(l)&amp;amp;:k&amp;lt; l\end{cases}$$&lt;/li>
&lt;li>Stationary time-invariant covariance: $$\mathrm{K}_ \mathcal{Y}(s)=\begin{cases}CA^{|s|}\bar{S}_ \mathcal{X}C^\top+CA^{|s|-1}B\bar{S}_ WD^\top&amp;amp;:s\neq 0 \\C\bar{S}_ \mathcal{X}C^\top+D\bar{S}_ WD^\top&amp;amp;:s=0\end{cases}$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="gaussian-stochastic-sequence">Gaussian Stochastic Sequence&lt;/h3>
&lt;ul>
&lt;li>Jointly Gaussian $\Rightarrow, \nLeftarrow$ Marginally Gaussian&lt;/li>
&lt;li>$c^\top X$ is Gaussian $\Leftrightarrow X$ is Gaussian&lt;/li>
&lt;li>Conditional Gaussian: if $X$ and $Y$ are Gaussian, then $X|Y \sim \mathcal{N}(\mu_{X|Y},S_{X|Y})$ where $\mu_{X|Y}=\mu_X+S_{XY}S_Y^{-1}(Y-\mu_Y)$, $S_{X|Y}=S_X-S_{XY}S_Y^{-1}S_{YX}$&lt;/li>
&lt;li>A linear controllable GMS $\mathcal{X}(k)$ is stationary iff. $A(k)=A, B(k)=B$ (time-invariant) and $A$ is asymptotically stable.&lt;/li>
&lt;li>All LTI stationary GMS are also ergodic in all finite momoents&lt;/li>
&lt;li>Solve $\mathcal{X}(k+1)=A\mathcal{X}(k)+B\mathcal{W}(k)$ when $\mathcal{X}$ is stationary ($\max_i|\lambda_i(A)|&amp;lt;1$)
&lt;ol>
&lt;li>solve $\bar{\mu}_ \mathcal{X}=A\bar{\mu}_ \mathcal{X}+B\bar{\mu}_\mathcal{W}$ for $\bar{\mu}$&lt;/li>
&lt;li>solve $\bar{S}_ \mathcal{X}=A\bar{S}_ \mathcal{X}A^\top+B\bar{S}_ \mathcal{W}B^\top$ for $\bar{S}_ \mathcal{X}$&lt;/li>
&lt;li>calculate $\Sigma_ \mathcal{X}(k:l|q)=\begin{bmatrix}\mathrm{K}_ \mathcal{X}(k,k|q)&amp;amp;\cdots&amp;amp;\mathrm{K}_ \mathcal{X}(k,l|q)\\ \vdots&amp;amp;\ddots&amp;amp;\vdots\\ \mathrm{K}_ \mathcal{X}(l,k|q)&amp;amp;\cdots&amp;amp;\mathrm{K}_ \mathcal{X}(l,l|q)\end{bmatrix}$ using $\bar{S}_ \mathcal{X}$&lt;/li>
&lt;li>Then $f_ \mathcal{X}(k:l;x(k:l))$ is determined with $\mu_ \mathcal{X}(k:l)=\{\bar{\mu}_ \mathcal{X},\bar{\mu}_ \mathcal{X}\ldots\bar{\mu}_ \mathcal{X}\}$ and $\Sigma_ \mathcal{X}(k:l)$ above.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;h3 id="observation--filtering">Observation &amp;amp; Filtering&lt;/h3>
&lt;blockquote>
&lt;p>For deterministic version, please check &lt;a href="{% post_path ControlSystemNotes %}#State-Estimation-Observer-Design">my notes for control system&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>(LTI) &lt;strong>Luenberger observer&lt;/strong>: $$\begin{align}\hat{\mathcal{X}}(k+1)&amp;amp;=A\hat{\mathcal{X}}(k)+L(\hat{\mathcal{Y}}(k)-\mathcal{Y}(k))+B\bar{\mu}_ \mathcal{W} \\ \hat{\mathcal{Y}}(k)&amp;amp;=C\hat{\mathcal{X}}(k)+D\bar{\mu}_\mathcal{W}\end{align}$$ where $L$ is the observer gain.
&lt;ul>
&lt;li>Note: We often assume that the process &amp;amp; measurement noise are decoupled and independent&lt;/li>
&lt;li>Combined form: $\hat{\mathcal{X}}(k+1)=[A+LC]\hat{\mathcal{X}}(k)-L\mathcal{Y}(k)+B\mu_\mathcal{W}(k)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>State estimation residual&lt;/strong> $r(k)=\mathcal{X}(k)-\hat{\mathcal{X}}(k)$
&lt;ul>
&lt;li>Combined form: $r(k+1)=[A+LC]r(k)+[B+LD]\tilde{\mathcal{W}}(k)$&lt;/li>
&lt;li>Stationary covariance can be solved by a Lyapunov equation $$\bar{S}_ r=[A+LC]\bar{S}_ r[A+LC]^T+[LD+B]\bar{S}_ \mathcal{W}[LD+B]^T$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>A common objective: minimize $\bar{S}_r=\mathbb{E}(rr^\top)$
&lt;ul>
&lt;li>Solutions: $L=-A\bar{S}_ rC^\top[C\bar{S}_ rC^\top+D\bar{S}_ \mathcal{W}D^\top]^{-1}$ (&lt;strong>Kalman observer gain&lt;/strong>)&lt;/li>
&lt;li>Or solve &lt;strong>(Discrete-time) Algebraic Riccati equation&lt;/strong> $$\bar{S}_ r=A\bar{S}_ rA^\top+B\bar{S}_ \mathcal{W}B^\top-A\bar{S}_ rC^\top[C\bar{S}_ rC^\top+D\bar{S}_ \mathcal{W}D^\top]^{-1}C\bar{S}_ rA^\top$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Innovation sequence&lt;/strong> $e(k)=\hat{\mathcal{Y}}(k)-\mathcal{Y}(k)$ with $L$ is optimal
&lt;ul>
&lt;li>We can find that $\mu_e=0$ and $\mathrm{K}_e(k+s,k)=\begin{cases}C\bar{S}_rC^\top+D\bar{S}_WD^T&amp;amp;:s=0 \\0&amp;amp;:s\neq0\end{cases}$. So the innovation sequence is iid. (only in Kalman observer)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>(Output) Probabilistically-equivalent model&lt;/strong>: $$\begin{align}\mathcal{X}(k+1)&amp;amp;=A\mathcal{X}(k)+Le(k) \\ \mathcal{Y}(k)&amp;amp;=C\mathcal{X}(k)-e(k)\end{align}$$&lt;/li>
&lt;/ul>
&lt;h2 id="markov-chains">Markov Chains&lt;/h2>
&lt;blockquote>
&lt;p>Content in this section comes from EECS 501
In this specific field, we often use stand-alone analysis methods.&lt;/p>
&lt;/blockquote>
&lt;h3 id="basic-definitions">Basic definitions&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>State distribution&lt;/strong>: We denote row vector $\pi_t$ as $\pi_t(x)=\mathbb{P}(\mathcal{X}_t=x),\; x\in S$ ($S$ is the set of states). Directly we have $\sum_x\pi(x)=1$&lt;/li>
&lt;li>&lt;strong>Time homogeneous&lt;/strong>: $\mathbb{P}(\mathcal{X}_ {t+1}=y|\mathcal{X}_ t=x)=\mathbb{P}(\mathcal{X}_{s+1}=y|\mathcal{X}_s=x)\;\forall s,t$&lt;/li>
&lt;li>&lt;strong>One-step transition probability matrix&lt;/strong>: $P_t=[P_{xy,t}]$ where $P_{xy,t}=\mathbb{P}(\mathcal{X}_ {t+1}=y|\mathcal{X}_ t=x)$.
&lt;ul>
&lt;li>Time-homo case: $P=[P_{xy}]$ where $P_{xy}=p(y|x)$&lt;/li>
&lt;li>Rows of the matrix sum up to 1.&lt;/li>
&lt;li>This matrix is also called &lt;strong>stochastic matrix&lt;/strong>.&lt;/li>
&lt;li>Extend this matrix to continuous states, then we use &lt;strong>transition kernel&lt;/strong> $T(x,y)$ to describe the transition probability,&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>m-step transition probability matrix&lt;/strong>: $P_{xy,t}^{(m)}=\mathbb{P}(\mathcal{X}_ {t+m}=y|\mathcal{X}_ t=x)$
&lt;ul>
&lt;li>&lt;strong>Chapman-Kolmogorov Equation&lt;/strong>: $P^{(n+m)}_t=P^{(n)}_t P^{(m)}_t$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="state-variables">State Variables&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Hitting time&lt;/strong>: $T_1(y)=\min\{n\geq 0:\mathcal{X}_ n=y\},\;T_k(y)=\min\{n&amp;gt;T_{k-1}(y):\mathcal{X}_ n=y\}$ where $n\in\mathbb{N}$
&lt;ul>
&lt;li>&lt;strong>Period&lt;/strong>: For state $i\in x$, its period is the greatest common divisor of $\{n&amp;gt;1|T_n(i)&amp;gt;0\}$. A Markov Chain is &lt;strong>aperiodic&lt;/strong> If all states have period 1.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Return probability&lt;/strong>: $f_{xy}=\mathbb{P}(T_1(y)&amp;lt;\infty|\mathcal{X}_0=x)$&lt;/li>
&lt;li>&lt;strong>Occupation time&lt;/strong>: $V(y)=\sum^\infty_{n=1}\unicode{x1D7D9}_{\mathcal{X_n}}(y)$&lt;/li>
&lt;li>Some properties
&lt;ul>
&lt;li>$f_{xy}=\mathbb{P}(V(y)\geqslant 1|\mathcal{X}_0=x)$&lt;/li>
&lt;li>$\mathbb{P}(V(y)=m|\mathcal{X}_ 0=x)=\begin{cases} 1-f_{xy}&amp;amp;:m=0\\f_{xy}f_{yy}^{m-1}(1-f_{yy})&amp;amp;:m\geqslant 1\end{cases}$&lt;/li>
&lt;li>$\mathbb{E}[ V(y)|\mathcal{X}_ 0=x]=\begin{cases} 0&amp;amp;:f_{xy}=0\\\infty&amp;amp;:f_{xy}&amp;gt;0,\ f_{yy}=1\\f_{xy}/(1-f_{yy})&amp;amp;:f_{xy}&amp;gt;0,\ f_{yy}&amp;lt;1\end{cases}$&lt;/li>
&lt;li>$\mathbb{E}[ V(y)|\mathcal{X}_ 0=x]=\sum^\infty_{n=1}P^{(n)}_{xy}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="state-classification">State Classification&lt;/h3>
&lt;blockquote>
&lt;p>Here we usually consider only time-homogeneous Markov Chains&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Accessible&lt;/strong> ($x\to y$): $\exists n\;\text{s.t.}\ P_{xy}^{(n)}&amp;gt;0$
&lt;ul>
&lt;li>$x\to y \Leftrightarrow f_{xy}&amp;gt;0$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Communicate&lt;/strong> ($x\leftrightarrow y$): $x\to y\;\text{and}\;y\to x$. This is a &lt;a class="link" href="https://en.wikipedia.org/wiki/Equivalence_relation" target="_blank" rel="noopener"
>equivalence relation&lt;/a>.&lt;/li>
&lt;li>&lt;strong>Equivalent class&lt;/strong>: set of states that communicate with each other&lt;/li>
&lt;li>&lt;strong>Irreducible&lt;/strong>: a Markov chain with only one communicating class&lt;/li>
&lt;li>&lt;strong>Absorbing/Closed state&lt;/strong>: $P_{xx}=1$
&lt;ul>
&lt;li>&lt;strong>Absorbing class&lt;/strong>: set $C$ is absorbing iff $\forall x \in C, \sum_{y\in C}P_{xy}=1$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Transient state&lt;/strong>: $f_{xx}&amp;lt;1 \Leftrightarrow \mathbb{E}[V_i|\mathcal{X}_0=i]&amp;lt;\infty$&lt;/li>
&lt;li>&lt;strong>Recurrent state&lt;/strong>: $f_{xx}=1 \Leftrightarrow \mathbb{E}[V_i|\mathcal{X}_0=i]=\infty$
&lt;ul>
&lt;li>&lt;strong>Positive recurrent&lt;/strong>: $\mathbb{E}[T_1(x)|\mathcal{X}_0=x]&amp;lt;0$&lt;/li>
&lt;li>&lt;strong>Null recurrent&lt;/strong>: $\mathbb{E}[T_1(x)|\mathcal{X}_0=x]=0$&lt;/li>
&lt;li>These two kind of recurrent states also make up communicating classes&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Some properties
&lt;ul>
&lt;li>If $x$ is (positive) recurrent and $x\to y$, then $y$ is also (positive) recurrent and $f_{xy}=f_{yx}=1$&lt;/li>
&lt;li>Every closed and finite subset of $X$ contains at least one (positive) recurrent state.&lt;/li>
&lt;li>All states of a communicating class are either positive recurrent, null recurrent or transient.&lt;/li>
&lt;li>Method to determine whether class $C$ is recurrent/transient
&lt;ol>
&lt;li>If $C$ is non-closed, it&amp;rsquo;s transient&lt;/li>
&lt;li>If $C$ is closed and finite, then $C$ is positive recurrent&lt;/li>
&lt;li>If $C$ is closed and infinite, then $C$ can be either positive/null recurrent or transient&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>A typical example is the &lt;a class="link" href="https://en.wikipedia.org/wiki/Birth%E2%80%93death_process" target="_blank" rel="noopener"
>birth-death chain&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Stationary distribution&lt;/strong>: $\bar{\pi}\;\text{s.t.}\;\bar{\pi}=\bar{\pi} P$
&lt;ul>
&lt;li>Stationary in limit form: $\bar{\pi}=\lim_{n\to \infty} \frac{1}{n}\sum^{n-1}_{t=0} \pi_t$
&lt;ul>
&lt;li>This is a &lt;strong>Cesaro&lt;/strong> limit, we use this to deal with the problem that $\lim_{n\to \infty} \pi_t$ might not exist if the chain is periodic.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Reversibility criterion for stationary: $\forall i,j \in S, \pi_i P_{ij} = \pi_j P_{ji}$ (a.k.a detailed balance condition), then the process is reversible and therefore stationary.&lt;/li>
&lt;li>Existence for stationary distribution satisfying $\bar{\pi}=\bar{\pi} P$
&lt;ol>
&lt;li>If the chain has single positive recurrent class, then exists unique solution: $\bar{\pi}(x)=0$ for all transient or null recurrent $x$&lt;/li>
&lt;li>If the chain has multiple positive recurrent class, then there are multiple solutions, for each positive recurrent $x$ we have a $\bar{\pi}^i$.&lt;/li>
&lt;li>If the chain has only transient and null recurrent states, there is no solution$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Convergence of stationary distribution
&lt;ul>
&lt;li>If the chain has positive recurrent class, then $\frac{1}{n}\sum^n_{t=1}\mathbb{P}(\mathcal{X}_t=j)\xrightarrow[n\to \infty]{}\bar{\pi}_j,\;\forall \mu_0$&lt;/li>
&lt;li>(Ergodic) If the chain is positive recurrent, then $\frac{1}{n}\sum^n_{t=1}\unicode{x1D7D9}_{\mathcal{X}_t}(j)\xrightarrow[n\to \infty]{\text{a.s.}}\bar{\pi}_j$&lt;/li>
&lt;li>If the chain is positive recurrent and aperiodic, then $\mathbb{P}(\mathcal{X}_n=j)\xrightarrow[n\to \infty]{}\bar{\pi}_j$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="continuous-time-stochastic-system">Continuous-Time Stochastic System&lt;/h2>
&lt;h3 id="stochastic-sequences-1">Stochastic Sequences&lt;/h3>
&lt;ul>
&lt;li>Definition: Given $t\in\mathbb{T}\subset\mathbb{R}$ a sequence of time, $\mathcal{X}(t,\omega): (\Omega,\mathcal{F},\mathbb{P})\to(\mathbb{R}^n,\mathcal{F}_ \mathcal{X},\mathbb{P}_ \mathcal{X})$ is a continuous stochastic sequence.&lt;/li>
&lt;li>Most definitions are similar to discrete sequence (including &lt;strong>stationarity&lt;/strong>), while the sequence is often defined as $\mathcal{X}(\mathcal{G}),\;\mathcal{G}={t_1,t_2,\ldots,t_N}\subset\mathbb{T}, t_i&amp;lt; t_{i+1}$&lt;/li>
&lt;li>&lt;strong>Stochastic Differential Equation (SDE)&lt;/strong>: $$\mathrm{d}\mathcal{X}(t)=F(\mathcal{X}(t),t)\mathrm{d}t+\sum^r_{i=1}G_i(\mathcal{X}(t),t)\mathrm{d}\mathcal{W}_i(t)$$ Here $\mathcal{W}$ is often a certain kind of &lt;em>noise&lt;/em> random process.&lt;/li>
&lt;/ul>
&lt;h3 id="markov-sequence-1">Markov Sequence&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Markov Sequence&lt;/strong>: A ss. $\mathcal{X}(k)$ is called a (continuous-time) Markov sequence if for the set of times $\mathcal{G}={t_1,t_2,\ldots,t_N}$ with $t_i &amp;lt; t_{i+1}$, we have $$\begin{split}f_ \mathcal{X}\left(t_N;x(t_N)\middle|\mathcal{X}(t_{N-1})=x(t_{N-1}),\mathcal{X}(t_{N-2})=x(t_{N-2}),\ldots\right)\qquad \\ =f_ \mathcal{X}(t_N;x(t_N)|\mathcal{X}(t_{N-1})=x(t_{N-1}))\end{split}$$&lt;/li>
&lt;li>&lt;strong>Hidden Markov Model&lt;/strong>: Definition similar to discrete case. A &lt;strong>continuous-time stochastic state space&lt;/strong> is of the form $$\begin{align}\dot{\mathcal{X}}(t)&amp;amp;=F(\mathcal{X},t)+G(\mathcal{X},t)\mathrm{d}\mathcal{W}(t)/\mathrm{d}t \\ \mathcal{Y}(t)&amp;amp;=H(\mathcal{X},t)+J(\mathcal{X},t)\mathrm{d}\mathcal{W}(t)/\mathrm{d}t\end{align}$$ where $\mathcal{W}$ is usually &lt;a class="link" href="#Wiener-Process" >Wiener process&lt;/a> and white noise $\mathrm{d}\mathcal{W}(t)/\mathrm{d}t$ is often written as $\mathcal{U}(t)$.
&lt;ul>
&lt;li>The state space is &lt;strong>affine&lt;/strong> if $F(\mathcal{X}(t),t)=A(t)\mathcal{X}(t)+u(t),\;N(\mathcal{X},t)=C(t)\mathcal{X}(t)+v(t)$&lt;/li>
&lt;li>The state space is &lt;strong>bilinear&lt;/strong> if $G(\mathcal{X},t)\mathrm{d}\mathcal{W}(t)=\sum^r_{i=1}B_i(t)\mathcal{X}\mathrm{d}\mathcal{W}_i(t)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="poisson-counters">Poisson Counters&lt;/h3>
&lt;ul>
&lt;li>Definition: It&amp;rsquo;s a stochastic Markow process $\mathcal{N}(t)\in\{0,1,2,\ldots\}$ with the characteristic that it jumps up by only one integer at a time.
&lt;ul>
&lt;li>Characteristics: transition times are random, transition step is always 1.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Transition rule: $\frac{\partial}{\partial t}p_\mathcal{N}=\begin{cases}-\lambda p_\mathcal{N}(t;n)&amp;amp;:n=0\\-\lambda p_\mathcal{N}(t;n)+\lambda p_\mathcal{N}(t;n-1)&amp;amp;:n&amp;gt;0\end{cases}$
&lt;ul>
&lt;li>From the rule we can conclude that $p_\mathcal{N}(t;n)=\frac{1}{n!}(\lambda t)^n e^{-\lambda t}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Bidirectional counter: $\mathcal{N}(t)$ is defined as one-directional. $\mathcal{N}_1(t)-\mathcal{N}_2(t)$ is called &lt;strong>bidirectional poisson counter&lt;/strong>.&lt;/li>
&lt;li>Expectation: $\mathbb{E}[\mathcal{N}(t)]=\lambda t,\;\mathbb{E}[\mathrm{d}\mathcal{N}(t)]=\lambda\mathrm{d}t$&lt;/li>
&lt;li>&lt;strong>Ito calculus&lt;/strong> for Poisson counter:
&lt;ul>
&lt;li>&lt;strong>Ito sense&lt;/strong>: $n(t)$ is a realization of poisson counter $\mathcal{N}$. $x(t)$ is a solution in Ito sense to $\mathrm{d}x(t)=F(x(t),t)\mathrm{d}t+G(x(t),t)\mathrm{d}n(t)$ if
&lt;ol>
&lt;li>On all intervals where $n(t)$ is constant, $\dot{x}(t)=F(x(t),t)$&lt;/li>
&lt;li>If $n(t)$ jumps at time $t_1$, $\lim_{t\to t_1^+}x(t)=\lim_{t\to t_1^-}x(t)+G\left(\lim_{t\to t_1^+}x(t),t_1\right)$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>Ito rule&lt;/strong>: Given a fuction $\psi(\mathcal{X}(t),t)$, taking Taylor expansion we have $$\mathrm{d}\psi=\left\{\frac{\partial\psi}{\partial t}+(\nabla_\mathcal{X}\psi)F(\mathcal{X},t)\right\}\mathrm{d}t+\sum^r_{i=1}\left\{\psi\left(\mathcal{X}+G_i(\mathcal{X},t),t\right)-\psi(\mathcal{X},t)\right\}\mathrm{d}\mathcal{N}_i(t)$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="wiener-process">Wiener-Process&lt;/h3>
&lt;ul>
&lt;li>Definition (&lt;strong>Brownian Motion&lt;/strong>): It&amp;rsquo;s a bidirectional poisson counter with infinite rate, i.e. $\mathcal{W}=\lim_{\lambda\to\infty}\frac{1}{\sqrt(\lambda)}(\mathcal{N}_1-\mathcal{N}_2)$ where $\mathcal{N}_1,\;\mathcal{N}_2$ have rate $\lambda/2$
&lt;ul>
&lt;li>Characteristic: It&amp;rsquo;s a Gaussian with zero mean and variance $t$&lt;/li>
&lt;li>Note: Actual Wiener Process has stronger continuity property than Brownian motion.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Expectation: $\mathbb{E}[\mathcal{W}]=\mathbb{E}[d\mathcal{W}]=0,\;\mathbb{E}[\mathcal{W}(\tau)\mathcal{W}(t)]=\mathbb{E}[\mathcal{W}^2(\min\{t,\tau\})]=\min\{t,\tau\}$&lt;/li>
&lt;li>Principle of independent increments: If the interval $[r,t), [\sigma,s)$ don&amp;rsquo;t overlap, then $\mathcal{W}(t)-\mathcal{W}(\tau)$ and $\mathcal{W}(s)-\mathcal{W}(\sigma)$ are uncorrelated.&lt;/li>
&lt;li>&lt;strong>Ito calculus&lt;/strong> for Wiener Process
&lt;ul>
&lt;li>&lt;strong>Ito rule&lt;/strong>: Given a fuction $\psi(\mathcal{X}(t),t)$, taking Taylor expansion we have $$\begin{split}\mathrm{d}\psi=\frac{\partial\psi}{\partial t}\mathrm{d}t+(\nabla_\mathcal{X}\psi)F(\mathcal{X},t)\mathrm{d}t+\sum^r_{i=1}\left(\nabla_\mathcal{X}\psi(\mathcal{X},t)\right)G_i(\mathcal{X},t)\mathrm{d}\mathcal{W}_ i \\ -\sum^r_{i=1}\frac{1}{2}G_i^\top(\mathcal{X},t)\left(\mathrm{H}_\mathcal{X}\psi(\mathcal{X},t)\right)G_i(\mathcal{X},t)\mathrm{d}t\end{split}$$
&lt;blockquote>
&lt;p>Note $\nabla_{\mathcal{X}}=\begin{bmatrix}\frac{\partial\psi}{\partial x_1}&amp;amp;\cdots&amp;amp;\frac{\partial\psi}{\partial x_n}\end{bmatrix}$ and $\mathrm{H}_{\mathcal{X}}=\begin{bmatrix}\frac{\partial^2\psi}{\partial x_1^2}&amp;amp;\cdots&amp;amp;\frac{\partial^2\psi}{\partial x_1\partial x_n}\\ \vdots&amp;amp;\ddots&amp;amp;\vdots \\ \frac{\partial^2\psi}{\partial x_n\partial x_1}&amp;amp;\cdots&amp;amp;\frac{\partial^2\psi}{\partial x_n^2}\end{bmatrix}$ are the gradient and Hessian operator respectively. By default, the operator target is $\mathcal{X}$ is not noted.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>White noise&lt;/strong>: It is Guassian distributed stationary stochastic process with $\mu_\mathcal{U}(t)=0,\;\mathrm{K}_ \mathcal{U}(t,\tau)=\Phi_ \mathcal{U}\delta(t-\tau)$, where $\Phi_\mathcal{U}$ is &lt;strong>spectral intensity&lt;/strong>.
&lt;ul>
&lt;li>We often consider white noise as derivative of Wiener process: $\mathcal{U}\sim\mathrm{d}\mathcal{W}/\mathrm{d}t$.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="linear-stochastic-sequence-1">Linear Stochastic Sequence&lt;/h3>
&lt;p>$$\begin{align}\mathrm{d}\mathcal{X}(t)&amp;amp;=\{A(t)\mathcal{X}(t)+u(t)\}\mathrm{d}t+B(k)\mathrm{d}\mathcal{W}(t) \\ \mathcal{Y}(t)&amp;amp;=C(k)\mathcal{X}(k)+D(k)\mathcal{W}(k)\end{align}$$&lt;/p>
&lt;ul>
&lt;li>Differential equation of expectations
&lt;ul>
&lt;li>&lt;strong>Mean&lt;/strong>: $\frac{\mathrm{d}}{\mathrm{d}t}\mu_\mathcal{X}(t)=A(t)\mu_\mathcal{X}(t)$&lt;/li>
&lt;li>&lt;strong>Autocovariance&lt;/strong>: $\frac{\mathrm{d}}{\mathrm{d}t}S_\mathcal{X}(t)=A(t)S_\mathcal{X}(t)+S_\mathcal{X}A^\top(t)+B(t)B^\top(t)$ (called &lt;strong>Lyapunov differential equation&lt;/strong>)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Stationary LTI case
&lt;ul>
&lt;li>Useful equation: $\mathrm{R}_ \mathcal{X}(t,\tau)=\begin{cases} S_ \mathcal{X}\exp\{A^\top(\tau-t)\}&amp;amp;,\tau&amp;gt;t \\ \exp\{A(t-\tau)\}S_ \mathcal{X}&amp;amp;,\tau&amp;lt; t\end{cases}$&lt;/li>
&lt;li>&lt;strong>(Continuous-time) algerbraic Lyapunov equation&lt;/strong>: $A\bar{S}_ \mathcal{X}+\bar{S}_\mathcal{X}A^\top+BB^\top=0$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="nonlinear-stochastic-sequence">Nonlinear Stochastic Sequence&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>The Fokker-Planck Equation (FPE)&lt;/strong>: Consider the Wiener-process excited general SDE, we have $$\frac{\partial f_ \mathcal{X}(x;t)}{\partial t}=-\nabla\left(F(\mathcal{X})f_ \mathcal{X}(x;t)\right)+\frac{1}{2}\mathrm{tr}\left[\mathrm{H}\left(\Gamma(\mathcal{X})f_ \mathcal{X}(x;t)\right)\right]$$
here $\Gamma(\mathcal{X})=G(\mathcal{X})G^\top(\mathcal{X})$&lt;/p>
&lt;ul>
&lt;li>$\mathcal{X}$ is stationary means $\frac{\partial f_\mathcal{X}(x;t)}{\partial t}=0$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Gaussian closure&lt;/strong>: An approximate solution for FPE is supposing $f_\mathcal{X}$ as multivariate Gaussian with some $S_\mathcal{X}$ and ${\mu_\mathcal{X}}$. That is we focus on 1st-order and 2nd-order estimation. Denote the estimated distribution as $\hat{f}_\mathcal{X}(x;t)$&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Estimated expectation&lt;/strong>: $\hat{\mathbb{E}}\phi(\mathcal{X})=\int\cdots\int\phi(x)\hat{f}_\mathcal{X}(x;t)\mathrm{d}x$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Solution:$$\begin{split}h(x,t)=-\nabla F+\tilde{x}^\top S^{-1}F-(\nabla\Gamma)S^{-1}\tilde{x}+\frac{1}{2}\mathrm{tr}\left[\mathrm{H}\Gamma \right] \\ +\frac{1}{2}\mathrm{tr}\left[\left(-S^{-1}+S^{-1}\tilde{x}\tilde{x}^\top S^{-1}\right)\Gamma\right]\end{split}$$ and $$\begin{align}\dot{\mu}_\mathcal{X}(t)&amp;amp;=S(t)\hat{\mathbb{E}}[\nabla^\top h(x)] \\ \dot{S} _\mathcal{X}(t)&amp;amp;=S(t)\hat{\mathbb{E}}[\mathrm{H} h(x)]S(t)\end{align}$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Useful simplification: If $G$ is constant (independent of $\mathcal{X}$), then the ODE of $\dot{\mu}$ and $\dot{S}$ become $$\begin{align}\dot{\mu}_ \mathcal{X}(t)&amp;amp;=\hat{\mathbb{E}}[F(\mathcal{X})] \\ \dot{S} _\mathcal{X}(t)&amp;amp;=\hat{A}^\top S+S\hat{A}+\Gamma\end{align}$$ where $\hat{A}=\hat{\mathbb{E}}\left[\frac{\partial F}{\partial\mathcal{X}}\right]$&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Equivalent linearization (Quasi-linearization)&lt;/strong>: In the estimation, $\mathcal{X}$ evolves equivalently to $\mathrm{d}\mathcal{X}(t)=\hat{A}(t)\mathcal{X}(t)\mathrm{d}t+G\mathrm{d}\mathcal{W}(t)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>No guaranteed bounds generally exist for the estimation error $$e(\mathcal{X},t)=\left(\frac{\partial\hat{f}_ \mathcal{X}}{\partial t}\right)-\left(-\nabla\left(F\hat{f}_ \mathcal{X}\right)+\frac{1}{2}\mathrm{tr}\left[\mathrm{H}\left(\Gamma\hat{f}_ \mathcal{X}\right)\right]\right)$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Estimation of $\mu(t),\;S(t)$ could also have error. And stationarity may not be the consistent between original system and estimated system&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="spectral-analysis">Spectral Analysis&lt;/h2>
&lt;blockquote>
&lt;p>Content in this section comes from MECHENG 549&lt;/p>
&lt;/blockquote>
&lt;h3 id="power-spectral-density">Power Spectral Density&lt;/h3>
&lt;ul>
&lt;li>Definition: The &lt;strong>power spectral density (PSD)&lt;/strong> of stochastic process $\mathcal{Y}$ is $$\Phi_\mathcal{Y}(\omega)\equiv\mathbb{E}\left[ \lim_{T\to\infty}\frac{1}{2T}Y_T(\omega)Y_T^\top(\omega)\right]$$ where $Y_T(\omega)$ is the Fourier transform of centered process $\mathcal{Y}(t)$.
&lt;ul>
&lt;li>White noise has the same PSD for whatever $\omega$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Wiener-Khinchin&lt;/strong> Theorem: $\Phi_\mathcal{Y}(\omega)=\int^\infty_{-\infty}e^{-i\omega\theta}\bar{\mathrm{K}}_\mathcal{Y}(\theta)\mathrm{d}\theta$&lt;/li>
&lt;li>&lt;strong>Signal propagation&lt;/strong>: If the system $P$ has input $\mathcal{Y}$ and output $\mathcal{Z}$, then $\Phi_\mathcal{Z}=|P(i\omega)\Phi_\mathcal{Y}(\omega)P^\top(i\omega)|$ where $P(i\omega)$ is the Fourier transform of the system.&lt;/li>
&lt;li>&lt;strong>Cross spectrum&lt;/strong>: If the system $G$ has input $\mathcal{Y}$ and output $\mathcal{Z}$, then the cross-spectrum is $\Phi_{\mathcal{ZY}}(\omega)=G(i\omega)\Phi_\mathcal{Y}(\omega)$&lt;/li>
&lt;/ul>
&lt;h3 id="periodograms">Periodograms&lt;/h3>
&lt;ul>
&lt;li>Definition: We want to estimate $\Phi_{\mathcal{Y}}(\omega)$ without the expectation, then we use $$Q_T(\omega,y)=\frac{1}{2T}\left|\int^T_{-T}e^{-i\omega t}y(t)\mathrm{d}t\right|^2$$ where $y$ is a sample realization of $\mathcal{Y}$.
&lt;ul>
&lt;li>We also use window functions to calculate the spectrum over a finite time interval (using &lt;strong>Bartlett&amp;rsquo;s procedure&lt;/strong>)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="stochastic-realizations">Stochastic realizations&lt;/h3>
&lt;p>We want to find a stochastic model which gives the same spectrum given. This is called &lt;strong>stochastic realization problem&lt;/strong>. We focus on scalar LTI case.&lt;/p>
&lt;p>If we know $P(s)=C[sI-A]^{-1}B+D$ (the Laplace transform of LTI system) and $P(s)=c\frac{\prod^m_{k=1}(s-z_k)}{\prod^n_{k=1}(s-p_k)}$ for some $m\leq n$ and real constant $c$. Then we have $$\Phi_\mathcal{Y}(\omega)=P(i\omega)P(-i\omega)=c^2\frac{\prod^m_{k=1}(\omega^2+z_k^2)}{\prod^n_{k=1}(\omega^2+p_k^2)}$$&lt;/p>
&lt;p>Lemma: For any valid PSD, there exists a spectral factorization $\Phi_\mathcal{Y}(\omega)=\frac{\sum^m_{k=0}a_k(\omega^2)^k}{\sum^n_{k=0}b_k(\omega^2)^k}=P(i\omega)P(-i\omega)$ where $P(s)$ is an asymptotically stable n-th order transfer function, iff&lt;/p>
&lt;ol>
&lt;li>$\Phi_\mathcal{Y}(\omega)$ is a ratio of polynomials of $\omega^2$&lt;/li>
&lt;li>All coefficients $a_k$ and $b_k$ are real&lt;/li>
&lt;li>The denominator has no positive real roots in $\omega^2$&lt;/li>
&lt;li>Any positive real roots in the numerator have even multiplicity&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;h2 id="notes-for-math-typing-in-hexo">Notes for math typing in Hexo&lt;/h2>
&lt;ol>
&lt;li>Escape &lt;code>\&lt;/code> by &lt;code>\\&lt;/code>. Especially escape &lt;code>{&lt;/code> by &lt;code>\\{&lt;/code> instead of &lt;code>\{&lt;/code>, and escape &lt;code>\\&lt;/code> by &lt;code>\\\\&lt;/code>.&lt;/li>
&lt;li>Be careful about &lt;code>_&lt;/code>, it&amp;rsquo;s used in markdown as italic indicator. Add space after &lt;code>_&lt;/code> is a useful solution.&lt;/li>
&lt;li>&lt;a class="link" href="https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference" target="_blank" rel="noopener"
>Some useful Mathjax tricks at StackExchange&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.overleaf.com/learn/latex/List_of_Greek_letters_and_math_symbols" target="_blank" rel="noopener"
>Several capital Greek characters should directly use its related Latin alphabet with &lt;code>\mathrm&lt;/code> command&lt;/a>.&lt;/li>
&lt;/ol>
&lt;p>Although I have migrated to Hugo, some tricks might still be relevant.&lt;/p>
&lt;/blockquote></description></item></channel></rss>