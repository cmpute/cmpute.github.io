<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Probability on JacobZ</title><link>https://zyxin.xyz/blog/en/tags/Probability/</link><description>Recent content in Probability on JacobZ</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Tue, 26 Mar 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://zyxin.xyz/blog/en/tags/Probability/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes for Stochastic System</title><link>https://zyxin.xyz/blog/en/2019-03/StochasticSystemNotes/</link><pubDate>Tue, 26 Mar 2019 00:00:00 +0000</pubDate><guid>https://zyxin.xyz/blog/en/2019-03/StochasticSystemNotes/</guid><description>&lt;blockquote>
&lt;p>Prerequisites: Knowledge of Elementary Calculus, Linear Algebra and Probability&lt;/p>
&lt;/blockquote>
&lt;h2 id="discrete-time-stochastic-system">Discrete-Time Stochastic System&lt;/h2>
&lt;h3 id="stochastic-sequences">Stochastic Sequences&lt;/h3>
&lt;ul>
&lt;li>Definition: Given $k\in\mathbb{K}\subseteq\mathbb{Z}$ a sequence of integers, $\mathcal{X}(k,\omega): (\Omega,\mathcal{F},\mathbb{P})\to(\mathbb{R}^n,\mathcal{F}_ \mathcal{X},\mathbb{P}_ \mathcal{X})$ is a random/stochastic sequence.&lt;/li>
&lt;li>Uncertainties: Consider a casual system $F$ relates some scalar inputs $u(k)$ to output $x(k)$
&lt;ul>
&lt;li>&lt;strong>Epistemic/Model uncertainty&lt;/strong>: $\mathcal{X}(k,\omega)=F(k,u(k),u(k-1),\ldots,\omega)$. (system is stochastic and input is deterministic).&lt;/li>
&lt;li>&lt;strong>Aleatoric/Input uncertainty&lt;/strong>: $\mathcal{X}(k,\omega)=f(k,U(k,\omega),u(k-1,\omega),\ldots)$ (system is deterministic and input is stochastic).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Realization&lt;/strong>: An outcome $\mathcal{X}(k,\omega)=x(k)$ given $\omega$ is called a realization of stochastic sequence $\mathcal{X}$&lt;/li>
&lt;li>Terminology and Convention
&lt;ul>
&lt;li>$\mathcal{X}(k,\omega)$ is often written as $\mathcal{X}(k)$ when there&amp;rsquo;s no ambiguity.&lt;/li>
&lt;li>$\mathbb{K}=\mathbb{Z}$ if not specified.&lt;/li>
&lt;li>Sequence over a set $\mathcal{K}_1\subseteq\mathbb{K}$ are denoted $\mathcal{X}(\mathcal{K}_1)$.&lt;/li>
&lt;li>$\mathcal{X}$ denotes $\mathcal{X}(\mathbb{K})$ if not specified.&lt;/li>
&lt;li>Consecutive subsequence: $$\mathcal{X}(k:l)=\{\mathcal{X}(k),\mathcal{X}(k+1),\ldots,\mathcal{X}(l)\},\;x(k:l)=\{x(k),x(k+1),\ldots,x(l)\}$$&lt;/li>
&lt;li>Abbreviations:
&lt;ul>
&lt;li>&lt;em>&lt;strong>SS&lt;/strong>&lt;/em> - stochastic sequence&lt;/li>
&lt;li>&lt;em>&lt;strong>IID&lt;/strong>&lt;/em> - independent indentically distributed&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="probabilistic-characterization">Probabilistic characterization&lt;/h3>
&lt;ul>
&lt;li>Distribution and density: $$F_ \mathcal{X}\left(k:l;x(k:l)\right)\equiv\mathbb{P}((\mathcal{X}_ i(k)\leqslant x_i(k))\cap\cdots\cap(\mathcal{X}_ i(l)\leqslant x_ i(l)),\;i=1\ldots n)$$ $$f_ \mathcal{X}\left(k:l;x(k:l)\right)\equiv \frac{\partial^{n(l-k+1)}}{\partial x_ 1(k)\cdots\partial x_ n(l)}F_ \mathcal{X}(k:l;x(k:l))$$
Here $k:l$ actually denotes a set of consecutive integers, it can be also changed to ordinary sets $\{k,l\}$ or single scalar $k$.&lt;/li>
&lt;li>&lt;strong>Ensemble Average&lt;/strong>: $\mathbb{E}[\psi(\mathcal{X}(k))]$, doing summation over different realization at same time $k$
&lt;ul>
&lt;li>&lt;strong>Mean&lt;/strong>: $\mu_ \mathcal{X}(k)\equiv\mathbb{E}[\mathcal{X}(k)]=\int^\infty_{-\infty}x(k)f_ \mathcal{X}(k;x(k))\mathrm{d}x(k)$&lt;/li>
&lt;li>&lt;strong>Conditional Mean&lt;/strong>: $\mu_ \mathcal{X}(l|k)\equiv\mathbb{E}[\mathcal{X}(l)|\mathcal{X}(k)=x(k)]$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Time Average&lt;/strong>: $\frac{1}{2N+1}\sum^N_{k=-N}\psi(\mathcal{X}(k))$, doing summation over different time k of same realization&lt;/li>
&lt;li>&lt;strong>Autocorrelation&lt;/strong>:
&lt;ul>
&lt;li>Scalar case: $r_ \mathcal{X}(k,l)\equiv\mathbb{E}[\mathcal{X}(k)\mathcal{X}(l)]=\int^\infty_{-\infty}\int^\infty_{-\infty}x(k)x(l)f_ \mathcal{X}(k,l;x(k,l))\mathrm{d}x(k)\mathrm{d}x(l)$&lt;/li>
&lt;li>Vector case: $R_ \mathcal{X}(k,l)\equiv\mathbb{E}[\mathcal{X}(k)\mathcal{X}^\top(l)]$&lt;/li>
&lt;li>Conditional autocorrelation: $R_ \mathcal{X}(k,l|q)\equiv\mathbb{E}[\mathcal{X}(k)\mathcal{X}^\top(l)|\mathcal{X}(q)=x(q)]$&lt;/li>
&lt;li>Often we denote $C_ \mathcal{X}(k)=R_ \mathcal{X}(k,k)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Autocovariance&lt;/strong>:
&lt;ul>
&lt;li>Scalar case: $\kappa_ \mathcal{X}(k,l)\equiv\mathbb{E}[(\mathcal{X}(k)-\mu_ \mathcal{X}(k))(\mathcal{X}(l)-\mu_ \mathcal{X}(l))]$&lt;/li>
&lt;li>Vector case: $\mathrm{K}_ \mathcal{X}(k,l)\equiv\mathbb{E}[(\mathcal{X}(k)-\mu_ \mathcal{X}(k))(\mathcal{X}(l)-\mu_ \mathcal{X}(l))^\top]$&lt;/li>
&lt;li>Conditional autocovariance: $\mathrm{K}_ \mathcal{X}(k,l|q)\equiv\mathbb{E}[(\mathcal{X}(k)-\mu_ \mathcal{X}(k|q))(\mathcal{X}(l)-\mu_ \mathcal{X}(l|q))^\top]$&lt;/li>
&lt;li>Often we denote $S_ \mathcal{X}(k|q)=\mathrm{K}_ \mathcal{X}(k,k|q)$&lt;/li>
&lt;li>Useful conclusion: $\mathrm{K}(a,b)=\mathrm{K}(b,a)^T$&lt;/li>
&lt;li>Normalized (&lt;strong>autocorrelation coefficient&lt;/strong>): $\rho_ \mathcal{X}(k,l)\equiv\mathrm{K}_ \mathcal{X}(k,l)/\sigma^2_{\mathcal{X}(k)}\sigma^2_{\mathcal{X}(l)}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Strong Stationarity&lt;/strong>(aka. strict sense): (necessarily identically distributed over time) $$\forall x(k:l)\in\mathbb{R}^{n(l-k+1)},\;\forall s\in\mathbb{Z},\;f_ \mathcal{X}(k:l;x(k:l))=f_ \mathcal{X}(k+s:l+s;x(k:l))$$&lt;/li>
&lt;li>&lt;strong>Weak Stationarity&lt;/strong>(aka. wide sense): $\forall k,l$ if $$\mu_ \mathcal{X}(k)=\mu_ \mathcal{X}(l)\;\text{and}\;\mathrm{K}_ \mathcal{X}(k,l)=\mathrm{K}_ \mathcal{X}(k+s,l+s)\equiv\bar{\mathrm{K}}_ \mathcal{X}(s)$$ Weak stationarity is necessary condition for stationarity. (Equal when Gaussian distributed)&lt;/li>
&lt;li>&lt;strong>Ergodicity&lt;/strong>: $\mathcal{X}$ is called ergodic in $\psi$ if
&lt;ol>
&lt;li>$\mathbb{E}[\psi(\mathcal{X})]$ is stationary&lt;/li>
&lt;li>Ensemble average is equal to Time average, that is $$ \frac{1}{2N+1}\sum^N_{k=-N}\psi(\mathcal{X}(k))\to\mathbb{E}[\psi(\mathcal{X})];\text{as};l\to \infty$$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;h3 id="markov-sequence">Markov Sequence&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Markov Sequence&lt;/strong>: A ss. $\mathcal{X}(k)$ is called a (discrete-time) Markov sequence if $$\begin{split}f_ \mathcal{X}\left(k;x(k)\middle|\mathcal{X}(k-1)=x(k-1),\mathcal{X}(k-2)=x(k-2),\ldots\right) \\=f_ \mathcal{X}(k;x(k)|\mathcal{X}(k-1)=x(k-1))\end{split}$$
&lt;ul>
&lt;li>We often make some assumption on the initial condition $\mathcal{X}(0)$, such as known, deteministic or uniformly distributed within certain domain.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Markov Chains&lt;/strong>: Markov sequence with discrete set of values(states) ${x_1\ldots x_m}$&lt;/li>
&lt;li>&lt;strong>Hidden Markov model&lt;/strong>: Sequence $\mathcal{Y}$ is called a Hidden Markov Model if it&amp;rsquo;s modeled by a system of the form $$\begin{align}\mathcal{X}(k+1)&amp;amp;=g(k,\mathcal{X}(k),\mathcal{W}(k)) \\ \mathcal{Y}(k)&amp;amp;=h(k,\mathcal{X}(k),\mathcal{W}(k))\end{align}$$
We also say that $\mathcal{Y}$ has a &lt;strong>(discrete-time) stochastic state space&lt;/strong>.&lt;/li>
&lt;li>&lt;strong>Guassian-Markov Sequence&lt;/strong> (GMS): $\mathcal{X}(k+1)=g(k,\mathcal{X}(k),\mathcal{W}(k))$ where $\mathcal{W}(k)$ is iid. Guassian&lt;/li>
&lt;/ul>
&lt;h3 id="linear-stochastic-sequence">Linear Stochastic Sequence&lt;/h3>
&lt;p>$$\begin{align}\mathcal{X}(k+1)&amp;amp;=A(k)\mathcal{X}(k)+B(k)\mathcal{W}(k) \\ \mathcal{Y}(k)&amp;amp;=C(k)\mathcal{X}(k)+D(k)\mathcal{W}(k)\end{align}$$&lt;/p>
&lt;blockquote>
&lt;p>For linear Markov sequences, the deterministic mean sequence and centered uncertain sequence completely decouple. So we often assume that $\mathcal{X}(k)$ and $\mathcal{Y}(k)$ are &lt;strong>centered&lt;/strong> in this case, with regard to deterministic inputs. The equation with deterministic inputs is often written as $$\mathcal{X}(k+1)=A(k)\mathcal{X}(k)+B_u(k)u(k)+B_\mathcal{W}(k)\mathcal{W}(k)$$&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>
&lt;p>Recursive-form expectations:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Mean&lt;/strong>: $\mu_ \mathcal{X}(k+1|q)=A(k)\mu_ \mathcal{X}(k|q)+B(k)\mu_\mathcal{W}(k)$&lt;/li>
&lt;li>&lt;strong>Covariance (Discrete-time algebraic Lyapunov/Stein difference equation)&lt;/strong>: $$S_ \mathcal{X}(k+1|q)=A(k)S_ \mathcal{X}(k|q)A^\top(k)+B(k)S_\mathcal{W}(k)B^\top(k)$$
&lt;blockquote>
&lt;p>Can be solved with &lt;code>dlyap&lt;/code> in MATLAB&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Convergence when $k\to\infty$:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Mean convergence&lt;/strong>: $\mu_ \mathcal{X}(k|q)$ converges requires $\max_i|\lambda_i(A)|&amp;lt;1$ ($\lambda$ denotes eigenvalue)&lt;/li>
&lt;li>&lt;strong>Covariance convergence&lt;/strong>: $S_ \mathcal{X}(k|q)$ converges requires $\max_i|\lambda_i(A)|&amp;lt;1$&lt;/li>
&lt;li>&lt;strong>(Discrete-time) Lyapunov equation&lt;/strong> (Stein equation): $\bar{S}_ \mathcal{X}=A\bar{S}_ \mathcal{X}A^\top+B\bar{S}_\mathcal{W}(k)B^\top$. Solution for this equation exists iff. $A$ is asymptotically stable (characterizing sequence is stationary).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Explicit state transition: By recursive substitution, $$\mathcal{X}(k)=\Psi(k,q)\mathcal{X}(q)+\sum^{k-1}_ {i=q}\Gamma(k,i)\mathcal{W}(i)$$ where state transition matrix $\Psi(k,q)=\begin{cases}I, &amp;amp;k=q \\ \prod^{k-1}_ {i=q}A(i),&amp;amp; k&amp;gt;q\end{cases}$ and $\Gamma(k,i)=\Psi(k,i+1)B(i)$.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Conditioned Mean sequence&lt;/strong>: $\mu_ \mathcal{X}(k|q)=\Psi(k,q)\mu_ \mathcal{X}(q)+\sum^{k-1}_ {i=q}\Gamma(k,i)\mu_\mathcal{W}(i)$&lt;/li>
&lt;li>&lt;strong>Conditioned Autocovariance Matrix&lt;/strong>: $$\mathrm{K}_ \mathcal{X}(k,l|q)=\Psi(k,q)S_ \mathcal{X}(q)\Psi^\top(l,q)+\sum^{min\{k,l\}-1}_ {i=q}\Gamma(k,i)S_\mathcal{W}(i)\Gamma^\top(l,i)$$
&lt;ul>
&lt;li>A special case: $S_ \mathcal{X}(k|q)=\mathrm{K}_ \mathcal{X}(k,k|q)=\sum^{k-1}_ {i=q}\Gamma(k,i)S_\mathcal{W}(i)\Gamma^\top(k,i)$, $S_ \mathcal{X}(k|k)=0$&lt;/li>
&lt;li>Useful equation (stationary centered case): $$\mathrm{K}_ \mathcal{X}(k,l)=\begin{cases} S_ \mathcal{X}(k)\cdot(A^\top)^{(l-k)}&amp;amp;,l&amp;gt;k\\ S_ \mathcal{X}(k)&amp;amp;,l=k\\ A^{(k-l)}S_ \mathcal{X}(k)&amp;amp;,l&amp;lt; k\end{cases}$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Conditional Autocorrelation Matrix&lt;/strong>: $R_ \mathcal{X}(k,l|q)=\mathrm{K}_ \mathcal{X}(k,l|q)+\mu_ \mathcal{X}(k|q)\mu_ \mathcal{X}^\top(l|q)$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Observation $\mathcal{Y}$ property:&lt;/p>
&lt;ul>
&lt;li>Mean: $\mu_ \mathcal{Y}(k|q)=C(k)\mu_ \mathcal{X}(k|q)+D(k)\mu_\mathcal{W}(k)$&lt;/li>
&lt;li>Covariance: $$\mathrm{K}_ \mathcal{Y}(k,l|q)=\begin{cases}C(k)\mathrm{K}_ \mathcal{X}(k,l|q)C^\top(l)+C(k)\Gamma(k,l)S_W(l)D^\top(l)&amp;amp;:k&amp;gt;l \\C(k)S_ \mathcal{X}C^\top(k)+D(k)S_\mathcal{W}(k)D^\top(k)&amp;amp;:k=l\\ C(k)\mathrm{K}_ \mathcal{X}(k,l|q)C^\top(l)+D(k)S_\mathcal{W}(k)\Gamma^\top(l,k)C^\top(l)&amp;amp;:k&amp;lt; l\end{cases}$$&lt;/li>
&lt;li>Stationary time-invariant covariance: $$\mathrm{K}_ \mathcal{Y}(s)=\begin{cases}CA^{|s|}\bar{S}_ \mathcal{X}C^\top+CA^{|s|-1}B\bar{S}_ WD^\top&amp;amp;:s\neq 0 \\C\bar{S}_ \mathcal{X}C^\top+D\bar{S}_ WD^\top&amp;amp;:s=0\end{cases}$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="gaussian-stochastic-sequence">Gaussian Stochastic Sequence&lt;/h3>
&lt;ul>
&lt;li>Jointly Gaussian $\Rightarrow, \nLeftarrow$ Marginally Gaussian&lt;/li>
&lt;li>$c^\top X$ is Gaussian $\Leftrightarrow X$ is Gaussian&lt;/li>
&lt;li>Conditional Gaussian: if $X$ and $Y$ are Gaussian, then $X|Y \sim \mathcal{N}(\mu_{X|Y},S_{X|Y})$ where $\mu_{X|Y}=\mu_X+S_{XY}S_Y^{-1}(Y-\mu_Y)$, $S_{X|Y}=S_X-S_{XY}S_Y^{-1}S_{YX}$&lt;/li>
&lt;li>A linear controllable GMS $\mathcal{X}(k)$ is stationary iff. $A(k)=A, B(k)=B$ (time-invariant) and $A$ is asymptotically stable.&lt;/li>
&lt;li>All LTI stationary GMS are also ergodic in all finite momoents&lt;/li>
&lt;li>Solve $\mathcal{X}(k+1)=A\mathcal{X}(k)+B\mathcal{W}(k)$ when $\mathcal{X}$ is stationary ($\max_i|\lambda_i(A)|&amp;lt;1$)
&lt;ol>
&lt;li>solve $\bar{\mu}_ \mathcal{X}=A\bar{\mu}_ \mathcal{X}+B\bar{\mu}_\mathcal{W}$ for $\bar{\mu}$&lt;/li>
&lt;li>solve $\bar{S}_ \mathcal{X}=A\bar{S}_ \mathcal{X}A^\top+B\bar{S}_ \mathcal{W}B^\top$ for $\bar{S}_ \mathcal{X}$&lt;/li>
&lt;li>calculate $\Sigma_ \mathcal{X}(k:l|q)=\begin{bmatrix}\mathrm{K}_ \mathcal{X}(k,k|q)&amp;amp;\cdots&amp;amp;\mathrm{K}_ \mathcal{X}(k,l|q)\\ \vdots&amp;amp;\ddots&amp;amp;\vdots\\ \mathrm{K}_ \mathcal{X}(l,k|q)&amp;amp;\cdots&amp;amp;\mathrm{K}_ \mathcal{X}(l,l|q)\end{bmatrix}$ using $\bar{S}_ \mathcal{X}$&lt;/li>
&lt;li>Then $f_ \mathcal{X}(k:l;x(k:l))$ is determined with $\mu_ \mathcal{X}(k:l)=\{\bar{\mu}_ \mathcal{X},\bar{\mu}_ \mathcal{X}\ldots\bar{\mu}_ \mathcal{X}\}$ and $\Sigma_ \mathcal{X}(k:l)$ above.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;h3 id="observation--filtering">Observation &amp;amp; Filtering&lt;/h3>
&lt;blockquote>
&lt;p>For deterministic version, please check &lt;a href="{% post_path ControlSystemNotes %}#State-Estimation-Observer-Design">my notes for control system&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>(LTI) &lt;strong>Luenberger observer&lt;/strong>: $$\begin{align}\hat{\mathcal{X}}(k+1)&amp;amp;=A\hat{\mathcal{X}}(k)+L(\hat{\mathcal{Y}}(k)-\mathcal{Y}(k))+B\bar{\mu}_ \mathcal{W} \\ \hat{\mathcal{Y}}(k)&amp;amp;=C\hat{\mathcal{X}}(k)+D\bar{\mu}_\mathcal{W}\end{align}$$ where $L$ is the observer gain.
&lt;ul>
&lt;li>Note: We often assume that the process &amp;amp; measurement noise are decoupled and independent&lt;/li>
&lt;li>Combined form: $\hat{\mathcal{X}}(k+1)=[A+LC]\hat{\mathcal{X}}(k)-L\mathcal{Y}(k)+B\mu_\mathcal{W}(k)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>State estimation residual&lt;/strong> $r(k)=\mathcal{X}(k)-\hat{\mathcal{X}}(k)$
&lt;ul>
&lt;li>Combined form: $r(k+1)=[A+LC]r(k)+[B+LD]\tilde{\mathcal{W}}(k)$&lt;/li>
&lt;li>Stationary covariance can be solved by a Lyapunov equation $$\bar{S}_ r=[A+LC]\bar{S}_ r[A+LC]^T+[LD+B]\bar{S}_ \mathcal{W}[LD+B]^T$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>A common objective: minimize $\bar{S}_r=\mathbb{E}(rr^\top)$
&lt;ul>
&lt;li>Solutions: $L=-A\bar{S}_ rC^\top[C\bar{S}_ rC^\top+D\bar{S}_ \mathcal{W}D^\top]^{-1}$ (&lt;strong>Kalman observer gain&lt;/strong>)&lt;/li>
&lt;li>Or solve &lt;strong>(Discrete-time) Algebraic Riccati equation&lt;/strong> $$\bar{S}_ r=A\bar{S}_ rA^\top+B\bar{S}_ \mathcal{W}B^\top-A\bar{S}_ rC^\top[C\bar{S}_ rC^\top+D\bar{S}_ \mathcal{W}D^\top]^{-1}C\bar{S}_ rA^\top$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Innovation sequence&lt;/strong> $e(k)=\hat{\mathcal{Y}}(k)-\mathcal{Y}(k)$ with $L$ is optimal
&lt;ul>
&lt;li>We can find that $\mu_e=0$ and $\mathrm{K}_e(k+s,k)=\begin{cases}C\bar{S}_rC^\top+D\bar{S}_WD^T&amp;amp;:s=0 \\0&amp;amp;:s\neq0\end{cases}$. So the innovation sequence is iid. (only in Kalman observer)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>(Output) Probabilistically-equivalent model&lt;/strong>: $$\begin{align}\mathcal{X}(k+1)&amp;amp;=A\mathcal{X}(k)+Le(k) \\ \mathcal{Y}(k)&amp;amp;=C\mathcal{X}(k)-e(k)\end{align}$$&lt;/li>
&lt;/ul>
&lt;h2 id="markov-chains">Markov Chains&lt;/h2>
&lt;blockquote>
&lt;p>Content in this section comes from EECS 501
In this specific field, we often use stand-alone analysis methods.&lt;/p>
&lt;/blockquote>
&lt;h3 id="basic-definitions">Basic definitions&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>State distribution&lt;/strong>: We denote row vector $\pi_t$ as $\pi_t(x)=\mathbb{P}(\mathcal{X}_t=x),\; x\in S$ ($S$ is the set of states). Directly we have $\sum_x\pi(x)=1$&lt;/li>
&lt;li>&lt;strong>Time homogeneous&lt;/strong>: $\mathbb{P}(\mathcal{X}_ {t+1}=y|\mathcal{X}_ t=x)=\mathbb{P}(\mathcal{X}_{s+1}=y|\mathcal{X}_s=x)\;\forall s,t$&lt;/li>
&lt;li>&lt;strong>One-step transition probability matrix&lt;/strong>: $P_t=[P_{xy,t}]$ where $P_{xy,t}=\mathbb{P}(\mathcal{X}_ {t+1}=y|\mathcal{X}_ t=x)$.
&lt;ul>
&lt;li>Time-homo case: $P=[P_{xy}]$ where $P_{xy}=p(y|x)$&lt;/li>
&lt;li>Rows of the matrix sum up to 1.&lt;/li>
&lt;li>This matrix is also called &lt;strong>stochastic matrix&lt;/strong>.&lt;/li>
&lt;li>Extend this matrix to continuous states, then we use &lt;strong>transition kernel&lt;/strong> $T(x,y)$ to describe the transition probability,&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>m-step transition probability matrix&lt;/strong>: $P_{xy,t}^{(m)}=\mathbb{P}(\mathcal{X}_ {t+m}=y|\mathcal{X}_ t=x)$
&lt;ul>
&lt;li>&lt;strong>Chapman-Kolmogorov Equation&lt;/strong>: $P^{(n+m)}_t=P^{(n)}_t P^{(m)}_t$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="state-variables">State Variables&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Hitting time&lt;/strong>: $T_1(y)=\min\{n\geq 0:\mathcal{X}_ n=y\},\;T_k(y)=\min\{n&amp;gt;T_{k-1}(y):\mathcal{X}_ n=y\}$ where $n\in\mathbb{N}$
&lt;ul>
&lt;li>&lt;strong>Period&lt;/strong>: For state $i\in x$, its period is the greatest common divisor of $\{n&amp;gt;1|T_n(i)&amp;gt;0\}$. A Markov Chain is &lt;strong>aperiodic&lt;/strong> If all states have period 1.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Return probability&lt;/strong>: $f_{xy}=\mathbb{P}(T_1(y)&amp;lt;\infty|\mathcal{X}_0=x)$&lt;/li>
&lt;li>&lt;strong>Occupation time&lt;/strong>: $V(y)=\sum^\infty_{n=1}\unicode{x1D7D9}_{\mathcal{X_n}}(y)$&lt;/li>
&lt;li>Some properties
&lt;ul>
&lt;li>$f_{xy}=\mathbb{P}(V(y)\geqslant 1|\mathcal{X}_0=x)$&lt;/li>
&lt;li>$\mathbb{P}(V(y)=m|\mathcal{X}_ 0=x)=\begin{cases} 1-f_{xy}&amp;amp;:m=0\\f_{xy}f_{yy}^{m-1}(1-f_{yy})&amp;amp;:m\geqslant 1\end{cases}$&lt;/li>
&lt;li>$\mathbb{E}[ V(y)|\mathcal{X}_ 0=x]=\begin{cases} 0&amp;amp;:f_{xy}=0\\\infty&amp;amp;:f_{xy}&amp;gt;0,\ f_{yy}=1\\f_{xy}/(1-f_{yy})&amp;amp;:f_{xy}&amp;gt;0,\ f_{yy}&amp;lt;1\end{cases}$&lt;/li>
&lt;li>$\mathbb{E}[ V(y)|\mathcal{X}_ 0=x]=\sum^\infty_{n=1}P^{(n)}_{xy}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="state-classification">State Classification&lt;/h3>
&lt;blockquote>
&lt;p>Here we usually consider only time-homogeneous Markov Chains&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Accessible&lt;/strong> ($x\to y$): $\exists n\;\text{s.t.}\ P_{xy}^{(n)}&amp;gt;0$
&lt;ul>
&lt;li>$x\to y \Leftrightarrow f_{xy}&amp;gt;0$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Communicate&lt;/strong> ($x\leftrightarrow y$): $x\to y\;\text{and}\;y\to x$. This is a &lt;a class="link" href="https://en.wikipedia.org/wiki/Equivalence_relation" target="_blank" rel="noopener"
>equivalence relation&lt;/a>.&lt;/li>
&lt;li>&lt;strong>Equivalent class&lt;/strong>: set of states that communicate with each other&lt;/li>
&lt;li>&lt;strong>Irreducible&lt;/strong>: a Markov chain with only one communicating class&lt;/li>
&lt;li>&lt;strong>Absorbing/Closed state&lt;/strong>: $P_{xx}=1$
&lt;ul>
&lt;li>&lt;strong>Absorbing class&lt;/strong>: set $C$ is absorbing iff $\forall x \in C, \sum_{y\in C}P_{xy}=1$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Transient state&lt;/strong>: $f_{xx}&amp;lt;1 \Leftrightarrow \mathbb{E}[V_i|\mathcal{X}_0=i]&amp;lt;\infty$&lt;/li>
&lt;li>&lt;strong>Recurrent state&lt;/strong>: $f_{xx}=1 \Leftrightarrow \mathbb{E}[V_i|\mathcal{X}_0=i]=\infty$
&lt;ul>
&lt;li>&lt;strong>Positive recurrent&lt;/strong>: $\mathbb{E}[T_1(x)|\mathcal{X}_0=x]&amp;lt;0$&lt;/li>
&lt;li>&lt;strong>Null recurrent&lt;/strong>: $\mathbb{E}[T_1(x)|\mathcal{X}_0=x]=0$&lt;/li>
&lt;li>These two kind of recurrent states also make up communicating classes&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Some properties
&lt;ul>
&lt;li>If $x$ is (positive) recurrent and $x\to y$, then $y$ is also (positive) recurrent and $f_{xy}=f_{yx}=1$&lt;/li>
&lt;li>Every closed and finite subset of $X$ contains at least one (positive) recurrent state.&lt;/li>
&lt;li>All states of a communicating class are either positive recurrent, null recurrent or transient.&lt;/li>
&lt;li>Method to determine whether class $C$ is recurrent/transient
&lt;ol>
&lt;li>If $C$ is non-closed, it&amp;rsquo;s transient&lt;/li>
&lt;li>If $C$ is closed and finite, then $C$ is positive recurrent&lt;/li>
&lt;li>If $C$ is closed and infinite, then $C$ can be either positive/null recurrent or transient&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>A typical example is the &lt;a class="link" href="https://en.wikipedia.org/wiki/Birth%E2%80%93death_process" target="_blank" rel="noopener"
>birth-death chain&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Stationary distribution&lt;/strong>: $\bar{\pi}\;\text{s.t.}\;\bar{\pi}=\bar{\pi} P$
&lt;ul>
&lt;li>Stationary in limit form: $\bar{\pi}=\lim_{n\to \infty} \frac{1}{n}\sum^{n-1}_{t=0} \pi_t$
&lt;ul>
&lt;li>This is a &lt;strong>Cesaro&lt;/strong> limit, we use this to deal with the problem that $\lim_{n\to \infty} \pi_t$ might not exist if the chain is periodic.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Reversibility criterion for stationary: $\forall i,j \in S, \pi_i P_{ij} = \pi_j P_{ji}$ (a.k.a detailed balance condition), then the process is reversible and therefore stationary.&lt;/li>
&lt;li>Existence for stationary distribution satisfying $\bar{\pi}=\bar{\pi} P$
&lt;ol>
&lt;li>If the chain has single positive recurrent class, then exists unique solution: $\bar{\pi}(x)=0$ for all transient or null recurrent $x$&lt;/li>
&lt;li>If the chain has multiple positive recurrent class, then there are multiple solutions, for each positive recurrent $x$ we have a $\bar{\pi}^i$.&lt;/li>
&lt;li>If the chain has only transient and null recurrent states, there is no solution$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Convergence of stationary distribution
&lt;ul>
&lt;li>If the chain has positive recurrent class, then $\frac{1}{n}\sum^n_{t=1}\mathbb{P}(\mathcal{X}_t=j)\xrightarrow[n\to \infty]{}\bar{\pi}_j,\;\forall \mu_0$&lt;/li>
&lt;li>(Ergodic) If the chain is positive recurrent, then $\frac{1}{n}\sum^n_{t=1}\unicode{x1D7D9}_{\mathcal{X}_t}(j)\xrightarrow[n\to \infty]{\text{a.s.}}\bar{\pi}_j$&lt;/li>
&lt;li>If the chain is positive recurrent and aperiodic, then $\mathbb{P}(\mathcal{X}_n=j)\xrightarrow[n\to \infty]{}\bar{\pi}_j$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="continuous-time-stochastic-system">Continuous-Time Stochastic System&lt;/h2>
&lt;h3 id="stochastic-sequences-1">Stochastic Sequences&lt;/h3>
&lt;ul>
&lt;li>Definition: Given $t\in\mathbb{T}\subset\mathbb{R}$ a sequence of time, $\mathcal{X}(t,\omega): (\Omega,\mathcal{F},\mathbb{P})\to(\mathbb{R}^n,\mathcal{F}_ \mathcal{X},\mathbb{P}_ \mathcal{X})$ is a continuous stochastic sequence.&lt;/li>
&lt;li>Most definitions are similar to discrete sequence (including &lt;strong>stationarity&lt;/strong>), while the sequence is often defined as $\mathcal{X}(\mathcal{G}),\;\mathcal{G}={t_1,t_2,\ldots,t_N}\subset\mathbb{T}, t_i&amp;lt; t_{i+1}$&lt;/li>
&lt;li>&lt;strong>Stochastic Differential Equation (SDE)&lt;/strong>: $$\mathrm{d}\mathcal{X}(t)=F(\mathcal{X}(t),t)\mathrm{d}t+\sum^r_{i=1}G_i(\mathcal{X}(t),t)\mathrm{d}\mathcal{W}_i(t)$$ Here $\mathcal{W}$ is often a certain kind of &lt;em>noise&lt;/em> random process.&lt;/li>
&lt;/ul>
&lt;h3 id="markov-sequence-1">Markov Sequence&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Markov Sequence&lt;/strong>: A ss. $\mathcal{X}(k)$ is called a (continuous-time) Markov sequence if for the set of times $\mathcal{G}={t_1,t_2,\ldots,t_N}$ with $t_i &amp;lt; t_{i+1}$, we have $$\begin{split}f_ \mathcal{X}\left(t_N;x(t_N)\middle|\mathcal{X}(t_{N-1})=x(t_{N-1}),\mathcal{X}(t_{N-2})=x(t_{N-2}),\ldots\right)\qquad \\ =f_ \mathcal{X}(t_N;x(t_N)|\mathcal{X}(t_{N-1})=x(t_{N-1}))\end{split}$$&lt;/li>
&lt;li>&lt;strong>Hidden Markov Model&lt;/strong>: Definition similar to discrete case. A &lt;strong>continuous-time stochastic state space&lt;/strong> is of the form $$\begin{align}\dot{\mathcal{X}}(t)&amp;amp;=F(\mathcal{X},t)+G(\mathcal{X},t)\mathrm{d}\mathcal{W}(t)/\mathrm{d}t \\ \mathcal{Y}(t)&amp;amp;=H(\mathcal{X},t)+J(\mathcal{X},t)\mathrm{d}\mathcal{W}(t)/\mathrm{d}t\end{align}$$ where $\mathcal{W}$ is usually &lt;a class="link" href="#Wiener-Process" >Wiener process&lt;/a> and white noise $\mathrm{d}\mathcal{W}(t)/\mathrm{d}t$ is often written as $\mathcal{U}(t)$.
&lt;ul>
&lt;li>The state space is &lt;strong>affine&lt;/strong> if $F(\mathcal{X}(t),t)=A(t)\mathcal{X}(t)+u(t),\;N(\mathcal{X},t)=C(t)\mathcal{X}(t)+v(t)$&lt;/li>
&lt;li>The state space is &lt;strong>bilinear&lt;/strong> if $G(\mathcal{X},t)\mathrm{d}\mathcal{W}(t)=\sum^r_{i=1}B_i(t)\mathcal{X}\mathrm{d}\mathcal{W}_i(t)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="poisson-counters">Poisson Counters&lt;/h3>
&lt;ul>
&lt;li>Definition: It&amp;rsquo;s a stochastic Markow process $\mathcal{N}(t)\in\{0,1,2,\ldots\}$ with the characteristic that it jumps up by only one integer at a time.
&lt;ul>
&lt;li>Characteristics: transition times are random, transition step is always 1.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Transition rule: $\frac{\partial}{\partial t}p_\mathcal{N}=\begin{cases}-\lambda p_\mathcal{N}(t;n)&amp;amp;:n=0\\-\lambda p_\mathcal{N}(t;n)+\lambda p_\mathcal{N}(t;n-1)&amp;amp;:n&amp;gt;0\end{cases}$
&lt;ul>
&lt;li>From the rule we can conclude that $p_\mathcal{N}(t;n)=\frac{1}{n!}(\lambda t)^n e^{-\lambda t}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Bidirectional counter: $\mathcal{N}(t)$ is defined as one-directional. $\mathcal{N}_1(t)-\mathcal{N}_2(t)$ is called &lt;strong>bidirectional poisson counter&lt;/strong>.&lt;/li>
&lt;li>Expectation: $\mathbb{E}[\mathcal{N}(t)]=\lambda t,\;\mathbb{E}[\mathrm{d}\mathcal{N}(t)]=\lambda\mathrm{d}t$&lt;/li>
&lt;li>&lt;strong>Ito calculus&lt;/strong> for Poisson counter:
&lt;ul>
&lt;li>&lt;strong>Ito sense&lt;/strong>: $n(t)$ is a realization of poisson counter $\mathcal{N}$. $x(t)$ is a solution in Ito sense to $\mathrm{d}x(t)=F(x(t),t)\mathrm{d}t+G(x(t),t)\mathrm{d}n(t)$ if
&lt;ol>
&lt;li>On all intervals where $n(t)$ is constant, $\dot{x}(t)=F(x(t),t)$&lt;/li>
&lt;li>If $n(t)$ jumps at time $t_1$, $\lim_{t\to t_1^+}x(t)=\lim_{t\to t_1^-}x(t)+G\left(\lim_{t\to t_1^+}x(t),t_1\right)$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>Ito rule&lt;/strong>: Given a fuction $\psi(\mathcal{X}(t),t)$, taking Taylor expansion we have $$\mathrm{d}\psi=\left\{\frac{\partial\psi}{\partial t}+(\nabla_\mathcal{X}\psi)F(\mathcal{X},t)\right\}\mathrm{d}t+\sum^r_{i=1}\left\{\psi\left(\mathcal{X}+G_i(\mathcal{X},t),t\right)-\psi(\mathcal{X},t)\right\}\mathrm{d}\mathcal{N}_i(t)$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="wiener-process">Wiener-Process&lt;/h3>
&lt;ul>
&lt;li>Definition (&lt;strong>Brownian Motion&lt;/strong>): It&amp;rsquo;s a bidirectional poisson counter with infinite rate, i.e. $\mathcal{W}=\lim_{\lambda\to\infty}\frac{1}{\sqrt(\lambda)}(\mathcal{N}_1-\mathcal{N}_2)$ where $\mathcal{N}_1,\;\mathcal{N}_2$ have rate $\lambda/2$
&lt;ul>
&lt;li>Characteristic: It&amp;rsquo;s a Gaussian with zero mean and variance $t$&lt;/li>
&lt;li>Note: Actual Wiener Process has stronger continuity property than Brownian motion.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Expectation: $\mathbb{E}[\mathcal{W}]=\mathbb{E}[d\mathcal{W}]=0,\;\mathbb{E}[\mathcal{W}(\tau)\mathcal{W}(t)]=\mathbb{E}[\mathcal{W}^2(\min\{t,\tau\})]=\min\{t,\tau\}$&lt;/li>
&lt;li>Principle of independent increments: If the interval $[r,t), [\sigma,s)$ don&amp;rsquo;t overlap, then $\mathcal{W}(t)-\mathcal{W}(\tau)$ and $\mathcal{W}(s)-\mathcal{W}(\sigma)$ are uncorrelated.&lt;/li>
&lt;li>&lt;strong>Ito calculus&lt;/strong> for Wiener Process
&lt;ul>
&lt;li>&lt;strong>Ito rule&lt;/strong>: Given a fuction $\psi(\mathcal{X}(t),t)$, taking Taylor expansion we have $$\begin{split}\mathrm{d}\psi=\frac{\partial\psi}{\partial t}\mathrm{d}t+(\nabla_\mathcal{X}\psi)F(\mathcal{X},t)\mathrm{d}t+\sum^r_{i=1}\left(\nabla_\mathcal{X}\psi(\mathcal{X},t)\right)G_i(\mathcal{X},t)\mathrm{d}\mathcal{W}_ i \\ -\sum^r_{i=1}\frac{1}{2}G_i^\top(\mathcal{X},t)\left(\mathrm{H}_\mathcal{X}\psi(\mathcal{X},t)\right)G_i(\mathcal{X},t)\mathrm{d}t\end{split}$$
&lt;blockquote>
&lt;p>Note $\nabla_{\mathcal{X}}=\begin{bmatrix}\frac{\partial\psi}{\partial x_1}&amp;amp;\cdots&amp;amp;\frac{\partial\psi}{\partial x_n}\end{bmatrix}$ and $\mathrm{H}_{\mathcal{X}}=\begin{bmatrix}\frac{\partial^2\psi}{\partial x_1^2}&amp;amp;\cdots&amp;amp;\frac{\partial^2\psi}{\partial x_1\partial x_n}\\ \vdots&amp;amp;\ddots&amp;amp;\vdots \\ \frac{\partial^2\psi}{\partial x_n\partial x_1}&amp;amp;\cdots&amp;amp;\frac{\partial^2\psi}{\partial x_n^2}\end{bmatrix}$ are the gradient and Hessian operator respectively. By default, the operator target is $\mathcal{X}$ is not noted.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>White noise&lt;/strong>: It is Guassian distributed stationary stochastic process with $\mu_\mathcal{U}(t)=0,\;\mathrm{K}_ \mathcal{U}(t,\tau)=\Phi_ \mathcal{U}\delta(t-\tau)$, where $\Phi_\mathcal{U}$ is &lt;strong>spectral intensity&lt;/strong>.
&lt;ul>
&lt;li>We often consider white noise as derivative of Wiener process: $\mathcal{U}\sim\mathrm{d}\mathcal{W}/\mathrm{d}t$.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="linear-stochastic-sequence-1">Linear Stochastic Sequence&lt;/h3>
&lt;p>$$\begin{align}\mathrm{d}\mathcal{X}(t)&amp;amp;=\{A(t)\mathcal{X}(t)+u(t)\}\mathrm{d}t+B(k)\mathrm{d}\mathcal{W}(t) \\ \mathcal{Y}(t)&amp;amp;=C(k)\mathcal{X}(k)+D(k)\mathcal{W}(k)\end{align}$$&lt;/p>
&lt;ul>
&lt;li>Differential equation of expectations
&lt;ul>
&lt;li>&lt;strong>Mean&lt;/strong>: $\frac{\mathrm{d}}{\mathrm{d}t}\mu_\mathcal{X}(t)=A(t)\mu_\mathcal{X}(t)$&lt;/li>
&lt;li>&lt;strong>Autocovariance&lt;/strong>: $\frac{\mathrm{d}}{\mathrm{d}t}S_\mathcal{X}(t)=A(t)S_\mathcal{X}(t)+S_\mathcal{X}A^\top(t)+B(t)B^\top(t)$ (called &lt;strong>Lyapunov differential equation&lt;/strong>)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Stationary LTI case
&lt;ul>
&lt;li>Useful equation: $\mathrm{R}_ \mathcal{X}(t,\tau)=\begin{cases} S_ \mathcal{X}\exp\{A^\top(\tau-t)\}&amp;amp;,\tau&amp;gt;t \\ \exp\{A(t-\tau)\}S_ \mathcal{X}&amp;amp;,\tau&amp;lt; t\end{cases}$&lt;/li>
&lt;li>&lt;strong>(Continuous-time) algerbraic Lyapunov equation&lt;/strong>: $A\bar{S}_ \mathcal{X}+\bar{S}_\mathcal{X}A^\top+BB^\top=0$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="nonlinear-stochastic-sequence">Nonlinear Stochastic Sequence&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>The Fokker-Planck Equation (FPE)&lt;/strong>: Consider the Wiener-process excited general SDE, we have $$\frac{\partial f_ \mathcal{X}(x;t)}{\partial t}=-\nabla\left(F(\mathcal{X})f_ \mathcal{X}(x;t)\right)+\frac{1}{2}\mathrm{tr}\left[\mathrm{H}\left(\Gamma(\mathcal{X})f_ \mathcal{X}(x;t)\right)\right]$$
here $\Gamma(\mathcal{X})=G(\mathcal{X})G^\top(\mathcal{X})$&lt;/p>
&lt;ul>
&lt;li>$\mathcal{X}$ is stationary means $\frac{\partial f_\mathcal{X}(x;t)}{\partial t}=0$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Gaussian closure&lt;/strong>: An approximate solution for FPE is supposing $f_\mathcal{X}$ as multivariate Gaussian with some $S_\mathcal{X}$ and ${\mu_\mathcal{X}}$. That is we focus on 1st-order and 2nd-order estimation. Denote the estimated distribution as $\hat{f}_\mathcal{X}(x;t)$&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Estimated expectation&lt;/strong>: $\hat{\mathbb{E}}\phi(\mathcal{X})=\int\cdots\int\phi(x)\hat{f}_\mathcal{X}(x;t)\mathrm{d}x$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Solution:$$\begin{split}h(x,t)=-\nabla F+\tilde{x}^\top S^{-1}F-(\nabla\Gamma)S^{-1}\tilde{x}+\frac{1}{2}\mathrm{tr}\left[\mathrm{H}\Gamma \right] \\ +\frac{1}{2}\mathrm{tr}\left[\left(-S^{-1}+S^{-1}\tilde{x}\tilde{x}^\top S^{-1}\right)\Gamma\right]\end{split}$$ and $$\begin{align}\dot{\mu}_\mathcal{X}(t)&amp;amp;=S(t)\hat{\mathbb{E}}[\nabla^\top h(x)] \\ \dot{S} _\mathcal{X}(t)&amp;amp;=S(t)\hat{\mathbb{E}}[\mathrm{H} h(x)]S(t)\end{align}$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Useful simplification: If $G$ is constant (independent of $\mathcal{X}$), then the ODE of $\dot{\mu}$ and $\dot{S}$ become $$\begin{align}\dot{\mu}_ \mathcal{X}(t)&amp;amp;=\hat{\mathbb{E}}[F(\mathcal{X})] \\ \dot{S} _\mathcal{X}(t)&amp;amp;=\hat{A}^\top S+S\hat{A}+\Gamma\end{align}$$ where $\hat{A}=\hat{\mathbb{E}}\left[\frac{\partial F}{\partial\mathcal{X}}\right]$&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Equivalent linearization (Quasi-linearization)&lt;/strong>: In the estimation, $\mathcal{X}$ evolves equivalently to $\mathrm{d}\mathcal{X}(t)=\hat{A}(t)\mathcal{X}(t)\mathrm{d}t+G\mathrm{d}\mathcal{W}(t)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>No guaranteed bounds generally exist for the estimation error $$e(\mathcal{X},t)=\left(\frac{\partial\hat{f}_ \mathcal{X}}{\partial t}\right)-\left(-\nabla\left(F\hat{f}_ \mathcal{X}\right)+\frac{1}{2}\mathrm{tr}\left[\mathrm{H}\left(\Gamma\hat{f}_ \mathcal{X}\right)\right]\right)$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Estimation of $\mu(t),\;S(t)$ could also have error. And stationarity may not be the consistent between original system and estimated system&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="spectral-analysis">Spectral Analysis&lt;/h2>
&lt;blockquote>
&lt;p>Content in this section comes from MECHENG 549&lt;/p>
&lt;/blockquote>
&lt;h3 id="power-spectral-density">Power Spectral Density&lt;/h3>
&lt;ul>
&lt;li>Definition: The &lt;strong>power spectral density (PSD)&lt;/strong> of stochastic process $\mathcal{Y}$ is $$\Phi_\mathcal{Y}(\omega)\equiv\mathbb{E}\left[ \lim_{T\to\infty}\frac{1}{2T}Y_T(\omega)Y_T^\top(\omega)\right]$$ where $Y_T(\omega)$ is the Fourier transform of centered process $\mathcal{Y}(t)$.
&lt;ul>
&lt;li>White noise has the same PSD for whatever $\omega$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Wiener-Khinchin&lt;/strong> Theorem: $\Phi_\mathcal{Y}(\omega)=\int^\infty_{-\infty}e^{-i\omega\theta}\bar{\mathrm{K}}_\mathcal{Y}(\theta)\mathrm{d}\theta$&lt;/li>
&lt;li>&lt;strong>Signal propagation&lt;/strong>: If the system $P$ has input $\mathcal{Y}$ and output $\mathcal{Z}$, then $\Phi_\mathcal{Z}=|P(i\omega)\Phi_\mathcal{Y}(\omega)P^\top(i\omega)|$ where $P(i\omega)$ is the Fourier transform of the system.&lt;/li>
&lt;li>&lt;strong>Cross spectrum&lt;/strong>: If the system $G$ has input $\mathcal{Y}$ and output $\mathcal{Z}$, then the cross-spectrum is $\Phi_{\mathcal{ZY}}(\omega)=G(i\omega)\Phi_\mathcal{Y}(\omega)$&lt;/li>
&lt;/ul>
&lt;h3 id="periodograms">Periodograms&lt;/h3>
&lt;ul>
&lt;li>Definition: We want to estimate $\Phi_{\mathcal{Y}}(\omega)$ without the expectation, then we use $$Q_T(\omega,y)=\frac{1}{2T}\left|\int^T_{-T}e^{-i\omega t}y(t)\mathrm{d}t\right|^2$$ where $y$ is a sample realization of $\mathcal{Y}$.
&lt;ul>
&lt;li>We also use window functions to calculate the spectrum over a finite time interval (using &lt;strong>Bartlett&amp;rsquo;s procedure&lt;/strong>)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="stochastic-realizations">Stochastic realizations&lt;/h3>
&lt;p>We want to find a stochastic model which gives the same spectrum given. This is called &lt;strong>stochastic realization problem&lt;/strong>. We focus on scalar LTI case.&lt;/p>
&lt;p>If we know $P(s)=C[sI-A]^{-1}B+D$ (the Laplace transform of LTI system) and $P(s)=c\frac{\prod^m_{k=1}(s-z_k)}{\prod^n_{k=1}(s-p_k)}$ for some $m\leq n$ and real constant $c$. Then we have $$\Phi_\mathcal{Y}(\omega)=P(i\omega)P(-i\omega)=c^2\frac{\prod^m_{k=1}(\omega^2+z_k^2)}{\prod^n_{k=1}(\omega^2+p_k^2)}$$&lt;/p>
&lt;p>Lemma: For any valid PSD, there exists a spectral factorization $\Phi_\mathcal{Y}(\omega)=\frac{\sum^m_{k=0}a_k(\omega^2)^k}{\sum^n_{k=0}b_k(\omega^2)^k}=P(i\omega)P(-i\omega)$ where $P(s)$ is an asymptotically stable n-th order transfer function, iff&lt;/p>
&lt;ol>
&lt;li>$\Phi_\mathcal{Y}(\omega)$ is a ratio of polynomials of $\omega^2$&lt;/li>
&lt;li>All coefficients $a_k$ and $b_k$ are real&lt;/li>
&lt;li>The denominator has no positive real roots in $\omega^2$&lt;/li>
&lt;li>Any positive real roots in the numerator have even multiplicity&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;h2 id="notes-for-math-typing-in-hexo">Notes for math typing in Hexo&lt;/h2>
&lt;ol>
&lt;li>Escape &lt;code>\&lt;/code> by &lt;code>\\&lt;/code>. Especially escape &lt;code>{&lt;/code> by &lt;code>\\{&lt;/code> instead of &lt;code>\{&lt;/code>, and escape &lt;code>\\&lt;/code> by &lt;code>\\\\&lt;/code>.&lt;/li>
&lt;li>Be careful about &lt;code>_&lt;/code>, it&amp;rsquo;s used in markdown as italic indicator. Add space after &lt;code>_&lt;/code> is a useful solution.&lt;/li>
&lt;li>&lt;a class="link" href="https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference" target="_blank" rel="noopener"
>Some useful Mathjax tricks at StackExchange&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.overleaf.com/learn/latex/List_of_Greek_letters_and_math_symbols" target="_blank" rel="noopener"
>Several capital Greek characters should directly use its related Latin alphabet with &lt;code>\mathrm&lt;/code> command&lt;/a>.&lt;/li>
&lt;/ol>
&lt;p>Although I have migrated to Hugo, some tricks might still be relevant.&lt;/p>
&lt;/blockquote></description></item><item><title>Notes for Probability Theory (Basics)</title><link>https://zyxin.xyz/blog/en/2019-02/ProbabilityNotes/</link><pubDate>Sat, 23 Feb 2019 00:00:00 +0000</pubDate><guid>https://zyxin.xyz/blog/en/2019-02/ProbabilityNotes/</guid><description>&lt;h2 id="probability-space">Probability Space&lt;/h2>
&lt;ul>
&lt;li>Notation: $(\Omega,\mathcal{F},\mathbb{P})$
&lt;ul>
&lt;li>$\Omega$: &lt;strong>Sample space&lt;/strong>&lt;/li>
&lt;li>$\mathcal{F}$: &lt;strong>Event space&lt;/strong>. Required to be &lt;a class="link" href="http://mathworld.wolfram.com/Sigma-Algebra.html" target="_blank" rel="noopener"
>&lt;strong>σ-algebra&lt;/strong>&lt;/a>. We often use &lt;a class="link" href="http://mathworld.wolfram.com/BorelSigma-Algebra.html" target="_blank" rel="noopener"
>&lt;strong>Borel σ-algebra&lt;/strong>&lt;/a> for continuous $\Omega$).
&lt;ul>
&lt;li>Axioms for &lt;strong>σ-algebra&lt;/strong>
&lt;ol>
&lt;li>$\mathcal{F}$ is non-empty&lt;/li>
&lt;li>$A\in\mathcal{F} \Rightarrow A^C\in\mathcal{F}$ (closed under complement)&lt;/li>
&lt;li>$A_i \in\mathcal{F} \Rightarrow \bigcup^\infty_{k=1} A_k \in \mathcal{F}$ (closed under countable union)&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>For continuous case considering interval $\Omega=[a,b]$, $\mathcal{F}_0$ is the set of all subintervals of $\Omega$. Then its &lt;strong>Borel σ-algebra&lt;/strong> is the smallest σ-algebra that contains $\mathcal{F}_0$. Here the $\mathcal{F}_0$ is a &lt;a class="link" href="https://en.wikibooks.org/wiki/Measure_Theory/Basic_Structures_And_Definitions/Semialgebras,_Algebras_and_%CF%83-algebras" target="_blank" rel="noopener"
>&lt;strong>semialgebra&lt;/strong>&lt;/a>. We can find a containing σ-algebra for every semialgebra.&lt;/li>
&lt;li>Axioms for &lt;strong>semialgebra&lt;/strong>
&lt;ol>
&lt;li>$\emptyset, \Omega \in \mathcal{F}$&lt;/li>
&lt;li>$A_i \in\mathcal{F} \Rightarrow \bigcap^n_{k=1} A_k \in \mathcal{F}$ (closed under finite intersections)&lt;/li>
&lt;li>$\forall B \in\mathcal{F}, B^C=\bigcup^n_{k=1} A_k$ where $A_i \in \mathcal{F}$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>$\mathbb{P}$: &lt;strong>Probability measure&lt;/strong>.
&lt;ul>
&lt;li>Axioms for probability measure
&lt;ol>
&lt;li>$\mathbb{P}(\Omega) = 1$&lt;/li>
&lt;li>$\forall A\in\mathcal{F}, \mathbb{P}(A) \geqslant 0$&lt;/li>
&lt;li>$A_i, A_j \in\mathcal{F}$ are pairwise disjoint, then $\mathbb{P}(\bigcup^\infty_{k=1}A_k)=\sum^\infty_{k=1}\mathbb{P}(A_k)$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Product Space&lt;/strong>: Probability spaces can be combined using Cartesian product.&lt;/li>
&lt;li>&lt;strong>Independence&lt;/strong>: $\mathbb{P}(A_{k_1}\cap A_{k_2}\cap &amp;hellip;\cap A_{k_l})=\prod_{i=1}^l \mathbb{P}(A_i),\;\forall \{k_i\}_1^l\subset\{ 1..n\}$&lt;/li>
&lt;li>&lt;strong>Conditional probability&lt;/strong>: $\mathbb{P}\left(A_i \middle| A_j\right)=\mathbb{P}(A_i\cap A_j)/\mathbb{P}(A_j)$&lt;/li>
&lt;li>&lt;strong>Total probability&lt;/strong>: $\mathbb{P}(B)=\sum_{i=1}^n \mathbb{P}(B\cap A_i)=\sum_{i=1}^n \mathbb{P}\left(B\middle| A_i\right)\mathbb{P}(A_i)$, where $\{A_1,\cdots,A_n\}$ are disjoint and partition of $\Omega$.&lt;/li>
&lt;li>&lt;strong>Bayes&amp;rsquo; Rule&lt;/strong>: $$\mathbb{P}(A_j|B)=\frac{\mathbb{P}(B|A_j)\mathbb{P}(A_j)}{\sum^n_{i=1} \mathbb{P}(B|A_i)\mathbb{P}(A_i)}$$
&lt;ul>
&lt;li>Priori: $\mathbb{P}(B|A_j)$&lt;/li>
&lt;li>Posteriori: $\mathbb{P}(A_j|B)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="random-variables">Random Variables&lt;/h2>
&lt;blockquote>
&lt;p>Note: The equations are written in continuous case by default, one can get the equation for discrete case by changing integration into summation and changing differential into difference.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>Random Variable $\mathcal{X}$ is a mapping $\mathcal{X}: (\Omega,\mathcal{F},\mathbb{P})\to(\Omega_ \mathcal{X},\mathcal{F}_ \mathcal{X},\mathbb{P}_ \mathcal{X})$&lt;/li>
&lt;li>&lt;strong>Continuous &amp;amp; Discrete &amp;amp; Mixed Random Variable&lt;/strong>:
&lt;ul>
&lt;li>Can be defined upon whether $\Omega_\mathcal{X}$ is continuous&lt;/li>
&lt;li>Can be defined upon whether we can find continuous density function $f_\mathcal{X}$&lt;/li>
&lt;li>$\mathcal{F}_\mathcal{X}$ for continuous $\mathcal{X}$ is a &lt;strong>Borel σ-field&lt;/strong>&lt;/li>
&lt;li>All three kinds of random variables can be expressed by CDF or &amp;ldquo;extended&amp;rdquo; PDF with Dirac function and Lebesgue integration.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Scalar Random Variable&lt;/strong>: $\mathcal{X}: \Omega\to\mathbb{F}$
&lt;ul>
&lt;li>Formal definition: $\mathcal{X}: (\Omega,\mathcal{F},\mathbb{P})\to(\Omega_ \mathcal{X}\subset\mathbb{F},\mathcal{F}_ \mathcal{X}\subset\left\{\omega\middle| \omega\subset\Omega_ \mathcal{X}\right\},\mathbb{P}_ \mathcal{X}:\mathcal{F}_ \mathcal{X}\to[0,1])$&lt;/li>
&lt;li>&lt;strong>Cumulative Distribution Function&lt;/strong> (CDF): $F_ \mathcal{X}(x)=\mathbb{P}_\mathcal{X}(\mathcal{X}(\omega)\leqslant x)$&lt;/li>
&lt;li>&lt;strong>Probability Mass Function&lt;/strong> (PMF): $p_ \mathcal{X}(x)=\mathbb{P}_\mathcal{X}(\mathcal{X}(\omega)=x)$&lt;/li>
&lt;li>&lt;strong>Probability Density Function&lt;/strong> (PDF): $$f_ \mathcal{X}(x)=\mathbb{P}_ \mathcal{X}(x&amp;lt; \mathcal{X}(\omega)\leqslant x+ \mathrm{d}x)=\mathrm{d}F_ \mathcal{X}(x)/ \mathrm{d}x$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Vector Random Variable&lt;/strong> (Multiple Random Variables): $\mathcal{X}: \Omega\to\mathbb{F}^n$
&lt;ul>
&lt;li>Formal definition $\mathcal{X}: (\Omega,\mathcal{F},\mathbb{P})\to(\Omega_ \mathcal{X}\subset\mathbb{F}^n,\mathcal{F}_ \mathcal{X}\subset\left\{\omega\middle| \omega\subset\Omega_ \mathcal{X}\right\},\mathbb{P}_ \mathcal{X}:\mathcal{F}_ \mathcal{X}\to[0,1])$&lt;/li>
&lt;li>&lt;strong>Cumulative Distribution Function&lt;/strong> (CDF): $F_ \mathcal{X}(x)=\mathbb{P}_\mathcal{X}(\{\mathcal{X}(\omega)\}_i\leqslant x_i), i=1\ldots n$&lt;/li>
&lt;li>&lt;strong>Probability Mass Function&lt;/strong> (PMF): $p_ \mathcal{X}(x)=\mathbb{P}_\mathcal{X}(\{\mathcal{X}(\omega)\}_i=x)$&lt;/li>
&lt;li>&lt;strong>Probability Density Function&lt;/strong> (PDF): $$f_ \mathcal{X}(x)=\mathbb{P}_ \mathcal{X}(x_i&amp;lt; \{\mathcal{X}(\omega)\}_ i\leqslant x_i+ \mathrm{d}x_i)=\frac{\partial^n}{\partial x_1\partial x_2\ldots\partial x_n}F_ \mathcal{X}(x)$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Afterwards, we don&amp;rsquo;t distinguish $\mathbb{P}_\mathcal{X}$ with $\mathbb{P}$ if there&amp;rsquo;s no ambiguity.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Independence&lt;/strong>($\perp$ or $\perp$ with double vertical lines): $\mathbb{P}(\mathcal{X}\in A\cap \mathcal{Y}\in B)=\mathbb{P}(\mathcal{X}\in A)\mathbb{P}(\mathcal{Y}\in B)$
&lt;ul>
&lt;li>&lt;strong>Independent CDF&lt;/strong>:$F_{\mathcal{XY}}(x,y)=F_\mathcal{X}(x)F_\mathcal{Y}(y)$ iff $\mathcal{X}\perp\mathcal{Y}$&lt;/li>
&lt;li>&lt;strong>Independent PMF&lt;/strong>:$p_{\mathcal{XY}}(x,y)=p_\mathcal{X}(x)p_\mathcal{Y}(y)$ iff $\mathcal{X}\perp\mathcal{Y}$&lt;/li>
&lt;li>&lt;strong>Independent PDF&lt;/strong>:$f_{\mathcal{XY}}(x,y)=f_\mathcal{X}(x)f_\mathcal{Y}(y)$ iff $\mathcal{X}\perp\mathcal{Y}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Marginalization&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>Marginal distribution function&lt;/strong>: $F_{1:m}(x_{1:m})\equiv F\left(\begin{bmatrix}x_1&amp;amp;x_2&amp;amp;\ldots&amp;amp;x_m&amp;amp;\infty&amp;amp;\ldots&amp;amp;\infty\end{bmatrix}^\top\right)$&lt;/li>
&lt;li>&lt;strong>Marginal density function&lt;/strong>: $f_{1:m}(x_{1:m})=\int^\infty_{-\infty}\cdots\int^\infty_{-\infty} f(x) \mathrm{d}x_{m+1}\cdots \mathrm{d}x_n$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Conditional&lt;/strong> (on event $\mathcal{E}$)
&lt;ul>
&lt;li>&lt;strong>Conditional probability&lt;/strong>: $\mathbb{P}(\mathcal{X}\in\mathcal{D}|\mathcal{E})=\mathbb{P}(\{\omega|\mathcal{X}(\omega)\in\mathcal{D}\}\cap \mathcal{E})/\mathbb{P}(\mathcal{E})$ on a event $\mathcal{E}\in\mathcal{F}$ and a set $\mathcal{D}\subset\mathcal{F}$.&lt;/li>
&lt;li>&lt;strong>Conditional CDF&lt;/strong>: $F_ \mathcal{X}(x|\mathcal{E})=\mathbb{P}(\mathcal{X}_i\leqslant x_i|\mathcal{E})$&lt;/li>
&lt;li>&lt;strong>Conditional PMF&lt;/strong>: $p_ \mathcal{X}(x|\mathcal{E})=\mathbb{P}(\mathcal{X}_i=x_i|\mathcal{E})$&lt;/li>
&lt;li>&lt;strong>Conditional PDF&lt;/strong>: $f_ \mathcal{X}(x|\mathcal{E})=\frac{\partial^n}{\partial x_1\partial x_2\ldots\partial x_n}F_ \mathcal{X}(x|\mathcal{E})$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Conditional&lt;/strong> (on variable $\mathcal{Y}$)
&lt;ul>
&lt;li>&lt;strong>Conditional probability&lt;/strong>: $$\mathbb{P}(\mathcal{X}\in\mathcal{D}_1|\mathcal{Y}\in\mathcal{D}_2)=\mathbb{P}(\{\omega|\mathcal{X}(\omega)\in\mathcal{D}_1,\mathcal{Y}(\omega)\in\mathcal{D}_2\})/\mathbb{P}(\mathcal{Y}(\omega)\in\mathcal{D}_2)$$&lt;/li>
&lt;li>&lt;strong>Conditional PDF&lt;/strong> (similar for PMF): $$f_{\mathcal{X}|\mathcal{Y}\in\mathcal{D}}(x)=\frac{\int_{\mathcal{Y}\in\mathcal{D}}f_{\mathcal{XY}}(x,y)}{\int_{\mathcal{Y}\in\mathcal{D}}f_{\mathcal{Y}}(y)},\;f_{\mathcal{X}|\mathcal{Y}}(x|y)=f_{\mathcal{X}|\mathcal{Y}=y}(x)=\frac{f_{\mathcal{XY}}(x,y)}{f_{\mathcal{Y}}(y)}$$
&lt;ul>
&lt;li>Using total probability we have $f_ \mathcal{X}(x)=\int f_{\mathcal{X}|\mathcal{Y}}(x|y)f_ \mathcal{Y}(y)\mathrm{d}y$. This can be further integrated into Bayes&amp;rsquo; rule.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Conditional CDF&lt;/strong>: Can be defined similarly, of defined as $F_{\mathcal{X}|\mathcal{Y}\in\mathcal{D}}(x)\equiv\int f_{\mathcal{X}|\mathcal{Y}\in\mathcal{D}}(x)\mathrm{d}x$
&lt;ul>
&lt;li>Similarly we have $F_ \mathcal{X}(x)=\int F_{\mathcal{X}|\mathcal{Y}}(x|y)f_ \mathcal{Y}(y)\mathrm{d}y$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Substitution law&lt;/strong>: $\mathbb{P}((\mathcal{X},\mathcal{Y})\in \mathcal{D}|\mathcal{X}=x)=\mathbb{P}((x,\mathcal{Y})\in \mathcal{D})$
&lt;ul>
&lt;li>Common usage: Suppose $\mathcal{Z}=\psi(\mathcal{X},\mathcal{Y})$, then $p_\mathcal{Z}(z)=\int_x \mathbb{P}\left(\psi(x,\mathcal{Y})=z\right)p_\mathcal{X}(x)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="uncertainty-propagation">Uncertainty Propagation&lt;/h2>
&lt;p>Suppose $\mathcal{Y}=\psi(\mathcal{X})$ or specifically $y=\psi(x_1)=\psi(x_2)=\cdots=\psi(x_K)$&lt;/p>
&lt;ul>
&lt;li>Scalar case: $$f_ \mathcal{Y}(y)=\sum^K_{k=1} f_ \mathcal{X}(\psi^{-1}_ k(y))\left| \frac{\partial\psi}{\partial x}\biggr|_{x=\psi^{-1}_k(y)} \right|^{-1}$$&lt;/li>
&lt;li>Vector case: $$f_ \mathcal{Y}(y)=\sum^K_{k=1} f_ \mathcal{X}(\psi^{-1}_ k(y))\left|\det(J)\right|^{-1},\text{where Jacobian }J=\frac{\partial\psi}{\partial x}\biggr|_{x=\psi^{-1}_k(y)}$$&lt;/li>
&lt;li>Trivial Case — Summation: $\mathcal{Y}=\mathcal{X}_ 1+\mathcal{X}_ 2$, then $f_ \mathcal{Y}=\int^\infty_{-\infty}f_ {\mathcal{X}_ 1\mathcal{X}_ 2}(x_1,x_2-x_1) \mathrm{d}x_1$
Another way is to use the method of choice: $F_ \mathcal{Y}(y)=\int_ {\psi(x)\leq y}f_\mathcal{X}(x)\mathrm{d}x$&lt;/li>
&lt;/ul>
&lt;h2 id="expectation--moments">Expectation &amp;amp; Moments&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Expectation&lt;/strong>: $\mathbb{E}_ \mathcal{X}[\psi(\mathcal{X})]=\int^\infty_\infty\cdots\int^\infty_\infty\psi(x)f_ \mathcal{X}(x) \mathrm{d}x_1\ldots \mathrm{d}x_n$&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>A more rigorous way to define the expectation is $\mathbb{E}_ \mathcal{X}[\psi(\mathcal{X})]=\int \psi(\mathcal{X})dF_ \mathcal{X}(\mathcal{X})$. This definition uses Lebesgue Integral and works on both discrete and continuous (or even mixed) variables. See &lt;a class="link" href="http://www.randomservices.org/random/dist/Integral.html" target="_blank" rel="noopener"
>this post&lt;/a> and &lt;a class="link" href="" >this discussion&lt;/a> for more information.
We write $\mathbb{E}_ \mathcal{X}$ as $\mathbb{E}$ if there&amp;rsquo;s no ambiguity (when only one random variable is included). And $\mathbb{E}[\mathcal{X}]$ will be abbreviated as $\mathbb{E}\mathcal{X}$ afterwards.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Linearity of expectation&lt;/strong>: $\mathbb{E}[A\mathcal{\mathcal{X}}]=A\mathbb{E}[\mathcal{X}] (\forall \mathcal{X},\mathcal{Y},\forall A\in\mathbb{R}^{m\times n})$&lt;/li>
&lt;li>&lt;strong>Independent expectation&lt;/strong>: $\mathbb{E}[\prod^n_i\mathcal{X}_i]=\prod^n_i\left(\mathbb{E}\mathcal{X}_i\right)(\forall\;\text{indep.}\;\mathcal{X}_i)$&lt;/li>
&lt;li>&lt;strong>Conditional expectation&lt;/strong>: $\mathbb{E}[\mathcal{Y}|\mathcal{X}=x]=\int^\infty_{-\infty}yf_{\mathcal{Y}|\mathcal{X}}(y|x)\mathrm{d}y$
&lt;ul>
&lt;li>&lt;strong>Total expectation/Smoothing&lt;/strong>: $\mathbb{E}_ \mathcal{X}[\mathbb{E}_ {\mathcal{Y}|\mathcal{X}}(\mathcal{Y}|\mathcal{X})]=\mathbb{E}\mathcal{Y}$&lt;/li>
&lt;li>&lt;strong>Substitution law&lt;/strong>: $\mathbb{E}[g(\mathcal{X},\mathcal{Y})|\mathcal{X}=x]=\mathbb{E}[\psi(x,\mathcal{Y})|\mathcal{X}=x]$&lt;/li>
&lt;li>$\mathbb{E}[\psi(\mathcal{X})|\mathcal{X}]=\psi(\mathcal{X})$&lt;/li>
&lt;li>$\mathbb{E}[\psi(\mathcal{X})\mathcal{Y}|\mathcal{X}]=\psi(\mathcal{X})\mathbb{E}(\mathcal{Y}|\mathcal{X})$&lt;/li>
&lt;li>&lt;strong>Towering&lt;/strong>: $\mathbb{E}_ {\mathcal{Y}|\mathcal{Z}}[\mathbb{E}_ \mathcal{X}(\mathcal{X}|\mathcal{Y},\mathcal{Z})|\mathcal{Z}]=\mathbb{E}_ {\mathcal{X}|\mathcal{Z}}[\mathcal{X}|\mathcal{Z}]$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Moment&lt;/strong> (p-th order): $\mu_p(\mathcal{X})=\mathbb{E}[\mathcal{X}^p]$
&lt;ul>
&lt;li>&lt;strong>Mean&lt;/strong>: $\mu_ \mathcal{X}=\mathbb{E}[\mathcal{X}]$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Central moment&lt;/strong> (p-th order): $\mathbb{E}[(\mathcal{X}-\mu_ \mathcal{X})^p]$
&lt;ul>
&lt;li>&lt;strong>Variance&lt;/strong>: $\sigma_ \mathcal{X}^2=\mathbb{E}[(\mathcal{X}-\mu_ \mathcal{X})^2]$, sometimes written as $\sigma_ \mathcal{X}^2=\mathbb{V}(\mathcal{X})$&lt;/li>
&lt;li>&lt;strong>Skewness&lt;/strong>: $\tilde{\mu}_ 3=\mathbb{E}[(\mathcal{X}-\mu_ \mathcal{X})^3]/\sigma_\mathcal{X}^3$ ($\tilde{\mu}_ 3&amp;gt;0$ right-skewed, $\tilde{\mu}_ 3&amp;lt;0$ left-skewed)&lt;/li>
&lt;li>&lt;strong>Kurtosis&lt;/strong>: $\tilde{\mu}_ 4=\mathbb{E}[(\mathcal{X}-\mu_ \mathcal{X})^4]/\sigma_\mathcal{X}^4$&lt;/li>
&lt;li>&lt;strong>Excessive Kurtosis&lt;/strong>: $\gamma = \mu_ 4-3$
&lt;img src="https://zyxin.xyz/blog/blog/en/2019-02/ProbabilityNotes/skewness_and_kurtosis.jpg"
width="1266"
height="237"
loading="lazy"
alt="Skewness and Kurtosis"
class="gallery-image"
data-flex-grow="534"
data-flex-basis="1282px"
>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Correlation&lt;/strong>: $\text{corr}(\mathcal{X}_i,\mathcal{X}_j)=\mathbb{E}[\mathcal{X}_i\mathcal{X}_j]$
&lt;ul>
&lt;li>&lt;strong>Correlation matrix&lt;/strong>: $C=\mathbb{E}[\mathcal{X}_i\mathcal{X}_j^\top]$&lt;/li>
&lt;li>&lt;strong>Correlation coefficient&lt;/strong>: $\rho(\mathcal{X}_ i,\mathcal{X}_ j)=\frac{\text{corr}(\mathcal{X}_ i,\mathcal{X}_ j)}{\sigma_{\mathcal{X}_ i}^2\sigma_{\mathcal{X}_ j}^2}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Covariance&lt;/strong>: $\text{cov}(\mathcal{X}_i,\mathcal{X}_j)=\mathbb{E}[(\mathcal{X}_i-\mu_i)(\mathcal{X}_j-\mu_j)]$
&lt;ul>
&lt;li>&lt;strong>Covariance matrix&lt;/strong>: $S=\mathbb{E}\left[(\mathcal{X}-\mu_ \mathcal{X})(\mathcal{X}-\mu_ \mathcal{X})^\top\right]$&lt;/li>
&lt;li>Properties: $\text{cov}(\mathcal{X}+c)=\text{cov}(\mathcal{X}), \text{cov}(A\mathcal{X},\mathcal{X}B)=A\text{cov}(\mathcal{X})+\text{cov}(\mathcal{X})B^\top$&lt;/li>
&lt;li>&lt;strong>Uncorrelated&lt;/strong>: $\rho(\mathcal{X}_ i,\mathcal{X}_ j)=0 \Leftrightarrow\text{cov}(\mathcal{X}_i,\mathcal{X}_j)=0\Leftrightarrow\text{corr}(\mathcal{X}_i,\mathcal{X}_j)=\mathbb{E}[\mathcal{X}_i]\mathbb{E}[\mathcal{X}_j]$ (uncorrelated is necessary for independent)&lt;/li>
&lt;li>Cases where uncorrelated implies independence: (1) Jointly Gaussian (2) Bernoulli&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Centered variable&lt;/strong>: $\tilde{\mathcal{X}}=\mathcal{X}-\mu_ \mathcal{X}$&lt;/li>
&lt;/ul>
&lt;h2 id="transform-methods">Transform methods&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Probability generating function&lt;/strong>(PGF): similiar to Z-tranform $$G_ \mathcal{X}(z)\equiv\mathbb{E}[z^\mathcal{X}]=\sum_{x_i}z^{x_i}p_ \mathcal{X}(x_i)$$
&lt;ul>
&lt;li>For $\mathcal{F}_ \mathcal{X}=\mathbb{N}$, we have $$\frac{\mathrm{d}^k}{\mathrm{d}z^k}G_ \mathcal{X}(z)\Biggr|_{z=1}=\mathbb{E}[\mathcal{X}(\mathcal{X}-1)\cdots(\mathcal{X}-(k-1))]$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Moment generating function&lt;/strong>(MGF): similar to Laplace transform $$M_ \mathcal{X}(z)\equiv\mathbb{E}[e^{s\mathcal{X}}]=\int^\infty_{-\infty}e^{sx}f_ \mathcal{X}(x)\mathrm{d}x$$
&lt;ul>
&lt;li>Generally we have $$\frac{\mathrm{d}^k}{\mathrm{d}s^k}M_ \mathcal{X}(s)\Biggr|_{s=0}=\mathbb{E}[\mathcal{X}^k]$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Characteristic function&lt;/strong>(CF): similar to Fourier transform $$\phi_ \mathcal{X}(\omega)\equiv\mathbb{E}[e^{j\omega \mathcal{X}}]=\int^\infty_{-\infty}e^{j\omega x}f_ \mathcal{X}(x)\mathrm{d}x$$
&lt;ul>
&lt;li>Generally we have $$\frac{\mathrm{d}^k}{\mathrm{d}\omega^k}\phi_ \mathcal{X}(\omega)\Biggr|_{\omega=0}=j^k\mathbb{E}[\mathcal{X}^k]$$&lt;/li>
&lt;li>&lt;strong>Independent&lt;/strong>: $\mathcal{X}\perp \!\!\! \perp\mathcal{Y}$ iff. $\phi_ \mathcal{XY}(\omega)=\phi_ \mathcal{X}(\omega)\phi_ \mathcal{Y}(\omega)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Joint characteristic function&lt;/strong>: for vector case $\mathcal{X}\in\mathbb{R}^n$, we define vector $u$ and $$\phi_ \mathcal{X}(u)\equiv\mathbb{E}[e^{ju^\top \mathcal{X}}]$$
&lt;ul>
&lt;li>Trivial usage: if $\mathcal{Y}=\mathcal{X}_ 1+\mathcal{X}_ 2$, then $\phi_ \mathcal{Y}(u)=\phi_{\mathcal{X}_ 1}(u)\phi_{\mathcal{X}_ 2}(u)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="common-distributions1">Common distributions&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/h2>
&lt;ul>
&lt;li>&lt;em>&lt;strong>Bernoulli&lt;/strong>&lt;/em>: $\Omega_\mathcal{X}=\{0,1\}$ $$p_\mathcal{X}(1)=p,\;p_\mathcal{X}(0)=q=1-p$$
&lt;ul>
&lt;li>$\mu_\mathcal{X}=p,\;\sigma^2_\mathcal{X}=pq,\;G_\mathcal{X}(z)=q+pz$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Binomial&lt;/strong>&lt;/em> - $\mathcal{B}(n,p)$: $\Omega_\mathcal{X}=\{0,1,\ldots,n\}$ $$p_\mathcal{X}(k)=\binom{n}{k}p^k(1-p)^{n-k}$$
&lt;ul>
&lt;li>$\mu_\mathcal{X}=np,\;\sigma^2_\mathcal{X}=np(1-p),\;G_\mathcal{X}(z)=(1-p+pe^z)^n$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Multinomial&lt;/strong>&lt;/em>: $\Omega_\mathcal{X}=\{0,1,\ldots,n\}^k$ $$p_\mathcal{X}(x)=\binom{n}{x_1!\cdots x_k!}p_1^{x_1}\cdots p_k^{x_k}$$&lt;/li>
&lt;li>&lt;em>&lt;strong>Geometric&lt;/strong>&lt;/em>: $\Omega_\mathcal{X}=\mathbb{N}$ $$p_\mathcal{X}(k)=(1-p)^{k-1}p$$
&lt;ul>
&lt;li>$\mu_\mathcal{X}=\frac{1}{p},\;\sigma^2_\mathcal{X}=\frac{1-p}{p^2},\;G_\mathcal{X}(z)=\frac{pz}{1-(1-p)z}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Poisson&lt;/strong>&lt;/em>: $\Omega_\mathcal{X}=\mathbb{N}$ $$p_\mathcal{X}(k)=\frac{\lambda^k}{k!}\exp\{-\lambda\}$$
&lt;ul>
&lt;li>$\mu_\mathcal{X}=\lambda,\;\sigma^2_\mathcal{X}=\lambda,\;G_\mathcal{X}(z)=\exp\{\lambda(z-1)\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Uniform&lt;/strong>&lt;/em> - $\mathcal{U}(a,b)$: $\Omega_\mathcal{X}=[a,b]$ $$f(x)=
\begin{cases}
1/(b-a)&amp;amp;,\;x \in [a,b] \\
0&amp;amp;,\;\text{otherwise}
\end{cases}$$
&lt;ul>
&lt;li>$\mu_\mathcal{X}=\frac{1}{2}(a+b),\;\sigma^2_\mathcal{X}=\frac{1}{12}(b-a)^2,\;M_\mathcal{X}(s)=\frac{e^{sb}-e^{sa}}{s(b-a)}\;(s\neq 0)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Normal&lt;/strong>&lt;/em> - $\mathcal{N}(\mu,\sigma)$: $\Omega_\mathcal{X}=\mathbb{R}$
$$f(x)=\frac{1}{\sqrt{2\pi\sigma}}\exp\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}$$
&lt;ul>
&lt;li>$M_\mathcal{X}(s)=\exp\left\{\mu s+\frac{1}{2}\sigma^2s^2\right\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Joint Normal&lt;/strong>&lt;/em>: $\Omega_\mathcal{X}=\mathbb{R}^n$
$$f(x)=\frac{1}{\sqrt{(2\pi)^n \det(S)}}\exp\left\{-\frac{1}{2}(x-\mu)^\top S^{-1}(x-\mu)\right\}$$
&lt;ul>
&lt;li>$\phi_\mathcal{X}(u)=\exp\left\{ju^\top \mu-\frac{1}{2}u^\top Su\right\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Rayleigh&lt;/strong>&lt;/em>: $\Omega_\mathcal{X}=[0,\infty]$ $$f(x)=\frac{x}{\sigma^2}\exp\left\{-\frac{x^2}{2\sigma^2}\right\}H(x)$$ where $H(x)$ is Heaviside step function&lt;/li>
&lt;li>&lt;em>&lt;strong>Exponential&lt;/strong>&lt;/em> - $\mathcal{E}(\lambda)$: $\Omega_\mathcal{X}=[0,\infty]$ $$f(x)=\frac{1}{\mu}\exp\left\{-\frac{x}{\mu}\right\}H(x)$$
&lt;ul>
&lt;li>$\mu_\mathcal{X}=1/\lambda,\;\sigma^2_\mathcal{X}=1/\lambda^2,\;M_\mathcal{X}(s)=\lambda/(\lambda-s)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Laplacian&lt;/strong>&lt;/em>: $$f(x)=\frac{1}{2b}\exp\left\{-\frac{|x-\mu|}{b}\right\}$$&lt;/li>
&lt;/ul>
&lt;h2 id="derivation-of-the-distributions">Derivation of the distributions&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Bernoulli → Binomial&lt;/strong>: $\mathcal{X}_ i\sim\text{Bernoulli}( p) \Rightarrow \mathcal{Y}=\sum_{i=1}^n \mathcal{X}_ i\sim\mathcal{B}(n,p)$&lt;/li>
&lt;li>&lt;strong>Bernoulli → Geometric&lt;/strong>: $\mathcal{X}_i\sim\text{Bernoulli}( p) \Rightarrow \mathcal{Y}\sim\text{Geometric}( p)$ denoting the first $\mathcal{X}_i=1$&lt;/li>
&lt;li>&lt;strong>Binomial → Poisson&lt;/strong>: $\mathcal{X}_i\sim\mathcal{B}(n,p,k=1) \Rightarrow \mathcal{Y}\sim\text{Poisson}(\lambda)$ when $n\to \infty$ with $p=\frac{\lambda\tau}{n}$&lt;/li>
&lt;li>&lt;strong>Binomial → Exponential&lt;/strong>: $\mathcal{X}_i\sim\mathcal{B}(n,p,k\neq0) \Rightarrow \mathcal{Y}\sim\mathcal{E}(\lambda)$ when $n\to \infty$ with $p=\frac{\lambda\tau}{n}$
&lt;blockquote>
&lt;p>Actually $\mathcal{B}(n,p,k) \Rightarrow e^{-\lambda \tau}\frac{(\lambda \tau)^k}{k!}$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;a class="link" href="https://en.wikipedia.org/wiki/Laplace_distribution#Related_distributions" target="_blank" rel="noopener"
>&lt;strong>Exponential → Laplacian&lt;/strong>&lt;/a>: $\mathcal{X}_1, \mathcal{X}_2 \sim\mathcal{E}(\lambda) \Rightarrow \mathcal{X}_1-\mathcal{X}_2\sim\text{Laplacian}(\lambda^{-1})$&lt;/li>
&lt;/ul>
&lt;h2 id="concentration-inequalities">Concentration Inequalities&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Cauchy-Schwarz Inequality&lt;/strong>: $S=\left(\mathbb{E}[\mathcal{X}\mathcal{X}_j]\right)^2\leqslant\left(\mathbb{E}[\mathcal{X}_i]\right)^2\left(\mathbb{E}[\mathcal{X}_j]\right)^2$&lt;/li>
&lt;li>&lt;strong>Markov Inequality&lt;/strong>: $\mathbb{P}(\mathcal{X}\geqslant a)\leqslant \mathbb{E}\mathcal{X}/a,\;a&amp;gt;0$&lt;/li>
&lt;li>&lt;strong>Chebychev Inequality&lt;/strong>: $\mathbb{P}(|\mathcal{X}-\mu|\geqslant\delta)\leqslant\sigma^2/\delta^2$&lt;/li>
&lt;li>&lt;strong>Jenson Inequality&lt;/strong>: $\psi(\mathbb{E} \mathcal{X}) \leqslant \mathbb{E}\psi(\mathcal{X})$ for any convex function $\psi$&lt;/li>
&lt;li>&lt;strong>Chernoff bound&lt;/strong>: $\mathbb{P}(\mathcal{X}\geqslant a)\leqslant \underset{s\geqslant 0}{\min},e^{-as}M_ \mathcal{X}(s)$&lt;/li>
&lt;li>&lt;strong>Law of Large Numbers&lt;/strong>: let $\mathcal{X}_i$ be samples drawn from $(\mathbb{R}^n,\mathcal{F}^n,\mathbb{P})$, and $\mathbb{P}$ is such that $\mathcal{X}_k$ has mean $\mu$ and covariance $S$
&lt;ul>
&lt;li>Weak version: if $\mathcal{Y}_ k=\frac{1}{k}\sum^k_{j=1}\mathcal{X}_j$ then $$\underset{k\to\infty}{\lim} \mathbb{P}\left(\left\Vert \mathcal{Y}_k-\mu\right\Vert&amp;gt;\epsilon\right)=0$$&lt;/li>
&lt;li>Strong version: if $\mathcal{Y}_ k=\frac{1}{k}\sum^k_{j=1}\mathcal{X}_j$ then $$\underset{k\to\infty}{\lim} \mathcal{Y}_k=\mu$$&lt;/li>
&lt;li>Central Limit Theorem: if $\mathcal{Y}_ k=\frac{1}{\sqrt{k}}\sum^k_{j=1}S^{-1/2}\left(\mathcal{X}_ j-\mu\right)$ then $$\underset{k\to\infty}{\lim} f_{\mathcal{Y}_k}(y_k)=\mathcal{N}(0,I)$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="estimation-theory">Estimation Theory&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Hilbert Space Projection Theorem&lt;/strong>: A Hilbert space is a complete inner product space. Let $\mathcal{H}$ be a Hilbert space, $\mathcal{M}$ be a closed subspace of $\mathcal{H}$ and $z\in\mathcal{H}$. Then there is a unique $\hat{z}\in\mathcal{M}$ which is closest to $z$: $$\Vert z-\hat{z}\Vert &amp;lt; \Vert z-y\Vert, \forall y\in\mathcal{M}, y\neq\hat{z}$$
&lt;ul>
&lt;li>&lt;strong>Orthogonality Principle&lt;/strong>: $\hat{z}$ is the closest point iff. $\langle z-\hat{z},y\rangle=0, \forall y\in\mathcal{M}$. In estimation we formulate inner product as $\langle \mathcal{X}, \mathcal{Y}\rangle=\mathbb{E}[\mathcal{X}\mathcal{Y}^T]$, it&amp;rsquo;s $\mathbb{E}[(\mathcal{Y}-\mathbb{E}[\mathcal{Y}|\mathcal{X}])h(\mathcal{X})]=0, \forall h(\cdot)$.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Estimation Problem&lt;/strong>: Given random vector \mathcal{X} and random variable $\mathcal{Y}$ with joint PDF $f_{\mathcal{XY}}(\cdot)$, we observe $\mathcal{X}=x$ and we want to form an estimate of $\mathcal{Y}$ as $\hat{\mathcal{Y}}=g(x)$&lt;/li>
&lt;li>&lt;strong>Minimum Mean Square Error(MMSE) Estimation&lt;/strong>: $\hat{\mathcal{Y}}=\mathbb{E}(\mathcal{Y}|\mathcal{X})$
&lt;ul>
&lt;li>Target: minimize $\mathbb{E}[(\mathcal{Y}-\hat{\mathcal{Y}})(\mathcal{Y}-\hat{\mathcal{Y}})^\top]$ with $\hat{\mathcal{Y}}=g(\mathcal{X})$ (arbitary $g(\cdot)$)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Linear Minimum MSE(LMMSE) Estimation&lt;/strong>: $\hat{A}$ satisfies $\mathbb{E}[\mathcal{YX^\top}]=A\mathbb{E}[\mathcal{XX^\top}]$
&lt;ul>
&lt;li>Target: minimize $\mathbb{E}[(\mathcal{Y}-\hat{\mathcal{Y}})(\mathcal{Y}-\hat{\mathcal{Y}})^\top]$ with $\hat{\mathcal{Y}}=A\mathcal{X}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Affine Minimum MSE(AMMSE) Estimation&lt;/strong>: $\hat{G}$ satisfies $\mathbb{E}[\mathcal{YX^\top}]=G\mathbb{E}[\mathcal{XX^\top}]$, $\hat{c}=\mu_{\mathcal{Y}}-\hat{G}\mu_{\mathcal{X}}$
&lt;ul>
&lt;li>Target: minimize $\mathbb{E}[(\mathcal{Y}-\hat{\mathcal{Y}})(\mathcal{Y}-\hat{\mathcal{Y}})^\top]$ with $\hat{\mathcal{Y}}=G\mathcal{X}+c$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="convergence">Convergence&lt;/h2>
&lt;ul>
&lt;li>Categories: Generally we assume $n\to\infty$ and giving $\mathcal{X}_n$ are random variables.
&lt;ul>
&lt;li>&lt;strong>Sure Convergence&lt;/strong> ($\mathcal{X}_n\xrightarrow{\text{surely}}\mathcal{X}$): $$\forall \omega\in\Omega, \mathcal{X}_n(\omega)\xrightarrow{n\to\infty}\mathcal{X}$$&lt;/li>
&lt;li>&lt;strong>Almost Sure Convergence&lt;/strong> ($\mathcal{X}_ n\xrightarrow{\text{a.s./w.p.1}}\mathcal{X}$, &lt;code>w.p.1&lt;/code>: with probability 1): $$\mathbb{P}(\{\omega\in\Omega:\lim_ {n\to\infty}\mathcal{X} _n(\omega)=\mathcal{X}\})=1$$
&lt;ul>
&lt;li>Equivalent definition: $$\mathbb{P}(\bigcup_{\epsilon&amp;gt;0}A(\epsilon))=0\;\text{where}\;A(\epsilon)=\bigcap_{N=1}\bigcup_{n=N}\{\omega:|\mathcal{X}_n(\omega)-\mathcal{X}(\omega)|\geqslant\epsilon\}$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Convergence in Probability&lt;/strong> ($\mathcal{X}_ n\xrightarrow{\text{in prob.}}\mathcal{X}$): $$\mathbb{P}(\{\omega\in\Omega:\lim_ {nn\to\infty}|\mathcal{X}_n(\omega)-\mathcal{X}(\omega)|\geqslant\epsilon\})=0,\;\forall\epsilon&amp;gt;0$$&lt;/li>
&lt;li>&lt;strong>Convergence in Distribution&lt;/strong> ($\mathcal{X}_ n\xrightarrow{\text{D}}\mathcal{X}$): $$\lim_ {n\to\infty} F_{\mathcal{X}_ n}(x)=F_{\mathcal{X}}(x)$$&lt;/li>
&lt;li>&lt;strong>Convergence in mean of order&lt;/strong>: ($\mathcal{X}_ n\xrightarrow{\text{mean r}}\mathcal{X}$, abbr. &lt;code>m.s.&lt;/code> for $r=2$): $$\mathbb{E}[|\mathcal{X}_n-\mathcal{X}|^r]\to 0$$&lt;/li>
&lt;li>$\mathcal{X}_n\xrightarrow{\text{a.s./mean r}}\mathcal{X}\Rightarrow\mathcal{X}_n\xrightarrow{\text{in prob.}}\mathcal{X}\Rightarrow\mathcal{X}_n\xrightarrow{\text{D}}\mathcal{X}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Strong Law of Large Numbers&lt;/strong> (requires iid.): $$S_n=\frac{1}{n}\sum\mathcal{X}_ i\xrightarrow{\text{a.s./m.s.}}\mu_ \mathcal{X}$$&lt;/li>
&lt;li>&lt;strong>Weak Law of Large Numbers&lt;/strong> (requires indentical uncorrelated distributed): $S_n\xrightarrow{\text{in prob.}}\mu_\mathcal{X}$&lt;/li>
&lt;li>&lt;strong>Central Limit Theorem&lt;/strong> (requires independent): $$\mathcal{Y}_n=\frac{1}{\sqrt{n}}\sum\frac{\mathcal{X}_i-m}{\sigma}\xrightarrow{\text{D}}\mathcal{N}(0,1)$$&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Law of Large Numbers and Central Limit Theorem together characterized the limiting behavior of the average of samples. See &lt;a class="link" href="https://en.wikipedia.org/wiki/Central_limit_theorem#Relation_to_the_law_of_large_numbers" target="_blank" rel="noopener"
>Wikipedia&lt;/a> and &lt;a class="link" href="http://www.cs.toronto.edu/~yuvalf/CLT.pdf" target="_blank" rel="noopener"
>a proof&lt;/a> to see their relationships.&lt;/p>
&lt;/blockquote>
&lt;h2 id="miscellaneous-corollaries">Miscellaneous Corollaries&lt;/h2>
&lt;ol>
&lt;li>For random valuable that takes positive values, $\mathbb{E}(X)=\int^\infty_0 \mathbb(\mathcal{X}&amp;gt;x)dx$&lt;/li>
&lt;li>If $\mathcal{X}_1,&amp;hellip;\mathcal{X}_n$ are IID continuous random variables, then $\mathbb{P}(\mathcal{X}_1&amp;lt;\mathcal{X}_2&amp;lt;\ldots&amp;lt;\mathcal{X}_n)=1/n!$&lt;/li>
&lt;li>Define $\mathcal{X}_ {(j)}$ to be the j-th smallest among $\mathcal{X}_ 1,&amp;hellip;\mathcal{X}_ n$. Suppose $\mathcal{X}_ 1,&amp;hellip;\mathcal{X}_ n$ are IID random variables with PDF $f$ and CDF $F$, then $$f_ {\mathcal{X}_ {(j)}}(x)=\frac{n!}{(n-j)!(j-1)!}[F(x)]^{j-1}[1-F(x)]^{n-j}f_ \mathcal{X}(x)$$&lt;/li>
&lt;/ol>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>There is a &lt;a class="link" href="http://www.math.wm.edu/~leemis/chart/UDR/UDR.html" target="_blank" rel="noopener"
>chart about &lt;em>Univariate Distribution Relationships&lt;/em>&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item></channel></rss>