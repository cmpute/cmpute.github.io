<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on JacobZ</title><link>https://zyxin.xyz/blog/en/post/</link><description>Recent content in Posts on JacobZ</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Thu, 05 Aug 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://zyxin.xyz/blog/en/post/index.xml" rel="self" type="application/rss+xml"/><item><title>Select Modern Programming Languages by One-line comments</title><link>https://zyxin.xyz/blog/en/2021-08/OneLinePerProgrammingLanguage/</link><pubDate>Thu, 05 Aug 2021 00:00:00 +0000</pubDate><guid>https://zyxin.xyz/blog/en/2021-08/OneLinePerProgrammingLanguage/</guid><description>&lt;p>In recent years, Many novel programming languages have emergerd, and new concepts continued to appear. I always wanted to learn about various new programming languages, and it would be nice to master one more language if it is prospective. Therefore, this article summarizes my understanding of some popular modern languages and their various characteristics. If you are also interested in trying a new language, then I hope this article can help you~&lt;/p>
&lt;p>For the popularity of languages, we can refer to &lt;a class="link" href="https://www.tiobe.com/tiobe-index/" target="_blank" rel="noopener"
>TIOBE rankings&lt;/a>, &lt;a class="link" href="https://madnight.github.io/githut" target="_blank" rel="noopener"
>Github rankings&lt;/a> and &lt;a class="link" href="https://insights.stackoverflow.com/survey/2020#technology-most-loved-dreaded-and-wanted-languages-loved" target="_blank" rel="noopener"
>StackOverflow rankings&lt;/a>. I won&amp;rsquo;t go into details in this post about the language features, since they&amp;rsquo;re still evolving over time. And for the same reason, the comments on each language only reflects my impression on each language listed here.&lt;/p>
&lt;p>This post focuses on general programming languange. For Domain-Specific Languages (DSL) like SQL, I won&amp;rsquo;t include them cuz you have to learn them only when needed.&lt;/p>
&lt;p>In terms of the history of the programming languages, there&amp;rsquo;s a very interesting picture below (&lt;a class="link" href="https://infographicnow.com/educational/languages/educational-infographic-timeline-of-programming-languages-infographic/" target="_blank" rel="noopener"
>source here&lt;/a>), if you&amp;rsquo;re interested in the details, you can refer to the wikipedia pages about &lt;a class="link" href="https://en.wikipedia.org/wiki/History_of_programming_languages" target="_blank" rel="noopener"
>the history&lt;/a> and &lt;a class="link" href="https://en.wikipedia.org/wiki/Timeline_of_programming_languages" target="_blank" rel="noopener"
>the timeline&lt;/a> (highly recommend the second one!), or refer to &lt;a class="link" href="https://www.levenez.com/lang/" target="_blank" rel="noopener"
>this website&lt;/a>.&lt;/p>
&lt;p>&lt;img src="https://zyxin.xyz/blog/blog/en/2021-08/OneLinePerProgrammingLanguage/timeline-of-programming-languages.jpg"
width="600"
height="1621"
loading="lazy"
alt="Timeline of the programming languages"
class="gallery-image"
data-flex-grow="37"
data-flex-basis="88px"
>&lt;/p>
&lt;h2 id="features-of-a-modern-languange">Features of a modern languange&lt;/h2>
&lt;p>Before the summary, first I should introduce some programming concepts for a better understanding of the languages. The comparison in depth of the concepts can be also found in &lt;a class="link" href="https://en.wikipedia.org/wiki/Comparison_of_programming_languages#:~:text=General%20comparison%20%20%20%20Language%20%20,%20%20%20%2020%20more%20rows%20" target="_blank" rel="noopener"
>wikipedia&lt;/a>. Here the introduction is also very brief, you can search these keywords for further information.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Object-Oriented&lt;/strong>: This should familiar to you if you have attended any class about programming. Object-oriented programming usually means that the core logic of the code is built around &amp;ldquo;classes&amp;rdquo;. A class contains a definition of an object and some associated methods, an derived class can be used to simplify the code.&lt;/li>
&lt;li>&lt;strong>Dynamic / Static Types&lt;/strong>: Static typing requires the program to specify the type of each variable while for dynamic typing that&amp;rsquo;s not a requirement. These two options are trade-off between the flexibilty and the safety of the program. A similar naming of this choice is &lt;strong>strong / weak typing&lt;/strong>. In weak typing languages, there&amp;rsquo;s a concept called &amp;ldquo;Duck-typing&amp;rdquo;, which means we only care about what interface the object/type provides, rather than ensure that the object has a specific type.&lt;/li>
&lt;li>&lt;strong>Meta-Programming / Generic Type&lt;/strong>: Meta-programming means that we can generate code with some &amp;ldquo;meta&amp;rdquo; code, a representative case is the template in C++. Meanwhile, generic types are similar to meta-programming, but it won&amp;rsquo;t generate code explicitly, it&amp;rsquo;s just some code supporting types as parameters.&lt;/li>
&lt;li>&lt;strong>Imperative / Declarative / Functional&lt;/strong>: In imperative Languages, you instruct the program, step by step, to do something; In declarative Languages, you only tell the program what you want to do; In functional Languages, the function is the first-class citizen and the program will achieve the goal by call functions sequentially. The functions usually can be stored in variables as well.&lt;/li>
&lt;li>&lt;strong>Parallelism&lt;/strong>: Parallelism means that the program supports running multiple code blocks in parallel (including processes, threads and coroutines)&lt;/li>
&lt;li>&lt;strong>Data Science&lt;/strong>: Some programming Languages are designed for data scientists, which mainly features the built-in support for high-precision and high-dimensional data.&lt;/li>
&lt;li>&lt;strong>Test-driven Development (TDD) / Design by Contract (Doc)&lt;/strong>: These are two different paradigm for building the program. Test-driving means that the target of coding is to passing some tests, while design by contract means the program has to meet some constraints during coding. Support for tests and contracts are not critical for programming languages, but in modern complex programs, they can drastically improve the efficiency and safety of the development.&lt;/li>
&lt;li>&lt;strong>Virtual Machine / Intermediate Language&lt;/strong>: A few languages support cross-platform compatibility through languange virtual machine, which will convert the intermediate codes to machine codes. The representatives are JVM, CLR and LLVM.&lt;/li>
&lt;li>&lt;strong>Garbage Collection (GC)&lt;/strong>: Garbage collection is a feature built in some languages&amp;rsquo; runtime. With this feature, you don&amp;rsquo;t need to worry about the lifetime of variables, since the garbage collector handles the deconstruction for you.&lt;/li>
&lt;/ul>
&lt;/hr>
&lt;blockquote>
&lt;p>Now I will lists the 1-line comments of the mainstream programming languages selected from the rankings mentioned above.&lt;/p>
&lt;/blockquote>
&lt;h2 id="why-to-choose-this-languange-in-about-1-sentence">Why to choose this languange in (about) 1 sentence&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>C&lt;/strong>: All time god which is closest to assembly languange with little dependency and high performance. The ABI written with C can be called from most other languages&lt;/li>
&lt;li>&lt;strong>CoffeeScript&lt;/strong>: Javascript with syntax sugar&lt;/li>
&lt;li>&lt;strong>C++&lt;/strong>: One of the most powerful Languages with full support of object-oriented programming and meta-programming, and full compatibility with C&lt;/li>
&lt;li>&lt;strong>C#&lt;/strong>: Full of syntax sugar and faster than Java. The recent advancements in open-source provide more resources.&lt;/li>
&lt;li>&lt;strong>D&lt;/strong>: Aiming at replacing C++ with many modern features including design by contract&lt;/li>
&lt;li>&lt;strong>Dart&lt;/strong>: Backed by Google and aiming at replacing Javascript, but that&amp;rsquo;s it&lt;/li>
&lt;li>&lt;strong>F#&lt;/strong>: Functional version of C#&lt;/li>
&lt;li>&lt;strong>Fortran&lt;/strong>: Old but blazing fast language, even faster than C&lt;/li>
&lt;li>&lt;strong>Go&lt;/strong>: Fast compilation with great coroutine support and produces single file executable without dependencies&lt;/li>
&lt;li>&lt;strong>Groovy&lt;/strong>: Dynamic typing version of Java built by Apache, similar goal to Ruby&lt;/li>
&lt;li>&lt;strong>Haskell&lt;/strong>: Representative of functional language&lt;/li>
&lt;li>&lt;strong>Java&lt;/strong>: Widely used in server applications with tons of available packages, equipped with garbage collector&lt;/li>
&lt;li>&lt;strong>Javascript&lt;/strong>: Popular in both front-end and backend development, being very flexible and supported by most browsers. Also comes with tons of availabe packages.&lt;/li>
&lt;li>&lt;strong>Julia&lt;/strong>: Science-computing oriented languange with builtin support to n-dimensional tensor and fast speed. Has the potential to be the next Fortran&lt;/li>
&lt;li>&lt;strong>Kotlin&lt;/strong>: Java with modern syntax sugar developed by JetBrains, compile to Java or JS&lt;/li>
&lt;li>&lt;strong>Matlab&lt;/strong>: Designed for engineers and scientists with many engineering packages. The Simulink package has no alternatives by now.&lt;/li>
&lt;li>&lt;strong>Objective-C&lt;/strong>: Any pros?&lt;/li>
&lt;li>&lt;strong>Perl&lt;/strong>: Suitable to be used as scripting or glue language, with great string processing utilites.&lt;/li>
&lt;li>&lt;strong>PHP&lt;/strong>: Server-oriented languange that can be embedded into HTML, flexible and simple&lt;/li>
&lt;li>&lt;strong>Python&lt;/strong>: Very flexible with everything being an object. Great readability and interoperability with C/C++. Tons of available packages&lt;/li>
&lt;li>&lt;strong>R&lt;/strong>: Designed for statistics with rich domain-specific packages&lt;/li>
&lt;li>&lt;strong>Ruby&lt;/strong>: Chained calling, sugar syntax and as flexible as Python&lt;/li>
&lt;li>&lt;strong>Rust&lt;/strong>: Memory safety ensured in language level. No garbage collector means that it&amp;rsquo;s fast!&lt;/li>
&lt;li>&lt;strong>Scala&lt;/strong>: Scala is like C++ on JVM, while Kotlin is like C# on JVM&lt;/li>
&lt;li>&lt;strong>Swift&lt;/strong>: Replacement for Obj-C developed by Apple. Similar to Java&lt;/li>
&lt;li>&lt;strong>Typescript&lt;/strong>: Strong typing version of Javascript&lt;/li>
&lt;li>&lt;strong>Vala&lt;/strong>: Aiming at replacing C/C++ in GUI programming in Linux (Elementary OS). Compile to C = fast speed.&lt;/li>
&lt;li>&lt;strong>Visual Basic&lt;/strong>: Builtin support by many Microsoft software&lt;/li>
&lt;/ul>
&lt;h2 id="why-to-avoid-this-languange-in-about-1-sentense">Why to avoid this languange in (about) 1 sentense&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>C&lt;/strong>: No modern language features, unsafe and hard for object-oriented programming&lt;/li>
&lt;li>&lt;strong>CoffeeScript&lt;/strong>: No compelling feature, Typescript might be a better choice&lt;/li>
&lt;li>&lt;strong>C++&lt;/strong>: Templates are very hard for debugging, slow compilation. Many modern features are implemented based on templates and std libraries, which are not elegant&lt;/li>
&lt;li>&lt;strong>C#&lt;/strong>: Confusing runtime standards, frequently changing API and sometimes break backward compatibility&lt;/li>
&lt;li>&lt;strong>D&lt;/strong>: No sponsor = poor eco-system&lt;/li>
&lt;li>&lt;strong>Dart&lt;/strong>: Again, typescript might be a better choice&lt;/li>
&lt;li>&lt;strong>F#&lt;/strong>: I don&amp;rsquo;t know who is using it, C# is good enough&lt;/li>
&lt;li>&lt;strong>Fortran&lt;/strong>: Old syntax and no modern language features&lt;/li>
&lt;li>&lt;strong>Go&lt;/strong>: No generics, don&amp;rsquo;t allow unused variables and modules&lt;/li>
&lt;li>&lt;strong>Groovy&lt;/strong>: There&amp;rsquo;re better alternatives outside of JVM Languages&lt;/li>
&lt;li>&lt;strong>Haskell&lt;/strong>: Learning it is like learning to be a mathematician&lt;/li>
&lt;li>&lt;strong>Java&lt;/strong>: Verbose, not as elegant and fast as C#&lt;/li>
&lt;li>&lt;strong>Javascript&lt;/strong>: Single-threaded, too flexible (see the meme below)
{% asset_img js-triangle.jpg Javascript等号三位一体%}&lt;/li>
&lt;li>&lt;strong>Julia&lt;/strong>: The package manangement is very hard to use. The syntax is also hard to understand&lt;/li>
&lt;li>&lt;strong>Kotlin&lt;/strong>: Slow to compile. No big problems other than that, but there&amp;rsquo;re better alternatives outside JVM&lt;/li>
&lt;li>&lt;strong>Matlab&lt;/strong>: Proprietary languange owned by Mathworks and you have to install a big Matlab software to run the code. Python and Julia are good enough to replace Matlab (if you are not a Simulink user)&lt;/li>
&lt;li>&lt;strong>Objective-C&lt;/strong>: Only used by Apple, hard to read&lt;/li>
&lt;li>&lt;strong>Perl&lt;/strong>: Hard to read, too flexible and slow. Python is better&lt;/li>
&lt;li>&lt;strong>PHP&lt;/strong>: Single threaded, only suitable for Web development. Javascript is better in terms of general abilities and community size&lt;/li>
&lt;li>&lt;strong>Python&lt;/strong>: Bad performance, single threaded (caused by GIL)&lt;/li>
&lt;li>&lt;strong>R&lt;/strong>: Even worse than Matlab&lt;/li>
&lt;li>&lt;strong>Rust&lt;/strong>: The compiler is too strict sometimes, and it&amp;rsquo;s hard to manipulate strings in Rust&lt;/li>
&lt;li>&lt;strong>Ruby&lt;/strong>: Bad performance, usually used only by backend engineers&lt;/li>
&lt;li>&lt;strong>Scala&lt;/strong>: Harder to learn than Kotlin, and worse interoperability with Java&lt;/li>
&lt;li>&lt;strong>Swift&lt;/strong>: Only a good choice for iOS and OSX development.&lt;/li>
&lt;li>&lt;strong>Typescript&lt;/strong>: Only a good choice for Web development&lt;/li>
&lt;li>&lt;strong>Vala&lt;/strong>: Only used by Gnome and ElementaryOS, very small community&lt;/li>
&lt;li>&lt;strong>Visual Basic&lt;/strong>: Don&amp;rsquo;t use it if you can!&lt;/li>
&lt;/ul></description></item><item><title>Notes for Algebra Basics</title><link>https://zyxin.xyz/blog/en/2020-06/AlgebraBasicsNotes/</link><pubDate>Sat, 27 Jun 2020 00:00:00 +0000</pubDate><guid>https://zyxin.xyz/blog/en/2020-06/AlgebraBasicsNotes/</guid><description>&lt;blockquote>
&lt;ul>
&lt;li>In this note, $\{x_i\}^b_a$ denotes set $\{x_a, x_{a+1}, \ldots, x_b\}$&lt;/li>
&lt;li>&lt;em>TODO: add Jordan Form&lt;/em>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h2 id="algebraic-structures">Algebraic Structures&lt;/h2>
&lt;h3 id="operation">Operation&lt;/h3>
&lt;ul>
&lt;li>Definition: an (binary, closed) &lt;strong>operation&lt;/strong> $\ast$ on a set $S$ is a mapping of $S\times S\to S$&lt;/li>
&lt;li>&lt;strong>Commutative&lt;/strong>: $x\ast y=y\ast x,\;\forall x,y\in S$&lt;/li>
&lt;li>&lt;strong>Associative&lt;/strong>: $(x\ast y)\ast z=x\ast (y\ast z),\;\forall x,y,z\in S$&lt;/li>
&lt;/ul>
&lt;h3 id="group">Group&lt;/h3>
&lt;ul>
&lt;li>Definition: a &lt;strong>group&lt;/strong> is a pair $(\mathcal{S},\ast)$ with following axioms
&lt;ol>
&lt;li>$\ast$ is associative on $\mathcal{S}$&lt;/li>
&lt;li>(Identity element) $\exists e\in \mathcal{S}\text{ s.t. }x\ast e=e\ast x=x,\;\forall x\in \mathcal{S}$&lt;/li>
&lt;li>(Inverse element) $\forall x\in \mathcal{S}, \exists x&amp;rsquo; \in \mathcal{S}\text{ s.t. }x\ast x&amp;rsquo;=x&amp;rsquo;\ast x=e$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>Abelian&lt;/strong>: a group is called &lt;strong>abelian group&lt;/strong> if $\ast$ is also commutative&lt;/li>
&lt;/ul>
&lt;h3 id="ring">Ring&lt;/h3>
&lt;ul>
&lt;li>Definition: a &lt;strong>ring&lt;/strong> is a triplet $(\mathcal{R},+,\ast)$ consisting of a set of &lt;code>scalars&lt;/code> $\mathcal{R}$ and two operators + and $\ast$ with following axioms
&lt;ol>
&lt;li>$(\mathcal{R},+)$ is an abelian group with identity denoted $0$&lt;/li>
&lt;li>$\forall a,b,c \in \mathcal{R}\text{ s.t. }a\ast(b\ast c) = (a\ast b)\ast c$&lt;/li>
&lt;li>$\exists 1\in\mathcal{R}, \forall a\in\mathcal{R}\text{ s.t. }a\cdot 1=a$&lt;/li>
&lt;li>$\ast$ is distributive over $+$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;h3 id="field">Field&lt;/h3>
&lt;ul>
&lt;li>Definition: a &lt;strong>field&lt;/strong> $(\mathcal{F},+,\ast)$ is a ring where $(\mathcal{F}\backslash\{0\},\ast)$ is also an abelian group.
&lt;blockquote>
&lt;p>Difference from ring to field is that $\ast$ need to be commutative and have a multiplicative inverse&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;h3 id="vector-space">Vector Space&lt;/h3>
&lt;ul>
&lt;li>Definition: a &lt;strong>vector space&lt;/strong> (aka. &lt;strong>linear space&lt;/strong>) is a triplet $(\mathcal{U},\oplus,\cdot)$ defined over a field $(\mathcal{F},+,\ast)$ with following axioms, where set $\mathcal{U}$ is called &lt;code>vectors&lt;/code>, operator $\oplus$ is called &lt;code>vector addition&lt;/code> and mapping $\cdot$ is called &lt;code>scalar multiplication&lt;/code>:
&lt;ol>
&lt;li>(&lt;strong>Null vector&lt;/strong>) $(\mathcal{U},+)$ is an abelian group with identity element $\emptyset$&lt;/li>
&lt;li>Scalar multiplication is a mapping of $\mathcal{F}\times\mathcal{U}\to\mathcal{U}$&lt;/li>
&lt;li>$\alpha\cdot(x\oplus y) = \alpha\cdot x \oplus \alpha\cdot y,\;\forall x,y\in\mathcal{U};\alpha\in\mathcal{F}$&lt;/li>
&lt;li>$(\alpha+\beta)\cdot x = \alpha\cdot x\oplus\beta\cdot x,\;\forall x\in\mathcal{U};\alpha,\beta\in\mathcal{F}$&lt;/li>
&lt;li>$(\alpha\ast\beta)\cdot x=\alpha\cdot(\beta\cdot x),\;\forall x\in\mathcal{U};\alpha,\beta\in\mathcal{F}$&lt;/li>
&lt;li>$1_\mathcal{F}\cdot x=x$&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>Usually we don&amp;rsquo;t distinguish vector addition $\oplus$ and addition of scalar $+$. Juxtaposition is also commonly used for &lt;em>both&lt;/em> scalar multiplication $\cdot$ and multiplication of scalars $\ast$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Subspace&lt;/strong>: a subspace $\mathcal{V}$ of a linear space $\mathcal{U}$ over field $\mathcal{F}$ is a subset of $\mathcal{U}$ which is itself a linear space over $\mathcal{F}$ under same vector addition and scalar multiplication.&lt;/li>
&lt;/ul>
&lt;h4 id="basis--coordinate">Basis &amp;amp; Coordinate&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Linear Independence&lt;/strong>: Let $\mathcal{V}$ be a vector space over $\mathcal{F}$ and let $X=\{x_i\}^n_1\subset \mathcal{V}$
&lt;ul>
&lt;li>X is &lt;strong>linearly dependent&lt;/strong> if $\exists \alpha_1,\ldots,\alpha_n\in\mathcal{F}$ not all 0 s.t. $\sum^n_{i=1} \alpha_i x_i=0$.&lt;/li>
&lt;li>X is &lt;strong>linearly independent&lt;/strong> if $\sum^n_{i=1} \alpha_i x_i=0 \Rightarrow \alpha_1=\alpha_2=\ldots=\alpha_n=0$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Span&lt;/strong>: Given a set of vectors $V$, the set of linear combinations of vectors in $V$ is called the &lt;strong>span&lt;/strong> of it, denoted $\mathrm{span}\{V\}$&lt;/li>
&lt;li>&lt;strong>Basis&lt;/strong>: A set of linearly independent vectors in a linear space $\mathcal{V}$ is a &lt;strong>basis&lt;/strong> if every vector in $\mathcal{V}$ can be expressed as a &lt;em>unique linear combination&lt;/em> of these vectors. (see below &amp;ldquo;Coordinate&amp;rdquo;)
&lt;ul>
&lt;li>Basis Expansion: Let $(X,\mathcal{F})$ be a vector space of dimension n. If $\{v_i\}^k_1,\;1\leqslant k&amp;lt; n$ is linearly independent, then $\exists \{v_i\}^n_{k+1}$ such that $\{v_i\}_1^n$ is a basis.&lt;/li>
&lt;li>&lt;strong>Reciprocal Basis&lt;/strong>: Given basis $\{v_i\}^n_1$, a set ${r_i}^1_n$ that satifies $\langle r_i,v_j \rangle=\delta_i(j)$ is a reciprocal basis. It can be generated by Gram-Schmidt Process and $\forall x\in\mathcal{X}, x=\sum^n_{i=1}\langle r_i,x\rangle v_i$.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Dimension&lt;/strong>: &lt;em>Cardinality&lt;/em> of the basis is called the &lt;strong>dimension&lt;/strong> of that vector space, which is equal to &lt;em>the maximum number of linearly independent vectors&lt;/em> in the space. Denoted as $dim(\mathcal{V})$.
&lt;ul>
&lt;li>In an $n$-dimensional vector space, any set of $n$ linearly independent vectors is a basis.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Coordinate&lt;/strong>: For a vector $x$ in vector space $\mathcal{V}$, given a basis $\{e_1, \ldots, e_n\}$ we can write $x$ as $x=\sum^n_{i=1}\beta_i e_i=E\beta$ where $E=\begin{bmatrix}e_1&amp;amp;e_2&amp;amp;\ldots&amp;amp;e_n\end{bmatrix}$ and $\beta=\begin{bmatrix}\beta_1&amp;amp;\beta_2&amp;amp;\ldots&amp;amp;\beta_n\end{bmatrix}^\top$. Here $\beta$ is called the &lt;strong>representation&lt;/strong> (or &lt;strong>coordinate&lt;/strong>) of $x$ given the basis $E$.&lt;/li>
&lt;/ul>
&lt;h4 id="norm--inner-product">Norm &amp;amp; Inner product&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Inner Product&lt;/strong>: an operator on two vectors that produces a scalar result (i.e. $\langle\cdot,\cdot\rangle:\mathcal{V}\to\mathbb{R}\;or\;\mathbb{C}$) with following axioms:
&lt;ol>
&lt;li>(Symmetry) $\langle x,y \rangle=\overline{\langle y,x\rangle},\;\forall x,y\in\mathcal{V}$&lt;/li>
&lt;li>(Bilinearity) $\langle \alpha x+\beta y,z\rangle=\alpha\langle x,z\rangle+\beta\langle y,z\rangle,\;\forall x,y,z\in\mathcal{V};\alpha,\beta\in\mathbb{C}$&lt;/li>
&lt;li>(Pos. definiteness) $\langle x,x\rangle\geqslant 0,\;\forall x\in\mathcal{V}$ and $\langle x,x\rangle=0\Rightarrow x=0_\mathcal{V}$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>Inner Product Space&lt;/strong>: A linear space with a defined inner product&lt;/li>
&lt;li>&lt;strong>Orthogonality&lt;/strong>:
&lt;ul>
&lt;li>Perpedicularity of vectors ($x\perp y$): $\langle x,y\rangle=0$&lt;/li>
&lt;li>Perpedicularity of a vector to a set ($y\perp\mathcal{S},\mathcal{S}\subset\mathcal{V}$): $y\perp x,\;\forall x\in\mathcal{S}$&lt;/li>
&lt;li>&lt;strong>Orthogonal Set&lt;/strong>: set $\mathcal{S}\subset(\mathcal{U},\langle\cdot,\cdot\rangle)$ is orthogonal $\Leftrightarrow x\perp y,\;\forall x,y\in\mathcal{S},x\neq y$&lt;/li>
&lt;li>&lt;strong>Orthonormal Set&lt;/strong>: set $\mathcal{S}$ is orthonormal iff $\mathcal{S}$ is orthogonal and $\Vert x\Vert=1,\;\forall x\in\mathcal{S}$&lt;/li>
&lt;li>Orthogonality of sets ($\mathcal{X}\perp\mathcal{Y}$): $\langle x,y\rangle=0,\;\forall x\in\mathcal{X};y\in\mathcal{Y}$&lt;/li>
&lt;li>&lt;strong>Orthogonal Complement&lt;/strong>: Let $(\mathcal{V},\langle\cdot,\cdot\rangle)$ be an inner product space and let $\mathcal{U}\subset\mathcal{V}$ be a subspace of $\mathcal{V}$, the orthogonal complement of $\mathcal{U}$ is $\mathcal{U}^\perp=\left\{v\in\mathcal{V}\middle|\langle v,u\rangle=0,\;\forall u\in\mathcal{U}\right\}$.
&lt;ul>
&lt;li>$\mathcal{U}^\perp\subset\mathcal{V}$ is a subspace&lt;/li>
&lt;li>$\mathcal{V}=\mathcal{U}\overset{\perp}{\oplus}\mathcal{U}^\perp$ ($\oplus$: direct sum, $\overset{\perp}{\oplus}$: orthogonal sum)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Norm&lt;/strong>: A &lt;strong>norm&lt;/strong> on a linear space $\mathcal{V}$ is mapping $\Vert\cdot\Vert:\;\mathcal{V}\to\mathbb{R}$ such that:
&lt;ol>
&lt;li>(Positive definiteness) $\Vert x\Vert\geqslant 0\;\forall x\in \mathcal{V}$ and $\Vert x\Vert =0\Rightarrow x=0_\mathcal{V}$&lt;/li>
&lt;li>(Homogeneous) $\Vert \alpha x\Vert=|\alpha|\cdot\Vert x\Vert,\;\forall x\in\mathcal{V},\alpha\in\mathbb{R}$&lt;/li>
&lt;li>(Triangle inequality) $\Vert x+y\Vert\leqslant\Vert x\Vert+\Vert y\Vert$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>Distance&lt;/strong>: Norm can be used to measure distance between two vectors. Meanwhile, distance from a vector to a (sub)space is defined as $d(x,\mathcal{S})=\inf_{y\in\mathcal{S}} d(x,y)=\inf_{y\in\mathcal{S}} \Vert x-y\Vert$
&lt;ul>
&lt;li>&lt;strong>Projection Point&lt;/strong>: $x^* =\arg\min_{y\in\mathcal{S}}\Vert x-y\Vert$ is the projection point of $x$ on linear space $\mathcal{S}$.&lt;/li>
&lt;li>&lt;strong>Projection Theorem&lt;/strong>: $\exists !x^* \in\mathcal{S}$ s.t. $\Vert x-x^* \Vert=d(x,\mathcal{S})$ and we have $(x-x^*) \perp\mathcal{S}$&lt;/li>
&lt;li>&lt;strong>Orthogonal Projection&lt;/strong>: $P(x)=x^*:\mathcal{X}\to\mathcal{M}$ is called the orthogonal projection of $\mathcal{X}$ onto $\mathcal{M}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Normed Space&lt;/strong>: A linear space with a defined norm $\Vert\cdot\Vert$, denoted $(\mathcal{V},\mathcal{F},\Vert\cdot\Vert)$
&lt;blockquote>
&lt;p>A inner product space is always a normed space because we can define $\Vert x\Vert=\sqrt{\langle x,x\rangle}$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>Common $\mathbb{R}^n$ Norms:
&lt;ul>
&lt;li>Euclidean norm (2-norm): $\Vert x\Vert_2=\left(\sum^n_{i=1}|x_i|^2\right)^{1/2}=\left\langle x,x\right\rangle^{1/2}=\left(x^\top x\right)^{1/2}$&lt;/li>
&lt;li>$l_p$ norm (p-norm): $\Vert x\Vert_p=\left(\sum^n_{i=1}|x_i|^p\right)^{1/p}$&lt;/li>
&lt;li>$l_1$ norm: $\Vert x\Vert_1=\sum^n_{i=1}|x_i|$&lt;/li>
&lt;li>$l_\infty$ norm: $\Vert x\Vert_\infty=\max_{i}\{x_i\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Common matrix norms:
&lt;blockquote>
&lt;p>Matrix norms are also called &lt;strong>operator norms&lt;/strong>, can measure how much a linear operator &amp;ldquo;magnifies&amp;rdquo; what it operates on.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>A general form induced from $\mathbb{R}^n$ norm: $$\Vert A\Vert=\sup_{x\neq 0}\frac{\Vert Ax\Vert}{\Vert x\Vert}=\sup_{\Vert x\Vert=1}\Vert Ax\Vert$$&lt;/li>
&lt;li>$\Vert A\Vert_1=\max_j\left(\sum^n_{i=1}|a_{ij}|\right)$&lt;/li>
&lt;li>$\Vert A\Vert_2=\left[ \max_{\Vert x\Vert=1}\left\{(Ax)^* (Ax)\right\}\right]^{1/2}=\left[ \lambda_{max}(A^ *A)\right]^{1/2}$ ($\lambda_{max}$: largest eigenvalue)&lt;/li>
&lt;li>$\Vert A\Vert_\infty=\max_i\left(\sum^n_{j=1}|a_{ij}|\right)$&lt;/li>
&lt;li>(Frobenius Norm) $\Vert A\Vert_F=\left[ \sum^m_{i=1}\sum^n_{j=1}\left|a_{ij}\right|^2\right]^{1/2}=\left[ tr(A^*A)\right]^{1/2}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Useful inequations:
&lt;ul>
&lt;li>&lt;strong>Cauchy-Schwarz&lt;/strong>: $|\langle x,y\rangle|\leqslant\left\langle x,x\right\rangle^{1/2}\cdot\left\langle y,y\right\rangle^{1/2}$&lt;/li>
&lt;li>&lt;strong>Triangle&lt;/strong> (aka. $\Delta$): $\Vert x+y\Vert\leqslant\Vert x\Vert+\Vert y\Vert$
&lt;blockquote>
&lt;p>Lemma: $\Vert x-y\Vert \geqslant \left| \Vert x\Vert-\Vert y\Vert \right|$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Pythagorean&lt;/strong>: $x\perp y \Leftrightarrow \Vert x+y\Vert=\Vert x\Vert+\Vert y\Vert$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="gramian">Gramian&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Gram-Schmidt Process&lt;/strong>: A method to find orthogonal basis $\{v_i\}^n_1$ given an ordinary basis $\{y_i\}^n_1$. It&amp;rsquo;s done by perform $v_k=y_k-\sum^{k-1}_{j=1}\frac{\langle y_k,v_j\rangle}{\langle v_j,v_j \rangle}\cdot v_j$ iteratively from 1 to $n$. To get an orthonormal basis, just normalize these vectors.&lt;/li>
&lt;li>&lt;a class="link" href="https://en.wikipedia.org/wiki/Gramian_matrix" target="_blank" rel="noopener"
>&lt;strong>Gram Matrix&lt;/strong>&lt;/a>: The Gram matrix generated from vectors $\{y_i\}_ 1^k$ is denoted $G(y_ 1,y_ 2,\ldots,y_ k)$. Its element $G_{ij}=\langle y_i,y_j\rangle$
&lt;ul>
&lt;li>&lt;strong>Gram Determinant&lt;/strong>: $g(y_1,y_2,\ldots,y_n)=\det G$&lt;/li>
&lt;li>&lt;strong>Normal Equations&lt;/strong>: Given subspace $\mathcal{M}$ and its basis $\{y_i\}^n_1$, the projection point of $\forall x\in\mathcal{M}$ can be represented by $$x^*=\alpha y=\begin{bmatrix}\alpha_1&amp;amp;\alpha_2&amp;amp;\ldots&amp;amp;\alpha_n\end{bmatrix}\begin{bmatrix}y_1\\y_2\\ \vdots \\y_n\end{bmatrix},\;\beta=\begin{bmatrix}\langle x,y_1\rangle\\ \langle x,y_2\rangle\\ \vdots\\ \langle x,y_n\rangle\end{bmatrix} where\;G^\top\alpha=\beta$$
&lt;blockquote>
&lt;p>For least-squares problem $Ax=b$, consider $\mathcal{M}$ to be the column space of $A$, then $G=A^\top A,\;\beta=A^\top b,\;G^\top\alpha=\beta\Rightarrow\alpha=(A^\top A)^{-1}A^\top b$. Similarly for weighted least-squares problem ($\Vert x\Vert=x^\top Mx$), let $G=A^\top MA, \beta=A^\top Mb$, we can get $\alpha=(A^\top MA)^{-1}A^\top Mb$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="linear-algebra">Linear Algebra&lt;/h2>
&lt;h3 id="linear-operator">Linear Operator&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>Definition: a linear operator $\mathcal{A}$ (aka. linear transformation, linear mapping) is a function $f: V\to U$ that operate on a linear space $(\mathcal{V},\mathcal{F})$ to produce elements in another linear space $(\mathcal{U},\mathcal{F})$ and obey $$\mathcal{A}(\alpha_1 x_1+\alpha_2 x_2) = \alpha_1\mathcal{A}(x_1) + \alpha_2\mathcal{A}(x_2),\;\forall x_1,x_2\in V;\alpha_1, \alpha_2\in\mathcal{F}$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Range (Space)&lt;/strong>: $\mathcal{R}(\mathcal{A})=\left\{u\in U\middle|\mathcal{A}(v)=u,\;\forall v\in V\right\}$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Null Space&lt;/strong> (aka. &lt;strong>kernel&lt;/strong>): $\mathcal{N}(\mathcal{A})=\left\{v\in V\middle|\mathcal{A}(v)=\emptyset_U\right\}$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>$\mathcal{A}$-invariant subspace&lt;/strong>: Given vector space $(\mathcal{V},\mathcal{F})$ and linear operator $\mathcal{A}:\mathcal{V}\rightarrow \mathcal{V}$, $\mathcal{W}\subseteq\mathcal{V}$ is $A$-invariant if $\forall x\in\mathcal{W}$, $\mathcal{A}x\in\mathcal{W}$.&lt;/p>
&lt;ul>
&lt;li>Both $\mathcal{R}(\mathcal{A})$ and $\mathcal{N}(\mathcal{A})$ are $\mathcal{A}$-invariant&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Matrix Representation: Given bases for both $V$ and $U$ (respectively $\{v_i\}^n_1$ and $\{u_j\}^m_1$), matrix representation $A$ satisfies $\mathcal{A}(v_i)=\sum^m_{j=0}A_{ji}u_j$ so that $\beta=A\alpha$ where $\alpha$ and $\beta$ is the representation of a vector under $\{v_i\}$ and $\{u_j\}$ respectively.
{% asset_img linear_map_relations.png Relation between a linear map and its matrix
representations %}&lt;/p>
&lt;ul>
&lt;li>$P$ and $Q$ are change of basis matrices, $A=Q^{-1}\tilde{A}P,\;\tilde{A}=QAP^{-1}$&lt;/li>
&lt;li>The i-th column of $A$ is the coordinates of $\mathcal{A}(v_i)$ represented by the basis $\{u_j\}$, similarly i-th column of $\tilde{A}$ is $\mathcal{A}(\tilde{v}_i)$ represented in $\{\tilde{u}_j\}$&lt;/li>
&lt;li>The i-th column of $P$ is the coordinates of $v_i$ represented by the basis $\{\tilde{v}\}$, similarly i-th column of $Q$ is $u_j$ represented in $\{\tilde{u}\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Matrix Similarity ($A\sim B$): Two (square) matrix representations ($A,B$) of the same linear operator are called &lt;strong>similar&lt;/strong> (or &lt;strong>conjugate&lt;/strong>) and they satisfies $\exists P$ s.t. $B=PAP^{-1}$.&lt;/p>
&lt;blockquote>
&lt;p>From now on we don&amp;rsquo;t distinguish between linear operator $\mathcal{A}$ and its matrix representation where choice of basis doesn&amp;rsquo;t matter.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Rank&lt;/strong>: $rank(A)=\rho(A)\equiv dim(\mathcal{R}(A))$&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Sylvester&amp;rsquo;s Inequality&lt;/strong>: $\rho(A)+\rho(B)-n\leqslant \rho(AB)\leqslant \min\{\rho(A), \rho(B)\}$&lt;/li>
&lt;li>&lt;strong>Singularity&lt;/strong>: $\rho(A)&amp;lt; n$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Nullity&lt;/strong>: $null(A)=\nu(A)\equiv dim(\mathcal{N}(A))$&lt;/p>
&lt;ul>
&lt;li>$\rho(A)+\nu(A)=n$ ($n$ is the dimensionality of domain space)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Adjoint&lt;/strong>: The adjoint of the linear map $\mathcal{A}: \mathcal{V}\to\mathcal{W}$ is the linear map $\mathcal{A}^*: \mathcal{W}\to\mathcal{V}$ such that $\langle y,\mathcal{A}(x)\rangle_\mathcal{W}=\langle \mathcal{A}^ *(y),x\rangle_\mathcal{V}$&lt;/p>
&lt;blockquote>
&lt;p>For its matrix representation, adjoint of $A$ is $A^ *$, which is $A^\top$ for real numbers.&lt;br>
Properties of $\mathcal{A}^ *$ is similar to matrix $A^ *$&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>$\mathcal{U}=\mathcal{R}(A)\overset{\perp}{\oplus}\mathcal{N}(A^ *),\;\mathcal{V}=\mathcal{R}(A^ *)\overset{\perp}{\oplus}\mathcal{N}(A)$&lt;/li>
&lt;li>$\mathcal{N}(A^* )=\mathcal{N}(AA^* )\subseteq\mathcal{U},\;\mathcal{R}(A)=\mathcal{R}(AA^*)\subseteq\mathcal{U}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Self-adjoint&lt;/strong>: $\mathcal{A}$ is self-adjoint iff $\mathcal{A}^*=\mathcal{A}$.&lt;/p>
&lt;ul>
&lt;li>For self-adjoint $\mathcal{A}$, if $\mathcal{V}=\mathbb{C}^{n\times n}$ then $A$ is &lt;strong>hermitian&lt;/strong>; if $\mathcal{V}=\mathbb{R}^{n\times n}$ then $A$ is &lt;strong>symmetric&lt;/strong>.&lt;/li>
&lt;li>Self-adjoint matrices have real eigenvalues and orthogonal eigenvectors&lt;/li>
&lt;li>&lt;strong>Skew symmetric&lt;/strong>: $A^*=-A$
&lt;blockquote>
&lt;p>For quadratic form $x^\top Ax=x^\top(\frac{A+A^\top}{2}+\frac{A-A^\top}{2})x$, since $A-A^\top$ is skew symmetric, scalar $x^\top (A-A^\top) x=-x^\top (A-A^\top)x$, so the skew-symmetric part is zero. Therefore for quadratic form $x^\top Ax$ we can always assume $A$ is symmetric.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Definiteness&lt;/strong>: (for symmetric matrix $P$)&lt;/p>
&lt;ul>
&lt;li>Positive definite ($P\succ 0$): $\forall x\in\mathbb{R}^n\neq 0,\; x^\top Px&amp;gt;0 \Leftrightarrow$ all eigenvalues of $P$ are positive.&lt;/li>
&lt;li>Semi-positive definite ($P\succcurlyeq 0$): $x^\top Px\geqslant 0 \Leftrightarrow$ all eigenvalues of $P$ are non-negative.&lt;/li>
&lt;li>Negative definite ($P\prec 0$): $x^\top Px &amp;lt; 0 \Leftrightarrow$ all eigenvalues of $P$ are negative.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Orthogonal Matrix&lt;/strong>: $Q$ is orthogonal iff $Q^\top Q=I$, iff columns of $Q$ are orthonormal.&lt;/p>
&lt;ul>
&lt;li>If $A\in\mathbb{R}^{n\times b}$ is symmetric, then $\exists$ orthogonal $Q$ s.t. $Q^\top AQ=\Lambda=\mathrm{diag}\{\lambda_1,\ldots,\lambda_n\}$ (see &lt;a class="link" href="#Eigendecomposition-and-Jordan-Form" >Eigen-decomposition&lt;/a> section below)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Orthogonal Projection&lt;/strong>: Given linear space $\mathcal{X}$ and subspace $\mathcal{M}$, $P(x)=x^*:\mathcal{X}\to\mathcal{M}$ ($x^ *$ is the projection point) is called orthogonal projection. If $\{v_i\}$ is a orthonormal basis of $\mathcal{M}$, then $P(x)=\sum_i \langle x,v_i\rangle v_i$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="eigenvalue-and-canonical-forms">Eigenvalue and Canonical Forms&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Eigenvalue&lt;/strong> and &lt;strong>Eigenvector&lt;/strong>: Given mapping $\mathcal{A}:\mathcal{V}\rightarrow\mathcal{V}$, if $\exists \lambda\in\mathcal{F}, v\neq \emptyset_{\mathcal{V}}\in\mathcal{V}$ s.t. $\mathcal{A}(v) = \lambda v$, then $\lambda$ is the &lt;strong>eigenvalue&lt;/strong>, $v$ is the &lt;strong>eigenvector&lt;/strong> (aka. &lt;strong>spectrum&lt;/strong>).
&lt;ul>
&lt;li>If eigenvalues are all distinct, then the associated eigenvectors form a basis.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Eigenspace&lt;/strong>: $\mathcal{N}_\lambda = \mathcal{N}(\mathcal{A}-\lambda \mathcal{I})$.
&lt;ul>
&lt;li>$q=dim(\mathcal{N}_\lambda)$ is called the &lt;strong>geometric multiplicity&lt;/strong> (几何重度)&lt;/li>
&lt;li>$\mathcal{N}_\lambda$ is an $\mathcal{A}$-invariant subspace.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Characteristic Polynomial&lt;/strong>: $\phi(s)\equiv\mathcal{det}(A-s I)$ is a polynomial of degree $n$ in $s$
&lt;ul>
&lt;li>Its solutions are the eigenvalues of $A$.&lt;/li>
&lt;li>The multiplicity $m_i$ of root term $(s-\lambda_i)$ here is called &lt;strong>algebraic multiplicity&lt;/strong> (代数重度) of $\lambda_i$.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Cayley-Hamilton Theorem&lt;/strong>: $\phi(A)=\mathbf{0}$
&lt;blockquote>
&lt;p>Proof needs the eigendecomposition or Jordan decomposition descibed below&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Minimal Polynomial&lt;/strong>: $\psi(s)$ is the minimal polynomial of $A$ iff $\psi(s)$ is the polynomial of least degree for which $\psi(A)=0$ and $\psi$ is monic (coefficient of highest order term is 1)
&lt;ul>
&lt;li>The multiplicity $\eta_i$ of root term $(s-\lambda_i)$ here is called the &lt;strong>index&lt;/strong> of $\lambda_i$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Eigendecomposition&lt;/strong> (aka. &lt;strong>Spectral Decomposition&lt;/strong>) is directly derived from the definition of eigenvalues: $$A=Q\Lambda Q^{-1}, \Lambda=\mathrm{diag}\left\{\lambda_1,\lambda_2,\ldots,\lambda_n\right\}$$
where $Q$ is a square matrix whose $i$-th column is the eigenvector $q_i$ corresponding to eigenvalue $\lambda_i$.
&lt;ul>
&lt;li>Feasibility: $A$ can be diagonalized (using eigendecomposition) iff. $q_i=m_i$ for all $\lambda_i$.&lt;/li>
&lt;li>If $A$ has $n$ distinct eigenvalues, then $A$ can be diagonalized.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Generalized eigenvector&lt;/strong>: A vector $v$ is a generalized eigenvector of rank $k$ associated with eigenvalue $\lambda$ iff $v\in\mathcal{N}\left((A-\lambda I)^k\right)$ but $v\notin\mathcal{N}\left((A-\lambda I)^{k-1}\right)$
&lt;ul>
&lt;li>If $v$ is a generalized eigenvector of rank $k$, $(A-\lambda I)v$ is a generalized eigenvector of rank $k-1$. This creates a chain of generalized eigenvectors (called &lt;strong>Jordan Chain&lt;/strong>) from rank $k$ to $1$, and they are linearly independent.&lt;/li>
&lt;li>$\eta$ (index, 幂零指数) of $\lambda$ is the smallest integer s.t. $dim\left(\mathcal{N}\left((A-\lambda I)^\eta\right)\right)$&lt;/li>
&lt;li>The space spanned by the chain of generalized eigenvectors from rank $\eta$ is called the &lt;strong>generalized eigenspace&lt;/strong> (with dimension $\eta$).&lt;/li>
&lt;li>Different generalized eigenspaces associated with the same and with different eigenvalues are orthogonal.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Jordan Decomposition&lt;/strong>: Similar to eigendecomposition, but works for all square matrices. $A=PJP^{-1}$ where $J=\mathrm{diag}\{J_1,J_2,\ldots,J_p\}$ is the &lt;strong>Jordan Form&lt;/strong> of A consisting of Jordan Blocks.
&lt;ul>
&lt;li>&lt;strong>Jordan Block&lt;/strong>: $J_i=\begin{bmatrix} \lambda &amp;amp; 1 &amp;amp;&amp;amp;&amp;amp; \\&amp;amp;\lambda&amp;amp;1&amp;amp;&amp;amp;\\&amp;amp;&amp;amp;\lambda&amp;amp;\ddots&amp;amp;\\&amp;amp;&amp;amp;&amp;amp;\ddots&amp;amp;1\\&amp;amp;&amp;amp;&amp;amp;&amp;amp;\lambda\end{bmatrix}$&lt;/li>
&lt;li>Each Jordan block corresponds to a generalized eigenspace&lt;/li>
&lt;li>$q_i$ = the count of Jordan blocks associated with $\lambda_i$&lt;/li>
&lt;li>$m_i$ = the count of $\lambda_i$ on diagonal of $J$&lt;/li>
&lt;li>$\eta_i$ = the dimension of the largest Jordan block associated with $\lambda_i$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>$\Lambda$ in eigendecomposition, $J$ in Jordan Form and $\Sigma$ in SVD (see below) are three kinds of &lt;strong>&lt;a class="link" href="https://en.wikipedia.org/wiki/Canonical_form#Linear_algebra" target="_blank" rel="noopener"
>Canonical Forms&lt;/a>&lt;/strong> of a matrix $A$&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Function of matrics&lt;/strong>: Let $f(\cdot)$ be an analytic function and $\lambda_i$ be an eigenvalue of $A$. If $p(\cdot)$ is a polynomial that satisfies $p(\lambda_i)=f(\lambda_i)$ and $\frac{\mathrm{d}^k}{\mathrm{d}s^k} p(\lambda_i)=\frac{\mathrm{d}^k}{\mathrm{d}s^k} f(\lambda_i)$ for $k=1,\ldots,\eta_i-1$, then $f(A)\equiv p(A)$.
&lt;blockquote>
&lt;ul>
&lt;li>This extends the functions applicable to matrics from polynomials (trivial) to any analytical functions&lt;/li>
&lt;li>By Cayley-Hamilton, we can always choose $p$ to be order $n-1$&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Sylvester&amp;rsquo;s Formula&lt;/strong>: $f(A)=\sum^k_{i=1}f(\lambda_i)A_i$ ($f$ being analytic)&lt;/li>
&lt;/ul>
&lt;h3 id="svd-and-linear-equations">SVD and Linear Equations&lt;/h3>
&lt;p>SVD Decomposition is useful in various fields and teached by a lot of courses, its complete version is formulated as $$A=U\Sigma V^*, \Sigma=\begin{bmatrix}\mathbf{\sigma}&amp;amp;\mathbf{0}\\ \mathbf{0}&amp;amp;\mathbf{0}\end{bmatrix}, \mathbf{\sigma}=\mathrm{diag}\left\{\sqrt{\lambda_1},\sqrt{\lambda_2},\ldots,\sqrt{\lambda_r}\right\},V=\begin{bmatrix}V_1&amp;amp;V_2\end{bmatrix},U=\begin{bmatrix}U_1&amp;amp;U_2\end{bmatrix}$$
where&lt;/p>
&lt;ul>
&lt;li>$r=\rho(A)$ is the rank of matrix $A$&lt;/li>
&lt;li>$\sigma_i$ are called &lt;strong>sigular values&lt;/strong>, $\lambda_i$ are eigenvalues of $A^* A$&lt;/li>
&lt;li>Columns of $V_1$ span $\mathcal{R}(A^ *A)=\mathcal{R}(A^ *)$, columns of $V_2$ span $\mathcal{N}(A^ *A)=\mathcal{N}(A)$&lt;/li>
&lt;li>Columns of $U_1=AV_1\sigma^{-1}$ span $\mathcal{R}(A)$, columns of $U_2$ span $\mathcal{N}(A^*)$&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>SVD can be derived by doing eigenvalue decomposition on $A^* A$&lt;/p>
&lt;/blockquote>
&lt;p>With SVD introduced, we can efficiently solve general linear equation $Ax=b$ as $x=x_r+x_n$ where $x_r\in\mathcal{R}(A^\top)$ and $x_n\in\mathcal{N}(A)$.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>$Ax=b$&lt;/th>
&lt;th>tall $A$ ($m&amp;gt;n$)&lt;/th>
&lt;th>fat $A$ ($m&amp;lt; n$)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Overdetermined, &lt;br> Least Squares, &lt;br> use Normal Equations&lt;/td>
&lt;td>Underdetermined, &lt;br> Quadratic Programming, &lt;br> use Lagrange Multiplies&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>I.$b\in\mathcal{R}(A)$&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1.$\mathcal{N}(A)={0}$&lt;/td>
&lt;td>$x$ exist &amp;amp; is unique&lt;/td>
&lt;td>$x=(A^\top A)^{-1}A^\top b=A^+b$&lt;/td>
&lt;td>$x=A^\top(AA^\top)^{-1}b=A^+b$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2.$\mathcal{N}(A)\neq{0}$&lt;/td>
&lt;td>$x$ exist &amp;amp; not unique&lt;/td>
&lt;td>$x_r=(A^\top A)^{-1}A^\top b=A^+b$&lt;/td>
&lt;td>$x_r=A^\top(AA^\top)^{-1}b=A^+b$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>II.$b\notin\mathcal{R}(A)$&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1.$\mathcal{N}(A)={0}$&lt;/td>
&lt;td>$x$ not exists, $x_r$ exist &amp;amp; is unique&lt;/td>
&lt;td>$x_r=(A^\top A)^{-1}A^\top b=A^+b$&lt;/td>
&lt;td>$x_r=A^\top(AA^\top)^{-1}b=A^+b$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2.$\mathcal{N}(A)\neq{0}$&lt;/td>
&lt;td>$x$ not exists, $x_r$ not exist&lt;/td>
&lt;td>$(A^\top A)^{-1}$ invertible&lt;/td>
&lt;td>$(AA^\top)^{-1}$ invertible&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>$A^+=(A^\top A)^{-1}A^\top$ is left pseudo-inverse, $A^+=A^\top (AA^\top)^{-1}$ is right pseudo-inverse.&lt;/li>
&lt;li>$A^+$ can be unified by the name &lt;strong>Moore-Penrose Inverse&lt;/strong> and calculated using SVD by $A^+=V\Sigma^+ U^\top$ where $\Sigma^+$ take inverse of non-zeros.&lt;/li>
&lt;/ul>
&lt;h3 id="miscellaneous">Miscellaneous&lt;/h3>
&lt;blockquote>
&lt;p>Selected theorems and lemmas useful in Linear Algebra. For more matrix properties see &lt;a class="link" href="https://zyxin.xyz/blog/en/2019-06/MatrixAlgebra/" >my post about Matrix Algebra&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>Matrix Square Root: $N^\top N=P$, then $N$ is the square root of $P$
&lt;blockquote>
&lt;p>Square root is not unique. Cholesky decomposition is often used as square root.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Schur Complement&lt;/strong>: Given matrices $A_{n\times n}, B_{n\times m}, C_{m\times m}$, the matrix $M=\begin{bmatrix}A&amp;amp;B\\ B^\top&amp;amp;C\end{bmatrix}$ is symmetric. Then the following are equivalent (TFAE)
&lt;ol>
&lt;li>$M\succ 0$&lt;/li>
&lt;li>$A\succ 0$ and $C-B^\top A^{-1}B\succ 0$ (LHS called Schur complement of $A$ in $M$)&lt;/li>
&lt;li>$C\succ 0$ and $A-B C^{-1}B^\top\succ 0$ (LHS called Schur complement of $C$ in $M$)&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Matrix Inverse Lemma: $(A+BCD)^{-1}=A^{-1}-A^{-1}B\left(C^{-1}+DA^{-1}B\right)^{-1}DA$&lt;/li>
&lt;li>Properties of $A^\top A$
&lt;ul>
&lt;li>$A^\top A \succeq 0$ and $A^\top A \succ 0 \Leftrightarrow A$ has full rank.&lt;/li>
&lt;li>$A^\top A$ and $AA^\top$ have same non-zero eigenvalues, but different eigenvectors.&lt;/li>
&lt;li>If $v$ is eigenvector of $A^\top A$ about $\lambda$, then $Av$ is eigenvector of $AA^\top$ about $\lambda$.&lt;/li>
&lt;li>If $v$ is eigenvector of $AA^\top$ about $\lambda$, then $A^\top v$ is eigenvector of $A^\top A$ about $\lambda$.&lt;/li>
&lt;li>$tr(A^\top A)=tr(AA^\top)=\sum_i\sum_j\left|A_{ij}\right|^2$&lt;/li>
&lt;li>$det(A)=\prod_i\lambda_i, tr(A)=\sum_i\lambda_i$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="real-analysis">Real Analysis&lt;/h2>
&lt;h3 id="set-theory">Set theory&lt;/h3>
&lt;blockquote>
&lt;p>$\text{~}S$ stands for complement of set $S$ in following contents. These concepts are discussed under normed space $(\mathcal{X}, \Vert\cdot\Vert)$&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Open Ball&lt;/strong>: Let $x_0\in\mathcal{X}$ and let $a\in\mathbb{R}, a&amp;gt;0$, then the open ball of radius $a$ about $x_0$ is $B_a(x_0)=\left\{x\in\mathcal{X}\middle| \Vert x-x_0\Vert &amp;lt; a\right\}$
&lt;ul>
&lt;li>Given subset $S\subset \mathcal{X}$, $d(x,S)=0\Leftrightarrow \forall\epsilon &amp;gt;0, B_\epsilon(x)\cap S\neq\emptyset$&lt;/li>
&lt;li>Given subset $S\subset \mathcal{X}$, $d(x,S)&amp;gt;0\Leftrightarrow \exists\epsilon &amp;gt;0, B_\epsilon(x)\cap S=\emptyset$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Interior Point&lt;/strong>: Given subset $S\subset\mathcal{X}$, $x\in S$ is an interior point of $S$ iff $\exists\epsilon &amp;gt;0, B_\epsilon(x)\subset S$
&lt;ul>
&lt;li>&lt;strong>Interior&lt;/strong>: $\mathring{S}=\{x\in \mathcal{X}|x\text{ is an interior point of }S\}=\{x\in\mathcal{X}|d(x,\text{~}S)&amp;gt;0\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Open Set&lt;/strong>: $S$ is open if $\mathring{S}=S$&lt;/li>
&lt;li>&lt;strong>Closure Point&lt;/strong>: Given subset $S\subset\mathcal{X}$, $x\in S$ is a closure point of $S$ iff $\forall\epsilon &amp;gt;0, B_\epsilon(x)\cap S\neq\emptyset$.
&lt;ul>
&lt;li>&lt;strong>Closure&lt;/strong>: $\bar{S}=\{x\in\mathcal{X}|x\text{ is a closure point of }S\}=\{x\in\mathcal{X}|d(x,S)=0\}$
&lt;blockquote>
&lt;p>Note that $\partial\mathcal{X}=\emptyset$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Closed Set&lt;/strong>: $S$ is closed if $\bar{S}=S$
&lt;blockquote>
&lt;p>$S$ is open $\Leftrightarrow$ $\text{~}S$ is closed, $S$ is closed $\Leftrightarrow$ $\text{~}S$ is open. Set being both open and closed is called &lt;strong>clopen&lt;/strong>(e.g. the whole set $\mathcal{X}$), empty set is clopen by convention.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Set Boundary&lt;/strong>: $\partial S=\bar{S}\cap\overline{\text{~}S}=\bar{S}\backslash\mathring{S}$&lt;/li>
&lt;/ul>
&lt;h3 id="sequences">Sequences&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Sequence&lt;/strong>($\{x_n\}$): a set of vectors indexed by the counting numbers
&lt;ul>
&lt;li>&lt;strong>Subsequence&lt;/strong>: Let $1\leqslant n_1&amp;lt; n_2&amp;lt;\ldots$ be an infinite set of increasing integers, then $\{x_{n_i}\}$ is a subsequence of $\{x_n\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Convergence&lt;/strong>($\{x_n\}\to x\in\mathcal{X}$): $\forall \epsilon&amp;gt;0,\exists N(\epsilon)&amp;lt;\infty\text{ s.t. }\forall n\geqslant N, \Vert x_n-x\Vert &amp;lt;\epsilon$
&lt;ul>
&lt;li>If $x_n \to x$ and $x_n \to y$, then $x=y$&lt;/li>
&lt;li>If $x_n \to x_0$ and $\{x_{n_i}\}$ is a subsequence of $\{x_n\}$, then $\{x_{n_i}\} \to x_0$&lt;/li>
&lt;li>&lt;strong>Cauchy Convergence&lt;/strong> (necessary condition for convergence): $\{x_n\}$ is cauchy if $\forall \epsilon&amp;gt;0,\exists N(\epsilon)&amp;lt;\infty$ s.t. $\forall n,m\geqslant N, \Vert x_n-x_m\Vert &amp;lt;\epsilon$&lt;/li>
&lt;li>If $\mathcal{X}$ is finite dimensional, $\{x_n\}$ is cauchy $\Rightarrow$ $\{x_n\}$ has a limit in $\mathcal{X}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Limit Point&lt;/strong>: Given subset $S\subset\mathcal{X}$, $x$ is a limit point of $S$ if $\exists \{x_n\}$ s.t. $\forall n\geqslant 1, x_n\in S$ and $x_n\to x$
&lt;ul>
&lt;li>$x$ is a limit point of $S$ iff $x\in\bar{S}$&lt;/li>
&lt;li>$S$ is closed iff $S$ contains its limit points&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Complete Space&lt;/strong>: a normed space is &lt;strong>complete&lt;/strong> if every Cauchy sequence has a limit. A complete normed space $(\mathcal{X}, \Vert\cdot\Vert)$ is called a &lt;strong>Banach space&lt;/strong>.
&lt;ul>
&lt;li>$S\subset \mathcal{X}$ is complete if every Cauchy sequence with elements from $S$ has a limit in $S$&lt;/li>
&lt;li>$S\subset \mathcal{X}$ is complete $\Rightarrow S$ is closed&lt;/li>
&lt;li>$\mathcal{X}$ is complete and $S\subset\mathcal{X} \Rightarrow S$ is complete&lt;/li>
&lt;li>All finite dimensional subspaces of $X$ are complete&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Completion of Normed Space&lt;/strong>: $\mathcal{Y}=\bar{\mathcal{X}}=\mathcal{X}+\{$all limit points of Cauchy sequences in $\mathcal{X}\}$
&lt;blockquote>
&lt;p>E.g. $C[a,b]$ contains continuous functions over $[a,b]$. $(C[a,b], \Vert\cdot\Vert_1)$ is not complete, $(C[a,b], \Vert\cdot\Vert_\infty)$ is complete. Completion of $(C[a,b], \Vert\cdot\Vert_1)$ requires Lebesque integration.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Contraction Mapping&lt;/strong>: Let $S\subset\mathcal{X}$ be a subset and $T:S\to S$ is a contraction mapping if $\exists 0\leqslant c\leqslant 1$ such that, $\forall x,y \in S, \Vert T(x)-T(y)\Vert\leqslant c\Vert x-y\Vert$
&lt;ul>
&lt;li>&lt;strong>Fixed Point&lt;/strong>: $x^* \in\mathcal{X}$ is a fixed point of $T$ if $T(x^ *)=x^ *$&lt;/li>
&lt;li>&lt;a class="link" href="https://en.wikipedia.org/wiki/Banach_fixed-point_theorem" target="_blank" rel="noopener"
>&lt;strong>Contraction Mapping Theorem&lt;/strong> (不动点定理)&lt;/a>: If $T:S\to S$ is a contraction mapping in a complete subset $S$, then $\exists! x^ *\in\mathcal{X}\text{ s.t. }T(x^ *)=x^ *$. Moreover, $\forall x_0\in S$, the sequence $x_{k+1}=T(x_k),k\geqslant 0$ is Cauchy and converges to $x^ *$.
&lt;blockquote>
&lt;p>E.g. Newton Method: $x_{k+1}=x_k-\epsilon\left[\frac{\partial h}{\partial x}(x_k)\right]^{-1}\left(h(x_k)-y\right)$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="continuity-and-compactness">Continuity and Compactness&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Continuous&lt;/strong>: Let $(\mathcal{X},\Vert\cdot\Vert_\mathcal{X})$ and $(\mathcal{Y},\Vert\cdot\Vert_\mathcal{Y})$ be two normed spaces. A function $f:\mathcal{X}\to\mathcal{Y}$ is continuous at $x_0\in\mathcal{X}$ if $\forall\epsilon &amp;gt;0,\exists \delta(\epsilon,x_0)&amp;gt;0\text{ s.t. }\Vert x-x_0\Vert_\mathcal{X}&amp;lt;\delta \Rightarrow\Vert f(x)-f(x_0)\Vert_\mathcal{Y} &amp;lt;\epsilon$
&lt;ul>
&lt;li>$f$ is continuous on $S\subset\mathcal{X}$ if $f$ is continuous at $\forall x_0\in S$&lt;/li>
&lt;li>If $f$ in continuous at $x_0$ and $\{x_n\}$ is a sequence s.t. $x_n\to x_0$, then the sequence $\{f(x_n)\}$ in $\mathcal{Y}$ converges to $f(x_0)$&lt;/li>
&lt;li>If $f$ is discontinuous at $x_0$, then $\exists \{x_n\}\in\mathcal{X}$ s.t. $x_n\to x_0$ but $f(x_n)\nrightarrow f(x_0)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Compact&lt;/strong>: $S\subset\mathcal{X}$ is (sequentially) compact if every sequence in $S$ has a convergent subsequence with limit in $S$&lt;/li>
&lt;li>&lt;strong>Bounded&lt;/strong>: $S\subset\mathcal{S}$ is bounded if $\exists r&amp;lt;\infty$ such that $S\subset B_r(0)$
&lt;ul>
&lt;li>$S$ is compact $\Rightarrow$ $S$ is closed and bounded&lt;/li>
&lt;li>&lt;strong>Bolzano-Weierstrass Theorem&lt;/strong>: In a finite-dimensional normed space, $C$ is closed and bounded $\Leftrightarrow$ for $C$ is compact&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Weierstrass Theorem&lt;/strong>: If $C\subset\mathcal{X}$ is a compact subset and $f:C\to\mathbb{R}$ is continuous at each point of $C$, then $f$ achieves its extreme values, i.e. $\exists \bar{x}\in C\text{ s.t. }f(\bar{x})=\sup_{x\in C} f(x)$ and $\exists \underline{x}\in C\text{ s.t. }f(\underline{x})=\inf_{x\in C} f(x)$
&lt;ul>
&lt;li>$f:C\to\mathbb{R}$ continuous and $C$ compact $\Rightarrow$ $\sup_{x\in C}f(x)&amp;lt;\infty$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Notes for Control System</title><link>https://zyxin.xyz/blog/en/2020-06/ControlSystemNotes/</link><pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate><guid>https://zyxin.xyz/blog/en/2020-06/ControlSystemNotes/</guid><description>&lt;!-- $\require{mathtools}$ load the required TeX package -->
&lt;blockquote>
&lt;ul>
&lt;li>In this note, $f\in\mathbb{F}^\mathbb{G}$ stands for a function with domain in $\mathbb{G}$ and co-domain in $\mathbb{F}$, i.e. $f:\mathbb{F}\to\mathbb{G}$, $H(x)$ generally stands for Heaviside function (step function)&lt;/li>
&lt;li>Please read &lt;a class="link" href="https://zyxin.xyz/blog/en/2020-06/AlgebraBasicsNotes/" >the Algebra Basics notes&lt;/a> first if you are not familiar with related concepts.&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h2 id="transforms">Transforms&lt;/h2>
&lt;h3 id="laplace-transform">Laplace Transform&lt;/h3>
&lt;ul>
&lt;li>Definition: $F(s)=\mathcal{L}\{f(t)\}(s)=\int^\infty_0 f(t)e^{-st}\mathrm{d}t$
&lt;blockquote>
&lt;p>Note that the transform is not well defined for all functions in $\mathbb{C}^\mathbb{R}$. And the transform is only valid for $s$ in a region of convergence, which is usually separated by 0.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>Laplace Transform is a linear map from $(\mathbb{C}^\mathbb{R}, \mathbb{C})$ to $(\mathbb{C}^\mathbb{C}, \mathbb{C})$ and it&amp;rsquo;s one-to-one.&lt;/li>
&lt;li>Properties: (see &lt;a class="link" href="https://en.wikipedia.org/wiki/Laplace_transform" target="_blank" rel="noopener"
>Wikipedia&lt;/a> or &lt;a class="link" href="https://lpsa.swarthmore.edu/LaplaceZTable/LaplacePropTable.html" target="_blank" rel="noopener"
>this page&lt;/a> for full list)
&lt;ul>
&lt;li>Derivative: $f&amp;rsquo;(t) \xleftrightarrow{\mathcal{L}} sF(s)-f(0^-)$&lt;/li>
&lt;li>Integration: $\int^t_0 f(\tau)d\tau \xleftrightarrow{\mathcal{L}} \frac{1}{s}F(s)$&lt;/li>
&lt;li>Delay: $f(t-a)H(t-a) \xleftrightarrow{\mathcal{L}} e^{-as}F(s)$&lt;/li>
&lt;li>Convolution: $\int^t_0 f(\tau)g(t-\tau)\mathrm{d}\tau \xleftrightarrow{\mathcal{L}} F(s)G(s)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Stationary Value: $\lim\limits_{t\to 0} f(t) = \lim\limits_{s\to \infty} sF(s), \lim\limits_{t\to \infty} f(t) = \lim\limits_{s\to 0} sF(s)$&lt;/li>
&lt;/ul>
&lt;h4 id="inverse-laplace-transform">Inverse Laplace Transform&lt;/h4>
&lt;blockquote>
&lt;p>Laplace transform is one-to-one, so we can apply inverse transform on functions in s-space&lt;/p>
&lt;/blockquote>
&lt;p>There are several ways to calculate Laplace transform, the first one is directly evaluating integration while the latter two are converting the function into certain formats that are convenient for table lookup:&lt;/p>
&lt;ol>
&lt;li>(Mellin&amp;rsquo;s) Inverse formula: $f(t)=\mathcal{L}^{-1}\{F(s)\}(t)=\frac{1}{2\pi j}\lim\limits_{T\to\infty} \int ^{\gamma+iT}_{\gamma-iT} e^{st}F(s)\mathrm{d}s$ where the integration is done along the vertical line $Re(s)=\gamma$ in the convex s-plane such that $\gamma$ is greater than the real part of all poles of $F(s)$.&lt;/li>
&lt;li>Power Series: $F(s) = \sum^\infty_{n=0} \frac{n!a_n}{s^{n+1}}\xleftrightarrow{\mathcal{L}} f(t) = \sum ^\infty_{n=0} a_n t^n $&lt;/li>
&lt;li>Partial Fractions: $F(s)=\frac{k_1}{s+a}+\frac{k_2}{s+b}+\ldots \xleftrightarrow{\mathcal{L}} f(t)=k_1 e^{-at} + k_2 e^{-bt} + \ldots$
&lt;ul>
&lt;li>To calculate partial fractions, one can use &lt;a class="link" href="http://tutorial.math.lamar.edu/Classes/Alg/DividingPolynomials.aspx" target="_blank" rel="noopener"
>Polynomial Division&lt;/a> or following lemma:&lt;/li>
&lt;li>Suppose $F(s)=\frac{N(s)}{D(s)}=\frac{N(s)}{\prod^n_{i=1} (s-p_i)^{r_i}}$ where $\mathrm{deg}(N(s)) &amp;lt; \mathrm{deg(D(s))}$ and each $p_i$ is a distinct root of $D(s)$ (i.e. pole) with multiplicity $r_i$, then $F(s)=\sum^n_{i=1}\sum^{r_i}_ {j=1} \frac{k_{ij}}{(s-p_i)j}$ where $k_{ij}=\frac{1}{(r_i-j)!}\left.\frac{\mathrm{d}^{r_i-j}}{\mathrm{d}s^{r_i-j}}(s-p_i)^{r_i}F(s)\right\vert_{s=p_i}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h3 id="z-transfrom">Z-Transfrom&lt;/h3>
&lt;ul>
&lt;li>Definition: $F(z)=\mathcal{Z}\{f(k)_ {k\in\mathbb{N}}\}(z)=\sum^\infty_{k=0} f(k)z^{-k}$&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Notice that $f$ is defined on natural numbers. In time domain, it&amp;rsquo;s usually corresponding to $f(kT)$. Z-transform is also only valid for $z$ in certain region (usually separated by 1)&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>Laplace Transform is a linear map from $(\mathbb{C}^\mathbb{N}, \mathbb{C})$ to $(\mathbb{C}^\mathbb{C}, \mathbb{C})$ and it&amp;rsquo;s one-to-one.&lt;/li>
&lt;li>Properties: (see &lt;a class="link" href="https://en.wikipedia.org/wiki/Z-transform" target="_blank" rel="noopener"
>Wikipedia&lt;/a> or &lt;a class="link" href="https://lpsa.swarthmore.edu/LaplaceZTable/LaplacePropTable.html" target="_blank" rel="noopener"
>this page&lt;/a> for full list)
&lt;ul>
&lt;li>Accumulation: $\sum^n_{k=-\infty} f(k) \xleftrightarrow{\mathcal{Z}} \frac{1}{1-z^{-1}}F(z)$&lt;/li>
&lt;li>Delay: $f(k-m) \xleftrightarrow{\mathcal{Z}} z^{-m}F(z)$&lt;/li>
&lt;li>Convolution: $\sum^k_{n=0}f_1(n)f_2(k-n) \xleftrightarrow{\mathcal{Z}} F_1(z)F_2(z)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Stationary Value: $\lim\limits_{t\to 0} f(t) = \lim\limits_{z\to \infty} F(z), \lim\limits_{t\to \infty} f(t) = \lim\limits_{z\to 1} (z-1)F(z)$&lt;/li>
&lt;/ul>
&lt;details>&lt;summary>Example: Z-Transform of PID controller&lt;/summary>
Assume the close-loop error input of the controller is $e(t)$, and $e(kT)$ after sampling. PID controller action in analog is
$$m(t)=K\left(e(t)+\frac{1}{T_i}\int^t_0e(t)\mathrm{d}t+T_d\frac{\mathrm{d}e(t)}{\mathrm{d}t}\right)$$
We can approximate by trapezoidal rule with two point difference:
$$m(kT)=K\left(e(kT)+\frac{T}{T_i}\sum^k_{h=1}\frac{e((h-1)T)+e(hT)}{2}+T_d\left(\frac{e(kT)-e((k-1)T)}{T}\right)\right)$$
Lets define $f(hT) = \frac{1}{2}\left(e((h-1)T)+e(hT)\right),\;f(0)=0$
Then $$\begin{split}\mathcal{Z}\left(\left\{\sum^k_{h=1}\frac{e((h-1)T)+e(hT)}{2}\right\}_k\right)(z)=\mathcal{Z}\left(\left\{\sum^k_{h=1}f(hT)\right\}_k\right)(z) \\ =\frac{1}{1-z^{-1}}(F(z)-F(0))=\frac{1}{1-z^{-1}}F(z)\end{split}$$
Notice that $$F(z)=\mathcal{Z}\left({f(hT)}_h\right)(z)=\frac{1+z^{-1}}{2}E(z)$$
so we can calculate the Z-transform of $m(kT)$
$$\begin{split} M(z)&amp;=K\left(1+\frac{T}{2T_i}\left(\frac{1+z^{-1}}{1-z^{-1}}\right)+\frac{T_d}{T}(1-z^{-1})\right)E(z)\\&amp;=K\left(1-\frac{T}{2T_i}+\frac{T}{T_i}\frac{1}{1-z^{-1}}+\frac{T_d}{T}(1-z^{-1})\right)E(z)\\&amp;=\left(K_p+K_i\left(\frac{1}{1-z^-1}\right)+K_d(1-z^{-1})\right)E(z) \end{split}$$
&lt;p>Here we have&lt;/p>
&lt;ul>
&lt;li>Proportional Gain $K_p=K-\frac{KT}{2T_i}$&lt;/li>
&lt;li>Integral Gain $K_I=\frac{KT}{T_i}$&lt;/li>
&lt;li>Derivative Gain $K_d=\frac{KT_d}{T}$&lt;/li>
&lt;/ul>
&lt;/details>
&lt;h4 id="inverse-z-transform">Inverse Z-Transform&lt;/h4>
&lt;ol>
&lt;li>Inverse formula: $f(k)=\mathcal{Z}^{-1}\{F(z)\}(k)=\frac{1}{2\pi j}\oint _\Gamma z^{k-1}F(z)\mathrm{d}z$ where the integration is done along any closed path $\Gamma$ that encloses all finite poles of $z^{k-1}X(z)$ in the z-plane.
&lt;ul>
&lt;li>According to residual theorem, we can write it as $f(k)=\sum_{p_i}Res(z^{k-1}f(z), pi)$ where $p_i$ are poles of $z^{k-1}f(k)$ and residual $Res(g(z),p)=\frac{1}{(m-1)!}\left.\frac{\mathrm{d}^{m-1}}{\mathrm{d}z^{m-1}}\left((z-p)^mg(z)\right)\right\vert_{z=p}$ with $m$ being the multiplicity of the pole $p$ in $g$.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Power Series: same as inverse laplace.&lt;/li>
&lt;li>Partial Fractions: same as inverse laplace.&lt;/li>
&lt;/ol>
&lt;h4 id="modified-z-transfrom">Modified Z-Transfrom&lt;/h4>
&lt;ul>
&lt;li>Definition: $F(z,m)=\mathcal{Z}_m(f,m)=\mathcal{Z}(\left\{f(kT-(1-m)T)\right\} _{k\in\mathbb{N}^+})(z)$&lt;/li>
&lt;li>We denote corresponding continuous form $\mathcal{L}(f(t-(1-m)T)\delta_ T(t))$ as $F^*(s,m)$&lt;/li>
&lt;li>Residual Theorem: $\mathcal{Z}_m(f,m)=z^{-1}\sum _{p_i} Res(\frac{F(s)e^{mTs}}{1-z^{-1}e^{Ts}}, p_i)$&lt;/li>
&lt;li>ModZ Transform is usually used when there&amp;rsquo;s delay in the system, use this transform to shift the signal with proper $m$ value.&lt;/li>
&lt;/ul>
&lt;h3 id="starred-transform">Starred Transform&lt;/h3>
&lt;ul>
&lt;li>Definition: $F^* (s)=\sum^\infty_{n=0}f(n*T)e^{-nTs}$&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Starred Transform is defined in continuous s-domain, but it only aggregates on discrete s values defined periodically by sampling time T, like Z-Transform. Starred Transform is usually exchangeable with Z-Transform with $z=e^{Ts}$.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>Sometimes we also see &lt;code>*&lt;/code> as an operator to sample a continuous signal. It converts a continuous signal to discrete delta functions. (See the &amp;ldquo;Sampler&amp;rdquo; section below)&lt;/li>
&lt;li>Calculation from Laplace Transform
&lt;ul>
&lt;li>$F^*(s)=\sum_{p_i\in\{poles\;of\;F(\lambda)\}} Res\left(F(\lambda)\frac{1}{1-e^{-T(s-\lambda)}}, p_i\right)$&lt;/li>
&lt;li>$F^*(s)=\frac{1}{T}\sum^\infty_{n=-\infty}F(s+jn\omega_s)+\frac{e(0)}{2}$ where $\omega_s=\frac{2\pi}{T}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Properties:
&lt;ul>
&lt;li>$F^*(s)$ is periodic in s plane with period $j\omega_s=\frac{2\pi j}{T}$&lt;/li>
&lt;li>If $F(s)$ has a pole at $s=s_0$, then $F^*(s)$ must have poles at $s=s_0+jn\omega_s$ for $m\in\mathbb{Z}$&lt;/li>
&lt;li>$A(s)=B(s)F^* (s) \Rightarrow A^* (s)=B^* (s)F^* (s)$, while usually $A(s)=B(s)F(s) \nRightarrow A^* (s)=B^* (s)F^* (s)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="fourier-transform">Fourier Transform&lt;/h3>
&lt;blockquote>
&lt;p>Fourier transform is basically to substitute $s=j\omega$ into Laplace transform. Additional properties are not discussed here.&lt;/p>
&lt;ul>
&lt;li>One important theorem (Shannon-Nyquist Sampling Theorem): Suppose $e:\mathbb{R}_+\to\mathbb{R}$ has a Fourier Transform with no frequency components greater than $f_0$, then $e$ is uniquely determined by the signal $e_s$ generated by ideally sampling $e$ with period $\frac{1}{2}f_0$.&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h2 id="state-space-representation">State Space Representation&lt;/h2>
&lt;h3 id="continuous-state-space-representation">Continuous State Space Representation&lt;/h3>
&lt;h4 id="definition">Definition&lt;/h4>
&lt;p>A continuous-time linear state-space system can be described by following two equations:
\begin{align}&amp;amp;\text{State equation}:\;&amp;amp;\dot{x}(t)&amp;amp;=A(t)x(t)+B(t)u(t),&amp;amp;\;x(t)\in\mathbb{R}^n,\;u(t)&amp;amp;\in\mathbb{R}^m \\&amp;amp;\text{Output equation}:\;&amp;amp;y(t)&amp;amp;=C(t)x(t)+D(t)u(t),&amp;amp;\;y(t)&amp;amp;\in\mathbb{R}^p\end{align}&lt;/p>
&lt;p>The input $u:[0,\infty)\to\mathbb{R}^m$, state $x:[0,\infty)\to\mathbb{R}^n$, and output $y:[0,\infty)\to\mathbb{R}^p$ are all &lt;em>signals&lt;/em>, i.e. functions of continuous time $t\in[0,\infty)$. The coefficients $A\in\mathbb{R}^{n\times n}$,$B\in\mathbb{R}^{n\times m}$,$C\in\mathbb{R}^{p\times n}$,$D\in\mathbb{R}^{p\times m}$&lt;/p>
&lt;p>This linear time-varying (LTV) system can be written compactly as
\begin{align*} \dot{x}&amp;amp;=A(t)x+B(t)u \\ y&amp;amp;=C(t)x+D(t)u\end{align*}
Similarly, linear time-invariant (LTI) system can be written as
\begin{align} \dot{x}&amp;amp;=Ax+Bu \\ y&amp;amp;=Cx+Du\end{align}&lt;/p>
&lt;p>For non-linear system, the equation will be written as&lt;/p>
&lt;table>&lt;tr>
&lt;th style="text-align:center">time-varying (NLTV) &lt;/th>
&lt;th style="text-align:center">time-invariant (NTLI) &lt;/th>
&lt;th style="text-align:center">time-invariant autonomous&lt;/th>&lt;/tr>&lt;tr>&lt;td>
&lt;p>\begin{align*}\dot{x}&amp;amp;=f(x,u,t)\\y&amp;amp;=g(x,u,t)\end{align*}&lt;/p>
&lt;/td>&lt;td>
&lt;p>\begin{align*}\dot{x}&amp;amp;=f(x,u)\\y&amp;amp;=g(x,u)\end{align*}&lt;/p>
&lt;/td>&lt;td>
&lt;p>\begin{align*}\dot{x}&amp;amp;=f(x)\\y&amp;amp;=g(x)\end{align*}&lt;/p>
&lt;/td>&lt;/tr>&lt;/table>
&lt;h4 id="solution">Solution&lt;/h4>
&lt;blockquote>
&lt;p>&lt;em>&lt;strong>Math prerequisites here:&lt;/strong>&lt;/em>&lt;/p>
&lt;ul>
&lt;li>For definition of function on matrix, see &lt;a href="{% post_path AlgebraBasicsNotes %}#Eigenvalue-and-Canonical-Forms">my notes for algebra basics&lt;/a>&lt;/li>
&lt;li>$e^A$ is matrix exponential, &lt;code>expm&lt;/code> in MATLAB
&lt;ol>
&lt;li>$\frac{\mathrm{d}}{\mathrm{d}t}e^{At}=Ae^{At}=e^{At}A$&lt;/li>
&lt;li>$e^{(A+B)t}\Leftrightarrow AB=BA$ &lt;strong>(be careful when commute matrices)&lt;/strong>&lt;/li>
&lt;li>$\mathcal{L}\{e^{At}\}=(sI-A)^{-1}$ (can be derived from property 1 and laplace derivative)&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>To calculate $e^A$
&lt;ol>
&lt;li>Eigenvalue decomposition&lt;/li>
&lt;li>Jordan form decomposition&lt;/li>
&lt;li>Directly evaluate infinite power series (converges quickly)&lt;/li>
&lt;li>Inverse Laplace transform&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>For more properties of the matrix function, see&lt;a class="link" href="https://zyxin.xyz/blog/en/2019-06/MatrixAlgebra/" >Matrix Algebra&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;ul>
&lt;li>
&lt;p>For homogeneous LTI system: $$\begin{align}x(t)=e^{A(t-t_0)}x_0\end{align}$$&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;&lt;em>homogeneous&lt;/em>&amp;rdquo; = zero-input, Eq.5 is also called &lt;strong>zero input response&lt;/strong> (ZIR).&lt;/li>
&lt;li>&amp;ldquo;&lt;em>homogeneous equation&lt;/em>&amp;rdquo; = 齐次方程&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>For LTI system:
$$\begin{align}x(t)=e^{A(t-t_0)}x(t_0)+\int^t_{t_0}e^{A(t-\tau)}Bu(\tau)d\tau\end{align}$$
This result requires $A$ to be time-invariant, $B,C,D$ can be time varying.&lt;/p>
&lt;ul>
&lt;li>The solution consists of two parts: ZIR and ZSR (&lt;strong>zero state response&lt;/strong>, $x(t_0)=0$), which are homogenenous solution (通解) and particular solution (特解) of the ODE.&lt;/li>
&lt;li>ZIR and ZSR are both linear mapping&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>For homogeneous LTV system: $$\begin{align}x(t)=\Phi(t,t_0)x_0\end{align}$$&lt;/p>
&lt;ul>
&lt;li>Matrix $\Phi$ is called the &lt;strong>state transition matrix&lt;/strong>, defined as $$\begin{equation}\begin{split}\Phi(t,t_0)\equiv I+\int^t_{t_0}A(s_1)\mathrm{d}s_1+\int^t_ {t_0}A(s_1)\int^{s_1}_ {t_0}A(s_2)\mathrm{d}s_2\mathrm{d}s_1+\\ \int^t_ {t_0}A(s_1)\int^{s_1}_ {t_0}A(s_2)\int^{s_2}_ {t_0}A(s_3)\mathrm{d}s_3\mathrm{d}s_2\mathrm{d}s_1+\cdots\end{split}\end{equation}$$&lt;/li>
&lt;li>Properties of $\Phi$:
&lt;ol>
&lt;li>$\Phi(t,t)=I$&lt;/li>
&lt;li>$\frac{\mathrm{d}}{\mathrm{d}t}\Phi(t,t_0)=A(t)\Phi(t,t_0)$&lt;/li>
&lt;li>(semigroup property) $\Phi(t,s)\Phi(s,\tau)=\Phi(t,\tau)$&lt;/li>
&lt;li>$\forall t,\tau\geqslant 0,\;[\Phi(t,\tau)]^{-1}=\Phi(\tau,t)$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Eq.6 can be directly derived by evaluating Eq.8&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>For LTV system:
$$\begin{align}x(t)=\Phi(t,t_0)x_0+\int^t_{t_0}\Phi(t,\tau)B(\tau)u(\tau)d\tau\end{align}$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Some conclusions:&lt;/p>
&lt;ul>
&lt;li>The solution given by Eq.9 is unique&lt;/li>
&lt;li>The set of all solutions to ZIR system forms a vector space of dimension $n$&lt;/li>
&lt;li>If $A(t)A(s)=A(s)A(t)$, then $\Phi(t,t_0)=e^{\int^t_{t_0}A(\tau)\mathrm{d}\tau}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Phase Portraits&lt;/strong>: A phase portrait is a graph of several zero-input responses on the phase plane ($\dot{x}(t)$ and $x(t)$ are phase variables)&lt;/p>
&lt;blockquote>
&lt;p>Usually in phase portraits, there are two straight lines corresponding to the eigenvector of A, other lines are growing in or opposite to the direction of the lines.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;h4 id="transfer-function">Transfer function&lt;/h4>
&lt;ul>
&lt;li>For LTI case, $\frac{Y(s)}{U(s)} = C(sI-A)^{-1}B+D$
&lt;blockquote>
&lt;p>This can be derived by take laplace transform of both sides of state equations&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;h3 id="discrete-state-space-representation">Discrete State Space Representation&lt;/h3>
&lt;h4 id="definition-1">Definition&lt;/h4>
&lt;p>A discrete-time linear state-space system can be described by following two equations:
$$\begin{align}&amp;amp;\text{State eq.}:\;&amp;amp;x(k+1)&amp;amp;=A(k)x(k)+B(k)u(k),&amp;amp;\;x\in\mathbb{R}^n,\;u&amp;amp;\in\mathbb{R}^m \\ &amp;amp;\text{Output eq.}:\;&amp;amp;y(k)&amp;amp;=C(k)x(k)+D(k)u(k),&amp;amp;\;y&amp;amp;\in\mathbb{R}^p\end{align}$$&lt;/p>
&lt;p>The input $u:\mathbb{N}\to\mathbb{R}^m$, state $x:\mathbb{N}\to\mathbb{R}^n$, and output $y:\mathbb{N}\to\mathbb{R}^p$ are all &lt;em>signals&lt;/em>, i.e. functions of continuous time $t\in\mathbb{N}$.&lt;/p>
&lt;p>Discrete LTI system is sometimes written compactly as $$\begin{align} x_{k+1}&amp;amp;=Ax_k+Bu_k \\ y_k&amp;amp;=Cx_k+Du_k \end{align}$$&lt;/p>
&lt;h4 id="transfer-function-1">Transfer function&lt;/h4>
&lt;ul>
&lt;li>For LTI case, $H(z)=C(zI-A)^{-1}B+D$ (pulse tranfer function)&lt;/li>
&lt;/ul>
&lt;h3 id="controllability--reachability">Controllability &amp;amp; Reachability&lt;/h3>
&lt;blockquote>
&lt;p>Note: hereafter $\mathfrak{R}$ denotes &lt;a href="{% post_path AlgebraBasicsNotes %}#Linear-Operator">range space&lt;/a>, $\mathfrak{N}$ denotes &lt;a href="{% post_path AlgebraBasicsNotes %}#Linear-Operator">null space&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Controllability&lt;/strong>: $\exists u$ that drives any initial state $x(t_0)=x_0$ to $x(t_1)=0$&lt;/li>
&lt;li>&lt;strong>Reachability&lt;/strong>: $\exists u$ that drives initial state $x(t_0)=0$ to any $x(t_1)=x_1$&lt;/li>
&lt;/ul>
&lt;p>Consider the continuous LTV system $\dot{x}=A(t)x+B(t)u,\;x\in\mathbb{R}^n,u\in\mathbb{R}^m$.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Reachable Subspace&lt;/strong>: Given $t_0$ &amp;amp; $t_1$, the reachable subspace $\mathcal{R}[t_0, t_1]$ consists of all states $x_1$ for which there exists and input $u:[t_0, t_1]\to\mathbb{R}^m$ that transfers the state from $x(t_0)=0$ to $x(t_1)=x_1$.&lt;/p>
&lt;ul>
&lt;li>$\mathcal{R}[t_0, t_1]\equiv\left\{x_1\in\mathbb{R}^n\middle|\exists u(\cdot),\;x_1=\int^{t_1}_{t_0}\Phi(t_1,\tau)B(\tau)u(\tau)\mathrm{d}\tau\right\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Controllable Subspace&lt;/strong>: Given $t_0$ &amp;amp; $t_1$, the controllable subspace $\mathcal{C}[t_0, t_1]$ consists of all states $x_0$ for which there exists an input $u:[t_0, t_1]\to\mathbb{R}^m$ that transfers the state from $x(t_0)=x_0$ to $x(t_1)=0$&lt;/p>
&lt;ul>
&lt;li>$\mathcal{C}[t_0, t_1]\equiv\left\{x_0\in\mathbb{R}^m\middle|\exists u(\cdot),\;0=\Phi(t_1,t_0)x_0+\int^{t_1}_{t_0}\Phi(t_1,\tau)B(\tau)u(\tau)\mathrm{d}\tau\right\}$&lt;/li>
&lt;li>or $\mathcal{C}[t_0, t_1]\equiv\left\{x_0\in\mathbb{R}^m\middle|\exists u(\cdot),\;x_0=\int^{t_1}_{t_0}\Phi(t_0,\tau)B(\tau)\left[-u(\tau)\right]\mathrm{d}\tau\right\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Reachability Grammian&lt;/strong>: $W_\mathcal{R}(t_0, t_1)\equiv\int^{t_1}_{t_0}\Phi(t_1,\tau)B(\tau)B(\tau)^\top\Phi^\top(t_1,\tau)\mathrm{d}\tau$ given times $t_1&amp;gt;t_0\geqslant0$&lt;/p>
&lt;ul>
&lt;li>The system is reachable at time $t_0$ iff $\exists t_1$ s.t. $W_\mathcal{R}(t_0,t_1)$ is non-singular.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>non-singular for some $t_1$ $\Rightarrow$ non-singular for any $t_1$&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>$\mathcal{R}[t_0,t_1]=\mathfrak{R}(W_\mathcal{R}(t_0,t_1))$&lt;/li>
&lt;li>if $x_1=W_\mathcal{R}(t_0,t_1)\eta_1\in\mathfrak{R}(W_\mathcal{R}(t_0,t_1))$, the control $u(t)=B^\top(t)\Phi^T(t_1,t)\eta_1$,$t\in[t_0,t_1]$ can be used to transfer the system from $x(t_0)=0$ to $x(t_1)=x_1$ (w/ minimum energy)&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>minimum energy = minimum $\int^T_0\Vert u(\tau)\Vert^2\mathrm{d}\tau$&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>For LTI system $W_\mathcal{R}(t_0,t_1)=\int^{t_1}_ {t_ 0}e^{A(t_1-\tau)}BB^\top e^{A^{\top} (t_1-\tau)}\mathrm{d}\tau=\int^{t_1-t_ 0}_ {0}e^{At}BB^\top e^{A^{\top}t}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;a class="link" href="https://en.wikipedia.org/wiki/Controllability_Gramian" target="_blank" rel="noopener"
>&lt;strong>Controllability Grammian&lt;/strong>&lt;/a>: $W_\mathcal{C}(t_0, t_1)\equiv\int^{t_1}_{t_0}\Phi(t_0,\tau)B(\tau)B(\tau)^\top\Phi^\top(t_0,\tau)\mathrm{d}\tau$ given times $t_1&amp;gt;t_0\geqslant0$&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The system is reachable at time $t_0$ iff $\exists t_1$ s.t. $W_\mathcal{C}(t_0,t_1)$ is non-singular.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$\mathcal{C}[t_0,t_1]=\mathfrak{R}(W_\mathcal{C}(t_0,t_1))$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>if $x_0=W_\mathcal{C}(t_0,t_1)\eta_0\in\mathfrak{R}(W_\mathcal{C}(t_0,t_1))$, control $u(t)=-B^\top(t)\Phi^\top(t_0,t)\eta_0$,$t\in[t_0,t_1]$ can be used to transfer the state from $x(t_0)=x_0$ to $x(t_1)=0$ (w/ minimum energy)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>For LTI system $W_\mathcal{C}(t_0,t_1)=\int^{t_1}_ {t_ 0}e^{A(t_0-\tau)}BB^\top e^{A^{\top} (t_0-\tau)}\mathrm{d}\tau=\int^{t_1-t_ 0}_ {0}e^{-At}BB^\top e^{-A^{\top}t}$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Controllability Matrix&lt;/strong>: For LTI system, controllability matrix $\mathcal{C}=[B\;|\;AB\;|\;A^2B\;\cdots\;A^{n-1}B]$&lt;/p>
&lt;blockquote>
&lt;p>The controllability matrix works for both continuous and discrete system, and it&amp;rsquo;s easier to be derived from discrete LTI equations:
In discrete LTI, $\mathcal{C}\mathbf{u}=-A^k x_0$ where $\mathbf{u}=\begin{bmatrix}u_{k-1} &amp;amp; u_{k-2} &amp;amp; \ldots &amp;amp; u_0\end{bmatrix}^\top$&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>For LTI, $\mathcal{R}[t_0,t_1]=\mathfrak{R}(W_\mathcal{R}[t_0,t_1])=\mathfrak{R}(\mathcal{C})=\mathfrak{R}(W_\mathcal{C}[t_0,t_1])=\mathcal{C}[t_0,t_1]$&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>This implies Controllability $\Leftrightarrow$ Reachability for LTI systems.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>The controllable subspace $\mathfrak{\mathcal{C}}$ is the smallest A-invariant subspace that contains $\mathfrak{\mathcal{B}}$&lt;/li>
&lt;li>If the controllability matrix has full rank, the LTI system (or the pair $(A,B)$) is &lt;strong>completely controllable&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>PBH-Eigenvector Test&lt;/strong>: An LTI system is not controllable iff there exists a nonzero &lt;em>left&lt;/em> eigenvector $v$ of $A$ such that $vB=0$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>PBH-Rank Test&lt;/strong>: An LTI system will be controllable iff $[\lambda I-A \;| \;B]$ has full row rank for all eigenvalue $\lambda$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>For LTI system, there exists an input $u(\cdot)$ that transfer the state from $x_0$ ito $x_1$ in finite time $T$ iff $x_1-e^{AT}x_0\in\mathfrak{R}(\mathcal{C})$&lt;/p>
&lt;ul>
&lt;li>The input that transfers any state $x_0$ to any other state $x_1$ in some finite time $T$ is $u(t)=B^\top e^{A^{\top}(T-t)}W_\mathcal{R}^{-1}(0,T)[x_1 -e^{AT}x_0]$, for $t\in[0,T]$ (w/ minimum energy)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="observability">Observability&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Observability&lt;/strong>: Given any input $u(t)$ and output $y(t)$ over $t\in[t_0,t_1]$, it&amp;rsquo;s sufficient to determine a unique initial state $\exists !x(t_0)$.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a class="link" href="https://en.wikipedia.org/wiki/Observability_Gramian" target="_blank" rel="noopener"
>&lt;strong>Observability Grammian&lt;/strong>&lt;/a>: $W_\mathcal{O}(t_0,t_1)\equiv\int^{t_1}_{t_0}\Phi^\top(t_1,\tau)C^\top(\tau)C(\tau)\Phi(t_1,\tau)\mathrm{d}\tau$&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The system is observable at time $t_0$ iff $\exists t_1$ s.t. $W_{\mathcal{O}}(0,t)$ is nonsingular.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>For LTI system $W_{\mathcal{O}}(t_0,t_1)=\int^{t_1}_{t_0} e^{A^{\top}(t_1-\tau)}C^\top Ce^{A(t_1-\tau)}\mathrm{d}\tau=\int^{t_1-t_0}_0 e^{A^{\top}\tau}C^\top Ce^{A\tau}\mathrm{d}\tau$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Observability Matrix&lt;/strong>: For LTI system, observability $\mathcal{O}=\begin{bmatrix}C\\CA\\CA^2\\ \vdots\\CA^{k-1}\end{bmatrix}$&lt;/p>
&lt;blockquote>
&lt;p>The controllability matrix works for both continuous and discrete system, and it&amp;rsquo;s easier to be derived from discrete LTI equations:
In discrete LTI, $\Psi_{k-1}=\mathcal{O}x_0$ where $$\Psi_k\equiv\begin{bmatrix}y_0\\y_1\\y_2\\ \vdots\\ y_{k-1}\end{bmatrix}-\begin{bmatrix} D &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ CB &amp;amp; D &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\ CAB &amp;amp; CB &amp;amp; D &amp;amp; \cdots &amp;amp; 0 \\ \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\ CA^{k-2}B &amp;amp; CA^{k-3}B &amp;amp; CA^{k-4}B &amp;amp; \cdots &amp;amp; 0 \end{bmatrix}\begin{bmatrix}u_0\\u_1\\u_2\\ \vdots \\ u_{k-1}\end{bmatrix}$$&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>If the controllability matrix has full rank, the LTI system (or the pair $(A,C)$) is &lt;strong>completely observable&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>PBH-Rank Test&lt;/strong>: An LTI system will be observable iff $\begin{bmatrix}A-\lambda I \\C\end{bmatrix}$ has full column rank for all eigenvalue $\lambda$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="duality">Duality&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Duality Theorem&lt;/strong>: The pair $(A,B)$ is controllable iff the pair $(A^\top, B^\top)$ is observable.
&lt;ul>
&lt;li>&lt;em>Controllability&lt;/em> only depends on matrix $A$ and $B$ while the &lt;em>Observability&lt;/em> only depends on matrix $A$ and $C$&lt;/li>
&lt;li>Duality theorem is useful for proof of observability conclusions from controllability&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Adjoint System&lt;/strong>:&lt;/li>
&lt;/ul>
&lt;table>
&lt;tr>&lt;th>&lt;/th>&lt;th> Original System &lt;/th>&lt;th> Adjoint System &lt;/th>&lt;/tr>
&lt;tr>&lt;td>Equations&lt;/td>&lt;td> $$\begin{align*} \dot{x}&amp;=A(t)x+B(t)u \\ y&amp;=C(t)x \end{align*}$$&lt;/td>&lt;td>$$\begin{align*} \dot{p}&amp;=-A^*(t)p-C^*(t)v \\ z&amp;=B^*(t)p\end{align*}$$&lt;/td>&lt;/tr>
&lt;tr>&lt;td>Initial Condition&lt;/td>&lt;td>$x(t_0)=x_0$&lt;/td>&lt;td>$p(t_1)=p_1$&lt;/td>&lt;/tr>
&lt;tr>&lt;td>State Trasition Matrix&lt;/td>&lt;td> $\Phi(t,t_0)$ &lt;/td>&lt;td> $\Phi^*(t_1,t)=\left(\Phi^*(t,t_1)\right)^{-1}$&lt;/td>&lt;/tr>
&lt;tr>&lt;td>Zero-State Response&lt;/td>&lt;td> $$\begin{split}L_u:\;&amp;u(\cdot)\to x(t_1)\\=&amp;\int^{t_1}_{t_0}\Phi(t_1,\tau)B(\tau)u(\tau)\mathrm{d}\tau\end{split}$$&lt;/td>&lt;td>$$\begin{split}P_u:\;&amp;v(\cdot)\to p(t_0)\\=&amp;\int^{t_1}_{t_0}\Phi^*(\tau,t_0)C^*(\tau)v(\tau)\mathrm{d}\tau\end{split}$$&lt;/td>&lt;/tr>
&lt;tr>&lt;td>Zero-Input Response&lt;/td>&lt;td> $$\begin{split}L_0:\;&amp;x_0\to y(\cdot)\\=&amp;C(\cdot)\Phi(\cdot,t_0)x_0\end{split}$$ &lt;/td>&lt;td> $$\begin{split}P_0:\;&amp;p_1\to z(\cdot)\\=&amp;B^*(\cdot)\Phi^*(t_1,\cdot)p_1\end{split}$$ &lt;/td>&lt;/tr>
&lt;tr>&lt;td rowspan="3"> Duality Theorem &lt;/td>&lt;td> Controllable ($\rho(L_u)=n$) &lt;/td>&lt;td> Observable ($\rho(P_0^*)=n$)&lt;/td>&lt;/tr>
&lt;tr>&lt;td> Observable ($\rho(L_0^*)=n$) &lt;/td>&lt;td> Controllable ($\rho(P_u)=n$) &lt;/td>&lt;/tr>
&lt;tr>&lt;td> A state is reachable ($x\in\mathfrak{R}(L_u)$) &lt;/td>&lt;td> A state is unobservable ($x\in\mathfrak{N}(L_0)$) &lt;/td>&lt;/tr>
&lt;/table>
Note that ZIR and ZSR are both linear mappings and $L_u^*=P_0,\;L_0^*=P_u$
&lt;h3 id="decomposition-and-realizations">Decomposition and Realizations&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>Similarity Transform of a (LTI) system: Based on Eq.3 and Eq.4, define $x=P\bar{x}$, then we have $$\begin{align}\dot{\bar{x}}&amp;amp;=P^{-1}AP\bar{x}+P^{-1}Bu&amp;amp;=\bar{A}\bar{x}+\bar{B}u \\ y&amp;amp;=CP\bar{x}+Du&amp;amp;=\bar{C}\bar{x}+Du\end{align}$$&lt;/p>
&lt;ul>
&lt;li>Similarity transform doesn&amp;rsquo;t affect transfer function.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Controllability Decomposition&lt;/strong>: For an uncontrollable LTI system, define matrix $V=[V_1\;V_2]$ where $V_1$ is a basis for $\mathfrak{R}(\mathcal{C})$ and $V_2$ complete a basis for $\mathbb{R}^n$, then after similarity transform with $\bar{x}=V^{-1}x$, we can partition the system like following:
$$\begin{align*}\dot{\bar{x}}&amp;amp;=\bar{A}\bar{x}+\bar{B}u&amp;amp;=&amp;amp;\begin{bmatrix}\bar{A}_ {11}&amp;amp;\bar{A}_ {12} \\ \mathbf{0} &amp;amp; \bar{A} _{22}\end{bmatrix}\begin{bmatrix}\bar{x} _1 \\ \bar{x} _2\end{bmatrix}+\begin{bmatrix}\bar{B} _1 \\ \mathbf{0}\end{bmatrix}u \\ y&amp;amp;=\bar{C}\bar{x}+Du&amp;amp;=&amp;amp; \begin{bmatrix}\bar{C} _1 &amp;amp; \bar{C} _2\end{bmatrix}\begin{bmatrix}\bar{x} _1 \\ \bar{x} _2\end{bmatrix} + Du\end{align*}$$
Here $\bar{x}_2$ is uncontrollable, and ZSR of the system with or without $\bar{x}_2$ is the same.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Observability Decomposition&lt;/strong>: For an unobservable LTI system, define matrix $U=\begin{bmatrix}U_1\\U_2\end{bmatrix}$ where $U_1$ is a basis for $\mathfrak{R}(\mathcal{O}^\top)$ and $U_2$ complete a basis for $\mathbb{R}^n$, then after similarity transform with $\hat{x}=Ux$, we can partition the system like following:
$$\begin{align*}\dot{\hat{x}}&amp;amp;=\hat{A}\hat{x}+\hat{B}u&amp;amp;=&amp;amp;\begin{bmatrix}\hat{A}_ {11}&amp;amp;\mathbf{0} \\ \hat{A}_ {21} &amp;amp; \hat{A} _{22}\end{bmatrix}\begin{bmatrix}\hat{x} _1 \\ \hat{x} _2\end{bmatrix}+\begin{bmatrix}\hat{B} _1 \\ \hat{B} _2\end{bmatrix}u \\ y&amp;amp;=\hat{C}\hat{x}+Du&amp;amp;=&amp;amp; \begin{bmatrix}\hat{C} _1 &amp;amp; \mathbf{0}\end{bmatrix}\begin{bmatrix}\hat{x} _1 \\ \hat{x} _2\end{bmatrix} + Du\end{align*}$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Realization&lt;/strong>: $\Sigma$ (system with Eq.3 and Eq.4) is a realization of $H(s)$ iff $H(s)=C(sI-A)^{-1}B+D$.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Equivalent&lt;/strong>: Two realizations are said to be equivalent if they have the same transfer function&lt;/li>
&lt;li>&lt;strong>Algebraically Equivalent&lt;/strong>: Two realizations have same transfer function and $n$ (dimension of states). (this implies a similarity transform between them)&lt;/li>
&lt;li>&lt;strong>Minimal Realization&lt;/strong>: $\Sigma$ is a minimal realization of $H(s)$ iff there doesn&amp;rsquo;t exists an equivalent realization $\bar{\Sigma}$ with $\bar{n}&amp;lt; n$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>$\Sigma$ is a minial realization iff $\Sigma$ is completely controllable and observable.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Kalman Cannonical Structure Theorem&lt;/strong> (aka. Kalman Decomposition): suppose $\rho(\mathcal{C})&amp;lt; n$ and $\rho(\mathcal{O})&amp;lt; n$, $\mathfrak{R}(\mathcal{C})$ are the controllable states, $\mathfrak{N}(\mathcal{O})$ are the unobservable states, define subspaces:&lt;/p>
&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Subspaces&lt;/th>
&lt;th style="text-align:center">Controllable&lt;/th>
&lt;th style="text-align:center">Observable&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">$V_2\equiv\mathfrak{R}(\mathcal{C})\cup\mathfrak{N}(\mathcal{O})$&lt;/td>
&lt;td style="text-align:center">Yes&lt;/td>
&lt;td style="text-align:center">No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">$V_1$ s.t. $V_1\oplus V_2=\mathfrak{R}(\mathcal{C})$&lt;/td>
&lt;td style="text-align:center">Yes&lt;/td>
&lt;td style="text-align:center">Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">$V_4$ s.t. $V_4\oplus V_2=\mathfrak{N}(\mathcal{O})$&lt;/td>
&lt;td style="text-align:center">No&lt;/td>
&lt;td style="text-align:center">No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">$V_3$ s.t. $V_1\oplus V_2\oplus V_3\oplus V_4=\mathbb{C}^n$&lt;/td>
&lt;td style="text-align:center">No&lt;/td>
&lt;td style="text-align:center">Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">Then let $$\begin{align*}\tilde{x}=\begin{bmatrix}\tilde{x}_ 1\\ \tilde{x}_ 2\\ \tilde{x}_ 3\\ \tilde{x}_ 4\end{bmatrix},\;\tilde{A}=&amp;amp;\begin{bmatrix}A_ {\mathrm{co}} &amp;amp;&amp;amp;A_{13}&amp;amp;\\A_{21}&amp;amp;A_{\mathrm{c\bar{o}}}&amp;amp;A_{23}&amp;amp;A_{24}\\&amp;amp;&amp;amp;A_{\mathrm{\bar{c}o}}&amp;amp;\\&amp;amp;&amp;amp;A_{43}&amp;amp;A_{\mathrm{\bar{c}\bar{o}}} \end{bmatrix},\;\tilde{B}=\begin{bmatrix}B_{\mathrm{co}}\\ B_{\mathrm{c\bar{o}}} \\ \mathbf{0} \\ \mathbf{0} \end{bmatrix} \\ \tilde{C}=&amp;amp;\begin{bmatrix}C_{\mathrm{co}}&amp;amp;\mathbf{0}\quad&amp;amp;C_{\mathrm{\bar{c}o}}&amp;amp;\mathbf{0}\quad\end{bmatrix}\end{align*}$$&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">$$\tilde{\Sigma}:\begin{cases} \dot{\tilde{x}}=A_{\mathrm{co}}\tilde{x}_ 1+B_{\mathrm{co}}u_1\\ y=C_{\mathrm{co}}\tilde{x}_1\end{cases}$$&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>$\tilde{\Sigma}$ is completely controllable and completely observable.&lt;/p>
&lt;hr/>
Consider SISO systems $$H(s)=\frac{b(s)}{a(s)}=\frac{b_{n-1}s^{n-1}+\ldots+b_1s+b_0}{s^n+a_{n-1}s^{n-1}+\ldots+a_1s+a_0}=\frac{\sum^{n-1}_{j=0} b_js^j}{s^n+\sum^{n-1}_{i=0} a_is^i}=\frac{Y(s)}{U(s)}$$
&lt;ul>
&lt;li>&lt;strong>Controllable Cannonical Form&lt;/strong>: $$\begin{align}\dot{x}&amp;amp;=\begin{bmatrix} 0&amp;amp;1&amp;amp;&amp;amp;\\ \vdots &amp;amp; &amp;amp; \ddots &amp;amp; \\ 0&amp;amp;&amp;amp;&amp;amp;1 \\-a_0&amp;amp;-a_1&amp;amp;\cdots&amp;amp;-a_{n-1}\end{bmatrix}x+\begin{bmatrix}0\\ \vdots \\ 0 \\ 1\end{bmatrix}u&amp;amp;=&amp;amp;A_cx+B_cu\\ y&amp;amp;=\begin{bmatrix}\quad b_0 &amp;amp;\quad b_1 &amp;amp;\cdots &amp;amp; \quad b_{n-1}\end{bmatrix}x&amp;amp;=&amp;amp;C_cx \end{align}$$
&lt;ul>
&lt;li>$(A_c, B_c)$ is controllable&lt;/li>
&lt;li>$(A_c, C_c)$ is observable if $a(s)$ and $b(s)$ have no common factors&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Observable Cannonical Form&lt;/strong>: $$\begin{align}\dot{x}&amp;amp;=\begin{bmatrix} 0&amp;amp;&amp;amp;&amp;amp;&amp;amp;-a_0\\ 1 &amp;amp; \ddots &amp;amp;&amp;amp;&amp;amp;-a_1 &amp;amp; \\ &amp;amp;\ddots&amp;amp;\ddots&amp;amp;&amp;amp;\vdots \\&amp;amp;&amp;amp;\ddots&amp;amp;0&amp;amp;-a_{n-2} \\ &amp;amp;&amp;amp;&amp;amp;1&amp;amp;-a_{n-1}\end{bmatrix}x+\begin{bmatrix}b_0\\ b_1 \\ \vdots \\ b_{n-2} \\ b_{n-1}\end{bmatrix}u&amp;amp;=&amp;amp;A_ox+B_ou\\ y&amp;amp;=\begin{bmatrix}0 &amp;amp; \;\cdots &amp;amp;\;\cdots &amp;amp; 0 &amp;amp; \quad 1\qquad \end{bmatrix}x&amp;amp;=&amp;amp;C_ox \end{align}$$
&lt;ul>
&lt;li>$(A_o, C_o)$ is observable&lt;/li>
&lt;li>$(A_o, B_o)$ is controllable if $a(s)$ and $b(s)$ have no common factors&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Model Cannonical Forms&lt;/strong>: Do &lt;strong>Spectral Decomposition&lt;/strong> (eigen-decomposition) or Jordan Decomposition, and then use the modal matrix (matrix of eigenvectors) to do similarity transform.&lt;/li>
&lt;li>&lt;strong>The Gilbert Realization&lt;/strong>: Let $G(s)$ be a $p\times m$ rational transfer function with simple poles (nonrepeated) at $\lambda_i,\;i=1,2,\ldots,k$. Calculate partial fraction expansion $$G(s)=\sum^k_{i=1}\frac{R_i}{s-\lambda_i},\qquad \text{Residue}\;R_i=\lim_{s\to\lambda_i}(s-\lambda_i)G(s)$$ Let $r_i=\rho(R_i)$, now write $R_i=C_iB_i$ where $C_ i\in\mathbb{R}^ {p\times r_ i},\;B_ i\in\mathbb{R}^ {r_ i\times p}$, then write $$A=\mathrm{blkdiag}\{\lambda_i I_{r_i}\},\;B^\top=[B_1^\top \;\cdots\; B^\top_k],\;C=[C_1\; \cdots\;C_k]$$, then $(A,B,C)$ is a realization of $G(s)$ with order $n=\sum^k_1 r_i$&lt;/li>
&lt;/ul>
&lt;p>For MIMO system the cannonical forms with be quite complex:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Controllable Cannonical Form (for MIMO)&lt;/strong>: Here we provide a way to convert from controllable LTI system to controllable. The collection of independent columns of $\mathcal{C}$ may be expressed as $$M=[b_1\;Ab_1\; \cdots\;A^{\mu_1-1}b_1\;|\;b_2\;Ab_2\;\cdots\;A^{\mu_2-1}b_2\;|\;\cdots\;|\;b_p\;Ab_p\;\cdots\;A^{\mu_p-1}b_p]$$ Construct $M^{-1}$ and then $T$:$$M^{-1}=\left[m_{11}^\top\;m_{12}^\top\;\cdots\;m_{1\mu_1}^\top\;\middle|\;\cdots\;\middle|\;m_{p1}^\top\;m_{p2}^\top\;\cdots\;m_{p\mu_p}^\top \right]^\top$$ $$T=\left[m_{1\mu_1}^\top\;(m_{1\mu_1}A)^\top\;\cdots\;\left(m_{1\mu_1}A^{\mu_1-1}\right)^\top\;\middle|\;\cdots\;\middle|\; m_{p\mu_p}^\top\;(m_{p\mu_p}A)^\top\;\cdots\;\left(m_{p\mu_p}A^{\mu_p-1}\right)^\top\right]^\top$$
Perform similarity transform with $\bar{x}=Tx$ and the canonical form will be obtained like following:
$$\bar{A}=\begin{bmatrix}\bar{A}_ {\mu_1\times\mu_1}&amp;amp;\mathbf{0}_ {\cdot\cdot}&amp;amp;\cdots&amp;amp;\mathbf{0}_ {\cdot\cdot} \\ \mathbf{0}_ {\cdot\cdot}&amp;amp;\bar{A}_ {\mu_2\times\mu_2}&amp;amp;\cdots&amp;amp;\mathbf{0}_ {\cdot\cdot}\\ \vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots \\ \mathbf{0}_ {\cdot\cdot}&amp;amp;\mathbf{0}_ {\cdot\cdot}&amp;amp;\cdots&amp;amp;\bar{A}_ {\mu_ p\times\mu_ p} \end{bmatrix},\quad\bar{B}=\begin{bmatrix}\mathbf{0}_ {\cdot n}\\ \mathbf{0}_ {\cdot (n-1)}\\ \vdots\\ \mathbf{0}_ {\cdot 1}\end{bmatrix}$$
Here $\bar{A}_ {\mu_ i\times\mu_ i}$ is the same structure as in SISO, $\mathbf{0}_ {\cdot\cdot}$ is a zero matrix except the last row, $\mathbf{0}_ {\cdot i}$ is a zero matrix except for the last row, and in the last row there are $i$ non-zeros on the right with the first element being 1.&lt;/li>
&lt;/ul>
&lt;h2 id="system-performance">System Performance&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>System Characteristic Equation&lt;/strong>: The polynomical with the roots equal to the poles of the output that are independent of the input.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>System Type&lt;/strong>: A plant $G$ can always be written as $G(s)=\frac{K\prod^m_{i=1}(s-s_i)}{s^N\prod^p_{j=1}(s-s_j)},\;z_i,z_j\neq 0$ or $G(z)=\frac{K\prod^m_{i=1}(z-z_i)}{(z-1)^N\prod^p_{j=1}(z-z_j)},\;z_i,z_j\neq 1$. Here $N$ is called the system type of $G(z)$.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Properties that matters for a controller:&lt;/p>
&lt;ol>
&lt;li>Stability&lt;/li>
&lt;li>Steady state accuracy&lt;/li>
&lt;li>Transient response&lt;/li>
&lt;li>Sensitivity&lt;/li>
&lt;li>Exogenous disturbance rejection&lt;/li>
&lt;li>Bounded control effort&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;h3 id="stability">Stability&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Stability&lt;/strong> means when the time goes to infinity, the system response is bounded.
&lt;ul>
&lt;li>A system is &lt;strong>stable&lt;/strong> if all its poles lies in the left half of $s$-plane or all inside the unit circle of $z$-plane.&lt;/li>
&lt;li>A system is &lt;strong>marginally stable&lt;/strong> if one of the pole is on the imaginary axis of $s$-plane or on the unit circle of $z$-plane.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Stability of linear systems is independent of input
&lt;ul>
&lt;li>The stability of a linear system can be evaluated by its characteristic equation $1-G_{op}(z)=0$, where $G_{op}$ is the open-loop transfer function (transfer function when input is eliminated, or feedback route is cut off).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Methods to evaluate stability
&lt;ul>
&lt;li>&lt;strong>Routh-Hurwitz Criterion&lt;/strong>: $s$-plane (omited here, see &lt;a class="link" href="https://en.wikipedia.org/wiki/Routh%E2%80%93Hurwitz_stability_criterion" target="_blank" rel="noopener"
>Wikipedia&lt;/a>)
&lt;ul>
&lt;li>For discrete system, a strategy is use bilinear transformation: $z=e^{\omega T}\approx \frac{1+\omega T/2}{1-\omega T/2}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Jury Criterion&lt;/strong>: $z$-plane (see &lt;a class="link" href="https://en.wikipedia.org/wiki/Jury_stability_criterion" target="_blank" rel="noopener"
>Wikipedia&lt;/a>)&lt;/li>
&lt;li>&lt;strong>Root Locus Method&lt;/strong>: both $s$- and $z$-plane (see &lt;a class="link" href="https://en.wikipedia.org/wiki/Root_locus" target="_blank" rel="noopener"
>Wikipedia&lt;/a>, &lt;code>rlocus&lt;/code> in MATLAB)&lt;/li>
&lt;li>&lt;strong>Nyquist Criterion&lt;/strong>: both $s$- and $z$-plane (see &lt;a class="link" href="https://en.wikipedia.org/wiki/Nyquist_stability_criterion" target="_blank" rel="noopener"
>Wikipedia&lt;/a>, &lt;code>nyquist&lt;/code> in MATLAB)
&lt;ul>
&lt;li>It works for both continuous and discrete systems, the difference is that in $s$-plane the detour point is at $s=0$ while in $z$-plane the detour point is at $z=1$.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Bode Diagrams&lt;/strong>: draw frequency response for (pulse) transfer function, works for both $s$- and $z$-plane (see &lt;a class="link" href="https://en.wikipedia.org/wiki/Bode_plot" target="_blank" rel="noopener"
>Wikipedia&lt;/a>, &lt;code>bode&lt;/code> in MATLAB)&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>A review of the stability judgement method &lt;a class="link" href="https://www.zhihu.com/question/60272694" target="_blank" rel="noopener"
>here at 知乎&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;h3 id="lyaponov-stability">Lyaponov Stability&lt;/h3>
&lt;blockquote>
&lt;p>Lyaponove Stability is only concerned with the effect of initial conditions on the response of the system (ZIR)&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Equilibrium Point&lt;/strong> $x_e$: Consider NLTV $\dot{x}(t)=f(x(t),u(t),t)$, equilibrium point satisfies $x(t_0)=x_e,\;u(t)\equiv 0\Rightarrow x(t)=x_e,\;\text{i.e. }f(x_e,0,t)=0,\;\forall t&amp;gt;t_0$
&lt;ul>
&lt;li>For discrete system, it&amp;rsquo;s $x(k+1)=x(k)=x_e$&lt;/li>
&lt;li>For LTI system, $x_e$ can be calculated from $Ax_e=0$, so the origin $x=0$ is always an equilibrium point.&lt;/li>
&lt;li>Set of equilibrium points in LTI systems are connected.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Lyapunov stability&lt;/strong>: An equilibrium point $x_e$ of the system $\dot{x}=A(t)x$ is &lt;strong>stable (in the sense of Lyapunov)&lt;/strong> iff $\forall \epsilon&amp;gt;0,\;\exists \delta(t_0,\epsilon)&amp;gt;0$ s.t. $\Vert x(t_0)-x_e\Vert&amp;lt;\delta\Rightarrow\Vert x(t)-x_e\Vert &amp;lt;\epsilon,\;\forall t&amp;gt;t_0$
&lt;ul>
&lt;li>$x_e$ is &lt;strong>uniformly stable&lt;/strong> if $\delta=\delta(\epsilon)$ (regardless of $t_0$)&lt;/li>
&lt;li>$x_e$ &lt;em>in LTI&lt;/em> is stable $\Rightarrow x_e$ is uniformly stable&lt;/li>
&lt;li>$x_e$ is &lt;strong>asymptotically stable&lt;/strong> if $\Vert x(t)-x_e\Vert\to 0$ as $t\to 0$&lt;/li>
&lt;li>$x_e$ is &lt;strong>exponentially stable&lt;/strong> if $\Vert x(t)-x_e\Vert \leqslant \gamma e^{-\lambda(t-t_0)}\Vert x(t_0)-x(e)\Vert$&lt;/li>
&lt;li>$x_e$ &lt;em>in LTI&lt;/em> is asymptotically stable $\Rightarrow x_e$ is exponentially stable&lt;/li>
&lt;li>$x_e$ is &lt;strong>globally stable&lt;/strong> if $\delta$ can be chosen arbitrarily large&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>For LTV system, the system is stable (the zero solution is stable) iff $\Phi(t,t_0)$ is bounded by $K(t_0)$.
&lt;ul>
&lt;li>If bounded by constant $K$, then the system is uniformly stable.&lt;/li>
&lt;li>If bounded by constant $K$ and $\Vert\Phi(t,0)\Vert\to 0$ as $t\to 0$, then the system is asymptotically stable.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>For LTI system $\dot{x}=Ax$, it is Lyapunov stable iff $\mathrm{Re}(\lambda_i)\leqslant 0$ or $\mathrm{Re}(\lambda_i)=0,\;\eta_i=1$. ($\eta_i$ is the multiplicity of $\lambda_i$)
&lt;ul>
&lt;li>If $\mathrm{Re}(\lambda_i)&amp;lt;0$, then the system is asymptotically stable&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Internal stability: concerns the state variables&lt;/li>
&lt;li>External stability: concerns the output variables&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Notes for contents below:&lt;/p>
&lt;ul>
&lt;li>Positive definite (pd.) function: function $V$ is pd. wrt. $p$ if $V(x)&amp;gt;0,\;x\neq p$ and $V(x)=0,\;x=p$&lt;/li>
&lt;li>$C^n$ denotes the set of continuous and at least n-th differentiable functions&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Lyapunov&amp;rsquo;s Direct Method&lt;/strong>: Let $\mathcal{U}$ be an open neighborhood of $p$ and let $V:\mathcal{U}-&amp;gt;\mathbb{R}$ be a countinuous positive definite $C^1$ function wrt. $p$, we have following two conclusions:&lt;/p>
&lt;ol>
&lt;li>If $\dot{V}\leqslant 0$ on $\mathcal{U}\backslash\{p\}$ then $p$ is a stable fixed point of $\dot{x}=f(x)$&lt;/li>
&lt;li>If $\dot{V}&amp;lt; 0$ on $\mathcal{U}\backslash\{p\}$ then $p$ is an asymptotically stable fixed point of $\dot{x}=f(x)$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Lyapunov Function&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>A function satisfying conclusion 1 is called a Lyapunov function&lt;/li>
&lt;li>A function satisfying conclusion 2 is called a &lt;strong>strict&lt;/strong> Lyapunov function&lt;/li>
&lt;li>A function that is $C^1$ and pd. is called a Lyapunov function candidate&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>The energy function usually can be used as Lyapunov function. If it&amp;rsquo;s only semi-positive definite, one can use &lt;a class="link" href="https://en.wikipedia.org/wiki/LaSalle%27s_invariance_principle" target="_blank" rel="noopener"
>LaSalle&amp;rsquo;s Theorem&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>For LTI system, the zero solution of $\dot{x}=Ax$ is asymptotically stable iff $\forall$ pd. hermitian matrices $Q$, equation $A^*P+PA=-Q$ has a unique hermitian solution $P$ that is positive definite.&lt;/p>
&lt;ul>
&lt;li>$A^*P+PA=-Q$ is called Lyapunov&amp;rsquo;s Matrix Equation&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>here $V(x)=x^* Px=\int^\infty_0 x^*(t)Qx(t)dt$, which can be also called cost-to-go, or generalized energy&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Lyapunov&amp;rsquo;s Indirect Method&lt;/strong> (Lyapunov&amp;rsquo;s First Method / Lyapunov&amp;rsquo;s Linearization Theorem): The nonlinear system $\dot{x}=f(x)$ is (locally) asymptotically stable near the equilibrium point $x_e$ if the linearized system $\dot{x}_L=\frac{\partial f}{\partial x}(x_e)x_L$ is asymptotically stable.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="bounded-input-bounded-output-stability">Bounded-Input Bounded-Output Stability&lt;/h3>
&lt;blockquote>
&lt;p>BIBO stability is only concerned with the response of the system to the input (ZSR).&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Bounded-Input Bounded-Output (BIBO) stability&lt;/strong>: The LTV system is said to be (uniformly) BIBO stable if there exists a finite constant $g$ s.t. $\forall u(\cdot)$, its forced response $y_f(\cdot)$ satisfies $$ \sup_{t\in[0,\infty)}\Vert y_f(t)\Vert \leqslant g \sup_{t\in[0,\infty)} \Vert u(t)\Vert $$
&lt;blockquote>
&lt;p>The impulse response can be analyzed to assess BIBO stability
The LTV system is uniformly BIBO stable iff every entry of $D(t)$ is bounded and $\sup_{t\geqslant 0}\int^t_0|g_{ij}(t,\tau)|d\tau &amp;lt;\infty$ for every entry $g_{ij}$ of the matrix $C(t)\Phi(t,\tau)B(\tau)$.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>BIBO stability is related with the stability descibed in classical control theory.&lt;/li>
&lt;li>Exponential Lyapunov Stability $\Rightarrow$ BIBO stability&lt;/li>
&lt;/ul>
&lt;h3 id="steady-state-accuracy">Steady State Accuracy&lt;/h3>
&lt;p>Steady state accurary can be derived from the property of Laplace/Z-transform as mentioned above (assuming stability)
$$\lim\limits_{t\to \infty} f(t) = \lim\limits_{z\to 1} (z-1)F(z) = \lim\limits_{s\to 0}sF(s)$$&lt;/p>
&lt;h3 id="transient-response">Transient Response&lt;/h3>
&lt;p>Some measurements of transient response (with step input):&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Rise time&lt;/strong> $t_r$: time from 10% to 90% of steady state value&lt;/li>
&lt;li>&lt;strong>Peak overshoot&lt;/strong>: $M_p$ for overshoot magnitude and $t_p$ for time&lt;/li>
&lt;li>&lt;strong>Settling time&lt;/strong> $t_s$: time after which the magnitude fall in $1-d$ to $1-d$ final value. $d$ is usually %2~5.&lt;/li>
&lt;/ul>
&lt;h3 id="sensitivity">Sensitivity&lt;/h3>
&lt;p>Given a transfer function $H(z)$ with parameter $\Theta\in\mathbb{R}$, then sensitivity is defined as $S_H=\frac{\partial H}{\partial \Theta}\cdot\frac{\Theta}{H} = \frac{\partial H/H}{\partial \Theta/\Theta}$&lt;/p>
&lt;h2 id="discretization-and-linearization">Discretization and Linearization&lt;/h2>
&lt;h3 id="discretization-example">Discretization Example&lt;/h3>
&lt;p>The following image shows a minial example of sampling and hold.
&lt;img src="https://zyxin.xyz/blog/blog/en/2020-06/ControlSystemNotes/snh.png"
width="769"
height="210"
loading="lazy"
alt="A minimal example of a conversion process with analog and digital signals"
class="gallery-image"
data-flex-grow="366"
data-flex-basis="878px"
>&lt;/p>
&lt;h3 id="sampling-ad">Sampling (A/D)&lt;/h3>
&lt;ul>
&lt;li>Ideal sampler (a.k.a impulse modulator) converts a continuous signal $e: \mathbb{R}_+ \to \mathbb{R}$ to a discrete one $\hat{e}: \mathbb{N}\to \mathbb{R}$, such that $$ \hat{e}=e(t)\delta(t-kT)=e(t)\delta_T(t); \forall k\in \mathbb{N} $$
&lt;ul>
&lt;li>Ideal sampler is actually applying starred transform.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>How to sample? A rule of thumb used to select sampling rates is chosing a rate of at least 5 samples per time constant.
&lt;ul>
&lt;li>The $\tau$ appearing in the transient response term $ke^{-t/\tau}$ of a first order analog system is called the time constant.&lt;/li>
&lt;li>If the sampling time is too large, it can make the system unstable.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="reconstructionhold-da">Reconstruction/Hold (D/A)&lt;/h2>
&lt;ul>
&lt;li>Zero order hold (ZOH): $ZOH(\{e(k)\}_{k\in\mathbb{N}})(t) = e(k)\;for\;kT\leq t\leq (k+1)T$
&lt;ul>
&lt;li>Alternative form: $ZOH(\{e(k)\})=\sum^\infty_{k=0}e(k)(H(t-kT)-H(t-(k+1)T))$&lt;/li>
&lt;li>Its Laplace Transform: $G_{ZOH}(s)=\frac{1-e^{-Ts}}{s}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>First order hold (FOH): (delayed version) $$FOH(\{e(k)\}_ {k\in\mathbb{N}})(t)=\sum_ {k\in\mathbb{N}}\left[e(kT)+\frac{t-kT}{T}(e(kT)-e((k-1)T)) \right]\left[H(t-kT)-H(t-(k+1)T) \right]$$&lt;/li>
&lt;/ul>
&lt;h2 id="state-space-representation-1">State Space Representation&lt;/h2>
&lt;p>Suppose we are given the LTI continuous system
$$\begin{align*} \dot{x}(t) &amp;amp;= Ax(t)+Bu(t) \\ y(t)&amp;amp;=Cx(t)+Du(t) \end{align*}$$
If the input is sampled and ZOH and the output is sampled, then
$$\begin{align*} x(k+1)&amp;amp;=\bar{A}x(k)+\bar{B}u(k) \\ y(k)&amp;amp;=\bar{C}x(k)+\bar{D}u(k)\end{align*}$$
where $\bar{A}=\Phi((k+1)T,kT)=e^{AkT}$&lt;/br>
$\bar{B}=\int^{(k+1)T}_{kT}\Phi((k+1)T,\tau)B\mathrm{d}\tau=A^{-1}(e^{AT}-I)$&lt;/br>
$\bar{C}=C$ and $\bar{D}=D$&lt;/p>
&lt;blockquote>
&lt;p>Steps to apply conversion:&lt;/p>
&lt;ol>
&lt;li>Dervice SS model for analog system&lt;/li>
&lt;li>Calculate discrete representation (&lt;code>c2d&lt;/code> in MATLAB)&lt;/li>
&lt;li>Calculate pulse transfer function (&lt;code>ss2tf&lt;/code> in MATLAB)
If there is a complex system with multiple sampling and holding, a general rule is&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>Each ZOH output is assumed to be an input&lt;/li>
&lt;li>Each sampler input is assumed to be an output
and then create continuous state space from analog part of the system, then discretize them to generate discrete equations&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h3 id="s-plane-and-z-plane">$s$-plane and $z$-plane&lt;/h3>
&lt;p>When converting $s$ to $z$, the complex variables are related by $z=e^{Ts}$. Suppose $s=\sigma+j\omega$, then $z=e^{T\sigma}\angle \omega T$&lt;/p>
&lt;blockquote>
&lt;p>Note: if frequencies differ in integer multiples of the sampling frequency $\frac{2\pi}{T}=\omega_s$, then they are sampled into the same location in the $z$-plane.&lt;/p>
&lt;/blockquote>
&lt;p>For transient response relationship, suppose $s$-plane poles occur at $s=\sigma\pm j\omega$, then the transient response if $Ae^{\sigma t}\cos(\omega t+\varphi)$. When sampling occurs at $z$-plane poles, then the transient response if $Ae^{\sigma kT}\cos(\omega kT+\varphi)$.&lt;/p>
&lt;details>&lt;summary>Example: 2nd order transfer function&lt;/summary>
$$G(s)=\frac{\omega_n^2}{s^2+2\xi\omega_ns+\omega_n^2}$$
The $z$-plane poles occur at $z=r\angle\pm\theta$ where $r=e^{-\xi\omega_n T}$ and $\theta=\omega_n T\sqrt{1-\xi^2}$.
&lt;p>Then we can get the inverse relationship&lt;/p>
&lt;ul>
&lt;li>$\xi=-\ln( r)/\sqrt{\ln^2( r)+\theta^2}$&lt;/li>
&lt;li>$\omega_n=(1/T)\sqrt{\ln^2( r)+\theta^2}$&lt;/li>
&lt;li>(time constant) $\tau=-T/\ln( r)$&lt;/li>
&lt;/ul>
&lt;/details>
&lt;h3 id="linearization">Linearization&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Jacobian Linearization&lt;/strong>: linearize $\dot{x}=f(x,u)$ at an equilibrium $(x_e, u_e)$ is $$\frac{\mathrm{d}z}{\mathrm{d}t}=Az+Bv,\quad\text{where}\;A=\left.\frac{\mathrm{d}f}{\mathrm{d}x}\right|_ {\begin{split}x=x_e\\u=u_e\end{split}},\;B=\left.\frac{\mathrm{d}f}{\mathrm{d}u}\right|_ {\begin{split}x=x_e\\u=u_e\end{split}},\;z=(x-x_e),\;v=(u-u_e)$$
&lt;ul>
&lt;li>change $(x_e, u_e)$ to a trajectory $(x_e(t), u_e(t))$ we can linearize the system about a trajectory.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="controllers">Controllers&lt;/h1>
&lt;h2 id="full-state-feedback">Full State Feedback&lt;/h2>
&lt;blockquote>
&lt;p>This method can be used for both continuous and discrete systems, just make sure to use corresponding method for choosing correct closed-loop transfer function.&lt;/p>
&lt;/blockquote>
&lt;p>For state space systems, with access to all of the state variables, we can change the $A$ matrix and thereby change the system dynamics by feedback.&lt;/p>
&lt;p>Consider SISO LTI system ($u\in\mathbb{R},y\in\mathbb{R}$), we define the input as $u\equiv Kx+Ev$ where $K\in\mathbb{R}^{1\times n},\;E\in\mathbb{R}$ is an input matrix and $v(t)\in\mathbb{R}^\mathbb{R}$ is the exogeneous (externally applied) input. The new system will be $$\begin{align}\dot{x}&amp;amp;=(A+BK)x+BEv\\y&amp;amp;=(C+DK)x+DEv\end{align}$$
The mission is to find a state update matrix $A_{\mathrm{CL}}\equiv A+BK$ with desired set of eigenvalues, therefore we can construct $A_{\mathrm{CL}}$ with specific eigenvalues and then calculate $K$. This process will be quite easy if the system is already in controllable cannonical form. (which can be constructed directly from transfer function or using similarity transform)&lt;/p>
&lt;p>Another way (SISO only) to calculate $K$ without controllable cannonical form is using the following formulae given the desired characteristic polynomial $\phi^{\star}(s)=s^n+\sum^{n-1}_ {i=0} a^\star_i s^i$ and original characteristic polynomial $\phi(s)=s^n+\sum^{n-1}_ {i=0} a_i s^i$&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Ackermann&amp;rsquo;s Formula&lt;/strong>: $K=-e^\top_n\mathcal{C}^{-1}\phi^\star(A)$ (here $e_i$ is unit vector with 1 at i-th position)&lt;/li>
&lt;li>&lt;strong>Bass-Gura&amp;rsquo;s Formula&lt;/strong>: $$K=-[(a^\star_{n-1}-a_{n-1}) \;\cdots\;(a^\star_0-a_0)]\begin{bmatrix}1&amp;amp;a_{n-1}&amp;amp;a_{n-2}&amp;amp;\cdots&amp;amp;a_1\\&amp;amp;1&amp;amp;a_{n-1}&amp;amp;\cdots&amp;amp;a_2\\ &amp;amp;&amp;amp;\ddots&amp;amp;\ddots&amp;amp;\vdots \\ &amp;amp;&amp;amp;&amp;amp;1&amp;amp;a_{n-1}\\ &amp;amp;&amp;amp;&amp;amp;&amp;amp;1\end{bmatrix}^{-1}\mathcal{C}^{-1}$$&lt;/li>
&lt;/ul>
&lt;p>Note that the zeros of transfer function will not be affected by state feedback.&lt;/p>
&lt;h3 id="state-estimation-observer-design">State Estimation (Observer Design)&lt;/h3>
&lt;blockquote>
&lt;p>Some times we don&amp;rsquo;t have the direct access to the state, we need construct an observer
For stochastic version, please check &lt;a href="{% post_path StochasticSystemNotes %}#Observation-Filtering">my notes for stochastic system&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>Assume a plant $\Sigma$ and an (&lt;strong>Luenberger&lt;/strong>) &lt;strong>observer&lt;/strong> $\hat{\Sigma}$:
$$\Sigma:\begin{cases}\dot{x}=Ax+Bu\\ y=Cx\end{cases},\quad \hat{\Sigma}:\begin{cases} \dot{\hat{x}}=A\hat{x}+Bu+L(y-\hat{y})\\ y=C\hat{x}\end{cases}$$&lt;/p>
&lt;p>Subtract observer dynamics from plant dynamics and define $e\equiv x-\hat{x}$, the dynamics for $e$ is $\dot{e}=(A-LC)e$ and $y-\hat{y}=Ce$. This error dynamic $A_e=A-LC$ can be easily changed with observable cannonical form. (which similarly can be constructed directly from transfer function or using similarity transform)&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Reduced-order Observer&lt;/strong>: If the state length of the system $n$ is large while $n-p$ is small, split the system and let $x_1$ holds the states that can be measured directly while $x_2$ holds states that are to be estimated, (i.e. $y=x_1+Du$). Define $z=\hat{x}_2-Lx_1$ then the system runs like $$\begin{align*}\begin{bmatrix}x_1 \\ \hat{x}_2\end{bmatrix}&amp;amp;=\begin{bmatrix} y-Du\\ z+Lx_1 \end{bmatrix}\qquad\begin{split}&amp;amp;\text{measurement} \\ &amp;amp;\text{observer}\end{split} \\ u&amp;amp;=K\begin{bmatrix}x_1 \\ \hat{x}_2 \end{bmatrix} + v \qquad\text{control law}\end{align*}$$ And then the error we care about is only $e=x_2-\hat{x}_2$.&lt;/li>
&lt;li>&lt;strong>Ackermann&amp;rsquo;s Formula&lt;/strong>: $L=\phi^\star(A)\mathcal{O}^{-1}e_n$ ($\phi^\star$ is the desired characteristic function for $A_e$)&lt;/li>
&lt;li>&lt;strong>Separation Principle&lt;/strong>: If a stable observer and stable state feedback are designed for an LTI system, then the combined observer and feedback will be stable.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Errors from state estimation&lt;/p>
&lt;ol>
&lt;li>Inaccurate knowledge of $A$ and $B$&lt;/li>
&lt;li>Initial condition uncertainty&lt;/li>
&lt;li>Disturbance or sensor error
It&amp;rsquo;s advised to choose observer poles to be 2-4x faster than closed loop poles&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;h3 id="lqrhttpsenwikipediaorgwikilineare28093quadratic_regulator">&lt;a class="link" href="https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic_regulator" target="_blank" rel="noopener"
>LQR&lt;/a>&lt;/h3>
&lt;blockquote>
&lt;p>Motivation: handle control constraints and time varying dynamics with performance metric (ideas of optimal control)
Note: $x^\top Ax$ is called a &lt;strong>quadratic form&lt;/strong>, $x^\top Ay$ is called a &lt;strong>bilinear form&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>A quadratic function $f(x)=x^\top Dx+C^\top x+c_0$ has one minimizer iff $D\succ 0$, or multiple minimizers iff $D\succeq 0$.&lt;/li>
&lt;li>(discrete finite time) &lt;strong>Linear Quadratic Regulator&lt;/strong> (LQR): the control problem is defined as $$\begin{align*}\min_{u\in\left(\mathbb{R}^m\right)^{\{0,\ldots,N\}}} J_{N}(u,x_0)&amp;amp;=\frac{1}{2}\sum^{N}_{k=0}(x^\top(k)Q(k)x(k)+u^\top(k)R(k)u(k)) \\ \mathrm{s.t.}\qquad x(k+1) &amp;amp;= A(k)x(k) + B(k)u(k)\quad \forall k\in\{0,\ldots,N-1\}\\ y(k)&amp;amp;=C(k)x(k)\\ x(0)&amp;amp;=x_0\end{align*}$$ where $Q(k)\succ 0$ and $R(k)\succ 0$&lt;/li>
&lt;li>&lt;strong>Bellman&amp;rsquo;s Principle of Optimality&lt;/strong>: If a closed loop control $u^\star$ is optimal over the interval $0\leqslant k\leqslant N$, it&amp;rsquo;s also optimal over any subinterval $m\leqslant k\leqslant N$ where $m\in\{0,\ldots,N\}$&lt;/li>
&lt;li>&lt;strong>The Minimum Principle&lt;/strong>: The optimal input to the LQR problem satisfies the following backward equations: $$\begin{align*}u^\star(k)&amp;amp;=-K(x)x(k) \\ K(k)&amp;amp;=\left[B^\top(k) P(k+1)B(k)+\frac{1}{2}R(k)\right]^{-1}B^\top(k)P(k+1)A(k) \\ P(k)&amp;amp;=A^\top(k)P(k+1)[A(k)- B(k)K(k)]+\frac{1}{2}Q(k)\end{align*}$$ and $P(N)=Q(N),\;K(N)=0$. The optimal cost is $J^\star_N=x^\top(0)P(0)x(0)$&lt;/li>
&lt;li>For infinite horizon, $K(k)$ start becoming constants. The optimal input for LQR problem (assuming the system became LTI when $N\to\infty$) is $u^*(k)=-Kx(k)$ where $$K=(B^\top PB+R/2)^{-1}B^\top PA$$ and $P\succ 0$ is the unique solution to the discrete-time &lt;strong>algebraic Riccati Equation&lt;/strong>: $$P=A^\top PA-A^\top PB\left(B^\top PB+R/2\right)^{-1}B^\top PA+Q/2$$&lt;/li>
&lt;/ul></description></item><item><title>Matrix Algebra</title><link>https://zyxin.xyz/blog/en/2019-06/MatrixAlgebra/</link><pubDate>Thu, 06 Jun 2019 00:00:00 +0000</pubDate><guid>https://zyxin.xyz/blog/en/2019-06/MatrixAlgebra/</guid><description>&lt;p>I encountered a lot of calculation of derivates and integration on matrices when I was learning linear systems and SLAM, but I haven&amp;rsquo;t learned much knowledge about them. Recently I know about the book &lt;em>Matrix Cookbook&lt;/em>, which thoroughly discussed the arithmetics of matrices. Therefore, I post it here for future references.&lt;/p>
&lt;h2 id="contents">Contents&lt;/h2>
&lt;ol>
&lt;li>基础内容&lt;/li>
&lt;li>求导&lt;/li>
&lt;li>逆&lt;/li>
&lt;li>复矩阵&lt;/li>
&lt;li>求解与分解&lt;/li>
&lt;li>统计与概率&lt;/li>
&lt;li>多元概率分布&lt;/li>
&lt;li>高斯&lt;/li>
&lt;li>特殊矩阵&lt;/li>
&lt;li>函数与运算符&lt;/li>
&lt;/ol>
&lt;h2 id="source">Source&lt;/h2>
&lt;p>The book can be downloaded from a website from &lt;a class="link" href="http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=3274" target="_blank" rel="noopener"
>Technical University of Denmark&lt;/a>. If the link failed, you can &lt;a class="link" href="https://zyxin.xyz/blog/blog/static/doc/matrixcookbook.pdf" >downloaded from here&lt;/a> directly.&lt;/p></description></item><item><title>Notes for Stochastic System</title><link>https://zyxin.xyz/blog/en/2019-03/StochasticSystemNotes/</link><pubDate>Tue, 26 Mar 2019 00:00:00 +0000</pubDate><guid>https://zyxin.xyz/blog/en/2019-03/StochasticSystemNotes/</guid><description>&lt;blockquote>
&lt;p>Prerequisites: Knowledge of Elementary Calculus, Linear Algebra and Probability&lt;/p>
&lt;/blockquote>
&lt;h2 id="discrete-time-stochastic-system">Discrete-Time Stochastic System&lt;/h2>
&lt;h3 id="stochastic-sequences">Stochastic Sequences&lt;/h3>
&lt;ul>
&lt;li>Definition: Given $k\in\mathbb{K}\subseteq\mathbb{Z}$ a sequence of integers, $\mathcal{X}(k,\omega): (\Omega,\mathcal{F},\mathbb{P})\to(\mathbb{R}^n,\mathcal{F}_ \mathcal{X},\mathbb{P}_ \mathcal{X})$ is a random/stochastic sequence.&lt;/li>
&lt;li>Uncertainties: Consider a casual system $F$ relates some scalar inputs $u(k)$ to output $x(k)$
&lt;ul>
&lt;li>&lt;strong>Epistemic/Model uncertainty&lt;/strong>: $\mathcal{X}(k,\omega)=F(k,u(k),u(k-1),\ldots,\omega)$. (system is stochastic and input is deterministic).&lt;/li>
&lt;li>&lt;strong>Aleatoric/Input uncertainty&lt;/strong>: $\mathcal{X}(k,\omega)=f(k,U(k,\omega),u(k-1,\omega),\ldots)$ (system is deterministic and input is stochastic).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Realization&lt;/strong>: An outcome $\mathcal{X}(k,\omega)=x(k)$ given $\omega$ is called a realization of stochastic sequence $\mathcal{X}$&lt;/li>
&lt;li>Terminology and Convention
&lt;ul>
&lt;li>$\mathcal{X}(k,\omega)$ is often written as $\mathcal{X}(k)$ when there&amp;rsquo;s no ambiguity.&lt;/li>
&lt;li>$\mathbb{K}=\mathbb{Z}$ if not specified.&lt;/li>
&lt;li>Sequence over a set $\mathcal{K}_1\subseteq\mathbb{K}$ are denoted $\mathcal{X}(\mathcal{K}_1)$.&lt;/li>
&lt;li>$\mathcal{X}$ denotes $\mathcal{X}(\mathbb{K})$ if not specified.&lt;/li>
&lt;li>Consecutive subsequence: $$\mathcal{X}(k:l)=\{\mathcal{X}(k),\mathcal{X}(k+1),\ldots,\mathcal{X}(l)\},\;x(k:l)=\{x(k),x(k+1),\ldots,x(l)\}$$&lt;/li>
&lt;li>Abbreviations:
&lt;ul>
&lt;li>&lt;em>&lt;strong>SS&lt;/strong>&lt;/em> - stochastic sequence&lt;/li>
&lt;li>&lt;em>&lt;strong>IID&lt;/strong>&lt;/em> - independent indentically distributed&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="probabilistic-characterization">Probabilistic characterization&lt;/h3>
&lt;ul>
&lt;li>Distribution and density: $$F_ \mathcal{X}\left(k:l;x(k:l)\right)\equiv\mathbb{P}((\mathcal{X}_ i(k)\leqslant x_i(k))\cap\cdots\cap(\mathcal{X}_ i(l)\leqslant x_ i(l)),\;i=1\ldots n)$$ $$f_ \mathcal{X}\left(k:l;x(k:l)\right)\equiv \frac{\partial^{n(l-k+1)}}{\partial x_ 1(k)\cdots\partial x_ n(l)}F_ \mathcal{X}(k:l;x(k:l))$$
Here $k:l$ actually denotes a set of consecutive integers, it can be also changed to ordinary sets $\{k,l\}$ or single scalar $k$.&lt;/li>
&lt;li>&lt;strong>Ensemble Average&lt;/strong>: $\mathbb{E}[\psi(\mathcal{X}(k))]$, doing summation over different realization at same time $k$
&lt;ul>
&lt;li>&lt;strong>Mean&lt;/strong>: $\mu_ \mathcal{X}(k)\equiv\mathbb{E}[\mathcal{X}(k)]=\int^\infty_{-\infty}x(k)f_ \mathcal{X}(k;x(k))\mathrm{d}x(k)$&lt;/li>
&lt;li>&lt;strong>Conditional Mean&lt;/strong>: $\mu_ \mathcal{X}(l|k)\equiv\mathbb{E}[\mathcal{X}(l)|\mathcal{X}(k)=x(k)]$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Time Average&lt;/strong>: $\frac{1}{2N+1}\sum^N_{k=-N}\psi(\mathcal{X}(k))$, doing summation over different time k of same realization&lt;/li>
&lt;li>&lt;strong>Autocorrelation&lt;/strong>:
&lt;ul>
&lt;li>Scalar case: $r_ \mathcal{X}(k,l)\equiv\mathbb{E}[\mathcal{X}(k)\mathcal{X}(l)]=\int^\infty_{-\infty}\int^\infty_{-\infty}x(k)x(l)f_ \mathcal{X}(k,l;x(k,l))\mathrm{d}x(k)\mathrm{d}x(l)$&lt;/li>
&lt;li>Vector case: $R_ \mathcal{X}(k,l)\equiv\mathbb{E}[\mathcal{X}(k)\mathcal{X}^\top(l)]$&lt;/li>
&lt;li>Conditional autocorrelation: $R_ \mathcal{X}(k,l|q)\equiv\mathbb{E}[\mathcal{X}(k)\mathcal{X}^\top(l)|\mathcal{X}(q)=x(q)]$&lt;/li>
&lt;li>Often we denote $C_ \mathcal{X}(k)=R_ \mathcal{X}(k,k)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Autocovariance&lt;/strong>:
&lt;ul>
&lt;li>Scalar case: $\kappa_ \mathcal{X}(k,l)\equiv\mathbb{E}[(\mathcal{X}(k)-\mu_ \mathcal{X}(k))(\mathcal{X}(l)-\mu_ \mathcal{X}(l))]$&lt;/li>
&lt;li>Vector case: $\mathrm{K}_ \mathcal{X}(k,l)\equiv\mathbb{E}[(\mathcal{X}(k)-\mu_ \mathcal{X}(k))(\mathcal{X}(l)-\mu_ \mathcal{X}(l))^\top]$&lt;/li>
&lt;li>Conditional autocovariance: $\mathrm{K}_ \mathcal{X}(k,l|q)\equiv\mathbb{E}[(\mathcal{X}(k)-\mu_ \mathcal{X}(k|q))(\mathcal{X}(l)-\mu_ \mathcal{X}(l|q))^\top]$&lt;/li>
&lt;li>Often we denote $S_ \mathcal{X}(k|q)=\mathrm{K}_ \mathcal{X}(k,k|q)$&lt;/li>
&lt;li>Useful conclusion: $\mathrm{K}(a,b)=\mathrm{K}(b,a)^T$&lt;/li>
&lt;li>Normalized (&lt;strong>autocorrelation coefficient&lt;/strong>): $\rho_ \mathcal{X}(k,l)\equiv\mathrm{K}_ \mathcal{X}(k,l)/\sigma^2_{\mathcal{X}(k)}\sigma^2_{\mathcal{X}(l)}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Strong Stationarity&lt;/strong>(aka. strict sense): (necessarily identically distributed over time) $$\forall x(k:l)\in\mathbb{R}^{n(l-k+1)},\;\forall s\in\mathbb{Z},\;f_ \mathcal{X}(k:l;x(k:l))=f_ \mathcal{X}(k+s:l+s;x(k:l))$$&lt;/li>
&lt;li>&lt;strong>Weak Stationarity&lt;/strong>(aka. wide sense): $\forall k,l$ if $$\mu_ \mathcal{X}(k)=\mu_ \mathcal{X}(l)\;\text{and}\;\mathrm{K}_ \mathcal{X}(k,l)=\mathrm{K}_ \mathcal{X}(k+s,l+s)\equiv\bar{\mathrm{K}}_ \mathcal{X}(s)$$ Weak stationarity is necessary condition for stationarity. (Equal when Gaussian distributed)&lt;/li>
&lt;li>&lt;strong>Ergodicity&lt;/strong>: $\mathcal{X}$ is called ergodic in $\psi$ if
&lt;ol>
&lt;li>$\mathbb{E}[\psi(\mathcal{X})]$ is stationary&lt;/li>
&lt;li>Ensemble average is equal to Time average, that is $$ \frac{1}{2N+1}\sum^N_{k=-N}\psi(\mathcal{X}(k))\to\mathbb{E}[\psi(\mathcal{X})];\text{as};l\to \infty$$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;h3 id="markov-sequence">Markov Sequence&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Markov Sequence&lt;/strong>: A ss. $\mathcal{X}(k)$ is called a (discrete-time) Markov sequence if $$\begin{split}f_ \mathcal{X}\left(k;x(k)\middle|\mathcal{X}(k-1)=x(k-1),\mathcal{X}(k-2)=x(k-2),\ldots\right) \\=f_ \mathcal{X}(k;x(k)|\mathcal{X}(k-1)=x(k-1))\end{split}$$
&lt;ul>
&lt;li>We often make some assumption on the initial condition $\mathcal{X}(0)$, such as known, deteministic or uniformly distributed within certain domain.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Markov Chains&lt;/strong>: Markov sequence with discrete set of values(states) ${x_1\ldots x_m}$&lt;/li>
&lt;li>&lt;strong>Hidden Markov model&lt;/strong>: Sequence $\mathcal{Y}$ is called a Hidden Markov Model if it&amp;rsquo;s modeled by a system of the form $$\begin{align}\mathcal{X}(k+1)&amp;amp;=g(k,\mathcal{X}(k),\mathcal{W}(k)) \\ \mathcal{Y}(k)&amp;amp;=h(k,\mathcal{X}(k),\mathcal{W}(k))\end{align}$$
We also say that $\mathcal{Y}$ has a &lt;strong>(discrete-time) stochastic state space&lt;/strong>.&lt;/li>
&lt;li>&lt;strong>Guassian-Markov Sequence&lt;/strong> (GMS): $\mathcal{X}(k+1)=g(k,\mathcal{X}(k),\mathcal{W}(k))$ where $\mathcal{W}(k)$ is iid. Guassian&lt;/li>
&lt;/ul>
&lt;h3 id="linear-stochastic-sequence">Linear Stochastic Sequence&lt;/h3>
&lt;p>$$\begin{align}\mathcal{X}(k+1)&amp;amp;=A(k)\mathcal{X}(k)+B(k)\mathcal{W}(k) \\ \mathcal{Y}(k)&amp;amp;=C(k)\mathcal{X}(k)+D(k)\mathcal{W}(k)\end{align}$$&lt;/p>
&lt;blockquote>
&lt;p>For linear Markov sequences, the deterministic mean sequence and centered uncertain sequence completely decouple. So we often assume that $\mathcal{X}(k)$ and $\mathcal{Y}(k)$ are &lt;strong>centered&lt;/strong> in this case, with regard to deterministic inputs. The equation with deterministic inputs is often written as $$\mathcal{X}(k+1)=A(k)\mathcal{X}(k)+B_u(k)u(k)+B_\mathcal{W}(k)\mathcal{W}(k)$$&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>
&lt;p>Recursive-form expectations:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Mean&lt;/strong>: $\mu_ \mathcal{X}(k+1|q)=A(k)\mu_ \mathcal{X}(k|q)+B(k)\mu_\mathcal{W}(k)$&lt;/li>
&lt;li>&lt;strong>Covariance (Discrete-time algebraic Lyapunov/Stein difference equation)&lt;/strong>: $$S_ \mathcal{X}(k+1|q)=A(k)S_ \mathcal{X}(k|q)A^\top(k)+B(k)S_\mathcal{W}(k)B^\top(k)$$
&lt;blockquote>
&lt;p>Can be solved with &lt;code>dlyap&lt;/code> in MATLAB&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Convergence when $k\to\infty$:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Mean convergence&lt;/strong>: $\mu_ \mathcal{X}(k|q)$ converges requires $\max_i|\lambda_i(A)|&amp;lt;1$ ($\lambda$ denotes eigenvalue)&lt;/li>
&lt;li>&lt;strong>Covariance convergence&lt;/strong>: $S_ \mathcal{X}(k|q)$ converges requires $\max_i|\lambda_i(A)|&amp;lt;1$&lt;/li>
&lt;li>&lt;strong>(Discrete-time) Lyapunov equation&lt;/strong> (Stein equation): $\bar{S}_ \mathcal{X}=A\bar{S}_ \mathcal{X}A^\top+B\bar{S}_\mathcal{W}(k)B^\top$. Solution for this equation exists iff. $A$ is asymptotically stable (characterizing sequence is stationary).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Explicit state transition: By recursive substitution, $$\mathcal{X}(k)=\Psi(k,q)\mathcal{X}(q)+\sum^{k-1}_ {i=q}\Gamma(k,i)\mathcal{W}(i)$$ where state transition matrix $\Psi(k,q)=\begin{cases}I, &amp;amp;k=q \\ \prod^{k-1}_ {i=q}A(i),&amp;amp; k&amp;gt;q\end{cases}$ and $\Gamma(k,i)=\Psi(k,i+1)B(i)$.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Conditioned Mean sequence&lt;/strong>: $\mu_ \mathcal{X}(k|q)=\Psi(k,q)\mu_ \mathcal{X}(q)+\sum^{k-1}_ {i=q}\Gamma(k,i)\mu_\mathcal{W}(i)$&lt;/li>
&lt;li>&lt;strong>Conditioned Autocovariance Matrix&lt;/strong>: $$\mathrm{K}_ \mathcal{X}(k,l|q)=\Psi(k,q)S_ \mathcal{X}(q)\Psi^\top(l,q)+\sum^{min\{k,l\}-1}_ {i=q}\Gamma(k,i)S_\mathcal{W}(i)\Gamma^\top(l,i)$$
&lt;ul>
&lt;li>A special case: $S_ \mathcal{X}(k|q)=\mathrm{K}_ \mathcal{X}(k,k|q)=\sum^{k-1}_ {i=q}\Gamma(k,i)S_\mathcal{W}(i)\Gamma^\top(k,i)$, $S_ \mathcal{X}(k|k)=0$&lt;/li>
&lt;li>Useful equation (stationary centered case): $$\mathrm{K}_ \mathcal{X}(k,l)=\begin{cases} S_ \mathcal{X}(k)\cdot(A^\top)^{(l-k)}&amp;amp;,l&amp;gt;k\\ S_ \mathcal{X}(k)&amp;amp;,l=k\\ A^{(k-l)}S_ \mathcal{X}(k)&amp;amp;,l&amp;lt; k\end{cases}$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Conditional Autocorrelation Matrix&lt;/strong>: $R_ \mathcal{X}(k,l|q)=\mathrm{K}_ \mathcal{X}(k,l|q)+\mu_ \mathcal{X}(k|q)\mu_ \mathcal{X}^\top(l|q)$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Observation $\mathcal{Y}$ property:&lt;/p>
&lt;ul>
&lt;li>Mean: $\mu_ \mathcal{Y}(k|q)=C(k)\mu_ \mathcal{X}(k|q)+D(k)\mu_\mathcal{W}(k)$&lt;/li>
&lt;li>Covariance: $$\mathrm{K}_ \mathcal{Y}(k,l|q)=\begin{cases}C(k)\mathrm{K}_ \mathcal{X}(k,l|q)C^\top(l)+C(k)\Gamma(k,l)S_W(l)D^\top(l)&amp;amp;:k&amp;gt;l \\C(k)S_ \mathcal{X}C^\top(k)+D(k)S_\mathcal{W}(k)D^\top(k)&amp;amp;:k=l\\ C(k)\mathrm{K}_ \mathcal{X}(k,l|q)C^\top(l)+D(k)S_\mathcal{W}(k)\Gamma^\top(l,k)C^\top(l)&amp;amp;:k&amp;lt; l\end{cases}$$&lt;/li>
&lt;li>Stationary time-invariant covariance: $$\mathrm{K}_ \mathcal{Y}(s)=\begin{cases}CA^{|s|}\bar{S}_ \mathcal{X}C^\top+CA^{|s|-1}B\bar{S}_ WD^\top&amp;amp;:s\neq 0 \\C\bar{S}_ \mathcal{X}C^\top+D\bar{S}_ WD^\top&amp;amp;:s=0\end{cases}$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="gaussian-stochastic-sequence">Gaussian Stochastic Sequence&lt;/h3>
&lt;ul>
&lt;li>Jointly Gaussian $\Rightarrow, \nLeftarrow$ Marginally Gaussian&lt;/li>
&lt;li>$c^\top X$ is Gaussian $\Leftrightarrow X$ is Gaussian&lt;/li>
&lt;li>Conditional Gaussian: if $X$ and $Y$ are Gaussian, then $X|Y \sim \mathcal{N}(\mu_{X|Y},S_{X|Y})$ where $\mu_{X|Y}=\mu_X+S_{XY}S_Y^{-1}(Y-\mu_Y)$, $S_{X|Y}=S_X-S_{XY}S_Y^{-1}S_{YX}$&lt;/li>
&lt;li>A linear controllable GMS $\mathcal{X}(k)$ is stationary iff. $A(k)=A, B(k)=B$ (time-invariant) and $A$ is asymptotically stable.&lt;/li>
&lt;li>All LTI stationary GMS are also ergodic in all finite momoents&lt;/li>
&lt;li>Solve $\mathcal{X}(k+1)=A\mathcal{X}(k)+B\mathcal{W}(k)$ when $\mathcal{X}$ is stationary ($\max_i|\lambda_i(A)|&amp;lt;1$)
&lt;ol>
&lt;li>solve $\bar{\mu}_ \mathcal{X}=A\bar{\mu}_ \mathcal{X}+B\bar{\mu}_\mathcal{W}$ for $\bar{\mu}$&lt;/li>
&lt;li>solve $\bar{S}_ \mathcal{X}=A\bar{S}_ \mathcal{X}A^\top+B\bar{S}_ \mathcal{W}B^\top$ for $\bar{S}_ \mathcal{X}$&lt;/li>
&lt;li>calculate $\Sigma_ \mathcal{X}(k:l|q)=\begin{bmatrix}\mathrm{K}_ \mathcal{X}(k,k|q)&amp;amp;\cdots&amp;amp;\mathrm{K}_ \mathcal{X}(k,l|q)\\ \vdots&amp;amp;\ddots&amp;amp;\vdots\\ \mathrm{K}_ \mathcal{X}(l,k|q)&amp;amp;\cdots&amp;amp;\mathrm{K}_ \mathcal{X}(l,l|q)\end{bmatrix}$ using $\bar{S}_ \mathcal{X}$&lt;/li>
&lt;li>Then $f_ \mathcal{X}(k:l;x(k:l))$ is determined with $\mu_ \mathcal{X}(k:l)=\{\bar{\mu}_ \mathcal{X},\bar{\mu}_ \mathcal{X}\ldots\bar{\mu}_ \mathcal{X}\}$ and $\Sigma_ \mathcal{X}(k:l)$ above.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;h3 id="observation--filtering">Observation &amp;amp; Filtering&lt;/h3>
&lt;blockquote>
&lt;p>For deterministic version, please check &lt;a href="{% post_path ControlSystemNotes %}#State-Estimation-Observer-Design">my notes for control system&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>(LTI) &lt;strong>Luenberger observer&lt;/strong>: $$\begin{align}\hat{\mathcal{X}}(k+1)&amp;amp;=A\hat{\mathcal{X}}(k)+L(\hat{\mathcal{Y}}(k)-\mathcal{Y}(k))+B\bar{\mu}_ \mathcal{W} \\ \hat{\mathcal{Y}}(k)&amp;amp;=C\hat{\mathcal{X}}(k)+D\bar{\mu}_\mathcal{W}\end{align}$$ where $L$ is the observer gain.
&lt;ul>
&lt;li>Note: We often assume that the process &amp;amp; measurement noise are decoupled and independent&lt;/li>
&lt;li>Combined form: $\hat{\mathcal{X}}(k+1)=[A+LC]\hat{\mathcal{X}}(k)-L\mathcal{Y}(k)+B\mu_\mathcal{W}(k)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>State estimation residual&lt;/strong> $r(k)=\mathcal{X}(k)-\hat{\mathcal{X}}(k)$
&lt;ul>
&lt;li>Combined form: $r(k+1)=[A+LC]r(k)+[B+LD]\tilde{\mathcal{W}}(k)$&lt;/li>
&lt;li>Stationary covariance can be solved by a Lyapunov equation $$\bar{S}_ r=[A+LC]\bar{S}_ r[A+LC]^T+[LD+B]\bar{S}_ \mathcal{W}[LD+B]^T$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>A common objective: minimize $\bar{S}_r=\mathbb{E}(rr^\top)$
&lt;ul>
&lt;li>Solutions: $L=-A\bar{S}_ rC^\top[C\bar{S}_ rC^\top+D\bar{S}_ \mathcal{W}D^\top]^{-1}$ (&lt;strong>Kalman observer gain&lt;/strong>)&lt;/li>
&lt;li>Or solve &lt;strong>(Discrete-time) Algebraic Riccati equation&lt;/strong> $$\bar{S}_ r=A\bar{S}_ rA^\top+B\bar{S}_ \mathcal{W}B^\top-A\bar{S}_ rC^\top[C\bar{S}_ rC^\top+D\bar{S}_ \mathcal{W}D^\top]^{-1}C\bar{S}_ rA^\top$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Innovation sequence&lt;/strong> $e(k)=\hat{\mathcal{Y}}(k)-\mathcal{Y}(k)$ with $L$ is optimal
&lt;ul>
&lt;li>We can find that $\mu_e=0$ and $\mathrm{K}_e(k+s,k)=\begin{cases}C\bar{S}_rC^\top+D\bar{S}_WD^T&amp;amp;:s=0 \\0&amp;amp;:s\neq0\end{cases}$. So the innovation sequence is iid. (only in Kalman observer)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>(Output) Probabilistically-equivalent model&lt;/strong>: $$\begin{align}\mathcal{X}(k+1)&amp;amp;=A\mathcal{X}(k)+Le(k) \\ \mathcal{Y}(k)&amp;amp;=C\mathcal{X}(k)-e(k)\end{align}$$&lt;/li>
&lt;/ul>
&lt;h2 id="markov-chains">Markov Chains&lt;/h2>
&lt;blockquote>
&lt;p>Content in this section comes from EECS 501
In this specific field, we often use stand-alone analysis methods.&lt;/p>
&lt;/blockquote>
&lt;h3 id="basic-definitions">Basic definitions&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>State distribution&lt;/strong>: We denote row vector $\pi_t$ as $\pi_t(x)=\mathbb{P}(\mathcal{X}_t=x),\; x\in S$ ($S$ is the set of states). Directly we have $\sum_x\pi(x)=1$&lt;/li>
&lt;li>&lt;strong>Time homogeneous&lt;/strong>: $\mathbb{P}(\mathcal{X}_ {t+1}=y|\mathcal{X}_ t=x)=\mathbb{P}(\mathcal{X}_{s+1}=y|\mathcal{X}_s=x)\;\forall s,t$&lt;/li>
&lt;li>&lt;strong>One-step transition probability matrix&lt;/strong>: $P_t=[P_{xy,t}]$ where $P_{xy,t}=\mathbb{P}(\mathcal{X}_ {t+1}=y|\mathcal{X}_ t=x)$.
&lt;ul>
&lt;li>Time-homo case: $P=[P_{xy}]$ where $P_{xy}=p(y|x)$&lt;/li>
&lt;li>Rows of the matrix sum up to 1.&lt;/li>
&lt;li>This matrix is also called &lt;strong>stochastic matrix&lt;/strong>.&lt;/li>
&lt;li>Extend this matrix to continuous states, then we use &lt;strong>transition kernel&lt;/strong> $T(x,y)$ to describe the transition probability,&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>m-step transition probability matrix&lt;/strong>: $P_{xy,t}^{(m)}=\mathbb{P}(\mathcal{X}_ {t+m}=y|\mathcal{X}_ t=x)$
&lt;ul>
&lt;li>&lt;strong>Chapman-Kolmogorov Equation&lt;/strong>: $P^{(n+m)}_t=P^{(n)}_t P^{(m)}_t$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="state-variables">State Variables&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Hitting time&lt;/strong>: $T_1(y)=\min\{n\geq 0:\mathcal{X}_ n=y\},\;T_k(y)=\min\{n&amp;gt;T_{k-1}(y):\mathcal{X}_ n=y\}$ where $n\in\mathbb{N}$
&lt;ul>
&lt;li>&lt;strong>Period&lt;/strong>: For state $i\in x$, its period is the greatest common divisor of $\{n&amp;gt;1|T_n(i)&amp;gt;0\}$. A Markov Chain is &lt;strong>aperiodic&lt;/strong> If all states have period 1.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Return probability&lt;/strong>: $f_{xy}=\mathbb{P}(T_1(y)&amp;lt;\infty|\mathcal{X}_0=x)$&lt;/li>
&lt;li>&lt;strong>Occupation time&lt;/strong>: $V(y)=\sum^\infty_{n=1}\unicode{x1D7D9}_{\mathcal{X_n}}(y)$&lt;/li>
&lt;li>Some properties
&lt;ul>
&lt;li>$f_{xy}=\mathbb{P}(V(y)\geqslant 1|\mathcal{X}_0=x)$&lt;/li>
&lt;li>$\mathbb{P}(V(y)=m|\mathcal{X}_ 0=x)=\begin{cases} 1-f_{xy}&amp;amp;:m=0\\f_{xy}f_{yy}^{m-1}(1-f_{yy})&amp;amp;:m\geqslant 1\end{cases}$&lt;/li>
&lt;li>$\mathbb{E}[ V(y)|\mathcal{X}_ 0=x]=\begin{cases} 0&amp;amp;:f_{xy}=0\\\infty&amp;amp;:f_{xy}&amp;gt;0,\ f_{yy}=1\\f_{xy}/(1-f_{yy})&amp;amp;:f_{xy}&amp;gt;0,\ f_{yy}&amp;lt;1\end{cases}$&lt;/li>
&lt;li>$\mathbb{E}[ V(y)|\mathcal{X}_ 0=x]=\sum^\infty_{n=1}P^{(n)}_{xy}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="state-classification">State Classification&lt;/h3>
&lt;blockquote>
&lt;p>Here we usually consider only time-homogeneous Markov Chains&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Accessible&lt;/strong> ($x\to y$): $\exists n\;\text{s.t.}\ P_{xy}^{(n)}&amp;gt;0$
&lt;ul>
&lt;li>$x\to y \Leftrightarrow f_{xy}&amp;gt;0$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Communicate&lt;/strong> ($x\leftrightarrow y$): $x\to y\;\text{and}\;y\to x$. This is a &lt;a class="link" href="https://en.wikipedia.org/wiki/Equivalence_relation" target="_blank" rel="noopener"
>equivalence relation&lt;/a>.&lt;/li>
&lt;li>&lt;strong>Equivalent class&lt;/strong>: set of states that communicate with each other&lt;/li>
&lt;li>&lt;strong>Irreducible&lt;/strong>: a Markov chain with only one communicating class&lt;/li>
&lt;li>&lt;strong>Absorbing/Closed state&lt;/strong>: $P_{xx}=1$
&lt;ul>
&lt;li>&lt;strong>Absorbing class&lt;/strong>: set $C$ is absorbing iff $\forall x \in C, \sum_{y\in C}P_{xy}=1$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Transient state&lt;/strong>: $f_{xx}&amp;lt;1 \Leftrightarrow \mathbb{E}[V_i|\mathcal{X}_0=i]&amp;lt;\infty$&lt;/li>
&lt;li>&lt;strong>Recurrent state&lt;/strong>: $f_{xx}=1 \Leftrightarrow \mathbb{E}[V_i|\mathcal{X}_0=i]=\infty$
&lt;ul>
&lt;li>&lt;strong>Positive recurrent&lt;/strong>: $\mathbb{E}[T_1(x)|\mathcal{X}_0=x]&amp;lt;0$&lt;/li>
&lt;li>&lt;strong>Null recurrent&lt;/strong>: $\mathbb{E}[T_1(x)|\mathcal{X}_0=x]=0$&lt;/li>
&lt;li>These two kind of recurrent states also make up communicating classes&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Some properties
&lt;ul>
&lt;li>If $x$ is (positive) recurrent and $x\to y$, then $y$ is also (positive) recurrent and $f_{xy}=f_{yx}=1$&lt;/li>
&lt;li>Every closed and finite subset of $X$ contains at least one (positive) recurrent state.&lt;/li>
&lt;li>All states of a communicating class are either positive recurrent, null recurrent or transient.&lt;/li>
&lt;li>Method to determine whether class $C$ is recurrent/transient
&lt;ol>
&lt;li>If $C$ is non-closed, it&amp;rsquo;s transient&lt;/li>
&lt;li>If $C$ is closed and finite, then $C$ is positive recurrent&lt;/li>
&lt;li>If $C$ is closed and infinite, then $C$ can be either positive/null recurrent or transient&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>A typical example is the &lt;a class="link" href="https://en.wikipedia.org/wiki/Birth%E2%80%93death_process" target="_blank" rel="noopener"
>birth-death chain&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;strong>Stationary distribution&lt;/strong>: $\bar{\pi}\;\text{s.t.}\;\bar{\pi}=\bar{\pi} P$
&lt;ul>
&lt;li>Stationary in limit form: $\bar{\pi}=\lim_{n\to \infty} \frac{1}{n}\sum^{n-1}_{t=0} \pi_t$
&lt;ul>
&lt;li>This is a &lt;strong>Cesaro&lt;/strong> limit, we use this to deal with the problem that $\lim_{n\to \infty} \pi_t$ might not exist if the chain is periodic.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Reversibility criterion for stationary: $\forall i,j \in S, \pi_i P_{ij} = \pi_j P_{ji}$ (a.k.a detailed balance condition), then the process is reversible and therefore stationary.&lt;/li>
&lt;li>Existence for stationary distribution satisfying $\bar{\pi}=\bar{\pi} P$
&lt;ol>
&lt;li>If the chain has single positive recurrent class, then exists unique solution: $\bar{\pi}(x)=0$ for all transient or null recurrent $x$&lt;/li>
&lt;li>If the chain has multiple positive recurrent class, then there are multiple solutions, for each positive recurrent $x$ we have a $\bar{\pi}^i$.&lt;/li>
&lt;li>If the chain has only transient and null recurrent states, there is no solution$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Convergence of stationary distribution
&lt;ul>
&lt;li>If the chain has positive recurrent class, then $\frac{1}{n}\sum^n_{t=1}\mathbb{P}(\mathcal{X}_t=j)\xrightarrow[n\to \infty]{}\bar{\pi}_j,\;\forall \mu_0$&lt;/li>
&lt;li>(Ergodic) If the chain is positive recurrent, then $\frac{1}{n}\sum^n_{t=1}\unicode{x1D7D9}_{\mathcal{X}_t}(j)\xrightarrow[n\to \infty]{\text{a.s.}}\bar{\pi}_j$&lt;/li>
&lt;li>If the chain is positive recurrent and aperiodic, then $\mathbb{P}(\mathcal{X}_n=j)\xrightarrow[n\to \infty]{}\bar{\pi}_j$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="continuous-time-stochastic-system">Continuous-Time Stochastic System&lt;/h2>
&lt;h3 id="stochastic-sequences-1">Stochastic Sequences&lt;/h3>
&lt;ul>
&lt;li>Definition: Given $t\in\mathbb{T}\subset\mathbb{R}$ a sequence of time, $\mathcal{X}(t,\omega): (\Omega,\mathcal{F},\mathbb{P})\to(\mathbb{R}^n,\mathcal{F}_ \mathcal{X},\mathbb{P}_ \mathcal{X})$ is a continuous stochastic sequence.&lt;/li>
&lt;li>Most definitions are similar to discrete sequence (including &lt;strong>stationarity&lt;/strong>), while the sequence is often defined as $\mathcal{X}(\mathcal{G}),\;\mathcal{G}={t_1,t_2,\ldots,t_N}\subset\mathbb{T}, t_i&amp;lt; t_{i+1}$&lt;/li>
&lt;li>&lt;strong>Stochastic Differential Equation (SDE)&lt;/strong>: $$\mathrm{d}\mathcal{X}(t)=F(\mathcal{X}(t),t)\mathrm{d}t+\sum^r_{i=1}G_i(\mathcal{X}(t),t)\mathrm{d}\mathcal{W}_i(t)$$ Here $\mathcal{W}$ is often a certain kind of &lt;em>noise&lt;/em> random process.&lt;/li>
&lt;/ul>
&lt;h3 id="markov-sequence-1">Markov Sequence&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Markov Sequence&lt;/strong>: A ss. $\mathcal{X}(k)$ is called a (continuous-time) Markov sequence if for the set of times $\mathcal{G}={t_1,t_2,\ldots,t_N}$ with $t_i &amp;lt; t_{i+1}$, we have $$\begin{split}f_ \mathcal{X}\left(t_N;x(t_N)\middle|\mathcal{X}(t_{N-1})=x(t_{N-1}),\mathcal{X}(t_{N-2})=x(t_{N-2}),\ldots\right)\qquad \\ =f_ \mathcal{X}(t_N;x(t_N)|\mathcal{X}(t_{N-1})=x(t_{N-1}))\end{split}$$&lt;/li>
&lt;li>&lt;strong>Hidden Markov Model&lt;/strong>: Definition similar to discrete case. A &lt;strong>continuous-time stochastic state space&lt;/strong> is of the form $$\begin{align}\dot{\mathcal{X}}(t)&amp;amp;=F(\mathcal{X},t)+G(\mathcal{X},t)\mathrm{d}\mathcal{W}(t)/\mathrm{d}t \\ \mathcal{Y}(t)&amp;amp;=H(\mathcal{X},t)+J(\mathcal{X},t)\mathrm{d}\mathcal{W}(t)/\mathrm{d}t\end{align}$$ where $\mathcal{W}$ is usually &lt;a class="link" href="#Wiener-Process" >Wiener process&lt;/a> and white noise $\mathrm{d}\mathcal{W}(t)/\mathrm{d}t$ is often written as $\mathcal{U}(t)$.
&lt;ul>
&lt;li>The state space is &lt;strong>affine&lt;/strong> if $F(\mathcal{X}(t),t)=A(t)\mathcal{X}(t)+u(t),\;N(\mathcal{X},t)=C(t)\mathcal{X}(t)+v(t)$&lt;/li>
&lt;li>The state space is &lt;strong>bilinear&lt;/strong> if $G(\mathcal{X},t)\mathrm{d}\mathcal{W}(t)=\sum^r_{i=1}B_i(t)\mathcal{X}\mathrm{d}\mathcal{W}_i(t)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="poisson-counters">Poisson Counters&lt;/h3>
&lt;ul>
&lt;li>Definition: It&amp;rsquo;s a stochastic Markow process $\mathcal{N}(t)\in\{0,1,2,\ldots\}$ with the characteristic that it jumps up by only one integer at a time.
&lt;ul>
&lt;li>Characteristics: transition times are random, transition step is always 1.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Transition rule: $\frac{\partial}{\partial t}p_\mathcal{N}=\begin{cases}-\lambda p_\mathcal{N}(t;n)&amp;amp;:n=0\\-\lambda p_\mathcal{N}(t;n)+\lambda p_\mathcal{N}(t;n-1)&amp;amp;:n&amp;gt;0\end{cases}$
&lt;ul>
&lt;li>From the rule we can conclude that $p_\mathcal{N}(t;n)=\frac{1}{n!}(\lambda t)^n e^{-\lambda t}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Bidirectional counter: $\mathcal{N}(t)$ is defined as one-directional. $\mathcal{N}_1(t)-\mathcal{N}_2(t)$ is called &lt;strong>bidirectional poisson counter&lt;/strong>.&lt;/li>
&lt;li>Expectation: $\mathbb{E}[\mathcal{N}(t)]=\lambda t,\;\mathbb{E}[\mathrm{d}\mathcal{N}(t)]=\lambda\mathrm{d}t$&lt;/li>
&lt;li>&lt;strong>Ito calculus&lt;/strong> for Poisson counter:
&lt;ul>
&lt;li>&lt;strong>Ito sense&lt;/strong>: $n(t)$ is a realization of poisson counter $\mathcal{N}$. $x(t)$ is a solution in Ito sense to $\mathrm{d}x(t)=F(x(t),t)\mathrm{d}t+G(x(t),t)\mathrm{d}n(t)$ if
&lt;ol>
&lt;li>On all intervals where $n(t)$ is constant, $\dot{x}(t)=F(x(t),t)$&lt;/li>
&lt;li>If $n(t)$ jumps at time $t_1$, $\lim_{t\to t_1^+}x(t)=\lim_{t\to t_1^-}x(t)+G\left(\lim_{t\to t_1^+}x(t),t_1\right)$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>Ito rule&lt;/strong>: Given a fuction $\psi(\mathcal{X}(t),t)$, taking Taylor expansion we have $$\mathrm{d}\psi=\left\{\frac{\partial\psi}{\partial t}+(\nabla_\mathcal{X}\psi)F(\mathcal{X},t)\right\}\mathrm{d}t+\sum^r_{i=1}\left\{\psi\left(\mathcal{X}+G_i(\mathcal{X},t),t\right)-\psi(\mathcal{X},t)\right\}\mathrm{d}\mathcal{N}_i(t)$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="wiener-process">Wiener-Process&lt;/h3>
&lt;ul>
&lt;li>Definition (&lt;strong>Brownian Motion&lt;/strong>): It&amp;rsquo;s a bidirectional poisson counter with infinite rate, i.e. $\mathcal{W}=\lim_{\lambda\to\infty}\frac{1}{\sqrt(\lambda)}(\mathcal{N}_1-\mathcal{N}_2)$ where $\mathcal{N}_1,\;\mathcal{N}_2$ have rate $\lambda/2$
&lt;ul>
&lt;li>Characteristic: It&amp;rsquo;s a Gaussian with zero mean and variance $t$&lt;/li>
&lt;li>Note: Actual Wiener Process has stronger continuity property than Brownian motion.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Expectation: $\mathbb{E}[\mathcal{W}]=\mathbb{E}[d\mathcal{W}]=0,\;\mathbb{E}[\mathcal{W}(\tau)\mathcal{W}(t)]=\mathbb{E}[\mathcal{W}^2(\min\{t,\tau\})]=\min\{t,\tau\}$&lt;/li>
&lt;li>Principle of independent increments: If the interval $[r,t), [\sigma,s)$ don&amp;rsquo;t overlap, then $\mathcal{W}(t)-\mathcal{W}(\tau)$ and $\mathcal{W}(s)-\mathcal{W}(\sigma)$ are uncorrelated.&lt;/li>
&lt;li>&lt;strong>Ito calculus&lt;/strong> for Wiener Process
&lt;ul>
&lt;li>&lt;strong>Ito rule&lt;/strong>: Given a fuction $\psi(\mathcal{X}(t),t)$, taking Taylor expansion we have $$\begin{split}\mathrm{d}\psi=\frac{\partial\psi}{\partial t}\mathrm{d}t+(\nabla_\mathcal{X}\psi)F(\mathcal{X},t)\mathrm{d}t+\sum^r_{i=1}\left(\nabla_\mathcal{X}\psi(\mathcal{X},t)\right)G_i(\mathcal{X},t)\mathrm{d}\mathcal{W}_ i \\ -\sum^r_{i=1}\frac{1}{2}G_i^\top(\mathcal{X},t)\left(\mathrm{H}_\mathcal{X}\psi(\mathcal{X},t)\right)G_i(\mathcal{X},t)\mathrm{d}t\end{split}$$
&lt;blockquote>
&lt;p>Note $\nabla_{\mathcal{X}}=\begin{bmatrix}\frac{\partial\psi}{\partial x_1}&amp;amp;\cdots&amp;amp;\frac{\partial\psi}{\partial x_n}\end{bmatrix}$ and $\mathrm{H}_{\mathcal{X}}=\begin{bmatrix}\frac{\partial^2\psi}{\partial x_1^2}&amp;amp;\cdots&amp;amp;\frac{\partial^2\psi}{\partial x_1\partial x_n}\\ \vdots&amp;amp;\ddots&amp;amp;\vdots \\ \frac{\partial^2\psi}{\partial x_n\partial x_1}&amp;amp;\cdots&amp;amp;\frac{\partial^2\psi}{\partial x_n^2}\end{bmatrix}$ are the gradient and Hessian operator respectively. By default, the operator target is $\mathcal{X}$ is not noted.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>White noise&lt;/strong>: It is Guassian distributed stationary stochastic process with $\mu_\mathcal{U}(t)=0,\;\mathrm{K}_ \mathcal{U}(t,\tau)=\Phi_ \mathcal{U}\delta(t-\tau)$, where $\Phi_\mathcal{U}$ is &lt;strong>spectral intensity&lt;/strong>.
&lt;ul>
&lt;li>We often consider white noise as derivative of Wiener process: $\mathcal{U}\sim\mathrm{d}\mathcal{W}/\mathrm{d}t$.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="linear-stochastic-sequence-1">Linear Stochastic Sequence&lt;/h3>
&lt;p>$$\begin{align}\mathrm{d}\mathcal{X}(t)&amp;amp;=\{A(t)\mathcal{X}(t)+u(t)\}\mathrm{d}t+B(k)\mathrm{d}\mathcal{W}(t) \\ \mathcal{Y}(t)&amp;amp;=C(k)\mathcal{X}(k)+D(k)\mathcal{W}(k)\end{align}$$&lt;/p>
&lt;ul>
&lt;li>Differential equation of expectations
&lt;ul>
&lt;li>&lt;strong>Mean&lt;/strong>: $\frac{\mathrm{d}}{\mathrm{d}t}\mu_\mathcal{X}(t)=A(t)\mu_\mathcal{X}(t)$&lt;/li>
&lt;li>&lt;strong>Autocovariance&lt;/strong>: $\frac{\mathrm{d}}{\mathrm{d}t}S_\mathcal{X}(t)=A(t)S_\mathcal{X}(t)+S_\mathcal{X}A^\top(t)+B(t)B^\top(t)$ (called &lt;strong>Lyapunov differential equation&lt;/strong>)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Stationary LTI case
&lt;ul>
&lt;li>Useful equation: $\mathrm{R}_ \mathcal{X}(t,\tau)=\begin{cases} S_ \mathcal{X}\exp\{A^\top(\tau-t)\}&amp;amp;,\tau&amp;gt;t \\ \exp\{A(t-\tau)\}S_ \mathcal{X}&amp;amp;,\tau&amp;lt; t\end{cases}$&lt;/li>
&lt;li>&lt;strong>(Continuous-time) algerbraic Lyapunov equation&lt;/strong>: $A\bar{S}_ \mathcal{X}+\bar{S}_\mathcal{X}A^\top+BB^\top=0$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="nonlinear-stochastic-sequence">Nonlinear Stochastic Sequence&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>The Fokker-Planck Equation (FPE)&lt;/strong>: Consider the Wiener-process excited general SDE, we have $$\frac{\partial f_ \mathcal{X}(x;t)}{\partial t}=-\nabla\left(F(\mathcal{X})f_ \mathcal{X}(x;t)\right)+\frac{1}{2}\mathrm{tr}\left[\mathrm{H}\left(\Gamma(\mathcal{X})f_ \mathcal{X}(x;t)\right)\right]$$
here $\Gamma(\mathcal{X})=G(\mathcal{X})G^\top(\mathcal{X})$&lt;/p>
&lt;ul>
&lt;li>$\mathcal{X}$ is stationary means $\frac{\partial f_\mathcal{X}(x;t)}{\partial t}=0$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Gaussian closure&lt;/strong>: An approximate solution for FPE is supposing $f_\mathcal{X}$ as multivariate Gaussian with some $S_\mathcal{X}$ and ${\mu_\mathcal{X}}$. That is we focus on 1st-order and 2nd-order estimation. Denote the estimated distribution as $\hat{f}_\mathcal{X}(x;t)$&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Estimated expectation&lt;/strong>: $\hat{\mathbb{E}}\phi(\mathcal{X})=\int\cdots\int\phi(x)\hat{f}_\mathcal{X}(x;t)\mathrm{d}x$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Solution:$$\begin{split}h(x,t)=-\nabla F+\tilde{x}^\top S^{-1}F-(\nabla\Gamma)S^{-1}\tilde{x}+\frac{1}{2}\mathrm{tr}\left[\mathrm{H}\Gamma \right] \\ +\frac{1}{2}\mathrm{tr}\left[\left(-S^{-1}+S^{-1}\tilde{x}\tilde{x}^\top S^{-1}\right)\Gamma\right]\end{split}$$ and $$\begin{align}\dot{\mu}_\mathcal{X}(t)&amp;amp;=S(t)\hat{\mathbb{E}}[\nabla^\top h(x)] \\ \dot{S} _\mathcal{X}(t)&amp;amp;=S(t)\hat{\mathbb{E}}[\mathrm{H} h(x)]S(t)\end{align}$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Useful simplification: If $G$ is constant (independent of $\mathcal{X}$), then the ODE of $\dot{\mu}$ and $\dot{S}$ become $$\begin{align}\dot{\mu}_ \mathcal{X}(t)&amp;amp;=\hat{\mathbb{E}}[F(\mathcal{X})] \\ \dot{S} _\mathcal{X}(t)&amp;amp;=\hat{A}^\top S+S\hat{A}+\Gamma\end{align}$$ where $\hat{A}=\hat{\mathbb{E}}\left[\frac{\partial F}{\partial\mathcal{X}}\right]$&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Equivalent linearization (Quasi-linearization)&lt;/strong>: In the estimation, $\mathcal{X}$ evolves equivalently to $\mathrm{d}\mathcal{X}(t)=\hat{A}(t)\mathcal{X}(t)\mathrm{d}t+G\mathrm{d}\mathcal{W}(t)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>No guaranteed bounds generally exist for the estimation error $$e(\mathcal{X},t)=\left(\frac{\partial\hat{f}_ \mathcal{X}}{\partial t}\right)-\left(-\nabla\left(F\hat{f}_ \mathcal{X}\right)+\frac{1}{2}\mathrm{tr}\left[\mathrm{H}\left(\Gamma\hat{f}_ \mathcal{X}\right)\right]\right)$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Estimation of $\mu(t),\;S(t)$ could also have error. And stationarity may not be the consistent between original system and estimated system&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="spectral-analysis">Spectral Analysis&lt;/h2>
&lt;blockquote>
&lt;p>Content in this section comes from MECHENG 549&lt;/p>
&lt;/blockquote>
&lt;h3 id="power-spectral-density">Power Spectral Density&lt;/h3>
&lt;ul>
&lt;li>Definition: The &lt;strong>power spectral density (PSD)&lt;/strong> of stochastic process $\mathcal{Y}$ is $$\Phi_\mathcal{Y}(\omega)\equiv\mathbb{E}\left[ \lim_{T\to\infty}\frac{1}{2T}Y_T(\omega)Y_T^\top(\omega)\right]$$ where $Y_T(\omega)$ is the Fourier transform of centered process $\mathcal{Y}(t)$.
&lt;ul>
&lt;li>White noise has the same PSD for whatever $\omega$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Wiener-Khinchin&lt;/strong> Theorem: $\Phi_\mathcal{Y}(\omega)=\int^\infty_{-\infty}e^{-i\omega\theta}\bar{\mathrm{K}}_\mathcal{Y}(\theta)\mathrm{d}\theta$&lt;/li>
&lt;li>&lt;strong>Signal propagation&lt;/strong>: If the system $P$ has input $\mathcal{Y}$ and output $\mathcal{Z}$, then $\Phi_\mathcal{Z}=|P(i\omega)\Phi_\mathcal{Y}(\omega)P^\top(i\omega)|$ where $P(i\omega)$ is the Fourier transform of the system.&lt;/li>
&lt;li>&lt;strong>Cross spectrum&lt;/strong>: If the system $G$ has input $\mathcal{Y}$ and output $\mathcal{Z}$, then the cross-spectrum is $\Phi_{\mathcal{ZY}}(\omega)=G(i\omega)\Phi_\mathcal{Y}(\omega)$&lt;/li>
&lt;/ul>
&lt;h3 id="periodograms">Periodograms&lt;/h3>
&lt;ul>
&lt;li>Definition: We want to estimate $\Phi_{\mathcal{Y}}(\omega)$ without the expectation, then we use $$Q_T(\omega,y)=\frac{1}{2T}\left|\int^T_{-T}e^{-i\omega t}y(t)\mathrm{d}t\right|^2$$ where $y$ is a sample realization of $\mathcal{Y}$.
&lt;ul>
&lt;li>We also use window functions to calculate the spectrum over a finite time interval (using &lt;strong>Bartlett&amp;rsquo;s procedure&lt;/strong>)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="stochastic-realizations">Stochastic realizations&lt;/h3>
&lt;p>We want to find a stochastic model which gives the same spectrum given. This is called &lt;strong>stochastic realization problem&lt;/strong>. We focus on scalar LTI case.&lt;/p>
&lt;p>If we know $P(s)=C[sI-A]^{-1}B+D$ (the Laplace transform of LTI system) and $P(s)=c\frac{\prod^m_{k=1}(s-z_k)}{\prod^n_{k=1}(s-p_k)}$ for some $m\leq n$ and real constant $c$. Then we have $$\Phi_\mathcal{Y}(\omega)=P(i\omega)P(-i\omega)=c^2\frac{\prod^m_{k=1}(\omega^2+z_k^2)}{\prod^n_{k=1}(\omega^2+p_k^2)}$$&lt;/p>
&lt;p>Lemma: For any valid PSD, there exists a spectral factorization $\Phi_\mathcal{Y}(\omega)=\frac{\sum^m_{k=0}a_k(\omega^2)^k}{\sum^n_{k=0}b_k(\omega^2)^k}=P(i\omega)P(-i\omega)$ where $P(s)$ is an asymptotically stable n-th order transfer function, iff&lt;/p>
&lt;ol>
&lt;li>$\Phi_\mathcal{Y}(\omega)$ is a ratio of polynomials of $\omega^2$&lt;/li>
&lt;li>All coefficients $a_k$ and $b_k$ are real&lt;/li>
&lt;li>The denominator has no positive real roots in $\omega^2$&lt;/li>
&lt;li>Any positive real roots in the numerator have even multiplicity&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;h2 id="notes-for-math-typing-in-hexo">Notes for math typing in Hexo&lt;/h2>
&lt;ol>
&lt;li>Escape &lt;code>\&lt;/code> by &lt;code>\\&lt;/code>. Especially escape &lt;code>{&lt;/code> by &lt;code>\\{&lt;/code> instead of &lt;code>\{&lt;/code>, and escape &lt;code>\\&lt;/code> by &lt;code>\\\\&lt;/code>.&lt;/li>
&lt;li>Be careful about &lt;code>_&lt;/code>, it&amp;rsquo;s used in markdown as italic indicator. Add space after &lt;code>_&lt;/code> is a useful solution.&lt;/li>
&lt;li>&lt;a class="link" href="https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference" target="_blank" rel="noopener"
>Some useful Mathjax tricks at StackExchange&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.overleaf.com/learn/latex/List_of_Greek_letters_and_math_symbols" target="_blank" rel="noopener"
>Several capital Greek characters should directly use its related Latin alphabet with &lt;code>\mathrm&lt;/code> command&lt;/a>.&lt;/li>
&lt;/ol>
&lt;p>Although I have migrated to Hugo, some tricks might still be relevant.&lt;/p>
&lt;/blockquote></description></item><item><title>Notes for Probability Theory (Basics)</title><link>https://zyxin.xyz/blog/en/2019-02/ProbabilityNotes/</link><pubDate>Sat, 23 Feb 2019 00:00:00 +0000</pubDate><guid>https://zyxin.xyz/blog/en/2019-02/ProbabilityNotes/</guid><description>&lt;h2 id="probability-space">Probability Space&lt;/h2>
&lt;ul>
&lt;li>Notation: $(\Omega,\mathcal{F},\mathbb{P})$
&lt;ul>
&lt;li>$\Omega$: &lt;strong>Sample space&lt;/strong>&lt;/li>
&lt;li>$\mathcal{F}$: &lt;strong>Event space&lt;/strong>. Required to be &lt;a class="link" href="http://mathworld.wolfram.com/Sigma-Algebra.html" target="_blank" rel="noopener"
>&lt;strong>σ-algebra&lt;/strong>&lt;/a>. We often use &lt;a class="link" href="http://mathworld.wolfram.com/BorelSigma-Algebra.html" target="_blank" rel="noopener"
>&lt;strong>Borel σ-algebra&lt;/strong>&lt;/a> for continuous $\Omega$).
&lt;ul>
&lt;li>Axioms for &lt;strong>σ-algebra&lt;/strong>
&lt;ol>
&lt;li>$\mathcal{F}$ is non-empty&lt;/li>
&lt;li>$A\in\mathcal{F} \Rightarrow A^C\in\mathcal{F}$ (closed under complement)&lt;/li>
&lt;li>$A_i \in\mathcal{F} \Rightarrow \bigcup^\infty_{k=1} A_k \in \mathcal{F}$ (closed under countable union)&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>For continuous case considering interval $\Omega=[a,b]$, $\mathcal{F}_0$ is the set of all subintervals of $\Omega$. Then its &lt;strong>Borel σ-algebra&lt;/strong> is the smallest σ-algebra that contains $\mathcal{F}_0$. Here the $\mathcal{F}_0$ is a &lt;a class="link" href="https://en.wikibooks.org/wiki/Measure_Theory/Basic_Structures_And_Definitions/Semialgebras,_Algebras_and_%CF%83-algebras" target="_blank" rel="noopener"
>&lt;strong>semialgebra&lt;/strong>&lt;/a>. We can find a containing σ-algebra for every semialgebra.&lt;/li>
&lt;li>Axioms for &lt;strong>semialgebra&lt;/strong>
&lt;ol>
&lt;li>$\emptyset, \Omega \in \mathcal{F}$&lt;/li>
&lt;li>$A_i \in\mathcal{F} \Rightarrow \bigcap^n_{k=1} A_k \in \mathcal{F}$ (closed under finite intersections)&lt;/li>
&lt;li>$\forall B \in\mathcal{F}, B^C=\bigcup^n_{k=1} A_k$ where $A_i \in \mathcal{F}$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>$\mathbb{P}$: &lt;strong>Probability measure&lt;/strong>.
&lt;ul>
&lt;li>Axioms for probability measure
&lt;ol>
&lt;li>$\mathbb{P}(\Omega) = 1$&lt;/li>
&lt;li>$\forall A\in\mathcal{F}, \mathbb{P}(A) \geqslant 0$&lt;/li>
&lt;li>$A_i, A_j \in\mathcal{F}$ are pairwise disjoint, then $\mathbb{P}(\bigcup^\infty_{k=1}A_k)=\sum^\infty_{k=1}\mathbb{P}(A_k)$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Product Space&lt;/strong>: Probability spaces can be combined using Cartesian product.&lt;/li>
&lt;li>&lt;strong>Independence&lt;/strong>: $\mathbb{P}(A_{k_1}\cap A_{k_2}\cap &amp;hellip;\cap A_{k_l})=\prod_{i=1}^l \mathbb{P}(A_i),\;\forall \{k_i\}_1^l\subset\{ 1..n\}$&lt;/li>
&lt;li>&lt;strong>Conditional probability&lt;/strong>: $\mathbb{P}\left(A_i \middle| A_j\right)=\mathbb{P}(A_i\cap A_j)/\mathbb{P}(A_j)$&lt;/li>
&lt;li>&lt;strong>Total probability&lt;/strong>: $\mathbb{P}(B)=\sum_{i=1}^n \mathbb{P}(B\cap A_i)=\sum_{i=1}^n \mathbb{P}\left(B\middle| A_i\right)\mathbb{P}(A_i)$, where $\{A_1,\cdots,A_n\}$ are disjoint and partition of $\Omega$.&lt;/li>
&lt;li>&lt;strong>Bayes&amp;rsquo; Rule&lt;/strong>: $$\mathbb{P}(A_j|B)=\frac{\mathbb{P}(B|A_j)\mathbb{P}(A_j)}{\sum^n_{i=1} \mathbb{P}(B|A_i)\mathbb{P}(A_i)}$$
&lt;ul>
&lt;li>Priori: $\mathbb{P}(B|A_j)$&lt;/li>
&lt;li>Posteriori: $\mathbb{P}(A_j|B)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="random-variables">Random Variables&lt;/h2>
&lt;blockquote>
&lt;p>Note: The equations are written in continuous case by default, one can get the equation for discrete case by changing integration into summation and changing differential into difference.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>Random Variable $\mathcal{X}$ is a mapping $\mathcal{X}: (\Omega,\mathcal{F},\mathbb{P})\to(\Omega_ \mathcal{X},\mathcal{F}_ \mathcal{X},\mathbb{P}_ \mathcal{X})$&lt;/li>
&lt;li>&lt;strong>Continuous &amp;amp; Discrete &amp;amp; Mixed Random Variable&lt;/strong>:
&lt;ul>
&lt;li>Can be defined upon whether $\Omega_\mathcal{X}$ is continuous&lt;/li>
&lt;li>Can be defined upon whether we can find continuous density function $f_\mathcal{X}$&lt;/li>
&lt;li>$\mathcal{F}_\mathcal{X}$ for continuous $\mathcal{X}$ is a &lt;strong>Borel σ-field&lt;/strong>&lt;/li>
&lt;li>All three kinds of random variables can be expressed by CDF or &amp;ldquo;extended&amp;rdquo; PDF with Dirac function and Lebesgue integration.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Scalar Random Variable&lt;/strong>: $\mathcal{X}: \Omega\to\mathbb{F}$
&lt;ul>
&lt;li>Formal definition: $\mathcal{X}: (\Omega,\mathcal{F},\mathbb{P})\to(\Omega_ \mathcal{X}\subset\mathbb{F},\mathcal{F}_ \mathcal{X}\subset\left\{\omega\middle| \omega\subset\Omega_ \mathcal{X}\right\},\mathbb{P}_ \mathcal{X}:\mathcal{F}_ \mathcal{X}\to[0,1])$&lt;/li>
&lt;li>&lt;strong>Cumulative Distribution Function&lt;/strong> (CDF): $F_ \mathcal{X}(x)=\mathbb{P}_\mathcal{X}(\mathcal{X}(\omega)\leqslant x)$&lt;/li>
&lt;li>&lt;strong>Probability Mass Function&lt;/strong> (PMF): $p_ \mathcal{X}(x)=\mathbb{P}_\mathcal{X}(\mathcal{X}(\omega)=x)$&lt;/li>
&lt;li>&lt;strong>Probability Density Function&lt;/strong> (PDF): $$f_ \mathcal{X}(x)=\mathbb{P}_ \mathcal{X}(x&amp;lt; \mathcal{X}(\omega)\leqslant x+ \mathrm{d}x)=\mathrm{d}F_ \mathcal{X}(x)/ \mathrm{d}x$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Vector Random Variable&lt;/strong> (Multiple Random Variables): $\mathcal{X}: \Omega\to\mathbb{F}^n$
&lt;ul>
&lt;li>Formal definition $\mathcal{X}: (\Omega,\mathcal{F},\mathbb{P})\to(\Omega_ \mathcal{X}\subset\mathbb{F}^n,\mathcal{F}_ \mathcal{X}\subset\left\{\omega\middle| \omega\subset\Omega_ \mathcal{X}\right\},\mathbb{P}_ \mathcal{X}:\mathcal{F}_ \mathcal{X}\to[0,1])$&lt;/li>
&lt;li>&lt;strong>Cumulative Distribution Function&lt;/strong> (CDF): $F_ \mathcal{X}(x)=\mathbb{P}_\mathcal{X}(\{\mathcal{X}(\omega)\}_i\leqslant x_i), i=1\ldots n$&lt;/li>
&lt;li>&lt;strong>Probability Mass Function&lt;/strong> (PMF): $p_ \mathcal{X}(x)=\mathbb{P}_\mathcal{X}(\{\mathcal{X}(\omega)\}_i=x)$&lt;/li>
&lt;li>&lt;strong>Probability Density Function&lt;/strong> (PDF): $$f_ \mathcal{X}(x)=\mathbb{P}_ \mathcal{X}(x_i&amp;lt; \{\mathcal{X}(\omega)\}_ i\leqslant x_i+ \mathrm{d}x_i)=\frac{\partial^n}{\partial x_1\partial x_2\ldots\partial x_n}F_ \mathcal{X}(x)$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Afterwards, we don&amp;rsquo;t distinguish $\mathbb{P}_\mathcal{X}$ with $\mathbb{P}$ if there&amp;rsquo;s no ambiguity.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Independence&lt;/strong>($\perp$ or $\perp$ with double vertical lines): $\mathbb{P}(\mathcal{X}\in A\cap \mathcal{Y}\in B)=\mathbb{P}(\mathcal{X}\in A)\mathbb{P}(\mathcal{Y}\in B)$
&lt;ul>
&lt;li>&lt;strong>Independent CDF&lt;/strong>:$F_{\mathcal{XY}}(x,y)=F_\mathcal{X}(x)F_\mathcal{Y}(y)$ iff $\mathcal{X}\perp\mathcal{Y}$&lt;/li>
&lt;li>&lt;strong>Independent PMF&lt;/strong>:$p_{\mathcal{XY}}(x,y)=p_\mathcal{X}(x)p_\mathcal{Y}(y)$ iff $\mathcal{X}\perp\mathcal{Y}$&lt;/li>
&lt;li>&lt;strong>Independent PDF&lt;/strong>:$f_{\mathcal{XY}}(x,y)=f_\mathcal{X}(x)f_\mathcal{Y}(y)$ iff $\mathcal{X}\perp\mathcal{Y}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Marginalization&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>Marginal distribution function&lt;/strong>: $F_{1:m}(x_{1:m})\equiv F\left(\begin{bmatrix}x_1&amp;amp;x_2&amp;amp;\ldots&amp;amp;x_m&amp;amp;\infty&amp;amp;\ldots&amp;amp;\infty\end{bmatrix}^\top\right)$&lt;/li>
&lt;li>&lt;strong>Marginal density function&lt;/strong>: $f_{1:m}(x_{1:m})=\int^\infty_{-\infty}\cdots\int^\infty_{-\infty} f(x) \mathrm{d}x_{m+1}\cdots \mathrm{d}x_n$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Conditional&lt;/strong> (on event $\mathcal{E}$)
&lt;ul>
&lt;li>&lt;strong>Conditional probability&lt;/strong>: $\mathbb{P}(\mathcal{X}\in\mathcal{D}|\mathcal{E})=\mathbb{P}(\{\omega|\mathcal{X}(\omega)\in\mathcal{D}\}\cap \mathcal{E})/\mathbb{P}(\mathcal{E})$ on a event $\mathcal{E}\in\mathcal{F}$ and a set $\mathcal{D}\subset\mathcal{F}$.&lt;/li>
&lt;li>&lt;strong>Conditional CDF&lt;/strong>: $F_ \mathcal{X}(x|\mathcal{E})=\mathbb{P}(\mathcal{X}_i\leqslant x_i|\mathcal{E})$&lt;/li>
&lt;li>&lt;strong>Conditional PMF&lt;/strong>: $p_ \mathcal{X}(x|\mathcal{E})=\mathbb{P}(\mathcal{X}_i=x_i|\mathcal{E})$&lt;/li>
&lt;li>&lt;strong>Conditional PDF&lt;/strong>: $f_ \mathcal{X}(x|\mathcal{E})=\frac{\partial^n}{\partial x_1\partial x_2\ldots\partial x_n}F_ \mathcal{X}(x|\mathcal{E})$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Conditional&lt;/strong> (on variable $\mathcal{Y}$)
&lt;ul>
&lt;li>&lt;strong>Conditional probability&lt;/strong>: $$\mathbb{P}(\mathcal{X}\in\mathcal{D}_1|\mathcal{Y}\in\mathcal{D}_2)=\mathbb{P}(\{\omega|\mathcal{X}(\omega)\in\mathcal{D}_1,\mathcal{Y}(\omega)\in\mathcal{D}_2\})/\mathbb{P}(\mathcal{Y}(\omega)\in\mathcal{D}_2)$$&lt;/li>
&lt;li>&lt;strong>Conditional PDF&lt;/strong> (similar for PMF): $$f_{\mathcal{X}|\mathcal{Y}\in\mathcal{D}}(x)=\frac{\int_{\mathcal{Y}\in\mathcal{D}}f_{\mathcal{XY}}(x,y)}{\int_{\mathcal{Y}\in\mathcal{D}}f_{\mathcal{Y}}(y)},\;f_{\mathcal{X}|\mathcal{Y}}(x|y)=f_{\mathcal{X}|\mathcal{Y}=y}(x)=\frac{f_{\mathcal{XY}}(x,y)}{f_{\mathcal{Y}}(y)}$$
&lt;ul>
&lt;li>Using total probability we have $f_ \mathcal{X}(x)=\int f_{\mathcal{X}|\mathcal{Y}}(x|y)f_ \mathcal{Y}(y)\mathrm{d}y$. This can be further integrated into Bayes&amp;rsquo; rule.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Conditional CDF&lt;/strong>: Can be defined similarly, of defined as $F_{\mathcal{X}|\mathcal{Y}\in\mathcal{D}}(x)\equiv\int f_{\mathcal{X}|\mathcal{Y}\in\mathcal{D}}(x)\mathrm{d}x$
&lt;ul>
&lt;li>Similarly we have $F_ \mathcal{X}(x)=\int F_{\mathcal{X}|\mathcal{Y}}(x|y)f_ \mathcal{Y}(y)\mathrm{d}y$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Substitution law&lt;/strong>: $\mathbb{P}((\mathcal{X},\mathcal{Y})\in \mathcal{D}|\mathcal{X}=x)=\mathbb{P}((x,\mathcal{Y})\in \mathcal{D})$
&lt;ul>
&lt;li>Common usage: Suppose $\mathcal{Z}=\psi(\mathcal{X},\mathcal{Y})$, then $p_\mathcal{Z}(z)=\int_x \mathbb{P}\left(\psi(x,\mathcal{Y})=z\right)p_\mathcal{X}(x)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="uncertainty-propagation">Uncertainty Propagation&lt;/h2>
&lt;p>Suppose $\mathcal{Y}=\psi(\mathcal{X})$ or specifically $y=\psi(x_1)=\psi(x_2)=\cdots=\psi(x_K)$&lt;/p>
&lt;ul>
&lt;li>Scalar case: $$f_ \mathcal{Y}(y)=\sum^K_{k=1} f_ \mathcal{X}(\psi^{-1}_ k(y))\left| \frac{\partial\psi}{\partial x}\biggr|_{x=\psi^{-1}_k(y)} \right|^{-1}$$&lt;/li>
&lt;li>Vector case: $$f_ \mathcal{Y}(y)=\sum^K_{k=1} f_ \mathcal{X}(\psi^{-1}_ k(y))\left|\det(J)\right|^{-1},\text{where Jacobian }J=\frac{\partial\psi}{\partial x}\biggr|_{x=\psi^{-1}_k(y)}$$&lt;/li>
&lt;li>Trivial Case — Summation: $\mathcal{Y}=\mathcal{X}_ 1+\mathcal{X}_ 2$, then $f_ \mathcal{Y}=\int^\infty_{-\infty}f_ {\mathcal{X}_ 1\mathcal{X}_ 2}(x_1,x_2-x_1) \mathrm{d}x_1$
Another way is to use the method of choice: $F_ \mathcal{Y}(y)=\int_ {\psi(x)\leq y}f_\mathcal{X}(x)\mathrm{d}x$&lt;/li>
&lt;/ul>
&lt;h2 id="expectation--moments">Expectation &amp;amp; Moments&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Expectation&lt;/strong>: $\mathbb{E}_ \mathcal{X}[\psi(\mathcal{X})]=\int^\infty_\infty\cdots\int^\infty_\infty\psi(x)f_ \mathcal{X}(x) \mathrm{d}x_1\ldots \mathrm{d}x_n$&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>A more rigorous way to define the expectation is $\mathbb{E}_ \mathcal{X}[\psi(\mathcal{X})]=\int \psi(\mathcal{X})dF_ \mathcal{X}(\mathcal{X})$. This definition uses Lebesgue Integral and works on both discrete and continuous (or even mixed) variables. See &lt;a class="link" href="http://www.randomservices.org/random/dist/Integral.html" target="_blank" rel="noopener"
>this post&lt;/a> and &lt;a class="link" href="" >this discussion&lt;/a> for more information.
We write $\mathbb{E}_ \mathcal{X}$ as $\mathbb{E}$ if there&amp;rsquo;s no ambiguity (when only one random variable is included). And $\mathbb{E}[\mathcal{X}]$ will be abbreviated as $\mathbb{E}\mathcal{X}$ afterwards.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;strong>Linearity of expectation&lt;/strong>: $\mathbb{E}[A\mathcal{\mathcal{X}}]=A\mathbb{E}[\mathcal{X}] (\forall \mathcal{X},\mathcal{Y},\forall A\in\mathbb{R}^{m\times n})$&lt;/li>
&lt;li>&lt;strong>Independent expectation&lt;/strong>: $\mathbb{E}[\prod^n_i\mathcal{X}_i]=\prod^n_i\left(\mathbb{E}\mathcal{X}_i\right)(\forall\;\text{indep.}\;\mathcal{X}_i)$&lt;/li>
&lt;li>&lt;strong>Conditional expectation&lt;/strong>: $\mathbb{E}[\mathcal{Y}|\mathcal{X}=x]=\int^\infty_{-\infty}yf_{\mathcal{Y}|\mathcal{X}}(y|x)\mathrm{d}y$
&lt;ul>
&lt;li>&lt;strong>Total expectation/Smoothing&lt;/strong>: $\mathbb{E}_ \mathcal{X}[\mathbb{E}_ {\mathcal{Y}|\mathcal{X}}(\mathcal{Y}|\mathcal{X})]=\mathbb{E}\mathcal{Y}$&lt;/li>
&lt;li>&lt;strong>Substitution law&lt;/strong>: $\mathbb{E}[g(\mathcal{X},\mathcal{Y})|\mathcal{X}=x]=\mathbb{E}[\psi(x,\mathcal{Y})|\mathcal{X}=x]$&lt;/li>
&lt;li>$\mathbb{E}[\psi(\mathcal{X})|\mathcal{X}]=\psi(\mathcal{X})$&lt;/li>
&lt;li>$\mathbb{E}[\psi(\mathcal{X})\mathcal{Y}|\mathcal{X}]=\psi(\mathcal{X})\mathbb{E}(\mathcal{Y}|\mathcal{X})$&lt;/li>
&lt;li>&lt;strong>Towering&lt;/strong>: $\mathbb{E}_ {\mathcal{Y}|\mathcal{Z}}[\mathbb{E}_ \mathcal{X}(\mathcal{X}|\mathcal{Y},\mathcal{Z})|\mathcal{Z}]=\mathbb{E}_ {\mathcal{X}|\mathcal{Z}}[\mathcal{X}|\mathcal{Z}]$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Moment&lt;/strong> (p-th order): $\mu_p(\mathcal{X})=\mathbb{E}[\mathcal{X}^p]$
&lt;ul>
&lt;li>&lt;strong>Mean&lt;/strong>: $\mu_ \mathcal{X}=\mathbb{E}[\mathcal{X}]$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Central moment&lt;/strong> (p-th order): $\mathbb{E}[(\mathcal{X}-\mu_ \mathcal{X})^p]$
&lt;ul>
&lt;li>&lt;strong>Variance&lt;/strong>: $\sigma_ \mathcal{X}^2=\mathbb{E}[(\mathcal{X}-\mu_ \mathcal{X})^2]$, sometimes written as $\sigma_ \mathcal{X}^2=\mathbb{V}(\mathcal{X})$&lt;/li>
&lt;li>&lt;strong>Skewness&lt;/strong>: $\tilde{\mu}_ 3=\mathbb{E}[(\mathcal{X}-\mu_ \mathcal{X})^3]/\sigma_\mathcal{X}^3$ ($\tilde{\mu}_ 3&amp;gt;0$ right-skewed, $\tilde{\mu}_ 3&amp;lt;0$ left-skewed)&lt;/li>
&lt;li>&lt;strong>Kurtosis&lt;/strong>: $\tilde{\mu}_ 4=\mathbb{E}[(\mathcal{X}-\mu_ \mathcal{X})^4]/\sigma_\mathcal{X}^4$&lt;/li>
&lt;li>&lt;strong>Excessive Kurtosis&lt;/strong>: $\gamma = \mu_ 4-3$
&lt;img src="https://zyxin.xyz/blog/blog/en/2019-02/ProbabilityNotes/skewness_and_kurtosis.jpg"
width="1266"
height="237"
loading="lazy"
alt="Skewness and Kurtosis"
class="gallery-image"
data-flex-grow="534"
data-flex-basis="1282px"
>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Correlation&lt;/strong>: $\text{corr}(\mathcal{X}_i,\mathcal{X}_j)=\mathbb{E}[\mathcal{X}_i\mathcal{X}_j]$
&lt;ul>
&lt;li>&lt;strong>Correlation matrix&lt;/strong>: $C=\mathbb{E}[\mathcal{X}_i\mathcal{X}_j^\top]$&lt;/li>
&lt;li>&lt;strong>Correlation coefficient&lt;/strong>: $\rho(\mathcal{X}_ i,\mathcal{X}_ j)=\frac{\text{corr}(\mathcal{X}_ i,\mathcal{X}_ j)}{\sigma_{\mathcal{X}_ i}^2\sigma_{\mathcal{X}_ j}^2}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Covariance&lt;/strong>: $\text{cov}(\mathcal{X}_i,\mathcal{X}_j)=\mathbb{E}[(\mathcal{X}_i-\mu_i)(\mathcal{X}_j-\mu_j)]$
&lt;ul>
&lt;li>&lt;strong>Covariance matrix&lt;/strong>: $S=\mathbb{E}\left[(\mathcal{X}-\mu_ \mathcal{X})(\mathcal{X}-\mu_ \mathcal{X})^\top\right]$&lt;/li>
&lt;li>Properties: $\text{cov}(\mathcal{X}+c)=\text{cov}(\mathcal{X}), \text{cov}(A\mathcal{X},\mathcal{X}B)=A\text{cov}(\mathcal{X})+\text{cov}(\mathcal{X})B^\top$&lt;/li>
&lt;li>&lt;strong>Uncorrelated&lt;/strong>: $\rho(\mathcal{X}_ i,\mathcal{X}_ j)=0 \Leftrightarrow\text{cov}(\mathcal{X}_i,\mathcal{X}_j)=0\Leftrightarrow\text{corr}(\mathcal{X}_i,\mathcal{X}_j)=\mathbb{E}[\mathcal{X}_i]\mathbb{E}[\mathcal{X}_j]$ (uncorrelated is necessary for independent)&lt;/li>
&lt;li>Cases where uncorrelated implies independence: (1) Jointly Gaussian (2) Bernoulli&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Centered variable&lt;/strong>: $\tilde{\mathcal{X}}=\mathcal{X}-\mu_ \mathcal{X}$&lt;/li>
&lt;/ul>
&lt;h2 id="transform-methods">Transform methods&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Probability generating function&lt;/strong>(PGF): similiar to Z-tranform $$G_ \mathcal{X}(z)\equiv\mathbb{E}[z^\mathcal{X}]=\sum_{x_i}z^{x_i}p_ \mathcal{X}(x_i)$$
&lt;ul>
&lt;li>For $\mathcal{F}_ \mathcal{X}=\mathbb{N}$, we have $$\frac{\mathrm{d}^k}{\mathrm{d}z^k}G_ \mathcal{X}(z)\Biggr|_{z=1}=\mathbb{E}[\mathcal{X}(\mathcal{X}-1)\cdots(\mathcal{X}-(k-1))]$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Moment generating function&lt;/strong>(MGF): similar to Laplace transform $$M_ \mathcal{X}(z)\equiv\mathbb{E}[e^{s\mathcal{X}}]=\int^\infty_{-\infty}e^{sx}f_ \mathcal{X}(x)\mathrm{d}x$$
&lt;ul>
&lt;li>Generally we have $$\frac{\mathrm{d}^k}{\mathrm{d}s^k}M_ \mathcal{X}(s)\Biggr|_{s=0}=\mathbb{E}[\mathcal{X}^k]$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Characteristic function&lt;/strong>(CF): similar to Fourier transform $$\phi_ \mathcal{X}(\omega)\equiv\mathbb{E}[e^{j\omega \mathcal{X}}]=\int^\infty_{-\infty}e^{j\omega x}f_ \mathcal{X}(x)\mathrm{d}x$$
&lt;ul>
&lt;li>Generally we have $$\frac{\mathrm{d}^k}{\mathrm{d}\omega^k}\phi_ \mathcal{X}(\omega)\Biggr|_{\omega=0}=j^k\mathbb{E}[\mathcal{X}^k]$$&lt;/li>
&lt;li>&lt;strong>Independent&lt;/strong>: $\mathcal{X}\perp \!\!\! \perp\mathcal{Y}$ iff. $\phi_ \mathcal{XY}(\omega)=\phi_ \mathcal{X}(\omega)\phi_ \mathcal{Y}(\omega)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Joint characteristic function&lt;/strong>: for vector case $\mathcal{X}\in\mathbb{R}^n$, we define vector $u$ and $$\phi_ \mathcal{X}(u)\equiv\mathbb{E}[e^{ju^\top \mathcal{X}}]$$
&lt;ul>
&lt;li>Trivial usage: if $\mathcal{Y}=\mathcal{X}_ 1+\mathcal{X}_ 2$, then $\phi_ \mathcal{Y}(u)=\phi_{\mathcal{X}_ 1}(u)\phi_{\mathcal{X}_ 2}(u)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="common-distributions1">Common distributions&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/h2>
&lt;ul>
&lt;li>&lt;em>&lt;strong>Bernoulli&lt;/strong>&lt;/em>: $\Omega_\mathcal{X}=\{0,1\}$ $$p_\mathcal{X}(1)=p,\;p_\mathcal{X}(0)=q=1-p$$
&lt;ul>
&lt;li>$\mu_\mathcal{X}=p,\;\sigma^2_\mathcal{X}=pq,\;G_\mathcal{X}(z)=q+pz$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Binomial&lt;/strong>&lt;/em> - $\mathcal{B}(n,p)$: $\Omega_\mathcal{X}=\{0,1,\ldots,n\}$ $$p_\mathcal{X}(k)=\binom{n}{k}p^k(1-p)^{n-k}$$
&lt;ul>
&lt;li>$\mu_\mathcal{X}=np,\;\sigma^2_\mathcal{X}=np(1-p),\;G_\mathcal{X}(z)=(1-p+pe^z)^n$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Multinomial&lt;/strong>&lt;/em>: $\Omega_\mathcal{X}=\{0,1,\ldots,n\}^k$ $$p_\mathcal{X}(x)=\binom{n}{x_1!\cdots x_k!}p_1^{x_1}\cdots p_k^{x_k}$$&lt;/li>
&lt;li>&lt;em>&lt;strong>Geometric&lt;/strong>&lt;/em>: $\Omega_\mathcal{X}=\mathbb{N}$ $$p_\mathcal{X}(k)=(1-p)^{k-1}p$$
&lt;ul>
&lt;li>$\mu_\mathcal{X}=\frac{1}{p},\;\sigma^2_\mathcal{X}=\frac{1-p}{p^2},\;G_\mathcal{X}(z)=\frac{pz}{1-(1-p)z}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Poisson&lt;/strong>&lt;/em>: $\Omega_\mathcal{X}=\mathbb{N}$ $$p_\mathcal{X}(k)=\frac{\lambda^k}{k!}\exp\{-\lambda\}$$
&lt;ul>
&lt;li>$\mu_\mathcal{X}=\lambda,\;\sigma^2_\mathcal{X}=\lambda,\;G_\mathcal{X}(z)=\exp\{\lambda(z-1)\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Uniform&lt;/strong>&lt;/em> - $\mathcal{U}(a,b)$: $\Omega_\mathcal{X}=[a,b]$ $$f(x)=
\begin{cases}
1/(b-a)&amp;amp;,\;x \in [a,b] \\
0&amp;amp;,\;\text{otherwise}
\end{cases}$$
&lt;ul>
&lt;li>$\mu_\mathcal{X}=\frac{1}{2}(a+b),\;\sigma^2_\mathcal{X}=\frac{1}{12}(b-a)^2,\;M_\mathcal{X}(s)=\frac{e^{sb}-e^{sa}}{s(b-a)}\;(s\neq 0)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Normal&lt;/strong>&lt;/em> - $\mathcal{N}(\mu,\sigma)$: $\Omega_\mathcal{X}=\mathbb{R}$
$$f(x)=\frac{1}{\sqrt{2\pi\sigma}}\exp\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}$$
&lt;ul>
&lt;li>$M_\mathcal{X}(s)=\exp\left\{\mu s+\frac{1}{2}\sigma^2s^2\right\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Joint Normal&lt;/strong>&lt;/em>: $\Omega_\mathcal{X}=\mathbb{R}^n$
$$f(x)=\frac{1}{\sqrt{(2\pi)^n \det(S)}}\exp\left\{-\frac{1}{2}(x-\mu)^\top S^{-1}(x-\mu)\right\}$$
&lt;ul>
&lt;li>$\phi_\mathcal{X}(u)=\exp\left\{ju^\top \mu-\frac{1}{2}u^\top Su\right\}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Rayleigh&lt;/strong>&lt;/em>: $\Omega_\mathcal{X}=[0,\infty]$ $$f(x)=\frac{x}{\sigma^2}\exp\left\{-\frac{x^2}{2\sigma^2}\right\}H(x)$$ where $H(x)$ is Heaviside step function&lt;/li>
&lt;li>&lt;em>&lt;strong>Exponential&lt;/strong>&lt;/em> - $\mathcal{E}(\lambda)$: $\Omega_\mathcal{X}=[0,\infty]$ $$f(x)=\frac{1}{\mu}\exp\left\{-\frac{x}{\mu}\right\}H(x)$$
&lt;ul>
&lt;li>$\mu_\mathcal{X}=1/\lambda,\;\sigma^2_\mathcal{X}=1/\lambda^2,\;M_\mathcal{X}(s)=\lambda/(\lambda-s)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>&lt;strong>Laplacian&lt;/strong>&lt;/em>: $$f(x)=\frac{1}{2b}\exp\left\{-\frac{|x-\mu|}{b}\right\}$$&lt;/li>
&lt;/ul>
&lt;h2 id="derivation-of-the-distributions">Derivation of the distributions&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Bernoulli → Binomial&lt;/strong>: $\mathcal{X}_ i\sim\text{Bernoulli}( p) \Rightarrow \mathcal{Y}=\sum_{i=1}^n \mathcal{X}_ i\sim\mathcal{B}(n,p)$&lt;/li>
&lt;li>&lt;strong>Bernoulli → Geometric&lt;/strong>: $\mathcal{X}_i\sim\text{Bernoulli}( p) \Rightarrow \mathcal{Y}\sim\text{Geometric}( p)$ denoting the first $\mathcal{X}_i=1$&lt;/li>
&lt;li>&lt;strong>Binomial → Poisson&lt;/strong>: $\mathcal{X}_i\sim\mathcal{B}(n,p,k=1) \Rightarrow \mathcal{Y}\sim\text{Poisson}(\lambda)$ when $n\to \infty$ with $p=\frac{\lambda\tau}{n}$&lt;/li>
&lt;li>&lt;strong>Binomial → Exponential&lt;/strong>: $\mathcal{X}_i\sim\mathcal{B}(n,p,k\neq0) \Rightarrow \mathcal{Y}\sim\mathcal{E}(\lambda)$ when $n\to \infty$ with $p=\frac{\lambda\tau}{n}$
&lt;blockquote>
&lt;p>Actually $\mathcal{B}(n,p,k) \Rightarrow e^{-\lambda \tau}\frac{(\lambda \tau)^k}{k!}$&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;a class="link" href="https://en.wikipedia.org/wiki/Laplace_distribution#Related_distributions" target="_blank" rel="noopener"
>&lt;strong>Exponential → Laplacian&lt;/strong>&lt;/a>: $\mathcal{X}_1, \mathcal{X}_2 \sim\mathcal{E}(\lambda) \Rightarrow \mathcal{X}_1-\mathcal{X}_2\sim\text{Laplacian}(\lambda^{-1})$&lt;/li>
&lt;/ul>
&lt;h2 id="concentration-inequalities">Concentration Inequalities&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Cauchy-Schwarz Inequality&lt;/strong>: $S=\left(\mathbb{E}[\mathcal{X}\mathcal{X}_j]\right)^2\leqslant\left(\mathbb{E}[\mathcal{X}_i]\right)^2\left(\mathbb{E}[\mathcal{X}_j]\right)^2$&lt;/li>
&lt;li>&lt;strong>Markov Inequality&lt;/strong>: $\mathbb{P}(\mathcal{X}\geqslant a)\leqslant \mathbb{E}\mathcal{X}/a,\;a&amp;gt;0$&lt;/li>
&lt;li>&lt;strong>Chebychev Inequality&lt;/strong>: $\mathbb{P}(|\mathcal{X}-\mu|\geqslant\delta)\leqslant\sigma^2/\delta^2$&lt;/li>
&lt;li>&lt;strong>Jenson Inequality&lt;/strong>: $\psi(\mathbb{E} \mathcal{X}) \leqslant \mathbb{E}\psi(\mathcal{X})$ for any convex function $\psi$&lt;/li>
&lt;li>&lt;strong>Chernoff bound&lt;/strong>: $\mathbb{P}(\mathcal{X}\geqslant a)\leqslant \underset{s\geqslant 0}{\min},e^{-as}M_ \mathcal{X}(s)$&lt;/li>
&lt;li>&lt;strong>Law of Large Numbers&lt;/strong>: let $\mathcal{X}_i$ be samples drawn from $(\mathbb{R}^n,\mathcal{F}^n,\mathbb{P})$, and $\mathbb{P}$ is such that $\mathcal{X}_k$ has mean $\mu$ and covariance $S$
&lt;ul>
&lt;li>Weak version: if $\mathcal{Y}_ k=\frac{1}{k}\sum^k_{j=1}\mathcal{X}_j$ then $$\underset{k\to\infty}{\lim} \mathbb{P}\left(\left\Vert \mathcal{Y}_k-\mu\right\Vert&amp;gt;\epsilon\right)=0$$&lt;/li>
&lt;li>Strong version: if $\mathcal{Y}_ k=\frac{1}{k}\sum^k_{j=1}\mathcal{X}_j$ then $$\underset{k\to\infty}{\lim} \mathcal{Y}_k=\mu$$&lt;/li>
&lt;li>Central Limit Theorem: if $\mathcal{Y}_ k=\frac{1}{\sqrt{k}}\sum^k_{j=1}S^{-1/2}\left(\mathcal{X}_ j-\mu\right)$ then $$\underset{k\to\infty}{\lim} f_{\mathcal{Y}_k}(y_k)=\mathcal{N}(0,I)$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="estimation-theory">Estimation Theory&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Hilbert Space Projection Theorem&lt;/strong>: A Hilbert space is a complete inner product space. Let $\mathcal{H}$ be a Hilbert space, $\mathcal{M}$ be a closed subspace of $\mathcal{H}$ and $z\in\mathcal{H}$. Then there is a unique $\hat{z}\in\mathcal{M}$ which is closest to $z$: $$\Vert z-\hat{z}\Vert &amp;lt; \Vert z-y\Vert, \forall y\in\mathcal{M}, y\neq\hat{z}$$
&lt;ul>
&lt;li>&lt;strong>Orthogonality Principle&lt;/strong>: $\hat{z}$ is the closest point iff. $\langle z-\hat{z},y\rangle=0, \forall y\in\mathcal{M}$. In estimation we formulate inner product as $\langle \mathcal{X}, \mathcal{Y}\rangle=\mathbb{E}[\mathcal{X}\mathcal{Y}^T]$, it&amp;rsquo;s $\mathbb{E}[(\mathcal{Y}-\mathbb{E}[\mathcal{Y}|\mathcal{X}])h(\mathcal{X})]=0, \forall h(\cdot)$.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Estimation Problem&lt;/strong>: Given random vector \mathcal{X} and random variable $\mathcal{Y}$ with joint PDF $f_{\mathcal{XY}}(\cdot)$, we observe $\mathcal{X}=x$ and we want to form an estimate of $\mathcal{Y}$ as $\hat{\mathcal{Y}}=g(x)$&lt;/li>
&lt;li>&lt;strong>Minimum Mean Square Error(MMSE) Estimation&lt;/strong>: $\hat{\mathcal{Y}}=\mathbb{E}(\mathcal{Y}|\mathcal{X})$
&lt;ul>
&lt;li>Target: minimize $\mathbb{E}[(\mathcal{Y}-\hat{\mathcal{Y}})(\mathcal{Y}-\hat{\mathcal{Y}})^\top]$ with $\hat{\mathcal{Y}}=g(\mathcal{X})$ (arbitary $g(\cdot)$)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Linear Minimum MSE(LMMSE) Estimation&lt;/strong>: $\hat{A}$ satisfies $\mathbb{E}[\mathcal{YX^\top}]=A\mathbb{E}[\mathcal{XX^\top}]$
&lt;ul>
&lt;li>Target: minimize $\mathbb{E}[(\mathcal{Y}-\hat{\mathcal{Y}})(\mathcal{Y}-\hat{\mathcal{Y}})^\top]$ with $\hat{\mathcal{Y}}=A\mathcal{X}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Affine Minimum MSE(AMMSE) Estimation&lt;/strong>: $\hat{G}$ satisfies $\mathbb{E}[\mathcal{YX^\top}]=G\mathbb{E}[\mathcal{XX^\top}]$, $\hat{c}=\mu_{\mathcal{Y}}-\hat{G}\mu_{\mathcal{X}}$
&lt;ul>
&lt;li>Target: minimize $\mathbb{E}[(\mathcal{Y}-\hat{\mathcal{Y}})(\mathcal{Y}-\hat{\mathcal{Y}})^\top]$ with $\hat{\mathcal{Y}}=G\mathcal{X}+c$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="convergence">Convergence&lt;/h2>
&lt;ul>
&lt;li>Categories: Generally we assume $n\to\infty$ and giving $\mathcal{X}_n$ are random variables.
&lt;ul>
&lt;li>&lt;strong>Sure Convergence&lt;/strong> ($\mathcal{X}_n\xrightarrow{\text{surely}}\mathcal{X}$): $$\forall \omega\in\Omega, \mathcal{X}_n(\omega)\xrightarrow{n\to\infty}\mathcal{X}$$&lt;/li>
&lt;li>&lt;strong>Almost Sure Convergence&lt;/strong> ($\mathcal{X}_ n\xrightarrow{\text{a.s./w.p.1}}\mathcal{X}$, &lt;code>w.p.1&lt;/code>: with probability 1): $$\mathbb{P}(\{\omega\in\Omega:\lim_ {n\to\infty}\mathcal{X} _n(\omega)=\mathcal{X}\})=1$$
&lt;ul>
&lt;li>Equivalent definition: $$\mathbb{P}(\bigcup_{\epsilon&amp;gt;0}A(\epsilon))=0\;\text{where}\;A(\epsilon)=\bigcap_{N=1}\bigcup_{n=N}\{\omega:|\mathcal{X}_n(\omega)-\mathcal{X}(\omega)|\geqslant\epsilon\}$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Convergence in Probability&lt;/strong> ($\mathcal{X}_ n\xrightarrow{\text{in prob.}}\mathcal{X}$): $$\mathbb{P}(\{\omega\in\Omega:\lim_ {nn\to\infty}|\mathcal{X}_n(\omega)-\mathcal{X}(\omega)|\geqslant\epsilon\})=0,\;\forall\epsilon&amp;gt;0$$&lt;/li>
&lt;li>&lt;strong>Convergence in Distribution&lt;/strong> ($\mathcal{X}_ n\xrightarrow{\text{D}}\mathcal{X}$): $$\lim_ {n\to\infty} F_{\mathcal{X}_ n}(x)=F_{\mathcal{X}}(x)$$&lt;/li>
&lt;li>&lt;strong>Convergence in mean of order&lt;/strong>: ($\mathcal{X}_ n\xrightarrow{\text{mean r}}\mathcal{X}$, abbr. &lt;code>m.s.&lt;/code> for $r=2$): $$\mathbb{E}[|\mathcal{X}_n-\mathcal{X}|^r]\to 0$$&lt;/li>
&lt;li>$\mathcal{X}_n\xrightarrow{\text{a.s./mean r}}\mathcal{X}\Rightarrow\mathcal{X}_n\xrightarrow{\text{in prob.}}\mathcal{X}\Rightarrow\mathcal{X}_n\xrightarrow{\text{D}}\mathcal{X}$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Strong Law of Large Numbers&lt;/strong> (requires iid.): $$S_n=\frac{1}{n}\sum\mathcal{X}_ i\xrightarrow{\text{a.s./m.s.}}\mu_ \mathcal{X}$$&lt;/li>
&lt;li>&lt;strong>Weak Law of Large Numbers&lt;/strong> (requires indentical uncorrelated distributed): $S_n\xrightarrow{\text{in prob.}}\mu_\mathcal{X}$&lt;/li>
&lt;li>&lt;strong>Central Limit Theorem&lt;/strong> (requires independent): $$\mathcal{Y}_n=\frac{1}{\sqrt{n}}\sum\frac{\mathcal{X}_i-m}{\sigma}\xrightarrow{\text{D}}\mathcal{N}(0,1)$$&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Law of Large Numbers and Central Limit Theorem together characterized the limiting behavior of the average of samples. See &lt;a class="link" href="https://en.wikipedia.org/wiki/Central_limit_theorem#Relation_to_the_law_of_large_numbers" target="_blank" rel="noopener"
>Wikipedia&lt;/a> and &lt;a class="link" href="http://www.cs.toronto.edu/~yuvalf/CLT.pdf" target="_blank" rel="noopener"
>a proof&lt;/a> to see their relationships.&lt;/p>
&lt;/blockquote>
&lt;h2 id="miscellaneous-corollaries">Miscellaneous Corollaries&lt;/h2>
&lt;ol>
&lt;li>For random valuable that takes positive values, $\mathbb{E}(X)=\int^\infty_0 \mathbb(\mathcal{X}&amp;gt;x)dx$&lt;/li>
&lt;li>If $\mathcal{X}_1,&amp;hellip;\mathcal{X}_n$ are IID continuous random variables, then $\mathbb{P}(\mathcal{X}_1&amp;lt;\mathcal{X}_2&amp;lt;\ldots&amp;lt;\mathcal{X}_n)=1/n!$&lt;/li>
&lt;li>Define $\mathcal{X}_ {(j)}$ to be the j-th smallest among $\mathcal{X}_ 1,&amp;hellip;\mathcal{X}_ n$. Suppose $\mathcal{X}_ 1,&amp;hellip;\mathcal{X}_ n$ are IID random variables with PDF $f$ and CDF $F$, then $$f_ {\mathcal{X}_ {(j)}}(x)=\frac{n!}{(n-j)!(j-1)!}[F(x)]^{j-1}[1-F(x)]^{n-j}f_ \mathcal{X}(x)$$&lt;/li>
&lt;/ol>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>There is a &lt;a class="link" href="http://www.math.wm.edu/~leemis/chart/UDR/UDR.html" target="_blank" rel="noopener"
>chart about &lt;em>Univariate Distribution Relationships&lt;/em>&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>The Pitfall Of Long Double</title><link>https://zyxin.xyz/blog/en/1-01/ThePitfallOfLongDouble/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zyxin.xyz/blog/en/1-01/ThePitfallOfLongDouble/</guid><description>&lt;p>{% note default %}&lt;/p>
&lt;h2 id="preface">Preface&lt;/h2>
&lt;p>This article is a repost, originally written by David on the Prelert&amp;rsquo;s website. The original article is now only &lt;a class="link" href="https://web.archive.org/web/20170422155137/http://info.prelert.com/blog/author/david" target="_blank" rel="noopener"
>accessible on WebArchive&lt;/a>.&lt;/p>
&lt;p>I found this article when I&amp;rsquo;m searching for the reason why Rust doesn&amp;rsquo;t have a corresponding type for &lt;code>long double&lt;/code> in C/C++, which have caused some interoperability issues (see &lt;a class="link" href="https://immunant.com/blog/2019/11/rust2020/#:~:text=long%20doubletypes%20in%20C%20are%20specified%20as%20being,with%20C%20code%2C%20Rust%20needs%20to%20support%20long" target="_blank" rel="noopener"
>here&lt;/a> and &lt;a class="link" href="https://github.com/rust-lang/rust-bindgen/issues/1549" target="_blank" rel="noopener"
>here&lt;/a>). On the contrary, the languages Zig and the newly born Carbon both support &lt;code>f16&lt;/code> and &lt;code>f128&lt;/code> types (Zig also supports &lt;code>f80&lt;/code> and Carbon also supports &lt;code>bfloat16&lt;/code>). But that&amp;rsquo;s not suprising because they all aim to provide max interoperability with C/C++. This article might explain some of the reason why Rust doesn&amp;rsquo;t support float types with higher precision.
{% endnote %}&lt;/p>
&lt;h2 id="contents">Contents&lt;/h2>
&lt;p>C++ provides three floating point data types: &lt;code>float&lt;/code>, &lt;code>double&lt;/code> and &lt;code>long double&lt;/code>. All the &lt;a class="link" href="http://webstore.ansi.org/RecordDetail.aspx?sku=INCITS%2FISO%2FIEC&amp;#43;14882-2012" target="_blank" rel="noopener"
>C++11 standard&lt;/a> says about these types is:&lt;/p>
&lt;blockquote>
&lt;p>The type double provides at least as much precision as float, and the type long double provides at least as much precision as double.&lt;/p>
&lt;p>The set of values of the type float is a subset of the set of values of the type double; the set of values of the type double is a subset of the set of values of the type long double. The value representation of floating-point types is implementation-defined.&lt;/p>
&lt;/blockquote>
&lt;p>However, almost all C++ compilers are part of a family that also includes a C compiler, and Annex F of the &lt;a class="link" href="http://webstore.ansi.org/RecordDetail.aspx?sku=INCITS/ISO/IEC%209899-1999%20%28R2005%29" target="_blank" rel="noopener"
>C99 standard&lt;/a> is more prescriptive:&lt;/p>
&lt;blockquote>
&lt;ul>
&lt;li>The float type matches the IEC 60559 single format.&lt;/li>
&lt;li>The double type matches the IEC 60559 double format.&lt;/li>
&lt;li>The long double type matches an IEC 60559 extended format, else a non-IEC 60559 extended format, else the IEC 60559 double format.&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Since only a complete masochist would write a C++ compiler that used different types for floating point than their closely related C compiler, in practice C++ adheres to the same rules. Certainly every C++ compiler I&amp;rsquo;ve ever worked with over the last 20 years has implemented the float and double types using the single and double precision representations defined in &lt;a class="link" href="http://en.wikipedia.org/wiki/IEEE_floating_point" target="_blank" rel="noopener"
>IEC 60559&lt;/a> (which is the same as &lt;a class="link" href="http://en.wikipedia.org/wiki/IEEE_754-1985" target="_blank" rel="noopener"
>IEEE 754&lt;/a>). But there is some variation in implementations of the last of these types, long double, and this can cause problems.&lt;/p>
&lt;p>Throughout my career in software development I&amp;rsquo;ve run into several issues with the long double type, and these fall into the two basic categories of:&lt;/p>
&lt;ol>
&lt;li>Lack of testing&lt;/li>
&lt;li>Portability&lt;/li>
&lt;/ol>
&lt;h3 id="lack-of-testing">Lack of testing&lt;/h3>
&lt;p>At the end of last year &lt;a class="link" href="https://web.archive.org/web/20170305103541/http://www.prelert.com/blog/linux-speeds-ahead/" target="_blank" rel="noopener"
>I wrote about a problem&lt;/a> that would fall into the first category. A &lt;a class="link" href="http://sourceware.org/bugzilla/show_bug.cgi?id=2445" target="_blank" rel="noopener"
>bug&lt;/a> in the x86_64 implementation of the &lt;code>powl()&lt;/code> function in glibc went unfixed for over 5 years. I suspect if the bug had been in the more widely used &lt;a class="link" href="http://pubs.opengroup.org/onlinepubs/9699919799/functions/pow.html" target="_blank" rel="noopener"
>pow()&lt;/a> function then more of a fuss would have been made and somebody would have fixed it sooner. Because the long double version of the function is less widely used, the bug was left to fester.&lt;/p>
&lt;p>Another example of the lack of testing long double gets is a problem I ran into with the IBM xlC/C++ compiler on &lt;a class="link" href="http://www-03.ibm.com/systems/power/software/aix/" target="_blank" rel="noopener"
>AIX&lt;/a> before joining Prelert. The name (hard link) through which the compiler is &lt;a class="link" href="http://publib.boulder.ibm.com/infocenter/comphelp/v8v101/index.jsp?topic=%2Fcom.ibm.xlcpp8a.doc%2Fcompiler%2Fref%2Ftucmpinv1.htm" target="_blank" rel="noopener"
>invoked&lt;/a> defines how it will behave, and when invoked using the name xlC128_r it uses a 128 bit representation for long double. At one time, even the most trivial programs compiled like this would core dump. Although the &lt;a class="link" href="http://www-01.ibm.com/support/docview.wss?uid=swg1IY96361" target="_blank" rel="noopener"
>bug report&lt;/a> shows an example calling &lt;a class="link" href="http://pubs.opengroup.org/onlinepubs/007908775/xsh/fork.html" target="_blank" rel="noopener"
>fork()&lt;/a>, even a simple &amp;ldquo;Hello world&amp;rdquo; program would core dump on exit if compiled with the &lt;a class="link" href="http://publib.boulder.ibm.com/infocenter/comphelp/v8v101/index.jsp?topic=%2Fcom.ibm.xlcpp8a.doc%2Fcompiler%2Fref%2Fruoptbrt.htm" target="_blank" rel="noopener"
>-brtl&lt;/a> flag! Clearly all the testing had been done on the more commonly used invocations of the compiler (where long double is not 128 bits in size).&lt;/p>
&lt;h3 id="portability">Portability&lt;/h3>
&lt;p>On the portability side, some gotchas to be aware of are:&lt;/p>
&lt;ol>
&lt;li>Microsoft &lt;a class="link" href="http://en.wikipedia.org/wiki/Visual_C%2B%2B" target="_blank" rel="noopener"
>Visual C++&lt;/a> represents long double using IEEE 754 double precision – just like double (the third option permitted by C99). Therefore, making a distinction between double and long double in your code is pointless if you only ever compile with Microsoft Visual C++. But if you have to support platforms other than Windows too and use long double then you&amp;rsquo;ve built in a key difference in behaviour between the platforms that may bite you. Most other x86 compilers treat long double as being the 80 bit extended precision type as used by the &lt;a class="link" href="http://en.wikipedia.org/wiki/X87" target="_blank" rel="noopener"
>x87&lt;/a> floating-point unit.&lt;/li>
&lt;li>On &lt;a class="link" href="http://en.wikipedia.org/wiki/SPARC" target="_blank" rel="noopener"
>SPARC&lt;/a> chips (OK I know they&amp;rsquo;re dying out) the long double type maps to a 128 bit representation, but, by default, compilers will generate code to do the operations in software rather than in hardware. This dates back to a time when most SPARC chips couldn&amp;rsquo;t do the operations in hardware and would request it be done in software using interrupts. Doing the 128 bit floating point operations in software unconditionally was faster than reacting to these interrupts. However, doing operations on long doubles in software is orders of magnitude slower than doing the same operations on doubles in hardware – some of our unit tests were 20 times slower when we encountered this problem, and the tests weren&amp;rsquo;t purely doing long double arithmetic. This is a case where code can be portable in terms of compiling and producing the correct results, but not in terms of having acceptable performance.&lt;/li>
&lt;/ol>
&lt;p>It&amp;rsquo;s instructive to look at what other portable languages do. Java has &lt;a class="link" href="http://docs.oracle.com/javase/tutorial/java/nutsandbolts/datatypes.html" target="_blank" rel="noopener"
>float and double types&lt;/a> corresponding to IEEE 754&amp;rsquo;s single and double precision representations (and unlike C++ the Java standard is &lt;a class="link" href="http://docs.oracle.com/javase/specs/jls/se7/html/jls-4.html#jls-4.2.3" target="_blank" rel="noopener"
>very explicit&lt;/a> about how floating point operations may be implemented). Java doesn&amp;rsquo;t make a long double type available to the programmer, presumably due to the portability issues I&amp;rsquo;ve outlined (although the standard allows the x87 extended precision format to be used in intermediate calculations done by the JVM). &lt;a class="link" href="http://docs.python.org/2/library/stdtypes.html#typesnumeric" target="_blank" rel="noopener"
>Python&lt;/a> just has a float type, which is &amp;ldquo;usually implemented as a double type in C&amp;rdquo;. So, if your overall system contains components written in other languages then you&amp;rsquo;ll avoid a data interchange problem by avoiding long double. The same goes if you want to store floating point numbers in a database table – for example &lt;a class="link" href="http://www.postgresql.org/" target="_blank" rel="noopener"
>PostgreSQL&lt;/a> offers &lt;a class="link" href="http://www.postgresql.org/docs/9.2/static/datatype-numeric.html#DATATYPE-FLOAT" target="_blank" rel="noopener"
>real and double&lt;/a> corresponding to IEEE 754&amp;rsquo;s single and double precision representations.&lt;/p>
&lt;p>A final advantage on x86 CPUs of sticking to float and double is that the compiler can then choose to do floating point calculations in the &lt;a class="link" href="http://en.wikipedia.org/wiki/Streaming_SIMD_Extensions" target="_blank" rel="noopener"
>SSE&lt;/a> unit of the CPU, which means two or four operations can potentially be done in parallel and function arguments passed in registers by &lt;a class="link" href="http://en.wikipedia.org/wiki/X86_calling_conventions#x86-64_calling_conventions" target="_blank" rel="noopener"
>64 bit calling conventions&lt;/a> are nicely in the SSE registers ready to be used. By contrast, long double variables can only be operated on in the x87 floating-point unit and are not passed in registers, slowing the program down.&lt;/p>
&lt;h3 id="conclusion">Conclusion&lt;/h3>
&lt;p>Some might say that using long double improves the accuracy of results. This may be true, but regardless of the amount of digits a fixed precision floating point type has it will be subject to &lt;a class="link" href="http://en.wikipedia.org/wiki/Loss_of_significance" target="_blank" rel="noopener"
>loss of significance&lt;/a> if a poorly chosen algorithm is applied to it. Using extended precision rather than double precision may mask this effect in some cases, but in the long term the only solutions are to use algorithms more appropriate for computer calculations or to somehow detect the loss of significance and replace the answer with an appropriate value.&lt;/p>
&lt;p>In my opinion, if you want to write portable C++ code that not only compiles on multiple architectures but also doesn&amp;rsquo;t have horrendous performance problems on some architectures, long double is best avoided. That&amp;rsquo;s what we do at Prelert – our C++ code doesn&amp;rsquo;t use long double and when we use Boost we define the macro &lt;a class="link" href="http://www.boost.org/doc/libs/1_54_0/libs/math/doc/html/math_toolkit/config_macros.html#math_toolkit.config_macros.boost_math_macros" target="_blank" rel="noopener"
>BOOST_MATH_NO_LONG_DOUBLE_MATH_FUNCTIONS&lt;/a> so that &lt;a class="link" href="http://www.boost.org/doc/libs/1_54_0/libs/math/doc/html/index.html" target="_blank" rel="noopener"
>Boost.Math&lt;/a> doesn&amp;rsquo;t either.&lt;/p></description></item><item><title>The Pitfall Of VLA in C</title><link>https://zyxin.xyz/blog/en/1-01/ThePitfallOfVLA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zyxin.xyz/blog/en/1-01/ThePitfallOfVLA/</guid><description>&lt;p>{% note default %}&lt;/p>
&lt;h2 id="preface">Preface&lt;/h2>
&lt;p>This article is a repost, originally &lt;a class="link" href="https://blog.joren.ga/vla-pitfalls" target="_blank" rel="noopener"
>written by Jorengarenar&lt;/a>. I found this article when I found that Rust doesn&amp;rsquo;t support dynamically stack allocation, and list this article in my blog for translation. If you want to read the Chinese version, please select the Chinese language at the bottom of this page.
{% endnote %}&lt;/p></description></item></channel></rss>