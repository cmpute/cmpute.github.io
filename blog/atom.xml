<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jacob Zhong</title>
  
  <subtitle>Blog</subtitle>
  <link href="/blog/atom.xml" rel="self"/>
  
  <link href="http://zyxin.xyz/blog/"/>
  <updated>2021-07-21T03:27:09.297Z</updated>
  <id>http://zyxin.xyz/blog/</id>
  
  <author>
    <name>Jacob Zhong</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ACGN收藏 - 音频格式</title>
    <link href="http://zyxin.xyz/blog/2021-07/ACGNAudioFormats/"/>
    <id>http://zyxin.xyz/blog/2021-07/ACGNAudioFormats/</id>
    <published>2021-07-21T03:26:42.000Z</published>
    <updated>2021-07-21T03:27:09.297Z</updated>
    
    <content type="html"><![CDATA[<p>在我的收藏里面，音乐是占大头的。我非常喜欢尝试各种风格的音乐，也非常喜欢日本音乐圈的多样性，因此收集了很多。在这过程中也了解到了一些音频格式的内容~我也<a href="https://github.com/cmpute/audio-codec-benchmark" target="_blank" rel="noopener">自己做过一个benchmark</a>，比较不同音频编码的性能区别，如果有兴趣的话可以自己尝试一下~。</p><p>我个人喜欢收藏无损音乐，目的不仅仅是因为高音质，而是无损意味着“无损”，音质与CD或者其他音源是完全一致的（当然，这个一致性音频编码本身并不能保证音源的完整性，但是如果有其他的辅助信息如EAC log，或者AccurateRip测试结果就完全可以保证了）。在这个情况下我把无损压成有损，就能保证这个有损是达到了预期的音质。如果是网上直接下载有损的话，一是很难确定这个有损有没有经过二次压缩，造成额外音质损失，二是有损也最好不要再转换格式了，同样是因为音质损失，这就造成了额外的不便。</p><p>本文就介绍一下我了解的与音频编码相关的知识，以及各种常用音频编码格式的比较。更全面的格式对比可以<a href="https://en.wikipedia.org/wiki/Comparison_of_audio_coding_formats" target="_blank" rel="noopener">参考Wikipedia页面</a>。另外需要指明的是，本文的介绍基于音乐收藏和本地播放的目的，与流媒体的需求不同，后者追求稳定的码率、低延迟甚至是低能耗。</p><a id="more"></a><h1 id="音频信号调制">音频信号调制<a class="header-anchor" href="#音频信号调制"> ❮</a></h1><blockquote><p>如果学过信号处理的读者可能已经了解本章内容了，可以跳过这节~</p></blockquote><p>在物理世界中，声音本质上是一种物体震动产生的波，如果要将物理世界中的波存储为数字世界可以存储的格式，则需要进行信号采样（模数转换）和信号调制。波形信号调制主要有两种方法，一种是脉冲编码调制（pulse-code modulation，PCM），一种则是脉冲密度调制（pulse-density modulation，PDM）。简而言之PCM就是通过数字信号的幅度和频率来分别表示模拟信号的幅度和频率，而PDM则通过数字信号的频率和幅度来分别表示模拟信号的幅度和频率（反过来了）。它们对应的音频存储格式是WAV和DSD（Direct Stream Digital），WAV由于编码简单是最广为使用的音频格式，而DSD由于技术和专利的限制则非常罕见，并且音频编辑比WAV复杂很多，因此只有在索尼的SACD上和一些高清音乐网站可以见到。</p><p>PCM的音质在频率上受限于其采样率，根据Nyquist采样定理，两倍以上的采样率可以真实还原出原波形，所以考虑到人耳的听力最高到20kHz，通常PCM音频的采样率都在40kHz以上（如常见的44.1kHz和48kHz）；在振幅上受限于其采样位深。因此采样率低会导致声音高频被裁掉，而采样位深低会导致振幅分辨率下降，音频的动态范围下降，这两者共同导致音频的失真。而PDM由于我没学过，就不评价其音质了。</p><p>在不同的采样方式之间是可能会产生额外失真的。高音质采样到低音质采样就不用说了，反过来也是可能的，如非整数倍地改变采样率（如44.1kHz到48kHz），PCM和PDM的转换。位深由于对应的是二进制的位数，非整数倍提高位深不会产生失真。</p><h1 id="有损（Lossy）编码">有损（Lossy）编码<a class="header-anchor" href="#有损（Lossy）编码"> ❮</a></h1><p>首先是有损编码，有损编码的音乐比较好找，因为（天国的）虾米、网易云、Spotify等网站都可以下到，现在很多平台都提供比较高音质的试听了。但是在曾经的年代，高音质有损编码也是比较难找的，以及现在放在手机上听歌我还是会转换成有损音质。</p><p>不同的有损格式对于“损失”音频的哪一部分、哪一频段是不一样的，他们适合的场景也不一样，比如有的格式设计之初的目的就睡尽量保留人声质量。如果真要比较哪种格式、哪种编码器的音质最好的话，只有A/B测试才是最可靠的，然而A/B测试也会受到被测对象的主观影响，所以如果想选择一个音质最好的编码器的话，可以自行A/B测试来做判断。</p><p>有损音频的音质可以通过码率（birate）直接进行优劣判断。音频的码率指的是每秒文件能够提供的信息量，以CD音质为例，普通CD一般采样率是<code>44.1kHz</code>（理论能够还原频率高达<code>22kHz</code>的波形），采样深度<code>16bit</code>，双声道，那么原始码率就是<code>44100*16*2=1141.2kbps</code>，注意这里的<code>kbps</code>是<code>kilo bits per second</code>。普通能下载到的有损音乐通常是MP3格式（虽然这年头很少有人再下载音乐了），在我高中那会，从QQ音乐等平台上上下载的MP3基本都是128kbps，只有虾米下载的是192kbps甚至320kbps，因此我还是非常喜欢虾米的。</p><p>码率是一个瞬时概念，对于音频编码（甚至视频编码）而言，码率是随时可能变化的。编码器通常提供两种码率控制方法：恒定比特率（Constant BiRate，CBR）和可变比特率（Variable BiRate，VBR）。选择CBR或者VBR需要试场景而定，CBR适合稳定的媒体串流，避免网络波动产生播放不畅，而VBR由于给了编码器更多空间根据媒体内容调节码率，通常而言可以达到更好的质量，适合本地存储回放。</p><p>一种客观的音质测试方式是直接计算编码后音频与原音频信号相差了多少。根据我的测试，有损音频的质量基本和码率成正比（见下图，如果用信号损失的对数值来看的话几乎是线性正比）。而如果使用<a href="https://en.wikipedia.org/wiki/Weighting_filter#Loudness_measurements_with_weighting_filters" target="_blank" rel="noopener">根据人听力敏感度加权</a>之后的频谱，那么可以看出在低码率时（如96kbps），AAC的音质较好，这正是AAC设计的目标，即在通话音质（一般就是96kbps）下能够有很好的表现。而在高码率时（如320kbps）Wavpack和MP3 CBR的表现更好，因此很多人说MP3格式应该被淘汰，但320k的MP3的音质还是非常好的。</p><p><img src="https://github.com/cmpute/audio-codec-benchmark/raw/master/figs/PLight_-_Bass_tek_2.wav.lossy_err.jpg" alt="有损音质与码率关系图"></p><p>简而言之，我的结论是在同等码率下各种有损格式的音质都差不多，更应该关注的是如何找到高码率的音源。下面介绍几个主流的有损音频编码格式。（我个人非常喜欢用Wavpack的有损模式，但是这个很非主流）</p><h2 id="MP3">MP3<a class="header-anchor" href="#MP3"> ❮</a></h2><p>MP3的名字来源于其最开始是作为MPEG-1标准中的第三种音频格式，它应该是（至少在中国）最广为流传的音频编码格式了。而在支持MP3的编码器中，<a href="https://lame.sourceforge.io/" target="_blank" rel="noopener">lame</a>是其中最常用的。MP3的编码特性是它会根据码率的设置进行低通滤波，320kbps CBR时滤波在20kHz左右比较接近CD音质的22kHz了，而192kbps CBR滤波则在16kHz左右，128kbps在12KHz左右。因此不同码率MP3的听感区别是非常明显的，通俗来讲音质越差的MP3越像是把喇叭蒙在鼓里的声音，各个频段的特点可以<a href="/blog/2020-12/AudiophileIntroduction/" title="参考我之前的博客">参考我之前的博客</a>。</p><p><a href="https://lame.sourceforge.io/" target="_blank" rel="noopener">lame编码器</a>虽然2012年之后就几乎没有怎么更新了，但是它应该仍然是所有提供mp3的音乐平台使用的主要编码器。它支持CBR、VBR和独有的的ABR。CBR可以指定码率，VBR无法直接指定码率，而通过指定参数<code>-V</code>来间接实现码率调整，而ABR则是在可变码率的同时支持指定一个目标平均码率。</p><p>MP3虽然是个很古老的格式，并且有不少为人诟病的缺点，但是因为高码率MP3的音质确实不错，而且MP3的硬件支持非常到位，因此到现在仍然是非常流行的音频格式。</p><h2 id="AAC">AAC<a class="header-anchor" href="#AAC"> ❮</a></h2><p>AAC全名为Advanced Audio Coding，AAC设计目标是成为MP3的后继者。虽然维基上说AAC在同等码率下能够得到比MP3更好的音质，但根据我的测试结果这个结论只在相对较低码率的时候成立。不过AAC设计的定位应该就是针对流媒体，以及现在的蓝牙音频，这些地方音频的码率都是受限的，所以也不能说错。AAC比MP3支持更多的采样率、通道数，在视频编码时其实用的非常多，但是其实它不是针对音乐收藏而设计的。</p><p>AAC音频文件的后缀名通常是<code>m4a</code>和<code>mp4</code>。这两者都是MPEG-4标准定义的流媒体容器后缀名，其中前者专门针对音频，而后者则是音频和视频都可以用。关于容器是什么，我会在之后的视频编码器博客中详细介绍。</p><p>AAC音频编码器除了万能的ffmpeg以外，还有以下这些专门针对AAC的编码器</p><ul><li><a href="http://wiki.hydrogenaud.io/index.php?title=Nero_AAC" target="_blank" rel="noopener">NeroAAC</a>：质量最好，但是是商用编码器，不开源。</li><li>QuickTime AAC：由苹果设计、应用在QuickTime和后来的iTunes中、口碑不错。<a href="https://github.com/nu774/qaac" target="_blank" rel="noopener">有第三方开源的实现（qaac）</a>。</li><li><a href="https://github.com/knik0/faac" target="_blank" rel="noopener">FAAC</a>：Free AAC，开源，但是感觉用的人不多</li></ul><p>总的而言，AAC的特性非常多，也是一个经过深思熟虑后设计的编码器，但由于是针对流媒体设计的，对音频收藏来说并没有什么吸引力。</p><h1 id="无损（Lossless）编码">无损（Lossless）编码<a class="header-anchor" href="#无损（Lossless）编码"> ❮</a></h1><p>无损编码即经过编码压缩之后不会损失信息的编码方式。如果仅仅是为了压缩而言的话，通用的文件压缩理论上也是可以用作音频编码的，但是通用压缩的效率肯定不如专门设计的音频压缩高，并且需要先解压才能播放。无损编码没有音质之差，它们的主要指标则是编码解码耗时，压缩率、对音频格式的支持以及其他附加功能。对音频格式的支持包括多声道（如5.1）、高采样率和位深（如常见的Hi-Res格式96kHz/24bit）、对DSD的支持等。</p><p>下面介绍一些常用的无损格式（其中我选择的就是WavPack）。</p><h2 id="FLAC">FLAC<a class="header-anchor" href="#FLAC"> ❮</a></h2><p>FLAC名为Free Lossless Audio Codec，虽然听起来非常老土和山寨，但应该是目前各平台最通用的格式，像是MP3在有损编码里的地位。FLAC开源、性能好、解码快、硬件支持好、压缩率也不错，无脑选flac一般没什么问题。FLAC是MPEG支持的格式，很多高质量的DVD和BD压缩出来的视频里都会用FLAC作为音频编码。因此通常情况下FLAC编码音频文件的后缀名是<code>.flac</code>，但有时你也能看到<code>.m4a</code>的后缀名。</p><h2 id="APE-TAK">APE/TAK<a class="header-anchor" href="#APE-TAK"> ❮</a></h2><p>APE（Monkey’s Audio）和TAK（Tom’s lossless Audio Kompressor）都是能够提供非常高压缩率的编码器，但是他们俩都是闭源的。在电驴（VeryCD）时代用APE的人非常多，可能就是由于其较高的压缩率吧，但是APE的编码和解码相当慢。TAK的编码解码都很快，估计是利用了多线程或者AVX加速。另外还有一款编码器叫OptimFrog，能够提供最高的压缩率，但是编码和解码都奇慢无比，更像是个Proof of concept的作品，而且还不开源，实际使用就不要考虑了。</p><h2 id="WavPack">WavPack<a class="header-anchor" href="#WavPack"> ❮</a></h2><p>这里隆重推荐我现在使用的WavPack。它开源、功能丰富、支持各种采样率位深和通道数，甚至支持DSD的编码，这个特性是别无二家了。</p><p>不过最吸引我的功能其实是支持混合编码（见后文）。WavPack由于其开源的特点，同样被各大音乐软件所支持，甚至ffmpeg和MKV视频容器都是支持WavPack的，不过MPEG-4仍然不支持比较遗憾。WavPack在硬件上支持可能没有FLAC广，但是WavPack的源码中同样包含了用汇编直接编写的几个核心函数，因此编解码的性能也是非常好的。</p><p>根据我的使用经验，WavPack有损音质好，无损体积小，编码也快，总之除了FLAC之外找WavPack就没错了！</p><h1 id="混合（Hybrid）编码">混合（Hybrid）编码<a class="header-anchor" href="#混合（Hybrid）编码"> ❮</a></h1><p>除了有损无损之外还有一种编码方式是混合编码，它指的是编码器在生成有损压缩音频后还生成一个修正文件（Correction File）。当修正文件和本体音频同时存在时原始音频可以被无损还原。这个编码方式的好处是你可以同时拥有大体积的高音质文件和小体积的低音质文件，非常适合我这样的收藏党，文件本体放在云上，然后小体积的有损部分可以经常下载下来听。有损部分也可以用作demo，如果听了demo之后喜欢上这首音乐了再去下载修正文件提高音质。</p><p>支持混合编码的主要有三种音频格式：LossyWav，WavPack和OptimFrog，其中最后一种非常难用，而且好像和LossyWAV一样不支持无损播放，即需要先解码再播放才能达到无损音质。</p><h2 id="LossyWAV">LossyWAV<a class="header-anchor" href="#LossyWAV"> ❮</a></h2><p>LossyWAV其实不算是一个完整的音频编码器，而是一个预处理软件。LossyWAV只能处理原始的PCM音频，然后生成的也是PCM音频，之后还需要使用其他的（无损）编码器来进行压缩。它的原理是分析原始音频，然后对其进行某种形式的变换使得音频更容易被压缩，从而降低生成的音频文件平均码率。由于这个变换是不可逆的，因此它也是有损压缩，但是LossyWAV支持生成修正文件，因此它可以看作一种混合编码方式。</p><p>经过以上描述，相信大家可以看出来编码过程非常麻烦，如果想要生成混合模式下的音频文件和修正文件，需要先从原始PCM音频生成有损PCM和修正PCM，然后再分别通过其他方式编码器将它们分别压缩，而无损解码的过程则是把他们反过来。因此LossyWAV在编解码速度和文件体积上都完全没有优势，并且这个原理也意味着LossyWav不支持无损播放。在能够选择WavPack的情况下还是不要用LossyWAV了。</p><h2 id="WavPack-2">WavPack<a class="header-anchor" href="#WavPack-2"> ❮</a></h2><p>WavPack内置对混合模式的支持，而且WavPack支持无损播放。这意味着只要播放器能够找到修正文件，那么播放器就能直接以无损音质播放音乐而不需要额外解码。这个特性对我来说就是killer！另外从前文图表可以看出，WavPack在有损模式下也能够达到很好的音质水平，甚至在特定码率下比MP3和AAC都要好。唯一遗憾的地方是混合模式下WavPack音频总体的压缩率是比较低的，通常会比无损模式下的体积要高出5%左右。不过这个年头存储空间越来越不值钱了，所以这个问题也完全可以忽略。</p><p>总而言之我完全找不到理由不使用WavPack，再次向大家推荐这个编码器！</p><hr><p>本文介绍了在ACG音乐收藏的过程中我了解到的音频编码知识，而在下一篇博客我还会介绍视频编码的内容~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在我的收藏里面，音乐是占大头的。我非常喜欢尝试各种风格的音乐，也非常喜欢日本音乐圈的多样性，因此收集了很多。在这过程中也了解到了一些音频格式的内容~我也&lt;a href=&quot;https://github.com/cmpute/audio-codec-benchmark&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自己做过一个benchmark&lt;/a&gt;，比较不同音频编码的性能区别，如果有兴趣的话可以自己尝试一下~。&lt;/p&gt;&lt;p&gt;我个人喜欢收藏无损音乐，目的不仅仅是因为高音质，而是无损意味着“无损”，音质与CD或者其他音源是完全一致的（当然，这个一致性音频编码本身并不能保证音源的完整性，但是如果有其他的辅助信息如EAC log，或者AccurateRip测试结果就完全可以保证了）。在这个情况下我把无损压成有损，就能保证这个有损是达到了预期的音质。如果是网上直接下载有损的话，一是很难确定这个有损有没有经过二次压缩，造成额外音质损失，二是有损也最好不要再转换格式了，同样是因为音质损失，这就造成了额外的不便。&lt;/p&gt;&lt;p&gt;本文就介绍一下我了解的与音频编码相关的知识，以及各种常用音频编码格式的比较。更全面的格式对比可以&lt;a href=&quot;https://en.wikipedia.org/wiki/Comparison_of_audio_coding_formats&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;参考Wikipedia页面&lt;/a&gt;。另外需要指明的是，本文的介绍基于音乐收藏和本地播放的目的，与流媒体的需求不同，后者追求稳定的码率、低延迟甚至是低能耗。&lt;/p&gt;
    
    </summary>
    
      <category term="ACGN" scheme="http://zyxin.xyz/blog/categories/ACGN/"/>
    
    
  </entry>
  
  <entry>
    <title>MegaFavNumbers - 最喜爱的百万数字</title>
    <link href="http://zyxin.xyz/blog/2021-07/MegaFavNumbers/"/>
    <id>http://zyxin.xyz/blog/2021-07/MegaFavNumbers/</id>
    <published>2021-07-12T01:46:26.000Z</published>
    <updated>2021-07-12T15:21:24.139Z</updated>
    
    <content type="html"><![CDATA[<p>这篇博客也是拖了很久了，简直是蹭热度都蹭不到热的。。。去年年底有一帮数学家和喜欢数学的人（Numberphile）发起了一个Youtube系列，叫<a href="https://www.youtube.com/hashtag/megafavnumbers" target="_blank" rel="noopener">#MegaFavNumbers</a>，也就是介绍自己最喜欢的大于一百万的数字。虽然没有要求这个数字是整数，但是Numberphile一般只关注整数（甚至仅自然数）。如果没有这个限制的话，那物理化学上就有很多常数了，例如某视频评论区有人提到阿伏伽德罗常数23333</p><p>如果让我来选的话我还真想不太出来，毕竟没学多少数学，顶多会选$2^{32}$这种程序员知道的数字，或者已知最大的质数、孪生质数云云。这个题目真的是很有意思了，很多有特殊性质的数字或者是某数列的第一个数都会比较小，很少会有一个非常大并且独一无二的数字，因此看了3Blue1Brown的视频之后我顿时就来了兴趣，<s>准备</s>写下这篇博客介绍以下各博主选择的数字，又了解一些平常不知道的冷知识~哈哈。我大致将这些数字分了个类，不过不是很严格。</p><a id="more"></a><h1 id="某特殊数列中第一个超过1M的">某特殊数列中第一个超过1M的<a class="header-anchor" href="#某特殊数列中第一个超过1M的"> ❮</a></h1><ul><li><a href="https://www.youtube.com/watch?v=b1wWGRZ9YTE" target="_blank" rel="noopener">3 628 800</a> by @Peter Pike：第一个超过1M的阶乘。作者讲了一堆和阶乘有关的可视化，但是数字本身其实比较直观了。</li><li><a href="https://www.youtube.com/watch?v=dJ6pej8SihI" target="_blank" rel="noopener">10<sup>69</sup>+69</a> by @Kevin Du：$10^x+x$数列中第个3质数（前两个是$10^1+1$，$10^9+9$），即<a href="https://oeis.org/A089379" target="_blank" rel="noopener">OEIS数列A089379</a>的第三个数 （不是很知道为什么没有选$10^9+9$ ┑(￣Д ￣)┍）</li><li><a href="https://www.youtube.com/watch?v=A7eJb8n8zAw" target="_blank" rel="noopener">≈1.1698e45</a> by @Stand-up Maths：满足$\tan( p )&gt;p$的第一个质数p，即<a href="https://oeis.org/A249836" target="_blank" rel="noopener">OEIS数列A249836</a>中的第一个质数。</li><li><a href="https://www.youtube.com/watch?v=Z3xq4ODNeZs&amp;t=208s" target="_blank" rel="noopener">$C^{104}_{39}$</a> by @Zoe Griffiths：在杨辉三角里出现超过5次的数中，大于1M的第一个数。神奇的是前一个数是24310，然后突然就变得很大了！</li><li><a href="https://www.youtube.com/watch?v=a9k_QmZbwX8" target="_blank" rel="noopener">640 320<sup>3</sup></a> by @Richard E. BORCHERDS：$\approx e^{\pi\sqrt{163}}-744$。取这个数的原因是它与$e^{\pi\sqrt{67}}-744$和$e^{\pi\sqrt{93}}-744$都神奇地非常接近一个整数，其背后的原因跟椭圆模函数$1/q+744+196884q+21493760q^2+…$有关。这个数由传奇印度数学家Srinivasa Ramanujan发现，也被称为Ramanujan常数，计算这个数需要支持任意精度浮点运算的计算器。</li><li><a href="https://www.youtube.com/watch?v=kIE2JZTwv_k" target="_blank" rel="noopener">23 240 400<sub>6</sub> = 720 720<sub>10</sub></a> by jan Misali：即六进制表示的720720。720720是接近1M的超级合数（令$d(n)$表示$n$的因数个数，$f_\epsilon(n)=d(n)/n^\epsilon$，超级合数则是满足$\forall k\in\mathbb{Z}^+$, $k&lt; n, d(n)&gt;d(k), f_\epsilon(n)\geq f_\epsilon(k)$的数$n$），即<a href="https://oeis.org/A002201" target="_blank" rel="noopener">OEIS数列A002201</a>中接近1M的一个很满足强迫症的数。博主为了让它超过1M换成了6进制哈哈哈哈。</li></ul><h1 id="某特殊数列最后一个数">某特殊数列最后一个数<a class="header-anchor" href="#某特殊数列最后一个数"> ❮</a></h1><ul><li><a href="https://www.youtube.com/watch?v=5BFDdVqAFZE" target="_blank" rel="noopener">73 939 133</a> by @Flammable Maths: 最大的可右截断素数（right truncatable prime），即<a href="http://oeis.org/A024770" target="_blank" rel="noopener">OEIS数列</a>最后一个数。</li><li><a href="https://www.youtube.com/watch?v=lKjR60jkUQE" target="_blank" rel="noopener">≈1.151322e38</a> by @Normalized Nerd：十进制下最后一个水仙花数，即<a href="http://oeis.org/A005188" target="_blank" rel="noopener">OEIS数列A005188</a>最后一个数。</li><li><a href="https://www.youtube.com/watch?v=RAKWgYDcB4k" target="_blank" rel="noopener">$3\times2^{402653209}-1$</a> by @timpa’s videos: 从4开始的Goodstein序列的最大一个数，即<a href="http://oeis.org/A005188" target="_blank" rel="noopener">OEIS数列A056193</a>中最大数。</li></ul><h1 id="某猜想的第一个正例或者反例">某猜想的第一个正例或者反例<a class="header-anchor" href="#某猜想的第一个正例或者反例"> ❮</a></h1><ul><li><a href="https://www.youtube.com/watch?v=eQCUPQdi6DY" target="_blank" rel="noopener">906 150 257</a> by @SparksMaths：<a href="https://en.wikipedia.org/wiki/P%C3%B3lya_conjecture" target="_blank" rel="noopener">Pólya猜想</a>的最小反例。<a href="https://www.zhihu.com/question/37164066/answer/71589759" target="_blank" rel="noopener">这里有个知乎回答提到了这个例子</a>。</li><li><a href="https://www.youtube.com/watch?v=R2eQVqdUQLI" target="_blank" rel="noopener">666 030 256, 696 630 544</a> by @singingbanana：偶亲和数猜想：“偶数亲和数之和为9的倍数”的第一个反例。（亲和数对：A的所有真因数之和等于B，B的所有真因数之和等于A）</li><li><a href="https://www.youtube.com/watch?v=vv0bHK44Q1s" target="_blank" rel="noopener">569 936 821 221 962 380 720</a> by @Numberphile：一个著名猜想的任意整数可以写成三个整数的三次方之和，其中$3=x^3+y^3+z^3$的解除了(1,1,1)，(4,4,-5)外找到的第三个解中的正数即为博主选择的数。</li><li><a href="https://www.youtube.com/watch?v=L4ArlAfKTLA" target="_blank" rel="noopener">≈8.42443e51</a> by @WillsWei：使得$n^{17}+9$和$(n+1)^{17}+9$不互质的第一个$n$。</li></ul><h1 id="来自非数学领域的数">来自非数学领域的数<a class="header-anchor" href="#来自非数学领域的数"> ❮</a></h1><ul><li><a href="https://www.youtube.com/watch?v=bknybcgfjAk" target="_blank" rel="noopener">1 094 795 585</a> by @LiveOverflow：<code>0x41414141</code>，即ASCII码表示的<code>AAAA</code>，被视作缓存溢出的标志</li><li><a href="https://www.youtube.com/watch?v=pCNVkUYUnrY" target="_blank" rel="noopener">≈1.01971e1400</a> by @The Comamba: $k\cdot 256^{211}+99$，其中<code>k</code>是一段破解DVD加密的代码的二进制表示。由于这段代码不合法，一个程序员用这个数把它加密成一个质数然后上传到了一个质数网站，也是很有想法了！</li><li><a href="https://www.youtube.com/watch?v=QqbDLoNHqDk" target="_blank" rel="noopener">6.187e34</a> by @Tom Rocks Maths: $1/l_p$，$l_p$代表普朗克长度。普朗克提出世界不是连续的，因此普朗克常数就可以用来用整数表达这个世界！</li><li><a href="https://www.youtube.com/watch?v=Zx5B0imgrS8" target="_blank" rel="noopener">1 056 006</a> by @Eddie Woo：悉尼歌剧院房顶的瓷砖数，surprise！哈哈哈哈！</li></ul><h1 id="其他">其他<a class="header-anchor" href="#其他"> ❮</a></h1><ul><li><a href="https://www.youtube.com/watch?v=4g_OjRB0wCE" target="_blank" rel="noopener">≈4.3252e19</a> by @David Dijon 和 @Philip Hintze: 魔方的组合可能数。</li><li><a href="https://www.youtube.com/watch?v=2SBqn9EaMg0" target="_blank" rel="noopener">302 575 350</a> by @blackpenredpen：买到Mega Million彩票的可能性。</li><li><a href="https://www.youtube.com/watch?v=P7Fbfu584ts" target="_blank" rel="noopener">12 345 679</a> by @TyYann：也是个很有名的数字了，12345679$\times$11 = 111111111。作者因为小时候的回忆而选择了它。</li><li><a href="https://www.youtube.com/watch?v=mH0oCDa74tE" target="_blank" rel="noopener">≈8.08e53</a> by @3Blue1Brown：“魔群”（Monster Group）的大小。魔群是“散在单群”（Sporadic Simple Groups）中最大的群。推荐看完整原视频，解释这个概念也是非常麻烦了。。。另外<a href="https://www.zhihu.com/question/47850518/answer/358979421" target="_blank" rel="noopener">知乎这也有个很棒的回答</a>。这个数是我觉得这系列里面最有意思的，一个数学中应该是非常基础的概念里面竟然会有这么大的尺寸，非常神奇！</li></ul><hr><p>时间有限，这里只总结了这么多。有一类数字没有加进来，就是专门生成大数字的运算符，所产生的最小数字。。。因为不好打出来所以没放。这个合作系列一共有200多个视频，如果有兴趣的话可以去<a href="https://www.youtube.com/hashtag/megafavnumbers" target="_blank" rel="noopener">Youtube列表里面查看</a>~另外对有兴趣探寻这些数字游戏的人，我也推荐<a href="https://projecteuler.net/" target="_blank" rel="noopener">Project Euler</a>，里面有很多找数字的题目，同时满足了对数字的好奇心和编程练习~</p><blockquote><p><strong>参考链接</strong>：</p><ul><li><a href="https://www.youtube.com/hashtag/megafavnumbers" target="_blank" rel="noopener">Youtube #MegaFavNumbers</a></li><li><a href="https://www.zhihu.com/question/37164066" target="_blank" rel="noopener">数学史上有哪些看似成立的算式形式猜想，最终被某个大数证明不成立？ - 知乎</a></li><li><a href="https://www.zhihu.com/question/47850518" target="_blank" rel="noopener">数学中的“怪兽群”是什么概念</a></li><li><a href="https://projecteuler.net/" target="_blank" rel="noopener">Project Euler</a></li><li><a href="https://johncarlosbaez.wordpress.com/2018/09/20/patterns-that-eventually-fail/" target="_blank" rel="noopener">Patterns That Eventually Fail</a></li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇博客也是拖了很久了，简直是蹭热度都蹭不到热的。。。去年年底有一帮数学家和喜欢数学的人（Numberphile）发起了一个Youtube系列，叫&lt;a href=&quot;https://www.youtube.com/hashtag/megafavnumbers&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;#MegaFavNumbers&lt;/a&gt;，也就是介绍自己最喜欢的大于一百万的数字。虽然没有要求这个数字是整数，但是Numberphile一般只关注整数（甚至仅自然数）。如果没有这个限制的话，那物理化学上就有很多常数了，例如某视频评论区有人提到阿伏伽德罗常数23333&lt;/p&gt;&lt;p&gt;如果让我来选的话我还真想不太出来，毕竟没学多少数学，顶多会选$2^{32}$这种程序员知道的数字，或者已知最大的质数、孪生质数云云。这个题目真的是很有意思了，很多有特殊性质的数字或者是某数列的第一个数都会比较小，很少会有一个非常大并且独一无二的数字，因此看了3Blue1Brown的视频之后我顿时就来了兴趣，&lt;s&gt;准备&lt;/s&gt;写下这篇博客介绍以下各博主选择的数字，又了解一些平常不知道的冷知识~哈哈。我大致将这些数字分了个类，不过不是很严格。&lt;/p&gt;
    
    </summary>
    
      <category term="Misc" scheme="http://zyxin.xyz/blog/categories/Misc/"/>
    
    
      <category term="Math" scheme="http://zyxin.xyz/blog/tags/Math/"/>
    
  </entry>
  
  <entry>
    <title>ACGN收藏 - 文件管理</title>
    <link href="http://zyxin.xyz/blog/2021-07/ACGNFileManagement/"/>
    <id>http://zyxin.xyz/blog/2021-07/ACGNFileManagement/</id>
    <published>2021-07-10T20:10:21.000Z</published>
    <updated>2021-07-12T02:40:53.068Z</updated>
    
    <content type="html"><![CDATA[<p>对于ACGN收藏来说，文件管理是一个基础任务，毕竟收藏的文件内容多种多样，例如光盘镜像、压制后的音频视频、小册子扫描、字幕甚至小游戏等。把文件按一定结构整理是必要的，我也专门为整理音乐写了<a href="https://github.com/cmpute/Fluss" target="_blank" rel="noopener">一些小工具</a>，不过整理文件的格式因人而异，也没有特别的难度，因此不需要特别描述我是怎么做的。我觉得值得一提的内容是如何对文件进行定期存档和备份，这也是我在硬盘被偷之后立马开始对收藏的文件进行的操作。备份有一个3-2-1的原则：3份备份，2份本地，1份云端，下面会介绍一些本地的备份和云端备份的方法以及我的选择。</p><h1 id="离线备份">离线备份<a class="header-anchor" href="#离线备份"> ❮</a></h1><p>离线备份就是把文件资料整理并存储到另一个设备上，需要考虑的功能有加密、压缩、增量更新、去重、冗余等。如果是最基本的备份，如果只想直接备份，不考虑加密压缩等的话，著名的<a href="https://rsync.samba.org/" target="_blank" rel="noopener">rsync</a>是个不错的选择，它可以同步两个目录（可以是挂载FTP的目录），并且有算法来进行去重以减少二进制的传输。</p><a id="more"></a><p>对我而言最大的需求是有冗余（指恢复记录，Recovery Record）和分卷，因为之前有在光盘上存一部分的音乐，而最后有几个压缩包已经无法恢复了，光这一条一卡几乎没剩下几条选项，可以参考<a href="https://en.wikipedia.org/wiki/List_of_archive_formats#Data_recovery" target="_blank" rel="noopener">维基百科</a>。内置支持恢复记录的格式最著名且常用的有WinRAR（虽然不开源），另外剩下的里面开源的只有DAR，FreeArc。FreeArc已经10年没更新了，并且代码是毛子用Haskell写的，注释都是俄语。。因此就不考虑了。如果考虑外部支持的话最常用的就是Par2标准。下面对比几种（文件级别）方案的区别</p><blockquote><p>如果有多盘的话那么RAID就是不二选择了。不过选择文件系统以及组RAID或者NAS都是比较折腾，而且多数情况下需要Linux，我现在平时还是难免用Windows当主力，换成Linux做备份还是麻烦，因此本文就不介绍支持备份功能的文件系统了。如有兴趣可以自行了解<a href="https://www.openzfs.org" target="_blank" rel="noopener">ZFS</a>、<a href="https://btrfs.wiki.kernel.org/index.php/Main_Page" target="_blank" rel="noopener">Btrfs</a>或者<a href="https://wiki.archlinux.org/title/XFS" target="_blank" rel="noopener">XFS</a>+<a href="https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)" target="_blank" rel="noopener">LVM</a>。这方面还有有很多博文可以参考（如<a href="https://markmcb.com/2020/01/07/five-years-of-btrfs" target="_blank" rel="noopener">这一篇ZFS和Btrfs的比较</a>，以及<a href="https://ownyourbits.com/2019/03/03/how-to-recover-a-btrfs-partition/" target="_blank" rel="noopener">这一篇如何从Btrfs恢复数据</a>）</p></blockquote><h2 id="WinRAR">WinRAR<a class="header-anchor" href="#WinRAR"> ❮</a></h2><p><a href="https://www.rarlab.com/" target="_blank" rel="noopener">WinRAR</a>除了不开源之外其实没有任何大毛病，它的解压部分也是开源的，因此不用担心以前的rar压缩包以后会打不开。主要的缺陷是WinRAR对增量更新几乎没有支持，最多<a href="https://x443.wordpress.com/2012/07/11/winrar-incremental-differential-backup/" target="_blank" rel="noopener">通过文件flag来实现</a>，因此不必指望RAR做增量更新了。如果只是想把收藏做个镜像，那WinRAR就很方便了，有不错的压缩和加密，而且支持分卷和恢复记录。</p><h2 id="DAR-Par2">DAR + Par2<a class="header-anchor" href="#DAR-Par2"> ❮</a></h2><p>DAR是一个设计来替代Tar的文档格式，内置对Par2的支持，并且支持增量更新，对大量数据的备份其实挺友好的。<a href="https://en.wikipedia.org/wiki/Parchive#Par2" target="_blank" rel="noopener">PAR2</a>是个给文件生成外部恢复记录的标准，可以生成一些恢复记录文件，当数据主体文件有一些损坏的时候，可以使用PAR2文件进行恢复，并且PAR2文件本身也是能够允许一部分损坏的。这个方案其实功能上来说很不错，但是由于是针对Linux设计的，对Windows支持用cygwin太不友好了。此外DAR的软件支持也不是很全，不知道为什么没有流行起来。</p><h2 id="7zip-Par2">7zip + Par2<a class="header-anchor" href="#7zip-Par2"> ❮</a></h2><p>如果不限打包软件（不要求对Par2的直接支持和增量更新）的话7zip应该是当前评价最高的压缩软件了。7zip + Par2是个不错的选择，不过设置Par2的参数就有一些麻烦了。这个方案相比WinRAR的优势仅仅在于7zip和Par2都是开源的。7zip有个额外的坏处是它的slice每个分区不能独立打开，rar的话每个slice包都有对应的文件可以解压。Par2相比WinRAR的修复好处在于它可以progressively提供冗余，就是下载的冗余文件不够的话可以下载更多冗余文件来进行修复，弱势是它不能处理32767以上个文件，因此必定需要跟某个archive格式一起使用。</p><h2 id="7zip-SeqBox">7zip + SeqBox<a class="header-anchor" href="#7zip-SeqBox"> ❮</a></h2><p>除了冗余数据之外，另一种保护对象是磁盘系统的文件头。SeqBox是一个用来保护<strong>单一</strong>文件在磁盘文件系统损坏的情况下仍能恢复数据的通用工具，其工作原理是将文件分割成尺寸小于硬盘扇区（sector）大小的块，每个文件块有独立的包含文件UID的文件头，这样哪怕分区表损坏，指定文件还是可以通过一次全盘扫描恢复出来。而BlockyArchive则是基于此之上的改进版，给每个文件块加上了冗余码，使得文件本身的损坏也可以得到恢复。这个方法对数据长期冷存储应该是很有用的。不过它会产生不小的额外存储开销，并且对应的功能其实更适合通过文件系统本身来解决，例如之前提到的著名的ZFS和Btrfs。</p><h1 id="在线备份">在线备份<a class="header-anchor" href="#在线备份"> ❮</a></h1><p>由于在线存储服务商通常都会提供数据完整性check以及数据冗余存储的功能，因此对recovery record的需求没有那么大（百度网盘除外！！！辣鸡网盘下载经常文件损坏）。有许多软件支持数据同步和备份，同步比如Google Drive自带的sync，Onedrive或者<a href="https://rclone.org/" target="_blank" rel="noopener">rclone</a>，他们的缺陷是没有加密、压缩，并且支持的snapshot功能有限。相比于本地备份，在线备份更关注的可能就是文件体积了，因为文件体积可能直接会影响收费策略，而冗余和备份通常会有云服务商来保证，因此去重对于在线备份来说是更重要的。</p><p>更针对性的备份软件则对这些都有支持，在<a href="https://alternativeto.net/software/time-machine/?license=opensource" target="_blank" rel="noopener">这个网站有一个开源软件的list</a>。这些软件通常支持将数据备份到另一个目录、NAS或者网盘，并且定期执行增量备份。由于Windows或者Mac目前还是不可避免地成为主力系统，因此只考虑支持Windows、Mac的情况下，再加上有GUI，可选项有<a href="https://www.duplicati.com/" target="_blank" rel="noopener">Duplicati</a>，<a href="https://duplicacy.com/" target="_blank" rel="noopener">Duplicacy</a>，<a href="https://www.urbackup.org/impressions.html" target="_blank" rel="noopener">UrBackup</a>和<a href="https://github.com/BlobBackup/BlobBackup" target="_blank" rel="noopener">BlobBackup</a>。这些软件有些是针对系统备份设计的，但其实我对系统备份没有什么需求，毕竟重装系统也没有很麻烦。Duplicacy有开源CLI，但GUI是收费的，性能很好。UrBackup的UI都很简陋，而且感觉更新不勤。BlockBackup是个定位简洁的产品，看下来Duplicati和Duplicacy还是个不错的选择，Duplicati支持的后端更多，而Duplicacy的性能更好并且更稳定。关于这些选择有不少比较，例如<a href="https://forum.duplicati.com/t/big-comparison-borg-vs-restic-vs-arq-5-vs-duplicacy-vs-duplicati/9952" target="_blank" rel="noopener">Duplicati的论坛里</a>，<a href="https://github.com/gilbertchen/benchmarking" target="_blank" rel="noopener">Duplicacy作者的benchmark</a>，可供参考。目前我的选择是Duplicacy，因为稳定并且高效。但Duplicacy由于算法特性，产生的文件块比较小，因此对于大数量的小文件备份不是很友好，如果之后要做日常文件备份的话可能还是会考虑Duplicati。</p><p>这里提以下去重（Deduplication）和<a href="http://dar.linux.free.fr/doc/usage_notes.html#Decremental_Backup" target="_blank" rel="noopener">增量（Incremental）/减量（Decremental）/差分（Differential）备份</a>的区别，通常增量备份仅仅会保留完整的新文件而可以跳过没有改动的文件（类似Git的模式），对文件中不同的部分一般不做处理，但在这种情况下如果有大文件进行了内容修改，则会产生大量的浪费，因此有专门的去重算法来针对文件整体内容进行去重，其本质上就是将所有文件看作一个大文件，然后通过特定的方法拆分（通常是使用<a href="https://en.wikipedia.org/wiki/Rolling_hash" target="_blank" rel="noopener">Rolling hash</a>）来达到快速查重的效果。这样的一个比较大的问题就是文件会被分成很多小块（通常只有几个MB），因此对于文件传输来说其实很低效（例如上传到网盘、拷入备用磁盘等），并且将文件分块太细也会带来一定的性能和容量损失。在文件内容大部分为大文件，并且不会内部进行小修改的时候，这样的操作其实比较浪费时间。</p><p>这里提到的在线备份工具都可以把本地磁盘看作一个备份目的地，因此也可以用作离线备份。另外离线备份也可以通过同步工具（如rclone）变成在线备份。上文提到的离线备份一般不能做到multi-version（除了ZFS），不过对于比如我这个音乐收集的任务来说，历史记录不是非常重要，因此也是个可行的方案。</p><h1 id="网站归档">网站归档<a class="header-anchor" href="#网站归档"> ❮</a></h1><p>还有一个比较另类的需求，我不仅想备份自己的文件，还想备份别人的<s>文件</s>网站。</p><p>很多同人社团的网站有很多信息，如Discography、世界观设定、Stuff List甚至一些正常的blog等，但是这些内容都不是持久的，很多同人社团停止活动之后网站也没了，因此也想备份他们的网站。这个需求通常可以通过知名网站<a href="https://archive.org/web/" target="_blank" rel="noopener">Internet Archive</a>完成，但是这个网站因为是公益性质的，一些多媒体资源并不一定有保存下来，还是自己搭建网站爬虫会比较可靠，Internet Archive可以作为补充。</p><p>网站爬取以前是通过IDM（Internet Download Manager）可以实现，但是IDM不免费因此后面也没有用了。而单页的存档方式之前很流行的一个格式是Firefox的<a href="https://en.wikipedia.org/wiki/Mozilla_Archive_Format" target="_blank" rel="noopener">maff</a>，不过Firefox也不再支持这个格式了。现在的计划是下载单独的网页用Save Page WE这个插件来完成，基本可以原封不动地备份一个网页，而对于备份整个网站，计划之后搭建一个自己的<a href="https://github.com/ArchiveBox/ArchiveBox" target="_blank" rel="noopener">ArchiveBox</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于ACGN收藏来说，文件管理是一个基础任务，毕竟收藏的文件内容多种多样，例如光盘镜像、压制后的音频视频、小册子扫描、字幕甚至小游戏等。把文件按一定结构整理是必要的，我也专门为整理音乐写了&lt;a href=&quot;https://github.com/cmpute/Fluss&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;一些小工具&lt;/a&gt;，不过整理文件的格式因人而异，也没有特别的难度，因此不需要特别描述我是怎么做的。我觉得值得一提的内容是如何对文件进行定期存档和备份，这也是我在硬盘被偷之后立马开始对收藏的文件进行的操作。备份有一个3-2-1的原则：3份备份，2份本地，1份云端，下面会介绍一些本地的备份和云端备份的方法以及我的选择。&lt;/p&gt;&lt;h1 id=&quot;离线备份&quot;&gt;离线备份&lt;a class=&quot;header-anchor&quot; href=&quot;#离线备份&quot;&gt; ❮&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;离线备份就是把文件资料整理并存储到另一个设备上，需要考虑的功能有加密、压缩、增量更新、去重、冗余等。如果是最基本的备份，如果只想直接备份，不考虑加密压缩等的话，著名的&lt;a href=&quot;https://rsync.samba.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;rsync&lt;/a&gt;是个不错的选择，它可以同步两个目录（可以是挂载FTP的目录），并且有算法来进行去重以减少二进制的传输。&lt;/p&gt;
    
    </summary>
    
      <category term="ACGN" scheme="http://zyxin.xyz/blog/categories/ACGN/"/>
    
    
  </entry>
  
  <entry>
    <title>ACGN 收藏者的自我修养</title>
    <link href="http://zyxin.xyz/blog/2021-07/ACGNCollection/"/>
    <id>http://zyxin.xyz/blog/2021-07/ACGNCollection/</id>
    <published>2021-07-10T19:06:42.000Z</published>
    <updated>2021-07-12T02:40:53.068Z</updated>
    
    <content type="html"><![CDATA[<p>很久没写博客了，这次想总结一下自己在ACGN收藏这条道路上越走越远，到底都走了哪些弯路哈哈哈哈 <s>（博客画风突变）</s>。这一篇算是一个开篇稿吧，想写的内容有挺多的，一些比较短的内容会放在这一篇底下。</p><a id="more"></a><p><a href="https://en.wikipedia.org/wiki/ACG_(subculture)" target="_blank" rel="noopener">ACGN</a>即Animation、Comics、Games、Novel，不知道这个年头还有多少人用这个词，但是这个词确实描述了我的兴趣爱好。虽然ACGN从名字上来看没有特定的文化限制，但是一般都是指源自日本的（日本的文化输出是不得不服啊），尤其是Animation这个词，通常在ACG里面的A其实指的是Anime（Animation的日式发音缩写），特指日本动漫。鄙人虽然喜欢看（日本）动漫小说等，算的上半个二刺螈，但是ACGN产业本身也是良莠不齐的文化产品，其中有很多优秀的产品，也不乏令人无语的奇葩。</p><p>个人认为日本ACGN的吸引力不仅仅在于产品制作精良，更在于其涉猎内容的广泛以及表达形式的多样，再加上各式产业的紧密衔接，让人很容易进这个坑里。我从初中入宅以来也接触了很多ACGN的内容，不过主要是看动漫去了，小说漫画看了个别，而日式游戏基本只接触过俩：雀龙门和东方系列。但真正让我入坑收藏的其实是从东方接触到的同人音乐，同人音乐的世界包罗万象，而又大多是限量发售，因此就勾起了我的收藏欲。之后渐渐的不仅收藏同人音乐，也去收藏起ACGN的产品了。</p><p>以前作为一个高中生，实在是没有什么存储资源存那么多的内容，不过现在好很多了，但是却也没有精力去整这些东西了。更悲催的是大二硬盘被偷了一次，导致我一半的收藏没了，也导致我很长一段时间再也没有收藏的欲望了。。。（于是落下了很多坑，悔不当初）</p><p>之后的几篇博文想介绍以下几个内容，虽是由我收藏的爱好衍生出来的一些技术，但也可以适用于很多其他的场合，因此有想把他们写下来的动力：</p><ol><li><a href="/blog/2021-07/ACGNFileManagement/" title="文件管理">文件管理</a></li><li>音频压制</li><li>音乐整理与播放器</li><li>视频压制</li><li>网页打包</li></ol><p>这篇就写到这里了，希望这个博客的坑最后也能填完orz。。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;很久没写博客了，这次想总结一下自己在ACGN收藏这条道路上越走越远，到底都走了哪些弯路哈哈哈哈 &lt;s&gt;（博客画风突变）&lt;/s&gt;。这一篇算是一个开篇稿吧，想写的内容有挺多的，一些比较短的内容会放在这一篇底下。&lt;/p&gt;
    
    </summary>
    
      <category term="ACGN" scheme="http://zyxin.xyz/blog/categories/ACGN/"/>
    
    
  </entry>
  
  <entry>
    <title>“音频发烧友” / “Audiophile” 入门及杂谈</title>
    <link href="http://zyxin.xyz/blog/2020-12/AudiophileIntroduction/"/>
    <id>http://zyxin.xyz/blog/2020-12/AudiophileIntroduction/</id>
    <published>2020-12-17T03:23:00.000Z</published>
    <updated>2021-07-12T02:40:53.068Z</updated>
    
    <content type="html"><![CDATA[<p>最近买了一些新的耳机，但是买完总感觉自己被收智商税了，于是就查查查了好多资料。这篇文章介绍我理解下音频发烧友的一些词汇是什么意思，如果你不烧耳机音响，但是想了解这个群体的，这个文章也能作为一个入门参考～Hifi领域有很多词汇我也没懂的，我也写在文章里了，如果有老烧路过请指教一二。我尝试用我学过的知识来客观解释音频领域的知识，我没上过信号处理，相关课程只上过自动控制和离散控制。</p><h1 id="发烧到底追求的是什么">发烧到底追求的是什么<a class="header-anchor" href="#发烧到底追求的是什么"> ❮</a></h1><p>刚好今天有看到<a href="https://www.youtube.com/watch?v=rM8sxFxmOUw" target="_blank" rel="noopener">一个Youtube视频</a>讲到，为什么音乐人并不care那些高端的音频设备。视频内容本身的观点是：一方面音乐人更关注的是音乐本身能不能打动人，另一方面是音乐人很多也没有那么多闲钱哈哈哈。以及底下的评论有很多人说自己是pro musician，然后疯狂喷audiophile追求的东西是虚无飘渺的。我承认烧音频领域有很多玄学都是脑放（脑补出来的），但是做耳机解码之类的厂家也是有很多pro audo engineer，不能否认这里面也是有很多技术门道的。根据这一帮自称pro musician的发言，我估计他们也没有多牛，他们的观点也有幸存者偏差在里面，并且本身不同级别不同类型的音乐也有不同的需求，因此这些评论也就看看就好。不过有一点我是同意的，听音乐最重要的还是音乐本身，对音质的追求不应放于对音乐本身的追求之上。</p><p>再打个比方，对音质的追求和对画质的追求其实是相似的，好的（照片）画质能让我们看清楚世界更多的细节，好的音质能让我们更真切地感受到被乐器包围的感觉。音频处理和视频处理也有很多相似的地方，因为他们都经常被看作信号来处理，后文我也会经常拿画质来打比方。</p><a id="more"></a><h2 id="什么是好的音质">什么是好的音质<a class="header-anchor" href="#什么是好的音质"> ❮</a></h2><p>在找到了自己喜欢的音乐之后，我们当然会希望手里的设备能更好的还原音乐本身，能听到每一点细节。因此对音质好的标准在我看来（我相信也是大多数人的观点），指的是<strong>耳朵听到的声音感觉和你站在录音的地方听到的声音感觉非常相似</strong>。由此可见在现场听，在乐器和人声面前听才能获得<strong>完美的音质</strong>，这是音质的金标准。当然这个也不是那么统一的，例如我并不觉得在歌手演唱会听到的音乐会比手机放出来的好听，因为演唱会非常嘈杂并且音响素质也不见得很好；但如果是在音乐厅听交响乐，我可以拍胸脯保证听到的声音远超电子设备播放出来的。又比如我还听很多电子音乐，里面很多音色都是直接合成出来的，那就无法通过这样的标准来定义了，这种情况下最好的音质可以定义成<strong>你听到的声音和音乐人以及调音师想让你听到的声音一致</strong>。</p><p>上面提到的音质是可以客观定义和测量的，但是另一部分人追求的音质则是他听到的声音符不符合他的口味，例如有些人喜欢温润的女声，有些人喜欢低沉的bass，这些其实都是主观的喜好。这才是发烧友的精髓——定制，就像搞机械键盘什么的，定制和折腾才是发烧友的精髓。不过我对这样的音质并没有什么追求，因为他们通常都可以通过简单的Eq（调整Equilizer）来解决。</p><h2 id="听到的声音的几个指标">听到的声音的几个指标<a class="header-anchor" href="#听到的声音的几个指标"> ❮</a></h2><p>通常对于音频发烧友来说，音质好不好是个比较笼统的词汇，因为音质好大抵是相似的，而音质差则各有各的差法。为了区分这些方面，audiophile们利用以及发明了很多与音质有关的词汇，我把比较常见的以及他们的意思列在下面了</p><table><thead><tr><th>声音基本概念</th><th>(声音的本质是声波)</th></tr></thead><tbody><tr><td>Loudness / 响度</td><td>声波的振幅，通常会取势能来计算平均振幅</td></tr><tr><td>Tone / 音调</td><td>（简单）声波的频率，真实的声音通常会是多个频率的叠加</td></tr><tr><td>Timbre / 音色</td><td>声波的形状，人通过音色区分声音的来源</td></tr></tbody></table><table><thead><tr><th>音频信号</th><th>(如何表征一个音频信号)</th></tr></thead><tbody><tr><td>Spectrogram / 频谱</td><td>频谱描述信号在各个频率上的幅度，一般通过Fourier变换计算，由于Fourier变换是可逆的，因此频谱可以唯一地对应一段声音</td></tr><tr><td>Frequency (Response) / 频率(响应)</td><td>频率响应描述输入信号和输出信号在频域上的差异</td></tr><tr><td>Phase / 相位</td><td>相位本身指周期信号中信号在周期的哪个位置，但是相位本身很少用，用的更多的是相位差。我们常用的是将相位差推广到非周期信号，然后用来描述多个声道之间的信号时间差</td></tr></tbody></table><table><thead><tr><th>可以量化的词汇</th><th>（客观描述音质）</th></tr></thead><tbody><tr><td>Bass / 低频</td><td>20Hz-20kHz的低频部分</td></tr><tr><td>Mid / 中频</td><td>20Hz-20kHz的中间部分</td></tr><tr><td>Treble / 高频</td><td>20-20kHz的高频部分</td></tr><tr><td>Imaging / 声像</td><td>声音的定位准不准，与信号相位有关。<a href="https://www.rtings.com/headphones/tests/sound-quality/imaging" target="_blank" rel="noopener">可参考Rtings的测量方法</a></td></tr><tr><td>Sound Stage / 声场</td><td>感受到的空间大小，听起来音源越分散，声场越大。这个主要是针对耳机还原音箱声场的感觉，具体解释参考<a href="https://site.douban.com/widget/notes/275603/note/118007253/" target="_blank" rel="noopener">豆瓣这篇文章</a>，测量方法参考<a href="https://www.rtings.com/headphones/tests/sound-quality/passive-soundstage" target="_blank" rel="noopener">Rtings的测试流程</a>。</td></tr><tr><td>Dynamic Range / 动态范围</td><td>在同一段声音里同时表现幅度很大和很小的信号的能力，可以参考图像的HDR技术。</td></tr><tr><td>Transient / 瞬态</td><td>这个词我是抱有疑问的，虽然控制器确实有响应时间这个参数，但是用在声音信号上感觉并不算很合适。好像一般通过追踪方波输入来看耳机的瞬态响应。</td></tr><tr><td>Signal to Noise Ratio (SNR) / 信噪比</td><td>字面意思，信号对噪声的比。这个噪音通常是音频电路的底噪。</td></tr><tr><td>Total Harmonic Distortion (THD) / 总谐波失真</td><td>输入一个纯净正弦信号，输出里这个信号的谐波就是谐波失真。</td></tr><tr><td>Intermodulation Distortion (IMD) / 互调失真</td><td>输出两个频率的信号，测输出信号的失真</td></tr><tr><td>Crosstalk / 串扰</td><td>多通道之间的信号干扰</td></tr></tbody></table><table><thead><tr><th>玄学词汇</th><th>（主观描述音质）</th></tr></thead><tbody><tr><td>Fidelity / Resolution / 解析力</td><td>这个词可能指的是低失真？有时候感觉也指超高频的频率响应。被各种厂家的广告用烂了，没有统一的解释</td></tr><tr><td>Punchy / 力度</td><td>通常指的是低频非常重</td></tr><tr><td>Congested / 拥挤 / Shouty</td><td>大概指的是声场小，或者是中高频gain太高</td></tr><tr><td>Sharp / 锐</td><td>一般是在频谱的某一小段中高频上有刺突</td></tr><tr><td>Clean / Clarity / 通透 / 纯净</td><td>应该都指的是中高频比较突出</td></tr><tr><td>Sound quality / 音质</td><td>虽然这里我们客观地讨论了什么是好音质，但是在audiophile社区里面这个词并不都是这么定义的</td></tr><tr><td>Tonality / 调性</td><td>这个词我着实没弄懂，本身是用来形容乐曲的谱调的，但是用来形容音质我也摸不找头脑</td></tr><tr><td>Layered / 层次感</td><td>这虽然我知道是什么意思，以及能听出来区别，但是觉得这个词很模糊。我猜测它与声像和声场都有关。</td></tr></tbody></table><p>其中低中高频的区别这里贴一张<a href="https://crinacle.com/2020/04/08/graphs-101-how-to-read-headphone-measurements/" target="_blank" rel="noopener">引自crinacle的图</a></p><img src="/blog/2020-12/AudiophileIntroduction/fr-chart.png" title="频率对应图"><h2 id="人能听出多大差别">人能听出多大差别<a class="header-anchor" href="#人能听出多大差别"> ❮</a></h2><p>在定义了什么是好音质以后，还有个问题是人能听出来多大的音质差别？首先一个基本常识是人的听力范围是在20Hz-20kHz之间（也有说16Hz-20kHz的），这是目前通用的标准，包括音频的采样率定在44100Hz也是参考了这个数据。另外在<a href="https://www.zhihu.com/question/274582289/answer/640857360" target="_blank" rel="noopener">这个知乎回答</a>里面，答主引用了这些数据：</p><blockquote><p>人耳所能感知到的纯单音变化最小幅度为0.3dB<br>人耳在最敏感的500Hz~2kHz段所能感知到的频率变化一般是0.2%<br>人耳所能感知的低次谐波失真变化最小量一般在1%上下</p></blockquote><p>以上这些数据可以作为参考，但是它们并不能作为硬性指标，例如说音频信号超过20kHz的部分就是完全没有意义的，我认为是不科学的。一是因为以上都是统计数据，不能否认现实世界有“金耳朵”的存在（不过至少我不是），二是以上数据来源于科学实验，其实验过程与我们听音乐的时候可能并不相同，有可能会导致音频敏感度的差别。不过至少这些数据让我们有一个大概的概念，如果一个音频设备带来的提升远小于这些数，那么极大概率你是听不出他们带来的区别的。</p><h1 id="发烧友字典">发烧友字典<a class="header-anchor" href="#发烧友字典"> ❮</a></h1><p>Hifi界另外一些让人摸不着头脑的地方就是，各种各样的词汇，以及这些词汇似乎指向的东西有时候也很不明确。。。这里把我自己学到的记一下。</p><h2 id="音乐播放器系统组成">音乐播放器系统组成<a class="header-anchor" href="#音乐播放器系统组成"> ❮</a></h2><p>我们这里不考虑录音室的系统组成，而只考虑用户的系统组成。</p><h3 id="音源">音源<a class="header-anchor" href="#音源"> ❮</a></h3><p>好像音源又叫<strong>前端</strong>？整那么邪乎干啥。。。音源有CD机、电脑、唱片机（俗称转盘turntable？），因为现在都是数字音乐了，因此重要的就是源文件的音质。音频格式有很多说法，首先分PCM和DSD两种，PCM是时域采样而DSD是频域采样。PCM又有很多指标，例如位深指音频每个采样的精度（通常是16bit，HiRes则有24bit以上），采样率指采样的频率，根据Nyquist-Shannon采样定律，频率高于20kHz理论上就能做到无损采样。然后音乐文件的格式又分有损和无损，有损格式如果比特率足够高还行，如果很低那就会非常严重地影响音质。然后音源会输出到数字界面。</p><h3 id="数字界面">数字界面<a class="header-anchor" href="#数字界面"> ❮</a></h3><p>虽然机油送我了一个数字界面，但是我并不能听出区别，以及我到现在也不是很清楚这个界面是干什么的。根据<a href="https://www.zhihu.com/question/30806888/answer/50247612" target="_blank" rel="noopener">我知乎看到的资料</a>，数字界面是把USB信号转换成DAC芯片能够直接读取的信号。外置数字界面的好处一个是时钟（可能）比内置更加精确，另一个是给没有USB或者火线（IEEE-1394协议）接口的DAC提供输入。这里就涉及另一个玄学的概念，叫<strong>时钟抖动（Jitter）</strong>。由于数字信号的采样（指PCM）是恒定频率的，因此如果数字线路的时钟频率不稳定，是会非常影响DAC转换结果的。抖动可以来源于时钟本身（如晶振），也可能来源于数字信号传输的接口芯片。不过就像知乎另一个回答说的，一般这种抖动都非常非常细微，我并不认为这对信号能有太多影响，并且我确实也听不出来。想感受一下多玄学的可以再看看<a href="http://www.erji.net/forum.php?mod=viewthread&amp;tid=7494&amp;extra=pageD1&amp;page=" target="_blank" rel="noopener">耳机大家庭的文章</a>。。。</p><h3 id="解码器（Digital-Analog-Converter，DAC）">解码器（Digital-Analog Converter，DAC）<a class="header-anchor" href="#解码器（Digital-Analog-Converter，DAC）"> ❮</a></h3><p>DAC就是数模转换器，用来将数字信号转成模拟信号。这个过程我觉得挺重要的，因为数模转换（模数转换）带来的信号损失还是很明显的。从控制理论里的零阶保持（ZOH）来理解的话，<a href="https://www.dummies.com/education/science/science-engineering/real-world-signals-and-systems-case-solving-the-dac-zoh-droop-problem-in-the-z-domain/" target="_blank" rel="noopener">转换过程会影响信号的相位和高频</a>。DAC的质量在整个音频管道中还是比较重要的。另外一个特性是DAC支持的格式，现在主流的hifi解码都支持高位深和DSD的音频了。</p><h3 id="放大器（Amplifier，Amp）">放大器（Amplifier，Amp）<a class="header-anchor" href="#放大器（Amplifier，Amp）"> ❮</a></h3><p>用于音箱的一般称功放（功率放大器），用于耳机的一般称耳放。放大器的作用就是把解码出来的模拟信号放大到合适的音量。很多设备如手机，甚至一些DAC都把功放集成进去了。独立的放大器设备有两个好处，一个的更好的电磁隔离，更少的底噪，另一个是可以提供更大的功率储备，用来推特别难推的耳机（如低阻低灵敏度的耳机），在极端状态下可以减少失真。</p><p>耳放还分两种：电子管耳放（胆机），晶体管耳放（石机）。我没听过胆机，但都说胆机声音温润，估计说到底就是胆机削低了高频。因此如果纯音质角度看，选一个低失真的耳放就可以了。</p><p>功放有时分前级后级，据我查到的资料说，前级是low-pass filter，用来处理低频，然后后级整体放大？这里我也不懂了，搜到各种不一样的说法，我觉得我还是别管这玩意了。（<a href="https://www.zhihu.com/question/30806888" target="_blank" rel="noopener">知乎参考在此</a>）</p><h3 id="接口">接口<a class="header-anchor" href="#接口"> ❮</a></h3><p>再讲一下不同音频设备之间的连接接口，数字的接口一般就是USB和S/PDIF了，模拟信号主要有TRS，TRRS，XLR等等，可以<a href="http://sound.zol.com.cn/512/5124960_all.html" target="_blank" rel="noopener">参考这篇文章</a>。这些接口本身没什么差别，虽然说有人对这个很在意，甚至还有人对墙上插座的接口很讲究，但是我觉得这都是玄学= =（就是不科学）</p><h3 id="回放设备">回放设备<a class="header-anchor" href="#回放设备"> ❮</a></h3><p>就是音箱或者耳机，这玩意也是有各种产品。音箱分有源音箱和无源音箱，有源就是内置了放大器的。耳机则分入耳（In-ear, IEM），和头戴式耳机（On-ear/Over-ear）。具体这就不展开了。</p><h2 id="耳机相关">耳机相关<a class="header-anchor" href="#耳机相关"> ❮</a></h2><h3 id="单元">单元<a class="header-anchor" href="#单元"> ❮</a></h3><ul><li>Balanced Armerture / BA / 动铁：平衡铁通过磁场变化，带动振膜运动。</li><li>Dynamic Driver / DD / 动圈：磁场直接驱动线圈，带动振膜运动。</li><li>Planar Magnetic / 平板：磁场直接驱动金属板运动。</li><li>Piezoelectric Ceramic / 压电陶瓷单元：压电晶体带动振膜变形发声。</li><li>Electrostatic / EST / 静电：电场带动振膜运动。<br>(可以参见<a href="https://www.youtube.com/watch?v=BKhS7X8rs74" target="_blank" rel="noopener">Linus的视频</a>)</li></ul><h3 id="线材">线材<a class="header-anchor" href="#线材"> ❮</a></h3><p>首先要说明的是，音频线材对声音的影响微乎其微。线材影响声音的原理是不同线材有不同的阻抗、容抗和感抗曲线（主要是阻抗和荣抗），因此可能会微微影响低阻耳机的频率响应。另外，说线材能提升音质的几乎就是扯淡。参见<a href="https://www.zhihu.com/question/274582289" target="_blank" rel="noopener">该知乎回答</a>。</p><ul><li>TPC: 电解铜</li><li>OFC: 无氧铜</li><li>OCC: 单结晶无氧铜</li><li>5N/6N/7N: （铜）纯度，几个N就有几个9。</li><li>Litz/Litz2: 绕线方式，参考<a href="https://www.newenglandwire.com/product/litz-wire-types-and-constructions/" target="_blank" rel="noopener">Litz官网</a>。并不知道不同绕线方法对感抗有没有什么影响。。<br>现在的线材基本都是无氧铜。个人认为为了好看和功能换线可以，为了换口味换线可以，为了增加屏蔽层减少外部信号噪音可以，但是为了“提升音质”就纯粹是智商税了。另外上面提到的都是传递模拟信号的线材，对那种audio-grade的USB线我是打死都不信有什么区别的，数字信号对这么点阻抗的变化根本不可能有什么反应。</li></ul><h3 id="耳塞-耳垫">耳塞/耳垫<a class="header-anchor" href="#耳塞-耳垫"> ❮</a></h3><p>耳塞(Tip)和耳垫(Pad)可以影响声音在进入耳朵之前的回响，因此也是会改变声音的。耳垫的影响比较大，耳塞我觉得比较小。不过同样的，我认为不同的耳塞耳垫都是相当于给耳机加了EQ，因此不必追求高音质的耳垫。有一点例外，如果耳塞耳垫有漏音的话，会严重影响音质，这种情况下就需要更换了。</p><hr><p>这大概就是我对audiophile各种知识的笔记了。在了解这么多之后，我还是觉得，选一个小巧、功能多、性能还过得去的DAC和amp，然后选个音质够用的耳机就行了，不用再换了。音质这玩意到最后音质提升的性价比实在太低了，还是找更多好听的音乐来的实在。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近买了一些新的耳机，但是买完总感觉自己被收智商税了，于是就查查查了好多资料。这篇文章介绍我理解下音频发烧友的一些词汇是什么意思，如果你不烧耳机音响，但是想了解这个群体的，这个文章也能作为一个入门参考～Hifi领域有很多词汇我也没懂的，我也写在文章里了，如果有老烧路过请指教一二。我尝试用我学过的知识来客观解释音频领域的知识，我没上过信号处理，相关课程只上过自动控制和离散控制。&lt;/p&gt;&lt;h1 id=&quot;发烧到底追求的是什么&quot;&gt;发烧到底追求的是什么&lt;a class=&quot;header-anchor&quot; href=&quot;#发烧到底追求的是什么&quot;&gt; ❮&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;刚好今天有看到&lt;a href=&quot;https://www.youtube.com/watch?v=rM8sxFxmOUw&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;一个Youtube视频&lt;/a&gt;讲到，为什么音乐人并不care那些高端的音频设备。视频内容本身的观点是：一方面音乐人更关注的是音乐本身能不能打动人，另一方面是音乐人很多也没有那么多闲钱哈哈哈。以及底下的评论有很多人说自己是pro musician，然后疯狂喷audiophile追求的东西是虚无飘渺的。我承认烧音频领域有很多玄学都是脑放（脑补出来的），但是做耳机解码之类的厂家也是有很多pro audo engineer，不能否认这里面也是有很多技术门道的。根据这一帮自称pro musician的发言，我估计他们也没有多牛，他们的观点也有幸存者偏差在里面，并且本身不同级别不同类型的音乐也有不同的需求，因此这些评论也就看看就好。不过有一点我是同意的，听音乐最重要的还是音乐本身，对音质的追求不应放于对音乐本身的追求之上。&lt;/p&gt;&lt;p&gt;再打个比方，对音质的追求和对画质的追求其实是相似的，好的（照片）画质能让我们看清楚世界更多的细节，好的音质能让我们更真切地感受到被乐器包围的感觉。音频处理和视频处理也有很多相似的地方，因为他们都经常被看作信号来处理，后文我也会经常拿画质来打比方。&lt;/p&gt;
    
    </summary>
    
      <category term="Misc" scheme="http://zyxin.xyz/blog/categories/Misc/"/>
    
    
      <category term="Audiophile" scheme="http://zyxin.xyz/blog/tags/Audiophile/"/>
    
  </entry>
  
  <entry>
    <title>Minecraft 1.12建服及侦测器BUD</title>
    <link href="http://zyxin.xyz/blog/2020-12/MCBud112/"/>
    <id>http://zyxin.xyz/blog/2020-12/MCBud112/</id>
    <published>2020-12-16T23:09:30.000Z</published>
    <updated>2021-07-12T02:40:53.108Z</updated>
    
    <content type="html"><![CDATA[<p>进来给实验室的服务器上装了个Minecraft服务器，给大家闲来无事上来种种菜，顺便体验一下新版本的特性。之前最高只玩过1.8，现在虽然更新到1.16了，但是听说很多Mod都还是只支持到1.12，所以就搭了1.12的服务器。基岩版的MC（Win10自带的那个）虽然性能很好，但是由于不购买就没法玩，所以考虑到大家肯定最开始都不想买，以及那个开服好像很麻烦，就还是搭了Java的服务器。</p><h1 id="一分钟上手Minecraft开服">一分钟上手Minecraft开服<a class="header-anchor" href="#一分钟上手Minecraft开服"> ❮</a></h1><p>以前玩Minecraft的时候都觉得开服务器好麻烦，要知道各种各样的配置方法，因此很佩服服主管理这些东西。直到有一天我搜到了这个：<a href="https://github.com/itzg/docker-minecraft-server" target="_blank" rel="noopener">docker-minecraft-server</a>，瞬间感觉一键开服不是梦了！这个repo把Minecraft的服务器版本以及Bukkit/Spigot服务器端Mod框架（可以理解成服务器上的Forge）都嵌进去了，简直不要太方便。数据也是从host的硬盘里mount进去的，因此如果你的服务器要转移或者备份也很方便。有了这个，开服只需要一行命令（假设你服务器上有docker）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 25565:25565 --name mc -e EULA=TRUE itzg/minecraft-server</span><br></pre></td></tr></table></figure><a id="more"></a><p>由于可以设置的环境变量非常多，因此我后来把配置都写到了docker-compose文件里面，这样修改设置后启动服务器就更简单了～目前我的设置如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">'3.8'</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">minecraft:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">itzg/minecraft-server</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"25565:25565"</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"/home/jacobz/Minecraft/docker-data:/data"</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="attr">MEMORY:</span> <span class="string">4G</span></span><br><span class="line">      <span class="attr">EULA:</span> <span class="string">"TRUE"</span></span><br><span class="line">      <span class="attr">VERSION:</span> <span class="number">1.12</span><span class="number">.2</span></span><br><span class="line">      <span class="attr">ENABLE_AUTOPAUSE:</span> <span class="string">"TRUE"</span></span><br><span class="line">      <span class="comment"># OVERRIDE_SERVER_PROPERTIES: "TRUE"</span></span><br><span class="line">      <span class="attr">MAX_TICK_TIME:</span> <span class="string">"-1"</span></span><br><span class="line">      <span class="attr">ONLINE_MODE:</span> <span class="string">"FALSE"</span></span><br><span class="line">      <span class="attr">TZ:</span> <span class="string">US/Eastern</span></span><br><span class="line">      <span class="attr">DIFFICULTY:</span> <span class="string">easy</span></span><br><span class="line">      <span class="attr">TYPE:</span> <span class="string">BUKKIT</span></span><br><span class="line">      <span class="attr">OPS:</span> <span class="string">cmpute</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">always</span></span><br></pre></td></tr></table></figure><h1 id="侦测器单片BUD">侦测器单片BUD<a class="header-anchor" href="#侦测器单片BUD"> ❮</a></h1><p>在服务器上玩了几天，最终还是想搭一个自动农场来解决温饱问题。再不去骗村民的情况下，最方便的食物我觉得就是南瓜饼了，它的原料（鸡蛋、糖、南瓜）都是非常好自动化的。因此我就想着顺便琢磨一下有侦测器之后自动农场有没有什么更方便的方法。甘蔗机在<a href="/blog/2017-08/MCTowerSugarcane/" title="我以前甘蔗机的博文">我以前甘蔗机的博文</a>里面有写到，侦测器搭甘蔗机的效率不如传统的BUD，因此主要可以改动的就是在南瓜机上了。感觉应该不是很难，因此我本地琢磨了一会，弄出来两种利用侦测器的单片BUD：</p><table><tr><th>上置型</th><th>下置型</th></tr><tr></tr><tr><td><link rel="stylesheet" href="/blog/css/minecraft.css" type="text/css"><p></p><div class="layered-blueprint" style="min-height:128px;width:128px"><input type="radio" id="mc_schematic_侧视图_0_活塞-沙子版本" class="layered-blueprint-radio" name="侧视图_0" checked><label for="mc_schematic_侧视图_0_活塞-沙子版本" class="layered-blueprint-tab">活塞+沙子版本</label><div class="layered-blueprint-layer"><table class="schematic" cellspacing="0" cellpadding="0" style="margin:0;line-height:0"><tbody><tr><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-128px -192px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-512px -96px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td></tr><tr><td><div><span class="sprite schematic-sprite" style="background-position:-32px -544px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-288px -352px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -224px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td></tr><tr><td><div><span class="text">O</span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-320px -352px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td></tr><tr><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td></tr></tbody></table></div><input type="radio" id="mc_schematic_侧视图_0_粘性活塞版本" class="layered-blueprint-radio" name="侧视图_0"><label for="mc_schematic_侧视图_0_粘性活塞版本" class="layered-blueprint-tab">粘性活塞版本</label><div class="layered-blueprint-layer"><table class="schematic" cellspacing="0" cellpadding="0" style="margin:0;line-height:0"><tbody><tr><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-512px -96px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td></tr><tr><td><div><span class="sprite schematic-sprite" style="background-position:-32px -544px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-288px -448px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -224px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td></tr><tr><td><div><span class="text">O</span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-320px -352px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td></tr><tr><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td></tr></tbody></table></div></div><p></p></td><td><link rel="stylesheet" href="/blog/css/minecraft.css" type="text/css"><p></p><div class="layered-blueprint" style="min-height:128px;width:128px"><input type="radio" id="mc_schematic_侧视图_12_活塞-沙子版本" class="layered-blueprint-radio" name="侧视图_12" checked><label for="mc_schematic_侧视图_12_活塞-沙子版本" class="layered-blueprint-tab">活塞+沙子版本</label><div class="layered-blueprint-layer"><table class="schematic" cellspacing="0" cellpadding="0" style="margin:0;line-height:0"><tbody><tr><td style="width:32px;height:32px"></td><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-256px -352px"><br></span></div></td><td><div><span class="text">O</span></div></td></tr><tr><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -224px"><br></span></div></td><td style="width:32px;height:32px"></td><td><div><span class="sprite schematic-sprite" style="background-position:-96px -544px"><br></span></div></td></tr><tr><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-512px -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-128px -192px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td></tr><tr><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-288px -352px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td></tr></tbody></table></div><input type="radio" id="mc_schematic_侧视图_12_粘性活塞版本" class="layered-blueprint-radio" name="侧视图_12"><label for="mc_schematic_侧视图_12_粘性活塞版本" class="layered-blueprint-tab">粘性活塞版本</label><div class="layered-blueprint-layer"><table class="schematic" cellspacing="0" cellpadding="0" style="margin:0;line-height:0"><tbody><tr><td style="width:32px;height:32px"></td><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-256px -352px"><br></span></div></td><td><div><span class="text">O</span></div></td></tr><tr><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -224px"><br></span></div></td><td style="width:32px;height:32px"></td><td><div><span class="sprite schematic-sprite" style="background-position:-96px -544px"><br></span></div></td></tr><tr><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-512px -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td></tr><tr><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-288px -448px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td></tr></tbody></table></div></div><p></p></td></tr><tr></tr></table><p>上图中O代表检测更新的地方，可以看见上置型的结构比下置的要稍微精简一点点，并且由于南瓜只能生成在泥土上，因此我最后使用了上置型的方法搭了自动南瓜机。对比，只需要把这个结构横着堆叠一下就行，在南瓜机上面有了侦测器确实可以大大减小粘性活塞的使用。不过由于这个结构比之前的方案宽度多了一格，因此没一层可能只能容纳两排南瓜了，因此如果要更密集的堆叠可能需要考虑改进这个结构。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;进来给实验室的服务器上装了个Minecraft服务器，给大家闲来无事上来种种菜，顺便体验一下新版本的特性。之前最高只玩过1.8，现在虽然更新到1.16了，但是听说很多Mod都还是只支持到1.12，所以就搭了1.12的服务器。基岩版的MC（Win10自带的那个）虽然性能很好，但是由于不购买就没法玩，所以考虑到大家肯定最开始都不想买，以及那个开服好像很麻烦，就还是搭了Java的服务器。&lt;/p&gt;&lt;h1 id=&quot;一分钟上手Minecraft开服&quot;&gt;一分钟上手Minecraft开服&lt;a class=&quot;header-anchor&quot; href=&quot;#一分钟上手Minecraft开服&quot;&gt; ❮&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;以前玩Minecraft的时候都觉得开服务器好麻烦，要知道各种各样的配置方法，因此很佩服服主管理这些东西。直到有一天我搜到了这个：&lt;a href=&quot;https://github.com/itzg/docker-minecraft-server&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;docker-minecraft-server&lt;/a&gt;，瞬间感觉一键开服不是梦了！这个repo把Minecraft的服务器版本以及Bukkit/Spigot服务器端Mod框架（可以理解成服务器上的Forge）都嵌进去了，简直不要太方便。数据也是从host的硬盘里mount进去的，因此如果你的服务器要转移或者备份也很方便。有了这个，开服只需要一行命令（假设你服务器上有docker）&lt;/p&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;docker run -d -p 25565:25565 --name mc -e EULA=TRUE itzg/minecraft-server&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Game" scheme="http://zyxin.xyz/blog/categories/Game/"/>
    
      <category term="Minecraft" scheme="http://zyxin.xyz/blog/categories/Game/Minecraft/"/>
    
    
      <category term="Redstone" scheme="http://zyxin.xyz/blog/tags/Redstone/"/>
    
      <category term="Automation" scheme="http://zyxin.xyz/blog/tags/Automation/"/>
    
      <category term="Docker" scheme="http://zyxin.xyz/blog/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Notes for Algebra Basics</title>
    <link href="http://zyxin.xyz/blog/2020-06/AlgebraBasicsNotes/"/>
    <id>http://zyxin.xyz/blog/2020-06/AlgebraBasicsNotes/</id>
    <published>2020-06-28T01:02:13.000Z</published>
    <updated>2021-07-12T02:40:53.068Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Selected notes from <code>ROB 501</code> and <code>ME 564</code>.<br>$\{x_i\}^b_a$ denotes set $\{x_a, x_{a+1}, \ldots, x_b\}$<br>TODO: add Jordan Form</p></blockquote><h1 id="Algebraic-Structures">Algebraic Structures<a class="header-anchor" href="#Algebraic-Structures"> ❮</a></h1><h2 id="Operation">Operation<a class="header-anchor" href="#Operation"> ❮</a></h2><ul><li>Definition: an (binary, closed) <strong>operation</strong> $\ast$ on a set $S$ is a mapping of $S\times S\to S$</li><li><strong>Commutative</strong>: $x\ast y=y\ast x,\;\forall x,y\in S$</li><li><strong>Associative</strong>: $(x\ast y)\ast z=x\ast (y\ast z),\;\forall x,y,z\in S$</li></ul><h2 id="Group">Group<a class="header-anchor" href="#Group"> ❮</a></h2><ul><li>Definition: a <strong>group</strong> is a pair $(\mathcal{S},\ast)$ with following axioms<ol><li>$\ast$ is associative on $\mathcal{S}$</li><li>(Identity element) $\exists e\in \mathcal{S}\text{ s.t. }x\ast e=e\ast x=x,\;\forall x\in \mathcal{S}$</li><li>(Inverse element) $\forall x\in \mathcal{S}, \exists x’ \in \mathcal{S}\text{ s.t. }x\ast x’=x’\ast x=e$</li></ol></li><li><strong>Abelian</strong>: a group is called <strong>abelian group</strong> if $\ast$ is also commutative</li></ul><a id="more"></a><h2 id="Ring">Ring<a class="header-anchor" href="#Ring"> ❮</a></h2><ul><li>Definition: a <strong>ring</strong> is a triplet $(\mathcal{R},+,\ast)$ consisting of a set of <code>scalars</code> $\mathcal{R}$ and two operators + and $\ast$ with following axioms<ol><li>$(\mathcal{R},+)$ is an abelian group with identity denoted $0$</li><li>$\forall a,b,c \in \mathcal{R}\text{ s.t. }a\ast(b\ast c) = (a\ast b)\ast c$</li><li>$\exists 1\in\mathcal{R}, \forall a\in\mathcal{R}\text{ s.t. }a\cdot 1=a$</li><li>$\ast$ is distributive over $+$</li></ol></li></ul><h2 id="Field">Field<a class="header-anchor" href="#Field"> ❮</a></h2><ul><li>Definition: a <strong>field</strong> $(\mathcal{F},+,\ast)$ is a ring where $(\mathcal{F}\backslash\{0\},\ast)$ is also an abelian group.<blockquote><p>Difference from ring to field is that $\ast$ need to be commutative and have a multiplicative inverse</p></blockquote></li></ul><h2 id="Vector-Space">Vector Space<a class="header-anchor" href="#Vector-Space"> ❮</a></h2><ul><li>Definition: a <strong>vector space</strong> (aka. <strong>linear space</strong>) is a triplet $(\mathcal{U},\oplus,\cdot)$ defined over a field $(\mathcal{F},+,\ast)$ with following axioms, where set $\mathcal{U}$ is called <code>vectors</code>, operator $\oplus$ is called <code>vector addition</code> and mapping $\cdot$ is called <code>scalar multiplication</code>:<ol><li>(<strong>Null vector</strong>) $(\mathcal{U},+)$ is an abelian group with identity element $\emptyset$</li><li>Scalar multiplication is a mapping of $\mathcal{F}\times\mathcal{U}\to\mathcal{U}$</li><li>$\alpha\cdot(x\oplus y) = \alpha\cdot x \oplus \alpha\cdot y,\;\forall x,y\in\mathcal{U};\alpha\in\mathcal{F}$</li><li>$(\alpha+\beta)\cdot x = \alpha\cdot x\oplus\beta\cdot x,\;\forall x\in\mathcal{U};\alpha,\beta\in\mathcal{F}$</li><li>$(\alpha\ast\beta)\cdot x=\alpha\cdot(\beta\cdot x),\;\forall x\in\mathcal{U};\alpha,\beta\in\mathcal{F}$</li><li>$1_\mathcal{F}\cdot x=x$</li></ol><blockquote><p>Usually we don’t distinguish vector addition $\oplus$ and addition of scalar $+$. Juxtaposition is also commonly used for <em>both</em> scalar multiplication $\cdot$ and multiplication of scalars $\ast$</p></blockquote></li><li><strong>Subspace</strong>: a subspace $\mathcal{V}$ of a linear space $\mathcal{U}$ over field $\mathcal{F}$ is a subset of $\mathcal{U}$ which is itself a linear space over $\mathcal{F}$ under same vector addition and scalar multiplication.</li></ul><h3 id="Basis-Coordinate">Basis &amp; Coordinate<a class="header-anchor" href="#Basis-Coordinate"> ❮</a></h3><ul><li><strong>Linear Independence</strong>: Let $\mathcal{V}$ be a vector space over $\mathcal{F}$ and let $X=\{x_i\}^n_1\subset \mathcal{V}$<ul><li>X is <strong>linearly dependent</strong> if $\exists \alpha_1,\ldots,\alpha_n\in\mathcal{F}$ not all 0 s.t. $\sum^n_{i=1} \alpha_i x_i=0$.</li><li>X is <strong>linearly independent</strong> if $\sum^n_{i=1} \alpha_i x_i=0 \Rightarrow \alpha_1=\alpha_2=\ldots=\alpha_n=0$</li></ul></li><li><strong>Span</strong>: Given a set of vectors $V$, the set of linear combinations of vectors in $V$ is called the <strong>span</strong> of it, denoted $\mathrm{span}\{V\}$</li><li><strong>Basis</strong>: A set of linearly independent vectors in a linear space $\mathcal{V}$ is a <strong>basis</strong> if every vector in $\mathcal{V}$ can be expressed as a <em>unique linear combination</em> of these vectors. (see below “Coordinate”)<ul><li>Basis Expansion: Let $(X,\mathcal{F})$ be a vector space of dimension n. If $\{v_i\}^k_1,\;1\leqslant k&lt; n$ is linearly independent, then $\exists \{v_i\}^n_{k+1}$ such that $\{v_i\}_1^n$ is a basis.</li><li><strong>Reciprocal Basis</strong>: Given basis $\{v_i\}^n_1$, a set ${r_i}^1_n$ that satifies $\langle r_i,v_j \rangle=\delta_i(j)$ is a reciprocal basis. It can be generated by Gram-Schmidt Process and $\forall x\in\mathcal{X}, x=\sum^n_{i=1}\langle r_i,x\rangle v_i$.</li></ul></li><li><strong>Dimension</strong>: <em>Cardinality</em> of the basis is called the <strong>dimension</strong> of that vector space, which is equal to <em>the maximum number of linearly independent vectors</em> in the space. Denoted as $dim(\mathcal{V})$.<ul><li>In an $n$-dimensional vector space, any set of $n$ linearly independent vectors is a basis.</li></ul></li><li><strong>Coordinate</strong>: For a vector $x$ in vector space $\mathcal{V}$, given a basis $\{e_1, \ldots, e_n\}$ we can write $x$ as $x=\sum^n_{i=1}\beta_i e_i=E\beta$ where $E=\begin{bmatrix}e_1&amp;e_2&amp;\ldots&amp;e_n\end{bmatrix}$ and $\beta=\begin{bmatrix}\beta_1&amp;\beta_2&amp;\ldots&amp;\beta_n\end{bmatrix}^\top$. Here $\beta$ is called the <strong>representation</strong> (or <strong>coordinate</strong>) of $x$ given the basis $E$.</li></ul><h3 id="Norm-Inner-product">Norm &amp; Inner product<a class="header-anchor" href="#Norm-Inner-product"> ❮</a></h3><ul><li><strong>Inner Product</strong>: an operator on two vectors that produces a scalar result (i.e. $\langle\cdot,\cdot\rangle:\mathcal{V}\to\mathbb{R}\;or\;\mathbb{C}$) with following axioms:<ol><li>(Symmetry) $\langle x,y \rangle=\overline{\langle y,x\rangle},\;\forall x,y\in\mathcal{V}$</li><li>(Bilinearity) $\langle \alpha x+\beta y,z\rangle=\alpha\langle x,z\rangle+\beta\langle y,z\rangle,\;\forall x,y,z\in\mathcal{V};\alpha,\beta\in\mathbb{C}$</li><li>(Pos. definiteness) $\langle x,x\rangle\geqslant 0,\;\forall x\in\mathcal{V}$ and $\langle x,x\rangle=0\Rightarrow x=0_\mathcal{V}$</li></ol></li><li><strong>Inner Product Space</strong>: A linear space with a defined inner product</li><li><strong>Orthogonality</strong>:<ul><li>Perpedicularity of vectors ($x\perp y$): $\langle x,y\rangle=0$</li><li>Perpedicularity of a vector to a set ($y\perp\mathcal{S},\mathcal{S}\subset\mathcal{V}$): $y\perp x,\;\forall x\in\mathcal{S}$</li><li><strong>Orthogonal Set</strong>: set $\mathcal{S}\subset(\mathcal{U},\langle\cdot,\cdot\rangle)$ is orthogonal $\Leftrightarrow x\perp y,\;\forall x,y\in\mathcal{S},x\neq y$</li><li><strong>Orthonormal Set</strong>: set $\mathcal{S}$ is orthonormal iff $\mathcal{S}$ is orthogonal and $\Vert x\Vert=1,\;\forall x\in\mathcal{S}$</li><li>Orthogonality of sets ($\mathcal{X}\perp\mathcal{Y}$): $\langle x,y\rangle=0,\;\forall x\in\mathcal{X};y\in\mathcal{Y}$</li><li><strong>Orthogonal Complement</strong>: Let $(\mathcal{V},\langle\cdot,\cdot\rangle)$ be an inner product space and let $\mathcal{U}\subset\mathcal{V}$ be a subspace of $\mathcal{V}$, the orthogonal complement of $\mathcal{U}$ is $\mathcal{U}^\perp=\left\{v\in\mathcal{V}\middle|\langle v,u\rangle=0,\;\forall u\in\mathcal{U}\right\}$.<ul><li>$\mathcal{U}^\perp\subset\mathcal{V}$ is a subspace</li><li>$\mathcal{V}=\mathcal{U}\overset{\perp}{\oplus}\mathcal{U}^\perp$ ($\oplus$: direct sum, $\overset{\perp}{\oplus}$: orthogonal sum)</li></ul></li></ul></li><li><strong>Norm</strong>: A <strong>norm</strong> on a linear space $\mathcal{V}$ is mapping $\Vert\cdot\Vert:\;\mathcal{V}\to\mathbb{R}$ such that:<ol><li>(Positive definiteness) $\Vert x\Vert\geqslant 0\;\forall x\in \mathcal{V}$ and $\Vert x\Vert =0\Rightarrow x=0_\mathcal{V}$</li><li>(Homogeneous) $\Vert \alpha x\Vert=|\alpha|\cdot\Vert x\Vert,\;\forall x\in\mathcal{V},\alpha\in\mathbb{R}$</li><li>(Triangle inequality) $\Vert x+y\Vert\leqslant\Vert x\Vert+\Vert y\Vert$</li></ol></li><li><strong>Distance</strong>: Norm can be used to measure distance between two vectors. Meanwhile, distance from a vector to a (sub)space is defined as $d(x,\mathcal{S})=\inf_{y\in\mathcal{S}} d(x,y)=\inf_{y\in\mathcal{S}} \Vert x-y\Vert$<ul><li><strong>Projection Point</strong>: $x^* =\arg\min_{y\in\mathcal{S}}\Vert x-y\Vert$ is the projection point of $x$ on linear space $\mathcal{S}$.</li><li><strong>Projection Theorem</strong>: $\exists !x^* \in\mathcal{S}$ s.t. $\Vert x-x^* \Vert=d(x,\mathcal{S})$ and we have $(x-x^*) \perp\mathcal{S}$</li><li><strong>Orthogonal Projection</strong>: $P(x)=x^*:\mathcal{X}\to\mathcal{M}$ is called the orthogonal projection of $\mathcal{X}$ onto $\mathcal{M}$</li></ul></li><li><strong>Normed Space</strong>: A linear space with a defined norm $\Vert\cdot\Vert$, denoted $(\mathcal{V},\mathcal{F},\Vert\cdot\Vert)$<blockquote><p>A inner product space is always a normed space because we can define $\Vert x\Vert=\sqrt{\langle x,x\rangle}$</p></blockquote></li><li>Common $\mathbb{R}^n$ Norms:<ul><li>Euclidean norm (2-norm): $\Vert x\Vert_2=\left(\sum^n_{i=1}|x_i|^2\right)^{1/2}=\left\langle x,x\right\rangle^{1/2}=\left(x^\top x\right)^{1/2}$</li><li>$l_p$ norm (p-norm): $\Vert x\Vert_p=\left(\sum^n_{i=1}|x_i|^p\right)^{1/p}$</li><li>$l_1$ norm: $\Vert x\Vert_1=\sum^n_{i=1}|x_i|$</li><li>$l_\infty$ norm: $\Vert x\Vert_\infty=\max_{i}\{x_i\}$</li></ul></li><li>Common matrix norms:<blockquote><p>Matrix norms are also called <strong>operator norms</strong>, can measure how much a linear operator “magnifies” what it operates on.</p></blockquote><ul><li>A general form induced from $\mathbb{R}^n$ norm: $$\Vert A\Vert=\sup_{x\neq 0}\frac{\Vert Ax\Vert}{\Vert x\Vert}=\sup_{\Vert x\Vert=1}\Vert Ax\Vert$$</li><li>$\Vert A\Vert_1=\max_j\left(\sum^n_{i=1}|a_{ij}|\right)$</li><li>$\Vert A\Vert_2=\left[ \max_{\Vert x\Vert=1}\left\{(Ax)^* (Ax)\right\}\right]^{1/2}=\left[ \lambda_{max}(A^ *A)\right]^{1/2}$ ($\lambda_{max}$: largest eigenvalue)</li><li>$\Vert A\Vert_\infty=\max_i\left(\sum^n_{j=1}|a_{ij}|\right)$</li><li>(Frobenius Norm) $\Vert A\Vert_F=\left[ \sum^m_{i=1}\sum^n_{j=1}\left|a_{ij}\right|^2\right]^{1/2}=\left[ tr(A^*A)\right]^{1/2}$</li></ul></li><li>Useful inequations:<ul><li><strong>Cauchy-Schwarz</strong>: $|\langle x,y\rangle|\leqslant\left\langle x,x\right\rangle^{1/2}\cdot\left\langle y,y\right\rangle^{1/2}$</li><li><strong>Triangle</strong> (aka. $\Delta$): $\Vert x+y\Vert\leqslant\Vert x\Vert+\Vert y\Vert$<blockquote><p>Lemma: $\Vert x-y\Vert \geqslant \left| \Vert x\Vert-\Vert y\Vert \right|$</p></blockquote></li><li><strong>Pythagorean</strong>: $x\perp y \Leftrightarrow \Vert x+y\Vert=\Vert x\Vert+\Vert y\Vert$</li></ul></li></ul><h3 id="Gramian">Gramian<a class="header-anchor" href="#Gramian"> ❮</a></h3><ul><li><strong>Gram-Schmidt Process</strong>: A method to find orthogonal basis $\{v_i\}^n_1$ given an ordinary basis $\{y_i\}^n_1$. It’s done by perform $v_k=y_k-\sum^{k-1}_{j=1}\frac{\langle y_k,v_j\rangle}{\langle v_j,v_j \rangle}\cdot v_j$ iteratively from 1 to $n$. To get an orthonormal basis, just normalize these vectors.</li><li><a href="https://en.wikipedia.org/wiki/Gramian_matrix" target="_blank" rel="noopener"><strong>Gram Matrix</strong></a>: The Gram matrix generated from vectors $\{y_i\}_ 1^k$ is denoted $G(y_ 1,y_ 2,\ldots,y_ k)$. Its element $G_{ij}=\langle y_i,y_j\rangle$<ul><li><strong>Gram Determinant</strong>: $g(y_1,y_2,\ldots,y_n)=\det G$</li><li><strong>Normal Equations</strong>: Given subspace $\mathcal{M}$ and its basis $\{y_i\}^n_1$, the projection point of $\forall x\in\mathcal{M}$ can be represented by $$x^*=\alpha y=\begin{bmatrix}\alpha_1&amp;\alpha_2&amp;\ldots&amp;\alpha_n\end{bmatrix}\begin{bmatrix}y_1\\y_2\\ \vdots \\y_n\end{bmatrix},\;\beta=\begin{bmatrix}\langle x,y_1\rangle\\ \langle x,y_2\rangle\\ \vdots\\ \langle x,y_n\rangle\end{bmatrix} where\;G^\top\alpha=\beta$$<blockquote><p>For least-squares problem $Ax=b$, consider $\mathcal{M}$ to be the column space of $A$, then $G=A^\top A,\;\beta=A^\top b,\;G^\top\alpha=\beta\Rightarrow\alpha=(A^\top A)^{-1}A^\top b$. Similarly for weighted least-squares problem ($\Vert x\Vert=x^\top Mx$), let $G=A^\top MA, \beta=A^\top Mb$, we can get $\alpha=(A^\top MA)^{-1}A^\top Mb$</p></blockquote></li></ul></li></ul><h1 id="Linear-Algebra">Linear Algebra<a class="header-anchor" href="#Linear-Algebra"> ❮</a></h1><h2 id="Linear-Operator">Linear Operator<a class="header-anchor" href="#Linear-Operator"> ❮</a></h2><ul><li><p>Definition: a linear operator $\mathcal{A}$ (aka. linear transformation, linear mapping) is a function $f: V\to U$ that operate on a linear space $(\mathcal{V},\mathcal{F})$ to produce elements in another linear space $(\mathcal{U},\mathcal{F})$ and obey $$\mathcal{A}(\alpha_1 x_1+\alpha_2 x_2) = \alpha_1\mathcal{A}(x_1) + \alpha_2\mathcal{A}(x_2),\;\forall x_1,x_2\in V;\alpha_1, \alpha_2\in\mathcal{F}$$</p></li><li><p><strong>Range (Space)</strong>: $\mathcal{R}(\mathcal{A})=\left\{u\in U\middle|\mathcal{A}(v)=u,\;\forall v\in V\right\}$</p></li><li><p><strong>Null Space</strong> (aka. <strong>kernel</strong>): $\mathcal{N}(\mathcal{A})=\left\{v\in V\middle|\mathcal{A}(v)=\emptyset_U\right\}$</p></li><li><p><strong>$\mathcal{A}$-invariant subspace</strong>: Given vector space $(\mathcal{V},\mathcal{F})$ and linear operator $\mathcal{A}:\mathcal{V}\rightarrow \mathcal{V}$, $\mathcal{W}\subseteq\mathcal{V}$ is $A$-invariant if $\forall x\in\mathcal{W}$, $\mathcal{A}x\in\mathcal{W}$.</p><ul><li>Both $\mathcal{R}(\mathcal{A})$ and $\mathcal{N}(\mathcal{A})$ are $\mathcal{A}$-invariant</li></ul></li><li><p>Matrix Representation: Given bases for both $V$ and $U$ (respectively $\{v_i\}^n_1$ and $\{u_j\}^m_1$), matrix representation $A$ satisfies $\mathcal{A}(v_i)=\sum^m_{j=0}A_{ji}u_j$ so that $\beta=A\alpha$ where $\alpha$ and $\beta$ is the representation of a vector under $\{v_i\}$ and $\{u_j\}$ respectively.</p><img src="/blog/2020-06/AlgebraBasicsNotes/linear_map_relations.png" title="Relation between a linear map and its matrix representations"><ul><li>$P$ and $Q$ are change of basis matrices, $A=Q^{-1}\tilde{A}P,\;\tilde{A}=QAP^{-1}$</li><li>The i-th column of $A$ is the coordinates of $\mathcal{A}(v_i)$ represented by the basis $\{u_j\}$, similarly i-th column of $\tilde{A}$ is $\mathcal{A}(\tilde{v}_i)$ represented in $\{\tilde{u}_j\}$</li><li>The i-th column of $P$ is the coordinates of $v_i$ represented by the basis $\{\tilde{v}\}$, similarly i-th column of $Q$ is $u_j$ represented in $\{\tilde{u}\}$</li></ul></li><li><p>Matrix Similarity ($A\sim B$): Two (square) matrix representations ($A,B$) of the same linear operator are called <strong>similar</strong> (or <strong>conjugate</strong>) and they satisfies $\exists P$ s.t. $B=PAP^{-1}$.</p><blockquote><p>From now on we don’t distinguish between linear operator $\mathcal{A}$ and its matrix representation where choice of basis doesn’t matter.</p></blockquote></li><li><p><strong>Rank</strong>: $rank(A)=\rho(A)\equiv dim(\mathcal{R}(A))$</p><ul><li><strong>Sylvester’s Inequality</strong>: $\rho(A)+\rho(B)-n\leqslant \rho(AB)\leqslant \min\{\rho(A), \rho(B)\}$</li><li><strong>Singularity</strong>: $\rho(A)&lt; n$</li></ul></li><li><p><strong>Nullity</strong>: $null(A)=\nu(A)\equiv dim(\mathcal{N}(A))$</p><ul><li>$\rho(A)+\nu(A)=n$ ($n$ is the dimensionality of domain space)</li></ul></li><li><p><strong>Adjoint</strong>: The adjoint of the linear map $\mathcal{A}: \mathcal{V}\to\mathcal{W}$ is the linear map $\mathcal{A}^*: \mathcal{W}\to\mathcal{V}$ such that $\langle y,\mathcal{A}(x)\rangle_\mathcal{W}=\langle \mathcal{A}^ *(y),x\rangle_\mathcal{V}$</p><blockquote><p>For its matrix representation, adjoint of $A$ is $A^ *$, which is $A^\top$ for real numbers.<br><br>Properties of $\mathcal{A}^ *$ is similar to matrix $A^ *$</p></blockquote><ul><li>$\mathcal{U}=\mathcal{R}(A)\overset{\perp}{\oplus}\mathcal{N}(A^ *),\;\mathcal{V}=\mathcal{R}(A^ *)\overset{\perp}{\oplus}\mathcal{N}(A)$</li><li>$\mathcal{N}(A^* )=\mathcal{N}(AA^* )\subseteq\mathcal{U},\;\mathcal{R}(A)=\mathcal{R}(AA^*)\subseteq\mathcal{U}$</li></ul></li><li><p><strong>Self-adjoint</strong>: $\mathcal{A}$ is self-adjoint iff $\mathcal{A}^*=\mathcal{A}$.</p><ul><li>For self-adjoint $\mathcal{A}$, if $\mathcal{V}=\mathbb{C}^{n\times n}$ then $A$ is <strong>hermitian</strong>; if $\mathcal{V}=\mathbb{R}^{n\times n}$ then $A$ is <strong>symmetric</strong>.</li><li>Self-adjoint matrices have real eigenvalues and orthogonal eigenvectors</li><li><strong>Skew symmetric</strong>: $A^*=-A$<blockquote><p>For quadratic form $x^\top Ax=x^\top(\frac{A+A^\top}{2}+\frac{A-A^\top}{2})x$, since $A-A^\top$ is skew symmetric, scalar $x^\top (A-A^\top) x=-x^\top (A-A^\top)x$, so the skew-symmetric part is zero. Therefore for quadratic form $x^\top Ax$ we can always assume $A$ is symmetric.</p></blockquote></li></ul></li><li><p><strong>Definiteness</strong>: (for symmetric matrix $P$)</p><ul><li>Positive definite ($P\succ 0$): $\forall x\in\mathbb{R}^n\neq 0,\; x^\top Px&gt;0 \Leftrightarrow$ all eigenvalues of $P$ are positive.</li><li>Semi-positive definite ($P\succcurlyeq 0$): $x^\top Px\geqslant 0 \Leftrightarrow$ all eigenvalues of $P$ are non-negative.</li><li>Negative definite ($P\prec 0$): $x^\top Px &lt; 0 \Leftrightarrow$ all eigenvalues of $P$ are negative.</li></ul></li><li><p><strong>Orthogonal Matrix</strong>: $Q$ is orthogonal iff $Q^\top Q=I$, iff columns of $Q$ are orthonormal.</p><ul><li>If $A\in\mathbb{R}^{n\times b}$ is symmetric, then $\exists$ orthogonal $Q$ s.t. $Q^\top AQ=\Lambda=\mathrm{diag}\{\lambda_1,\ldots,\lambda_n\}$ (see <a href="#Eigendecomposition-and-Jordan-Form">Eigen-decomposition</a> section below)</li></ul></li><li><p><strong>Orthogonal Projection</strong>: Given linear space $\mathcal{X}$ and subspace $\mathcal{M}$, $P(x)=x^*:\mathcal{X}\to\mathcal{M}$ ($x^ *$ is the projection point) is called orthogonal projection. If $\{v_i\}$ is a orthonormal basis of $\mathcal{M}$, then $P(x)=\sum_i \langle x,v_i\rangle v_i$</p></li></ul><h2 id="Eigenvalue-and-Canonical-Forms">Eigenvalue and Canonical Forms<a class="header-anchor" href="#Eigenvalue-and-Canonical-Forms"> ❮</a></h2><ul><li><strong>Eigenvalue</strong> and <strong>Eigenvector</strong>: Given mapping $\mathcal{A}:\mathcal{V}\rightarrow\mathcal{V}$, if $\exists \lambda\in\mathcal{F}, v\neq \emptyset_{\mathcal{V}}\in\mathcal{V}$ s.t. $\mathcal{A}(v) = \lambda v$, then $\lambda$ is the <strong>eigenvalue</strong>, $v$ is the <strong>eigenvector</strong> (aka. <strong>spectrum</strong>).<ul><li>If eigenvalues are all distinct, then the associated eigenvectors form a basis.</li></ul></li><li><strong>Eigenspace</strong>: $\mathcal{N}_\lambda = \mathcal{N}(\mathcal{A}-\lambda \mathcal{I})$.<ul><li>$q=dim(\mathcal{N}_\lambda)$ is called the <strong>geometric multiplicity</strong> (几何重度)</li><li>$\mathcal{N}_\lambda$ is an $\mathcal{A}$-invariant subspace.</li></ul></li><li><strong>Characteristic Polynomial</strong>: $\phi(s)\equiv\mathcal{det}(A-s I)$ is a polynomial of degree $n$ in $s$<ul><li>Its solutions are the eigenvalues of $A$.</li><li>The multiplicity $m_i$ of root term $(s-\lambda_i)$ here is called <strong>algebraic multiplicity</strong> (代数重度) of $\lambda_i$.</li></ul></li><li><strong>Cayley-Hamilton Theorem</strong>: $\phi(A)=\mathbf{0}$<blockquote><p>Proof needs the eigendecomposition or Jordan decomposition descibed below</p></blockquote></li><li><strong>Minimal Polynomial</strong>: $\psi(s)$ is the minimal polynomial of $A$ iff $\psi(s)$ is the polynomial of least degree for which $\psi(A)=0$ and $\psi$ is monic (coefficient of highest order term is 1)<ul><li>The multiplicity $\eta_i$ of root term $(s-\lambda_i)$ here is called the <strong>index</strong> of $\lambda_i$</li></ul></li><li><strong>Eigendecomposition</strong> (aka. <strong>Spectral Decomposition</strong>) is directly derived from the definition of eigenvalues: $$A=Q\Lambda Q^{-1}, \Lambda=\mathrm{diag}\left\{\lambda_1,\lambda_2,\ldots,\lambda_n\right\}$$<br>where $Q$ is a square matrix whose $i$-th column is the eigenvector $q_i$ corresponding to eigenvalue $\lambda_i$.<ul><li>Feasibility: $A$ can be diagonalized (using eigendecomposition) iff. $q_i=m_i$ for all $\lambda_i$.</li><li>If $A$ has $n$ distinct eigenvalues, then $A$ can be diagonalized.</li></ul></li><li><strong>Generalized eigenvector</strong>: A vector $v$ is a generalized eigenvector of rank $k$ associated with eigenvalue $\lambda$ iff $v\in\mathcal{N}\left((A-\lambda I)^k\right)$ but $v\notin\mathcal{N}\left((A-\lambda I)^{k-1}\right)$<ul><li>If $v$ is a generalized eigenvector of rank $k$, $(A-\lambda I)v$ is a generalized eigenvector of rank $k-1$. This creates a chain of generalized eigenvectors (called <strong>Jordan Chain</strong>) from rank $k$ to $1$, and they are linearly independent.</li><li>$\eta$ (index, 幂零指数) of $\lambda$ is the smallest integer s.t. $dim\left(\mathcal{N}\left((A-\lambda I)^\eta\right)\right)$</li><li>The space spanned by the chain of generalized eigenvectors from rank $\eta$ is called the <strong>generalized eigenspace</strong> (with dimension $\eta$).</li><li>Different generalized eigenspaces associated with the same and with different eigenvalues are orthogonal.</li></ul></li><li><strong>Jordan Decomposition</strong>: Similar to eigendecomposition, but works for all square matrices. $A=PJP^{-1}$ where $J=\mathrm{diag}\{J_1,J_2,\ldots,J_p\}$ is the <strong>Jordan Form</strong> of A consisting of Jordan Blocks.<ul><li><strong>Jordan Block</strong>: $J_i=\begin{bmatrix} \lambda &amp; 1 &amp;&amp;&amp; \\&amp;\lambda&amp;1&amp;&amp;\\&amp;&amp;\lambda&amp;\ddots&amp;\\&amp;&amp;&amp;\ddots&amp;1\\&amp;&amp;&amp;&amp;\lambda\end{bmatrix}$</li><li>Each Jordan block corresponds to a generalized eigenspace</li><li>$q_i$ = the count of Jordan blocks associated with $\lambda_i$</li><li>$m_i$ = the count of $\lambda_i$ on diagonal of $J$</li><li>$\eta_i$ = the dimension of the largest Jordan block associated with $\lambda_i$</li></ul></li></ul><blockquote><p>$\Lambda$ in eigendecomposition, $J$ in Jordan Form and $\Sigma$ in SVD (see below) are three kinds of <strong><a href="https://en.wikipedia.org/wiki/Canonical_form#Linear_algebra" target="_blank" rel="noopener">Canonical Forms</a></strong> of a matrix $A$</p></blockquote><ul><li><strong>Function of matrics</strong>: Let $f(\cdot)$ be an analytic function and $\lambda_i$ be an eigenvalue of $A$. If $p(\cdot)$ is a polynomial that satisfies $p(\lambda_i)=f(\lambda_i)$ and $\frac{\mathrm{d}^k}{\mathrm{d}s^k} p(\lambda_i)=\frac{\mathrm{d}^k}{\mathrm{d}s^k} f(\lambda_i)$ for $k=1,\ldots,\eta_i-1$, then $f(A)\equiv p(A)$.<blockquote><ul><li>This extends the functions applicable to matrics from polynomials (trivial) to any analytical functions</li><li>By Cayley-Hamilton, we can always choose $p$ to be order $n-1$</li></ul></blockquote></li><li><strong>Sylvester’s Formula</strong>: $f(A)=\sum^k_{i=1}f(\lambda_i)A_i$ ($f$ being analytic)</li></ul><h2 id="SVD-and-Linear-Equations">SVD and Linear Equations<a class="header-anchor" href="#SVD-and-Linear-Equations"> ❮</a></h2><p>SVD Decomposition is useful in various fields and teached by a lot of courses, its complete version is formulated as $$A=U\Sigma V^*, \Sigma=\begin{bmatrix}\mathbf{\sigma}&amp;\mathbf{0}\\ \mathbf{0}&amp;\mathbf{0}\end{bmatrix}, \mathbf{\sigma}=\mathrm{diag}\left\{\sqrt{\lambda_1},\sqrt{\lambda_2},\ldots,\sqrt{\lambda_r}\right\},V=\begin{bmatrix}V_1&amp;V_2\end{bmatrix},U=\begin{bmatrix}U_1&amp;U_2\end{bmatrix}$$<br>where</p><ul><li>$r=\rho(A)$ is the rank of matrix $A$</li><li>$\sigma_i$ are called <strong>sigular values</strong>, $\lambda_i$ are eigenvalues of $A^* A$</li><li>Columns of $V_1$ span $\mathcal{R}(A^ *A)=\mathcal{R}(A^ *)$, columns of $V_2$ span $\mathcal{N}(A^ *A)=\mathcal{N}(A)$</li><li>Columns of $U_1=AV_1\sigma^{-1}$ span $\mathcal{R}(A)$, columns of $U_2$ span $\mathcal{N}(A^*)$</li></ul><blockquote><p>SVD can be derived by doing eigenvalue decomposition on $A^* A$</p></blockquote><p>With SVD introduced, we can efficiently solve general linear equation $Ax=b$ as $x=x_r+x_n$ where $x_r\in\mathcal{R}(A^\top)$ and $x_n\in\mathcal{N}(A)$.</p><table><thead><tr><th></th><th>$Ax=b$</th><th>tall $A$ ($m&gt;n$)</th><th>fat $A$ ($m&lt; n$)</th></tr></thead><tbody><tr><td></td><td></td><td>Overdetermined,<br>Least Squares,<br>use Normal Equations</td><td>Underdetermined,<br>Quadratic Programming,<br>use Lagrange Multiplies</td></tr><tr><td>I.$b\in\mathcal{R}(A)$</td><td></td><td></td><td></td></tr><tr><td>1.$\mathcal{N}(A)={0}$</td><td>$x$ exist &amp; is unique</td><td>$x=(A^\top A)^{-1}A^\top b=A^+b$</td><td>$x=A^\top(AA^\top)^{-1}b=A^+b$</td></tr><tr><td>2.$\mathcal{N}(A)\neq{0}$</td><td>$x$ exist &amp; not unique</td><td>$x_r=(A^\top A)^{-1}A^\top b=A^+b$</td><td>$x_r=A^\top(AA^\top)^{-1}b=A^+b$</td></tr><tr><td>II.$b\notin\mathcal{R}(A)$</td><td></td><td></td><td></td></tr><tr><td>1.$\mathcal{N}(A)={0}$</td><td>$x$ not exists, $x_r$ exist &amp; is unique</td><td>$x_r=(A^\top A)^{-1}A^\top b=A^+b$</td><td>$x_r=A^\top(AA^\top)^{-1}b=A^+b$</td></tr><tr><td>2.$\mathcal{N}(A)\neq{0}$</td><td>$x$ not exists, $x_r$ not exist</td><td>$(A^\top A)^{-1}$ invertible</td><td>$(AA^\top)^{-1}$ invertible</td></tr></tbody></table><ul><li>$A^+=(A^\top A)^{-1}A^\top$ is left pseudo-inverse, $A^+=A^\top (AA^\top)^{-1}$ is right pseudo-inverse.</li><li>$A^+$ can be unified by the name <strong>Moore-Penrose Inverse</strong> and calculated using SVD by $A^+=V\Sigma^+ U^\top$ where $\Sigma^+$ take inverse of non-zeros.</li></ul><h2 id="Miscellaneous">Miscellaneous<a class="header-anchor" href="#Miscellaneous"> ❮</a></h2><blockquote><p>Selected theorems and lemmas useful in Linear Algebra. For more matrix properties see <a href="/blog/2019-06/MatrixAlgebra/" title="my post about Matrix Algebra">my post about Matrix Algebra</a></p></blockquote><ul><li>Matrix Square Root: $N^\top N=P$, then $N$ is the square root of $P$<blockquote><p>Square root is not unique. Cholesky decomposition is often used as square root.</p></blockquote></li><li><strong>Schur Complement</strong>: Given matrices $A_{n\times n}, B_{n\times m}, C_{m\times m}$, the matrix $M=\begin{bmatrix}A&amp;B\\ B^\top&amp;C\end{bmatrix}$ is symmetric. Then the following are equivalent (TFAE)<ol><li>$M\succ 0$</li><li>$A\succ 0$ and $C-B^\top A^{-1}B\succ 0$ (LHS called Schur complement of $A$ in $M$)</li><li>$C\succ 0$ and $A-B C^{-1}B^\top\succ 0$ (LHS called Schur complement of $C$ in $M$)</li></ol></li><li>Matrix Inverse Lemma: $(A+BCD)^{-1}=A^{-1}-A^{-1}B\left(C^{-1}+DA^{-1}B\right)^{-1}DA$</li><li>Properties of $A^\top A$<ul><li>$A^\top A \succeq 0$ and $A^\top A \succ 0 \Leftrightarrow A$ has full rank.</li><li>$A^\top A$ and $AA^\top$ have same non-zero eigenvalues, but different eigenvectors.</li><li>If $v$ is eigenvector of $A^\top A$ about $\lambda$, then $Av$ is eigenvector of $AA^\top$ about $\lambda$.</li><li>If $v$ is eigenvector of $AA^\top$ about $\lambda$, then $A^\top v$ is eigenvector of $A^\top A$ about $\lambda$.</li><li>$tr(A^\top A)=tr(AA^\top)=\sum_i\sum_j\left|A_{ij}\right|^2$</li><li>$det(A)=\prod_i\lambda_i, tr(A)=\sum_i\lambda_i$</li></ul></li></ul><h1 id="Real-Analysis">Real Analysis<a class="header-anchor" href="#Real-Analysis"> ❮</a></h1><h2 id="Set-theory">Set theory<a class="header-anchor" href="#Set-theory"> ❮</a></h2><blockquote><p>$\text{~}S$ stands for complement of set $S$ in following contents. These concepts are discussed under normed space $(\mathcal{X}, \Vert\cdot\Vert)$</p></blockquote><ul><li><strong>Open Ball</strong>: Let $x_0\in\mathcal{X}$ and let $a\in\mathbb{R}, a&gt;0$, then the open ball of radius $a$ about $x_0$ is $B_a(x_0)=\left\{x\in\mathcal{X}\middle| \Vert x-x_0\Vert &lt; a\right\}$<ul><li>Given subset $S\subset \mathcal{X}$, $d(x,S)=0\Leftrightarrow \forall\epsilon &gt;0, B_\epsilon(x)\cap S\neq\emptyset$</li><li>Given subset $S\subset \mathcal{X}$, $d(x,S)&gt;0\Leftrightarrow \exists\epsilon &gt;0, B_\epsilon(x)\cap S=\emptyset$</li></ul></li><li><strong>Interior Point</strong>: Given subset $S\subset\mathcal{X}$, $x\in S$ is an interior point of $S$ iff $\exists\epsilon &gt;0, B_\epsilon(x)\subset S$<ul><li><strong>Interior</strong>: $\mathring{S}=\{x\in \mathcal{X}|x\text{ is an interior point of }S\}=\{x\in\mathcal{X}|d(x,\text{~}S)&gt;0\}$</li></ul></li><li><strong>Open Set</strong>: $S$ is open if $\mathring{S}=S$</li><li><strong>Closure Point</strong>: Given subset $S\subset\mathcal{X}$, $x\in S$ is a closure point of $S$ iff $\forall\epsilon &gt;0, B_\epsilon(x)\cap S\neq\emptyset$.<ul><li><strong>Closure</strong>: $\bar{S}=\{x\in\mathcal{X}|x\text{ is a closure point of }S\}=\{x\in\mathcal{X}|d(x,S)=0\}$<blockquote><p>Note that $\partial\mathcal{X}=\emptyset$</p></blockquote></li></ul></li><li><strong>Closed Set</strong>: $S$ is closed if $\bar{S}=S$<blockquote><p>$S$ is open $\Leftrightarrow$ $\text{~}S$ is closed, $S$ is closed $\Leftrightarrow$ $\text{~}S$ is open. Set being both open and closed is called <strong>clopen</strong>(e.g. the whole set $\mathcal{X}$), empty set is clopen by convention.</p></blockquote></li><li><strong>Set Boundary</strong>: $\partial S=\bar{S}\cap\overline{\text{~}S}=\bar{S}\backslash\mathring{S}$</li></ul><h2 id="Sequences">Sequences<a class="header-anchor" href="#Sequences"> ❮</a></h2><ul><li><strong>Sequence</strong>($\{x_n\}$): a set of vectors indexed by the counting numbers<ul><li><strong>Subsequence</strong>: Let $1\leqslant n_1&lt; n_2&lt;\ldots$ be an infinite set of increasing integers, then $\{x_{n_i}\}$ is a subsequence of $\{x_n\}$</li></ul></li><li><strong>Convergence</strong>($\{x_n\}\to x\in\mathcal{X}$): $\forall \epsilon&gt;0,\exists N(\epsilon)&lt;\infty\text{ s.t. }\forall n\geqslant N, \Vert x_n-x\Vert &lt;\epsilon$<ul><li>If $x_n \to x$ and $x_n \to y$, then $x=y$</li><li>If $x_n \to x_0$ and $\{x_{n_i}\}$ is a subsequence of $\{x_n\}$, then $\{x_{n_i}\} \to x_0$</li><li><strong>Cauchy Convergence</strong> (necessary condition for convergence): $\{x_n\}$ is cauchy if $\forall \epsilon&gt;0,\exists N(\epsilon)&lt;\infty$ s.t. $\forall n,m\geqslant N, \Vert x_n-x_m\Vert &lt;\epsilon$</li><li>If $\mathcal{X}$ is finite dimensional, $\{x_n\}$ is cauchy $\Rightarrow$ $\{x_n\}$ has a limit in $\mathcal{X}$</li></ul></li><li><strong>Limit Point</strong>: Given subset $S\subset\mathcal{X}$, $x$ is a limit point of $S$ if $\exists \{x_n\}$ s.t. $\forall n\geqslant 1, x_n\in S$ and $x_n\to x$<ul><li>$x$ is a limit point of $S$ iff $x\in\bar{S}$</li><li>$S$ is closed iff $S$ contains its limit points</li></ul></li><li><strong>Complete Space</strong>: a normed space is <strong>complete</strong> if every Cauchy sequence has a limit. A complete normed space $(\mathcal{X}, \Vert\cdot\Vert)$ is called a <strong>Banach space</strong>.<ul><li>$S\subset \mathcal{X}$ is complete if every Cauchy sequence with elements from $S$ has a limit in $S$</li><li>$S\subset \mathcal{X}$ is complete $\Rightarrow S$ is closed</li><li>$\mathcal{X}$ is complete and $S\subset\mathcal{X} \Rightarrow S$ is complete</li><li>All finite dimensional subspaces of $X$ are complete</li></ul></li><li><strong>Completion of Normed Space</strong>: $\mathcal{Y}=\bar{\mathcal{X}}=\mathcal{X}+\{$all limit points of Cauchy sequences in $\mathcal{X}\}$<blockquote><p>E.g. $C[a,b]$ contains continuous functions over $[a,b]$. $(C[a,b], \Vert\cdot\Vert_1)$ is not complete, $(C[a,b], \Vert\cdot\Vert_\infty)$ is complete. Completion of $(C[a,b], \Vert\cdot\Vert_1)$ requires Lebesque integration.</p></blockquote></li><li><strong>Contraction Mapping</strong>: Let $S\subset\mathcal{X}$ be a subset and $T:S\to S$ is a contraction mapping if $\exists 0\leqslant c\leqslant 1$ such that, $\forall x,y \in S, \Vert T(x)-T(y)\Vert\leqslant c\Vert x-y\Vert$<ul><li><strong>Fixed Point</strong>: $x^* \in\mathcal{X}$ is a fixed point of $T$ if $T(x^ *)=x^ *$</li><li><a href="https://en.wikipedia.org/wiki/Banach_fixed-point_theorem" target="_blank" rel="noopener"><strong>Contraction Mapping Theorem</strong> (不动点定理)</a>: If $T:S\to S$ is a contraction mapping in a complete subset $S$, then $\exists! x^ *\in\mathcal{X}\text{ s.t. }T(x^ *)=x^ *$. Moreover, $\forall x_0\in S$, the sequence $x_{k+1}=T(x_k),k\geqslant 0$ is Cauchy and converges to $x^ *$.<blockquote><p>E.g. Newton Method: $x_{k+1}=x_k-\epsilon\left[\frac{\partial h}{\partial x}(x_k)\right]^{-1}\left(h(x_k)-y\right)$</p></blockquote></li></ul></li></ul><h2 id="Continuity-and-Compactness">Continuity and Compactness<a class="header-anchor" href="#Continuity-and-Compactness"> ❮</a></h2><ul><li><strong>Continuous</strong>: Let $(\mathcal{X},\Vert\cdot\Vert_\mathcal{X})$ and $(\mathcal{Y},\Vert\cdot\Vert_\mathcal{Y})$ be two normed spaces. A function $f:\mathcal{X}\to\mathcal{Y}$ is continuous at $x_0\in\mathcal{X}$ if $\forall\epsilon &gt;0,\exists \delta(\epsilon,x_0)&gt;0\text{ s.t. }\Vert x-x_0\Vert_\mathcal{X}&lt;\delta \Rightarrow\Vert f(x)-f(x_0)\Vert_\mathcal{Y} &lt;\epsilon$<ul><li>$f$ is continuous on $S\subset\mathcal{X}$ if $f$ is continuous at $\forall x_0\in S$</li><li>If $f$ in continuous at $x_0$ and $\{x_n\}$ is a sequence s.t. $x_n\to x_0$, then the sequence $\{f(x_n)\}$ in $\mathcal{Y}$ converges to $f(x_0)$</li><li>If $f$ is discontinuous at $x_0$, then $\exists \{x_n\}\in\mathcal{X}$ s.t. $x_n\to x_0$ but $f(x_n)\nrightarrow f(x_0)$</li></ul></li><li><strong>Compact</strong>: $S\subset\mathcal{X}$ is (sequentially) compact if every sequence in $S$ has a convergent subsequence with limit in $S$</li><li><strong>Bounded</strong>: $S\subset\mathcal{S}$ is bounded if $\exists r&lt;\infty$ such that $S\subset B_r(0)$<ul><li>$S$ is compact $\Rightarrow$ $S$ is closed and bounded</li><li><strong>Bolzano-Weierstrass Theorem</strong>: In a finite-dimensional normed space, $C$ is closed and bounded $\Leftrightarrow$ for $C$ is compact</li></ul></li><li><strong>Weierstrass Theorem</strong>: If $C\subset\mathcal{X}$ is a compact subset and $f:C\to\mathbb{R}$ is continuous at each point of $C$, then $f$ achieves its extreme values, i.e. $\exists \bar{x}\in C\text{ s.t. }f(\bar{x})=\sup_{x\in C} f(x)$ and $\exists \underline{x}\in C\text{ s.t. }f(\underline{x})=\inf_{x\in C} f(x)$<ul><li>$f:C\to\mathbb{R}$ continuous and $C$ compact $\Rightarrow$ $\sup_{x\in C}f(x)&lt;\infty$</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;&lt;p&gt;Selected notes from &lt;code&gt;ROB 501&lt;/code&gt; and &lt;code&gt;ME 564&lt;/code&gt;.&lt;br&gt;$\{x_i\}^b_a$ denotes set $\{x_a, x_{a+1}, \ldots, x_b\}$&lt;br&gt;TODO: add Jordan Form&lt;/p&gt;&lt;/blockquote&gt;&lt;h1 id=&quot;Algebraic-Structures&quot;&gt;Algebraic Structures&lt;a class=&quot;header-anchor&quot; href=&quot;#Algebraic-Structures&quot;&gt; ❮&lt;/a&gt;&lt;/h1&gt;&lt;h2 id=&quot;Operation&quot;&gt;Operation&lt;a class=&quot;header-anchor&quot; href=&quot;#Operation&quot;&gt; ❮&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Definition: an (binary, closed) &lt;strong&gt;operation&lt;/strong&gt; $\ast$ on a set $S$ is a mapping of $S\times S\to S$&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Commutative&lt;/strong&gt;: $x\ast y=y\ast x,\;\forall x,y\in S$&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Associative&lt;/strong&gt;: $(x\ast y)\ast z=x\ast (y\ast z),\;\forall x,y,z\in S$&lt;/li&gt;&lt;/ul&gt;&lt;h2 id=&quot;Group&quot;&gt;Group&lt;a class=&quot;header-anchor&quot; href=&quot;#Group&quot;&gt; ❮&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Definition: a &lt;strong&gt;group&lt;/strong&gt; is a pair $(\mathcal{S},\ast)$ with following axioms&lt;ol&gt;&lt;li&gt;$\ast$ is associative on $\mathcal{S}$&lt;/li&gt;&lt;li&gt;(Identity element) $\exists e\in \mathcal{S}\text{ s.t. }x\ast e=e\ast x=x,\;\forall x\in \mathcal{S}$&lt;/li&gt;&lt;li&gt;(Inverse element) $\forall x\in \mathcal{S}, \exists x’ \in \mathcal{S}\text{ s.t. }x\ast x’=x’\ast x=e$&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Abelian&lt;/strong&gt;: a group is called &lt;strong&gt;abelian group&lt;/strong&gt; if $\ast$ is also commutative&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://zyxin.xyz/blog/categories/Notes/"/>
    
      <category term="Math" scheme="http://zyxin.xyz/blog/categories/Notes/Math/"/>
    
    
      <category term="Math" scheme="http://zyxin.xyz/blog/tags/Math/"/>
    
      <category term="Algebra" scheme="http://zyxin.xyz/blog/tags/Algebra/"/>
    
  </entry>
  
  <entry>
    <title>Notes for Control System</title>
    <link href="http://zyxin.xyz/blog/2020-06/ControlSystemNotes/"/>
    <id>http://zyxin.xyz/blog/2020-06/ControlSystemNotes/</id>
    <published>2020-06-10T19:25:34.000Z</published>
    <updated>2021-07-12T02:40:53.098Z</updated>
    
    <content type="html"><![CDATA[<blockquote><ul><li>This note combines content from ME 564 Linear Systems and ME 561 Discrete Digital Control</li><li>Please read <a href="/blog/2020-06/AlgebraBasicsNotes/" title="the Algebra Basics notes">the Algebra Basics notes</a> first if you are not familiar with related concepts.</li><li>In this note, $f\in\mathbb{F}^\mathbb{G}$ stands for a function with domain in $\mathbb{G}$ and co-domain in $\mathbb{F}$, i.e. $f:\mathbb{F}\to\mathbb{G}$, $H(x)$ generally stands for Heaviside function (step function)</li></ul></blockquote><h1 id="Transforms">Transforms<a class="header-anchor" href="#Transforms"> ❮</a></h1><h2 id="Laplace-Transform">Laplace Transform<a class="header-anchor" href="#Laplace-Transform"> ❮</a></h2><ul><li>Definition: $F(s)=\mathcal{L}\{f(t)\}(s)=\int^\infty_0 f(t)e^{-st}\mathrm{d}t$<blockquote><p>Note that the transform is not well defined for all functions in $\mathbb{C}^\mathbb{R}$. And the transform is only valid for $s$ in a region of convergence, which is usually separated by 0.</p></blockquote></li><li>Laplace Transform is a linear map from $(\mathbb{C}^\mathbb{R}, \mathbb{C})$ to $(\mathbb{C}^\mathbb{C}, \mathbb{C})$ and it’s one-to-one.</li><li>Properties: (see <a href="https://en.wikipedia.org/wiki/Laplace_transform" target="_blank" rel="noopener">Wikipedia</a> or <a href="https://lpsa.swarthmore.edu/LaplaceZTable/LaplacePropTable.html" target="_blank" rel="noopener">this page</a> for full list)<ul><li>Derivative: $f’(t) \xleftrightarrow{\mathcal{L}} sF(s)-f(0^-)$</li><li>Integration: $\int^t_0 f(\tau)d\tau \xleftrightarrow{\mathcal{L}} \frac{1}{s}F(s)$</li><li>Delay: $f(t-a)H(t-a) \xleftrightarrow{\mathcal{L}} e^{-as}F(s)$</li><li>Convolution: $\int^t_0 f(\tau)g(t-\tau)\mathrm{d}\tau \xleftrightarrow{\mathcal{L}} F(s)G(s)$</li></ul></li><li>Stationary Value: $\lim\limits_{t\to 0} f(t) = \lim\limits_{s\to \infty} sF(s), \lim\limits_{t\to \infty} f(t) = \lim\limits_{s\to 0} sF(s)$</li></ul><a id="more"></a><h3 id="Inverse-Laplace-Transform">Inverse Laplace Transform<a class="header-anchor" href="#Inverse-Laplace-Transform"> ❮</a></h3><blockquote><p>Laplace transform is one-to-one, so we can apply inverse transform on functions in s-space</p></blockquote><p>There are several ways to calculate Laplace transform, the first one is directly evaluating integration while the latter two are converting the function into certain formats that are convenient for table lookup:</p><ol><li>(Mellin’s) Inverse formula: $f(t)=\mathcal{L}^{-1}\{F(s)\}(t)=\frac{1}{2\pi j}\lim\limits_{T\to\infty} \int ^{\gamma+iT}_{\gamma-iT} e^{st}F(s)\mathrm{d}s$ where the integration is done along the vertical line $Re(s)=\gamma$ in the convex s-plane such that $\gamma$ is greater than the real part of all poles of $F(s)$.</li><li>Power Series: $F(s) = \sum^\infty_{n=0} \frac{n!a_n}{s^{n+1}}\xleftrightarrow{\mathcal{L}} f(t) = \sum ^\infty_{n=0} a_n t^n $</li><li>Partial Fractions: $F(s)=\frac{k_1}{s+a}+\frac{k_2}{s+b}+\ldots \xleftrightarrow{\mathcal{L}} f(t)=k_1 e^{-at} + k_2 e^{-bt} + \ldots$<ul><li>To calculate partial fractions, one can use <a href="http://tutorial.math.lamar.edu/Classes/Alg/DividingPolynomials.aspx" target="_blank" rel="noopener">Polynomial Division</a> or following lemma:</li><li>Suppose $F(s)=\frac{N(s)}{D(s)}=\frac{N(s)}{\prod^n_{i=1} (s-p_i)^{r_i}}$ where $\mathrm{deg}(N(s)) &lt; \mathrm{deg(D(s))}$ and each $p_i$ is a distinct root of $D(s)$ (i.e. pole) with multiplicity $r_i$, then $F(s)=\sum^n_{i=1}\sum^{r_i}_ {j=1} \frac{k_{ij}}{(s-p_i)j}$ where $k_{ij}=\frac{1}{(r_i-j)!}\left.\frac{\mathrm{d}^{r_i-j}}{\mathrm{d}s^{r_i-j}}(s-p_i)^{r_i}F(s)\right\vert_{s=p_i}$</li></ul></li></ol><h2 id="Z-Transfrom">Z-Transfrom<a class="header-anchor" href="#Z-Transfrom"> ❮</a></h2><ul><li>Definition: $F(z)=\mathcal{Z}\{f(k)_ {k\in\mathbb{N}}\}(z)=\sum^\infty_{k=0} f(k)z^{-k}$</li></ul><blockquote><p>Notice that $f$ is defined on natural numbers. In time domain, it’s usually corresponding to $f(kT)$. Z-transform is also only valid for $z$ in certain region (usually separated by 1)</p></blockquote><ul><li>Laplace Transform is a linear map from $(\mathbb{C}^\mathbb{N}, \mathbb{C})$ to $(\mathbb{C}^\mathbb{C}, \mathbb{C})$ and it’s one-to-one.</li><li>Properties: (see <a href="https://en.wikipedia.org/wiki/Z-transform" target="_blank" rel="noopener">Wikipedia</a> or <a href="https://lpsa.swarthmore.edu/LaplaceZTable/LaplacePropTable.html" target="_blank" rel="noopener">this page</a> for full list)<ul><li>Accumulation: $\sum^n_{k=-\infty} f(k) \xleftrightarrow{\mathcal{Z}} \frac{1}{1-z^{-1}}F(z)$</li><li>Delay: $f(k-m) \xleftrightarrow{\mathcal{Z}} z^{-m}F(z)$</li><li>Convolution: $\sum^k_{n=0}f_1(n)f_2(k-n) \xleftrightarrow{\mathcal{Z}} F_1(z)F_2(z)$</li></ul></li><li>Stationary Value: $\lim\limits_{t\to 0} f(t) = \lim\limits_{z\to \infty} F(z), \lim\limits_{t\to \infty} f(t) = \lim\limits_{z\to 1} (z-1)F(z)$</li></ul><details><summary>Example: Z-Transform of PID controller</summary>Assume the close-loop error input of the controller is $e(t)$, and $e(kT)$ after sampling. PID controller action in analog is $$m(t)=K\left(e(t)+\frac{1}{T_i}\int^t_0e(t)\mathrm{d}t+T_d\frac{\mathrm{d}e(t)}{\mathrm{d}t}\right)$$ We can approximate by trapezoidal rule with two point difference: $$m(kT)=K\left(e(kT)+\frac{T}{T_i}\sum^k_{h=1}\frac{e((h-1)T)+e(hT)}{2}+T_d\left(\frac{e(kT)-e((k-1)T)}{T}\right)\right)$$ Lets define $f(hT) = \frac{1}{2}\left(e((h-1)T)+e(hT)\right),\;f(0)=0$ Then $$\begin{split}\mathcal{Z}\left(\left\{\sum^k_{h=1}\frac{e((h-1)T)+e(hT)}{2}\right\}_k\right)(z)=\mathcal{Z}\left(\left\{\sum^k_{h=1}f(hT)\right\}_k\right)(z) \\ =\frac{1}{1-z^{-1}}(F(z)-F(0))=\frac{1}{1-z^{-1}}F(z)\end{split}$$ Notice that $$F(z)=\mathcal{Z}\left({f(hT)}_h\right)(z)=\frac{1+z^{-1}}{2}E(z)$$ so we can calculate the Z-transform of $m(kT)$ $$\begin{split} M(z)&=K\left(1+\frac{T}{2T_i}\left(\frac{1+z^{-1}}{1-z^{-1}}\right)+\frac{T_d}{T}(1-z^{-1})\right)E(z)\\&=K\left(1-\frac{T}{2T_i}+\frac{T}{T_i}\frac{1}{1-z^{-1}}+\frac{T_d}{T}(1-z^{-1})\right)E(z)\\&=\left(K_p+K_i\left(\frac{1}{1-z^-1}\right)+K_d(1-z^{-1})\right)E(z) \end{split}$$<p>Here we have</p><ul><li>Proportional Gain $K_p=K-\frac{KT}{2T_i}$</li><li>Integral Gain $K_I=\frac{KT}{T_i}$</li><li>Derivative Gain $K_d=\frac{KT_d}{T}$</li></ul></details><h3 id="Inverse-Z-Transform">Inverse Z-Transform<a class="header-anchor" href="#Inverse-Z-Transform"> ❮</a></h3><ol><li>Inverse formula: $f(k)=\mathcal{Z}^{-1}\{F(z)\}(k)=\frac{1}{2\pi j}\oint _\Gamma z^{k-1}F(z)\mathrm{d}z$ where the integration is done along any closed path $\Gamma$ that encloses all finite poles of $z^{k-1}X(z)$ in the z-plane.<ul><li>According to residual theorem, we can write it as $f(k)=\sum_{p_i}Res(z^{k-1}f(z), pi)$ where $p_i$ are poles of $z^{k-1}f(k)$ and residual $Res(g(z),p)=\frac{1}{(m-1)!}\left.\frac{\mathrm{d}^{m-1}}{\mathrm{d}z^{m-1}}\left((z-p)^mg(z)\right)\right\vert_{z=p}$ with $m$ being the multiplicity of the pole $p$ in $g$.</li></ul></li><li>Power Series: same as inverse laplace.</li><li>Partial Fractions: same as inverse laplace.</li></ol><h3 id="Modified-Z-Transfrom">Modified Z-Transfrom<a class="header-anchor" href="#Modified-Z-Transfrom"> ❮</a></h3><ul><li>Definition: $F(z,m)=\mathcal{Z}_m(f,m)=\mathcal{Z}(\left\{f(kT-(1-m)T)\right\} _{k\in\mathbb{N}^+})(z)$</li><li>We denote corresponding continuous form $\mathcal{L}(f(t-(1-m)T)\delta_ T(t))$ as $F^*(s,m)$</li><li>Residual Theorem: $\mathcal{Z}_m(f,m)=z^{-1}\sum _{p_i} Res(\frac{F(s)e^{mTs}}{1-z^{-1}e^{Ts}}, p_i)$</li><li>ModZ Transform is usually used when there’s delay in the system, use this transform to shift the signal with proper $m$ value.</li></ul><h2 id="Starred-Transform">Starred Transform<a class="header-anchor" href="#Starred-Transform"> ❮</a></h2><ul><li>Definition: $F^* (s)=\sum^\infty_{n=0}f(n*T)e^{-nTs}$</li></ul><blockquote><p>Starred Transform is defined in continuous s-domain, but it only aggregates on discrete s values defined periodically by sampling time T, like Z-Transform. Starred Transform is usually exchangeable with Z-Transform with $z=e^{Ts}$.</p></blockquote><ul><li>Sometimes we also see <code>*</code> as an operator to sample a continuous signal. It converts a continuous signal to discrete delta functions. (See the “Sampler” section below)</li><li>Calculation from Laplace Transform<ul><li>$F^*(s)=\sum_{p_i\in\{poles\;of\;F(\lambda)\}} Res\left(F(\lambda)\frac{1}{1-e^{-T(s-\lambda)}}, p_i\right)$</li><li>$F^*(s)=\frac{1}{T}\sum^\infty_{n=-\infty}F(s+jn\omega_s)+\frac{e(0)}{2}$ where $\omega_s=\frac{2\pi}{T}$</li></ul></li><li>Properties:<ul><li>$F^*(s)$ is periodic in s plane with period $j\omega_s=\frac{2\pi j}{T}$</li><li>If $F(s)$ has a pole at $s=s_0$, then $F^*(s)$ must have poles at $s=s_0+jn\omega_s$ for $m\in\mathbb{Z}$</li><li>$A(s)=B(s)F^* (s) \Rightarrow A^* (s)=B^* (s)F^* (s)$, while usually $A(s)=B(s)F(s) \nRightarrow A^* (s)=B^* (s)F^* (s)$</li></ul></li></ul><h2 id="Fourier-Transform">Fourier Transform<a class="header-anchor" href="#Fourier-Transform"> ❮</a></h2><blockquote><p>Fourier transform is basically to substitute $s=j\omega$ into Laplace transform. Additional properties are not discussed here.</p><ul><li>One important theorem (Shannon-Nyquist Sampling Theorem): Suppose $e:\mathbb{R}_+\to\mathbb{R}$ has a Fourier Transform with no frequency components greater than $f_0$, then $e$ is uniquely determined by the signal $e_s$ generated by ideally sampling $e$ with period $\frac{1}{2}f_0$.</li></ul></blockquote><h1 id="State-Space-Representation">State Space Representation<a class="header-anchor" href="#State-Space-Representation"> ❮</a></h1><h2 id="Continuous-State-Space-Representation">Continuous State Space Representation<a class="header-anchor" href="#Continuous-State-Space-Representation"> ❮</a></h2><h3 id="Definition">Definition<a class="header-anchor" href="#Definition"> ❮</a></h3><p>A continuous-time linear state-space system can be described by following two equations:<br>\begin{align}&amp;\text{State equation}:\;&amp;\dot{x}(t)&amp;=A(t)x(t)+B(t)u(t),&amp;\;x(t)\in\mathbb{R}^n,\;u(t)&amp;\in\mathbb{R}^m \\&amp;\text{Output equation}:\;&amp;y(t)&amp;=C(t)x(t)+D(t)u(t),&amp;\;y(t)&amp;\in\mathbb{R}^p\end{align}</p><p>The input $u:[0,\infty)\to\mathbb{R}^m$, state $x:[0,\infty)\to\mathbb{R}^n$, and output $y:[0,\infty)\to\mathbb{R}^p$ are all <em>signals</em>, i.e. functions of continuous time $t\in[0,\infty)$. The coefficients $A\in\mathbb{R}^{n\times n}$,$B\in\mathbb{R}^{n\times m}$,$C\in\mathbb{R}^{p\times n}$,$D\in\mathbb{R}^{p\times m}$</p><p>This linear time-varying (LTV) system can be written compactly as<br>\begin{align*} \dot{x}&amp;=A(t)x+B(t)u \\ y&amp;=C(t)x+D(t)u\end{align*}<br>Similarly, linear time-invariant (LTI) system can be written as<br>\begin{align} \dot{x}&amp;=Ax+Bu \\ y&amp;=Cx+Du\end{align}</p><p>For non-linear system, the equation will be written as</p><table><tr><th style="text-align:center">time-varying (NLTV)</th><th style="text-align:center">time-invariant (NTLI)</th><th style="text-align:center">time-invariant autonomous</th></tr><tr><td><p>\begin{align*}\dot{x}&amp;=f(x,u,t)\\y&amp;=g(x,u,t)\end{align*}</p></td><td><p>\begin{align*}\dot{x}&amp;=f(x,u)\\y&amp;=g(x,u)\end{align*}</p></td><td><p>\begin{align*}\dot{x}&amp;=f(x)\\y&amp;=g(x)\end{align*}</p></td></tr></table><h3 id="Solution">Solution<a class="header-anchor" href="#Solution"> ❮</a></h3><blockquote><p><em><strong>Math prerequisites here:</strong></em></p><ul><li>For definition of function on matrix, see <a href="/blog/2020-06/AlgebraBasicsNotes/#Eigenvalue-and-Canonical-Forms">my notes for algebra basics</a></li><li>$e^A$ is matrix exponential, <code>expm</code> in MATLAB<ol><li>$\frac{\mathrm{d}}{\mathrm{d}t}e^{At}=Ae^{At}=e^{At}A$</li><li>$e^{(A+B)t}\Leftrightarrow AB=BA$ <strong>(be careful when commute matrices)</strong></li><li>$\mathcal{L}\{e^{At}\}=(sI-A)^{-1}$ (can be derived from property 1 and laplace derivative)</li></ol></li><li>To calculate $e^A$<ol><li>Eigenvalue decomposition</li><li>Jordan form decomposition</li><li>Directly evaluate infinite power series (converges quickly)</li><li>Inverse Laplace transform</li></ol></li><li>For more properties of the matrix function, see <a href="/blog/2019-06/MatrixAlgebra/" title="Matrix Algebra">Matrix Algebra</a></li></ul></blockquote><ul><li><p>For homogeneous LTI system: $$\begin{align}x(t)=e^{A(t-t_0)}x_0\end{align}$$</p><ul><li>“<em>homogeneous</em>” = zero-input, Eq.5 is also called <strong>zero input response</strong> (ZIR).</li><li>“<em>homogeneous equation</em>” = 齐次方程</li></ul></li><li><p>For LTI system:<br>$$\begin{align}x(t)=e^{A(t-t_0)}x(t_0)+\int^t_{t_0}e^{A(t-\tau)}Bu(\tau)d\tau\end{align}$$<br>This result requires $A$ to be time-invariant, $B,C,D$ can be time varying.</p><ul><li>The solution consists of two parts: ZIR and ZSR (<strong>zero state response</strong>, $x(t_0)=0$), which are homogenenous solution (通解) and particular solution (特解) of the ODE.</li><li>ZIR and ZSR are both linear mapping</li></ul></li><li><p>For homogeneous LTV system: $$\begin{align}x(t)=\Phi(t,t_0)x_0\end{align}$$</p><ul><li>Matrix $\Phi$ is called the <strong>state transition matrix</strong>, defined as $$\begin{equation}\begin{split}\Phi(t,t_0)\equiv I+\int^t_{t_0}A(s_1)\mathrm{d}s_1+\int^t_ {t_0}A(s_1)\int^{s_1}_ {t_0}A(s_2)\mathrm{d}s_2\mathrm{d}s_1+\\ \int^t_ {t_0}A(s_1)\int^{s_1}_ {t_0}A(s_2)\int^{s_2}_ {t_0}A(s_3)\mathrm{d}s_3\mathrm{d}s_2\mathrm{d}s_1+\cdots\end{split}\end{equation}$$</li><li>Properties of $\Phi$:<ol><li>$\Phi(t,t)=I$</li><li>$\frac{\mathrm{d}}{\mathrm{d}t}\Phi(t,t_0)=A(t)\Phi(t,t_0)$</li><li>(semigroup property) $\Phi(t,s)\Phi(s,\tau)=\Phi(t,\tau)$</li><li>$\forall t,\tau\geqslant 0,\;[\Phi(t,\tau)]^{-1}=\Phi(\tau,t)$</li></ol></li><li>Eq.6 can be directly derived by evaluating Eq.8</li></ul></li><li><p>For LTV system:<br>$$\begin{align}x(t)=\Phi(t,t_0)x_0+\int^t_{t_0}\Phi(t,\tau)B(\tau)u(\tau)d\tau\end{align}$$</p></li><li><p>Some conclusions:</p><ul><li>The solution given by Eq.9 is unique</li><li>The set of all solutions to ZIR system forms a vector space of dimension $n$</li><li>If $A(t)A(s)=A(s)A(t)$, then $\Phi(t,t_0)=e^{\int^t_{t_0}A(\tau)\mathrm{d}\tau}$</li></ul></li><li><p><strong>Phase Portraits</strong>: A phase portrait is a graph of several zero-input responses on the phase plane ($\dot{x}(t)$ and $x(t)$ are phase variables)</p><blockquote><p>Usually in phase portraits, there are two straight lines corresponding to the eigenvector of A, other lines are growing in or opposite to the direction of the lines.</p></blockquote></li></ul><h3 id="Transfer-function">Transfer function<a class="header-anchor" href="#Transfer-function"> ❮</a></h3><ul><li>For LTI case, $\frac{Y(s)}{U(s)} = C(sI-A)^{-1}B+D$<blockquote><p>This can be derived by take laplace transform of both sides of state equations</p></blockquote></li></ul><h2 id="Discrete-State-Space-Representation">Discrete State Space Representation<a class="header-anchor" href="#Discrete-State-Space-Representation"> ❮</a></h2><h3 id="Definition-2">Definition<a class="header-anchor" href="#Definition-2"> ❮</a></h3><p>A discrete-time linear state-space system can be described by following two equations:<br>$$\begin{align}&amp;\text{State eq.}:\;&amp;x(k+1)&amp;=A(k)x(k)+B(k)u(k),&amp;\;x\in\mathbb{R}^n,\;u&amp;\in\mathbb{R}^m \\ &amp;\text{Output eq.}:\;&amp;y(k)&amp;=C(k)x(k)+D(k)u(k),&amp;\;y&amp;\in\mathbb{R}^p\end{align}$$</p><p>The input $u:\mathbb{N}\to\mathbb{R}^m$, state $x:\mathbb{N}\to\mathbb{R}^n$, and output $y:\mathbb{N}\to\mathbb{R}^p$ are all <em>signals</em>, i.e. functions of continuous time $t\in\mathbb{N}$.</p><p>Discrete LTI system is sometimes written compactly as $$\begin{align} x_{k+1}&amp;=Ax_k+Bu_k \\ y_k&amp;=Cx_k+Du_k \end{align}$$</p><h3 id="Transfer-function-2">Transfer function<a class="header-anchor" href="#Transfer-function-2"> ❮</a></h3><ul><li>For LTI case, $H(z)=C(zI-A)^{-1}B+D$ (pulse tranfer function)</li></ul><h2 id="Controllability-Reachability">Controllability &amp; Reachability<a class="header-anchor" href="#Controllability-Reachability"> ❮</a></h2><blockquote><p>Note: hereafter $\mathfrak{R}$ denotes <a href="/blog/2020-06/AlgebraBasicsNotes/#Linear-Operator">range space</a>, $\mathfrak{N}$ denotes <a href="/blog/2020-06/AlgebraBasicsNotes/#Linear-Operator">null space</a>.</p></blockquote><ul><li><strong>Controllability</strong>: $\exists u$ that drives any initial state $x(t_0)=x_0$ to $x(t_1)=0$</li><li><strong>Reachability</strong>: $\exists u$ that drives initial state $x(t_0)=0$ to any $x(t_1)=x_1$</li></ul><p>Consider the continuous LTV system $\dot{x}=A(t)x+B(t)u,\;x\in\mathbb{R}^n,u\in\mathbb{R}^m$.</p><ul><li><p><strong>Reachable Subspace</strong>: Given $t_0$ &amp; $t_1$, the reachable subspace $\mathcal{R}[t_0, t_1]$ consists of all states $x_1$ for which there exists and input $u:[t_0, t_1]\to\mathbb{R}^m$ that transfers the state from $x(t_0)=0$ to $x(t_1)=x_1$.</p><ul><li>$\mathcal{R}[t_0, t_1]\equiv\left\{x_1\in\mathbb{R}^n\middle|\exists u(\cdot),\;x_1=\int^{t_1}_{t_0}\Phi(t_1,\tau)B(\tau)u(\tau)\mathrm{d}\tau\right\}$</li></ul></li><li><p><strong>Controllable Subspace</strong>: Given $t_0$ &amp; $t_1$, the controllable subspace $\mathcal{C}[t_0, t_1]$ consists of all states $x_0$ for which there exists an input $u:[t_0, t_1]\to\mathbb{R}^m$ that transfers the state from $x(t_0)=x_0$ to $x(t_1)=0$</p><ul><li>$\mathcal{C}[t_0, t_1]\equiv\left\{x_0\in\mathbb{R}^m\middle|\exists u(\cdot),\;0=\Phi(t_1,t_0)x_0+\int^{t_1}_{t_0}\Phi(t_1,\tau)B(\tau)u(\tau)\mathrm{d}\tau\right\}$</li><li>or $\mathcal{C}[t_0, t_1]\equiv\left\{x_0\in\mathbb{R}^m\middle|\exists u(\cdot),\;x_0=\int^{t_1}_{t_0}\Phi(t_0,\tau)B(\tau)\left[-u(\tau)\right]\mathrm{d}\tau\right\}$</li></ul></li><li><p><strong>Reachability Grammian</strong>: $W_\mathcal{R}(t_0, t_1)\equiv\int^{t_1}_{t_0}\Phi(t_1,\tau)B(\tau)B(\tau)^\top\Phi^\top(t_1,\tau)\mathrm{d}\tau$ given times $t_1&gt;t_0\geqslant0$</p><ul><li>The system is reachable at time $t_0$ iff $\exists t_1$ s.t. $W_\mathcal{R}(t_0,t_1)$ is non-singular.</li></ul><blockquote><p>non-singular for some $t_1$ $\Rightarrow$ non-singular for any $t_1$</p></blockquote><ul><li>$\mathcal{R}[t_0,t_1]=\mathfrak{R}(W_\mathcal{R}(t_0,t_1))$</li><li>if $x_1=W_\mathcal{R}(t_0,t_1)\eta_1\in\mathfrak{R}(W_\mathcal{R}(t_0,t_1))$, the control $u(t)=B^\top(t)\Phi^T(t_1,t)\eta_1$,$t\in[t_0,t_1]$ can be used to transfer the system from $x(t_0)=0$ to $x(t_1)=x_1$ (w/ minimum energy)</li></ul><blockquote><p>minimum energy = minimum $\int^T_0\Vert u(\tau)\Vert^2\mathrm{d}\tau$</p></blockquote><ul><li>For LTI system $W_\mathcal{R}(t_0,t_1)=\int^{t_1}_ {t_ 0}e^{A(t_1-\tau)}BB^\top e^{A^{\top} (t_1-\tau)}\mathrm{d}\tau=\int^{t_1-t_ 0}_ {0}e^{At}BB^\top e^{A^{\top}t}$</li></ul></li><li><p><a href="https://en.wikipedia.org/wiki/Controllability_Gramian" target="_blank" rel="noopener"><strong>Controllability Grammian</strong></a>: $W_\mathcal{C}(t_0, t_1)\equiv\int^{t_1}_{t_0}\Phi(t_0,\tau)B(\tau)B(\tau)^\top\Phi^\top(t_0,\tau)\mathrm{d}\tau$ given times $t_1&gt;t_0\geqslant0$</p><ul><li><p>The system is reachable at time $t_0$ iff $\exists t_1$ s.t. $W_\mathcal{C}(t_0,t_1)$ is non-singular.</p></li><li><p>$\mathcal{C}[t_0,t_1]=\mathfrak{R}(W_\mathcal{C}(t_0,t_1))$</p></li><li><p>if $x_0=W_\mathcal{C}(t_0,t_1)\eta_0\in\mathfrak{R}(W_\mathcal{C}(t_0,t_1))$, control $u(t)=-B^\top(t)\Phi^\top(t_0,t)\eta_0$,$t\in[t_0,t_1]$ can be used to transfer the state from $x(t_0)=x_0$ to $x(t_1)=0$ (w/ minimum energy)</p></li><li><p>For LTI system $W_\mathcal{C}(t_0,t_1)=\int^{t_1}_ {t_ 0}e^{A(t_0-\tau)}BB^\top e^{A^{\top} (t_0-\tau)}\mathrm{d}\tau=\int^{t_1-t_ 0}_ {0}e^{-At}BB^\top e^{-A^{\top}t}$</p></li></ul></li><li><p><strong>Controllability Matrix</strong>: For LTI system, controllability matrix $\mathcal{C}=[B\;|\;AB\;|\;A^2B\;\cdots\;A^{n-1}B]$</p><blockquote><p>The controllability matrix works for both continuous and discrete system, and it’s easier to be derived from discrete LTI equations:<br>In discrete LTI, $\mathcal{C}\mathbf{u}=-A^k x_0$ where $\mathbf{u}=\begin{bmatrix}u_{k-1} &amp; u_{k-2} &amp; \ldots &amp; u_0\end{bmatrix}^\top$</p></blockquote><ul><li>For LTI, $\mathcal{R}[t_0,t_1]=\mathfrak{R}(W_\mathcal{R}[t_0,t_1])=\mathfrak{R}(\mathcal{C})=\mathfrak{R}(W_\mathcal{C}[t_0,t_1])=\mathcal{C}[t_0,t_1]$</li></ul><blockquote><p>This implies Controllability $\Leftrightarrow$ Reachability for LTI systems.</p></blockquote><ul><li>The controllable subspace $\mathfrak{\mathcal{C}}$ is the smallest A-invariant subspace that contains $\mathfrak{\mathcal{B}}$</li><li>If the controllability matrix has full rank, the LTI system (or the pair $(A,B)$) is <strong>completely controllable</strong></li></ul></li><li><p><strong>PBH-Eigenvector Test</strong>: An LTI system is not controllable iff there exists a nonzero <em>left</em> eigenvector $v$ of $A$ such that $vB=0$</p></li><li><p><strong>PBH-Rank Test</strong>: An LTI system will be controllable iff $[\lambda I-A \;| \;B]$ has full row rank for all eigenvalue $\lambda$</p></li><li><p>For LTI system, there exists an input $u(\cdot)$ that transfer the state from $x_0$ ito $x_1$ in finite time $T$ iff $x_1-e^{AT}x_0\in\mathfrak{R}(\mathcal{C})$</p><ul><li>The input that transfers any state $x_0$ to any other state $x_1$ in some finite time $T$ is $u(t)=B^\top e^{A^{\top}(T-t)}W_\mathcal{R}^{-1}(0,T)[x_1 -e^{AT}x_0]$, for $t\in[0,T]$ (w/ minimum energy)</li></ul></li></ul><h2 id="Observability">Observability<a class="header-anchor" href="#Observability"> ❮</a></h2><ul><li><p><strong>Observability</strong>: Given any input $u(t)$ and output $y(t)$ over $t\in[t_0,t_1]$, it’s sufficient to determine a unique initial state $\exists !x(t_0)$.</p></li><li><p><a href="https://en.wikipedia.org/wiki/Observability_Gramian" target="_blank" rel="noopener"><strong>Observability Grammian</strong></a>: $W_\mathcal{O}(t_0,t_1)\equiv\int^{t_1}_{t_0}\Phi^\top(t_1,\tau)C^\top(\tau)C(\tau)\Phi(t_1,\tau)\mathrm{d}\tau$</p><ul><li><p>The system is observable at time $t_0$ iff $\exists t_1$ s.t. $W_{\mathcal{O}}(0,t)$ is nonsingular.</p></li><li><p>For LTI system $W_{\mathcal{O}}(t_0,t_1)=\int^{t_1}_{t_0} e^{A^{\top}(t_1-\tau)}C^\top Ce^{A(t_1-\tau)}\mathrm{d}\tau=\int^{t_1-t_0}_0 e^{A^{\top}\tau}C^\top Ce^{A\tau}\mathrm{d}\tau$</p></li></ul></li><li><p><strong>Observability Matrix</strong>: For LTI system, observability $\mathcal{O}=\begin{bmatrix}C\\CA\\CA^2\\ \vdots\\CA^{k-1}\end{bmatrix}$</p><blockquote><p>The controllability matrix works for both continuous and discrete system, and it’s easier to be derived from discrete LTI equations:<br>In discrete LTI, $\Psi_{k-1}=\mathcal{O}x_0$ where $$\Psi_k\equiv\begin{bmatrix}y_0\\y_1\\y_2\\ \vdots\\ y_{k-1}\end{bmatrix}-\begin{bmatrix} D &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\ CB &amp; D &amp; 0 &amp; \cdots &amp; 0 \\ CAB &amp; CB &amp; D &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ CA^{k-2}B &amp; CA^{k-3}B &amp; CA^{k-4}B &amp; \cdots &amp; 0 \end{bmatrix}\begin{bmatrix}u_0\\u_1\\u_2\\ \vdots \\ u_{k-1}\end{bmatrix}$$</p></blockquote><ul><li>If the controllability matrix has full rank, the LTI system (or the pair $(A,C)$) is <strong>completely observable</strong>.</li></ul></li><li><p><strong>PBH-Rank Test</strong>: An LTI system will be observable iff $\begin{bmatrix}A-\lambda I \\C\end{bmatrix}$ has full column rank for all eigenvalue $\lambda$</p></li></ul><h2 id="Duality">Duality<a class="header-anchor" href="#Duality"> ❮</a></h2><ul><li><strong>Duality Theorem</strong>: The pair $(A,B)$ is controllable iff the pair $(A^\top, B^\top)$ is observable.<ul><li><em>Controllability</em> only depends on matrix $A$ and $B$ while the <em>Observability</em> only depends on matrix $A$ and $C$</li><li>Duality theorem is useful for proof of observability conclusions from controllability</li></ul></li><li><strong>Adjoint System</strong>:</li></ul><table><tr><th></th><th>Original System</th><th>Adjoint System</th></tr><tr><td>Equations</td><td>$$\begin{align*} \dot{x}&=A(t)x+B(t)u \\ y&=C(t)x \end{align*}$$</td><td>$$\begin{align*} \dot{p}&=-A^*(t)p-C^*(t)v \\ z&=B^*(t)p\end{align*}$$</td></tr><tr><td>Initial Condition</td><td>$x(t_0)=x_0$</td><td>$p(t_1)=p_1$</td></tr><tr><td>State Trasition Matrix</td><td>$\Phi(t,t_0)$</td><td>$\Phi^*(t_1,t)=\left(\Phi^*(t,t_1)\right)^{-1}$</td></tr><tr><td>Zero-State Response</td><td>$$\begin{split}L_u:\;&u(\cdot)\to x(t_1)\\=&\int^{t_1}_{t_0}\Phi(t_1,\tau)B(\tau)u(\tau)\mathrm{d}\tau\end{split}$$</td><td>$$\begin{split}P_u:\;&v(\cdot)\to p(t_0)\\=&\int^{t_1}_{t_0}\Phi^*(\tau,t_0)C^*(\tau)v(\tau)\mathrm{d}\tau\end{split}$$</td></tr><tr><td>Zero-Input Response</td><td>$$\begin{split}L_0:\;&x_0\to y(\cdot)\\=&C(\cdot)\Phi(\cdot,t_0)x_0\end{split}$$</td><td>$$\begin{split}P_0:\;&p_1\to z(\cdot)\\=&B^*(\cdot)\Phi^*(t_1,\cdot)p_1\end{split}$$</td></tr><tr><td rowspan="3">Duality Theorem</td><td>Controllable ($\rho(L_u)=n$)</td><td>Observable ($\rho(P_0^*)=n$)</td></tr><tr><td>Observable ($\rho(L_0^*)=n$)</td><td>Controllable ($\rho(P_u)=n$)</td></tr><tr><td>A state is reachable ($x\in\mathfrak{R}(L_u)$)</td><td>A state is unobservable ($x\in\mathfrak{N}(L_0)$)</td></tr></table>Note that ZIR and ZSR are both linear mappings and $L_u^*=P_0,\;L_0^*=P_u$<h2 id="Decomposition-and-Realizations">Decomposition and Realizations<a class="header-anchor" href="#Decomposition-and-Realizations"> ❮</a></h2><ul><li><p>Similarity Transform of a (LTI) system: Based on Eq.3 and Eq.4, define $x=P\bar{x}$, then we have $$\begin{align}\dot{\bar{x}}&amp;=P^{-1}AP\bar{x}+P^{-1}Bu&amp;=\bar{A}\bar{x}+\bar{B}u \\ y&amp;=CP\bar{x}+Du&amp;=\bar{C}\bar{x}+Du\end{align}$$</p><ul><li>Similarity transform doesn’t affect transfer function.</li></ul></li><li><p><strong>Controllability Decomposition</strong>: For an uncontrollable LTI system, define matrix $V=[V_1\;V_2]$ where $V_1$ is a basis for $\mathfrak{R}(\mathcal{C})$ and $V_2$ complete a basis for $\mathbb{R}^n$, then after similarity transform with $\bar{x}=V^{-1}x$, we can partition the system like following:<br>$$\begin{align*}\dot{\bar{x}}&amp;=\bar{A}\bar{x}+\bar{B}u&amp;=&amp;\begin{bmatrix}\bar{A}_ {11}&amp;\bar{A}_ {12} \\ \mathbf{0} &amp; \bar{A} _{22}\end{bmatrix}\begin{bmatrix}\bar{x} _1 \\ \bar{x} _2\end{bmatrix}+\begin{bmatrix}\bar{B} _1 \\ \mathbf{0}\end{bmatrix}u \\ y&amp;=\bar{C}\bar{x}+Du&amp;=&amp; \begin{bmatrix}\bar{C} _1 &amp; \bar{C} _2\end{bmatrix}\begin{bmatrix}\bar{x} _1 \\ \bar{x} _2\end{bmatrix} + Du\end{align*}$$<br>Here $\bar{x}_2$ is uncontrollable, and ZSR of the system with or without $\bar{x}_2$ is the same.</p></li><li><p><strong>Observability Decomposition</strong>: For an unobservable LTI system, define matrix $U=\begin{bmatrix}U_1\\U_2\end{bmatrix}$ where $U_1$ is a basis for $\mathfrak{R}(\mathcal{O}^\top)$ and $U_2$ complete a basis for $\mathbb{R}^n$, then after similarity transform with $\hat{x}=Ux$, we can partition the system like following:<br>$$\begin{align*}\dot{\hat{x}}&amp;=\hat{A}\hat{x}+\hat{B}u&amp;=&amp;\begin{bmatrix}\hat{A}_ {11}&amp;\mathbf{0} \\ \hat{A}_ {21} &amp; \hat{A} _{22}\end{bmatrix}\begin{bmatrix}\hat{x} _1 \\ \hat{x} _2\end{bmatrix}+\begin{bmatrix}\hat{B} _1 \\ \hat{B} _2\end{bmatrix}u \\ y&amp;=\hat{C}\hat{x}+Du&amp;=&amp; \begin{bmatrix}\hat{C} _1 &amp; \mathbf{0}\end{bmatrix}\begin{bmatrix}\hat{x} _1 \\ \hat{x} _2\end{bmatrix} + Du\end{align*}$$</p></li><li><p><strong>Realization</strong>: $\Sigma$ (system with Eq.3 and Eq.4) is a realization of $H(s)$ iff $H(s)=C(sI-A)^{-1}B+D$.</p><ul><li><strong>Equivalent</strong>: Two realizations are said to be equivalent if they have the same transfer function</li><li><strong>Algebraically Equivalent</strong>: Two realizations have same transfer function and $n$ (dimension of states). (this implies a similarity transform between them)</li><li><strong>Minimal Realization</strong>: $\Sigma$ is a minimal realization of $H(s)$ iff there doesn’t exists an equivalent realization $\bar{\Sigma}$ with $\bar{n}&lt; n$</li></ul></li><li><p>$\Sigma$ is a minial realization iff $\Sigma$ is completely controllable and observable.</p></li><li><p><strong>Kalman Cannonical Structure Theorem</strong> (aka. Kalman Decomposition): suppose $\rho(\mathcal{C})&lt; n$ and $\rho(\mathcal{O})&lt; n$, $\mathfrak{R}(\mathcal{C})$ are the controllable states, $\mathfrak{N}(\mathcal{O})$ are the unobservable states, define subspaces:</p></li></ul><table><thead><tr><th style="text-align:left">Subspaces</th><th style="text-align:center">Controllable</th><th style="text-align:center">Observable</th></tr></thead><tbody><tr><td style="text-align:left">$V_2\equiv\mathfrak{R}(\mathcal{C})\cup\mathfrak{N}(\mathcal{O})$</td><td style="text-align:center">Yes</td><td style="text-align:center">No</td></tr><tr><td style="text-align:left">$V_1$ s.t. $V_1\oplus V_2=\mathfrak{R}(\mathcal{C})$</td><td style="text-align:center">Yes</td><td style="text-align:center">Yes</td></tr><tr><td style="text-align:left">$V_4$ s.t. $V_4\oplus V_2=\mathfrak{N}(\mathcal{O})$</td><td style="text-align:center">No</td><td style="text-align:center">No</td></tr><tr><td style="text-align:left">$V_3$ s.t. $V_1\oplus V_2\oplus V_3\oplus V_4=\mathbb{C}^n$</td><td style="text-align:center">No</td><td style="text-align:center">Yes</td></tr></tbody></table><p>Then let $$\begin{align*}\tilde{x}=\begin{bmatrix}\tilde{x}_ 1\\ \tilde{x}_ 2\\ \tilde{x}_ 3\\ \tilde{x}_ 4\end{bmatrix},\;\tilde{A}=&amp;\begin{bmatrix}A_ {\mathrm{co}} &amp;&amp;A_{13}&amp;\\A_{21}&amp;A_{\mathrm{c\bar{o}}}&amp;A_{23}&amp;A_{24}\\&amp;&amp;A_{\mathrm{\bar{c}o}}&amp;\\&amp;&amp;A_{43}&amp;A_{\mathrm{\bar{c}\bar{o}}} \end{bmatrix},\;\tilde{B}=\begin{bmatrix}B_{\mathrm{co}}\\ B_{\mathrm{c\bar{o}}} \\ \mathbf{0} \\ \mathbf{0} \end{bmatrix} \\ \tilde{C}=&amp;\begin{bmatrix}C_{\mathrm{co}}&amp;\mathbf{0}\quad&amp;C_{\mathrm{\bar{c}o}}&amp;\mathbf{0}\quad\end{bmatrix}\end{align*}$$<br>$$\tilde{\Sigma}:\begin{cases} \dot{\tilde{x}}=A_{\mathrm{co}}\tilde{x}_ 1+B_{\mathrm{co}}u_1\\ y=C_{\mathrm{co}}\tilde{x}_1\end{cases}$$</p><p>$\tilde{\Sigma}$ is completely controllable and completely observable.</p><hr>Consider SISO systems $$H(s)=\frac{b(s)}{a(s)}=\frac{b_{n-1}s^{n-1}+\ldots+b_1s+b_0}{s^n+a_{n-1}s^{n-1}+\ldots+a_1s+a_0}=\frac{\sum^{n-1}_{j=0} b_js^j}{s^n+\sum^{n-1}_{i=0} a_is^i}=\frac{Y(s)}{U(s)}$$<ul><li><strong>Controllable Cannonical Form</strong>: $$\begin{align}\dot{x}&amp;=\begin{bmatrix} 0&amp;1&amp;&amp;\\ \vdots &amp; &amp; \ddots &amp; \\ 0&amp;&amp;&amp;1 \\-a_0&amp;-a_1&amp;\cdots&amp;-a_{n-1}\end{bmatrix}x+\begin{bmatrix}0\\ \vdots \\ 0 \\ 1\end{bmatrix}u&amp;=&amp;A_cx+B_cu\\ y&amp;=\begin{bmatrix}\quad b_0 &amp;\quad b_1 &amp;\cdots &amp; \quad b_{n-1}\end{bmatrix}x&amp;=&amp;C_cx \end{align}$$<ul><li>$(A_c, B_c)$ is controllable</li><li>$(A_c, C_c)$ is observable if $a(s)$ and $b(s)$ have no common factors</li></ul></li><li><strong>Observable Cannonical Form</strong>: $$\begin{align}\dot{x}&amp;=\begin{bmatrix} 0&amp;&amp;&amp;&amp;-a_0\\ 1 &amp; \ddots &amp;&amp;&amp;-a_1 &amp; \\ &amp;\ddots&amp;\ddots&amp;&amp;\vdots \\&amp;&amp;\ddots&amp;0&amp;-a_{n-2} \\ &amp;&amp;&amp;1&amp;-a_{n-1}\end{bmatrix}x+\begin{bmatrix}b_0\\ b_1 \\ \vdots \\ b_{n-2} \\ b_{n-1}\end{bmatrix}u&amp;=&amp;A_ox+B_ou\\ y&amp;=\begin{bmatrix}0 &amp; \;\cdots &amp;\;\cdots &amp; 0 &amp; \quad 1\qquad \end{bmatrix}x&amp;=&amp;C_ox \end{align}$$<ul><li>$(A_o, C_o)$ is observable</li><li>$(A_o, B_o)$ is controllable if $a(s)$ and $b(s)$ have no common factors</li></ul></li><li><strong>Model Cannonical Forms</strong>: Do <strong>Spectral Decomposition</strong> (eigen-decomposition) or Jordan Decomposition, and then use the modal matrix (matrix of eigenvectors) to do similarity transform.</li><li><strong>The Gilbert Realization</strong>: Let $G(s)$ be a $p\times m$ rational transfer function with simple poles (nonrepeated) at $\lambda_i,\;i=1,2,\ldots,k$. Calculate partial fraction expansion $$G(s)=\sum^k_{i=1}\frac{R_i}{s-\lambda_i},\qquad \text{Residue}\;R_i=\lim_{s\to\lambda_i}(s-\lambda_i)G(s)$$ Let $r_i=\rho(R_i)$, now write $R_i=C_iB_i$ where $C_ i\in\mathbb{R}^ {p\times r_ i},\;B_ i\in\mathbb{R}^ {r_ i\times p}$, then write $$A=\mathrm{blkdiag}\{\lambda_i I_{r_i}\},\;B^\top=[B_1^\top \;\cdots\; B^\top_k],\;C=[C_1\; \cdots\;C_k]$$, then $(A,B,C)$ is a realization of $G(s)$ with order $n=\sum^k_1 r_i$</li></ul><p>For MIMO system the cannonical forms with be quite complex:</p><ul><li><strong>Controllable Cannonical Form (for MIMO)</strong>: Here we provide a way to convert from controllable LTI system to controllable. The collection of independent columns of $\mathcal{C}$ may be expressed as $$M=[b_1\;Ab_1\; \cdots\;A^{\mu_1-1}b_1\;|\;b_2\;Ab_2\;\cdots\;A^{\mu_2-1}b_2\;|\;\cdots\;|\;b_p\;Ab_p\;\cdots\;A^{\mu_p-1}b_p]$$ Construct $M^{-1}$ and then $T$:$$M^{-1}=\left[m_{11}^\top\;m_{12}^\top\;\cdots\;m_{1\mu_1}^\top\;\middle|\;\cdots\;\middle|\;m_{p1}^\top\;m_{p2}^\top\;\cdots\;m_{p\mu_p}^\top \right]^\top$$ $$T=\left[m_{1\mu_1}^\top\;(m_{1\mu_1}A)^\top\;\cdots\;\left(m_{1\mu_1}A^{\mu_1-1}\right)^\top\;\middle|\;\cdots\;\middle|\; m_{p\mu_p}^\top\;(m_{p\mu_p}A)^\top\;\cdots\;\left(m_{p\mu_p}A^{\mu_p-1}\right)^\top\right]^\top$$<br>Perform similarity transform with $\bar{x}=Tx$ and the canonical form will be obtained like following:<br>$$\bar{A}=\begin{bmatrix}\bar{A}_ {\mu_1\times\mu_1}&amp;\mathbf{0}_ {\cdot\cdot}&amp;\cdots&amp;\mathbf{0}_ {\cdot\cdot} \\ \mathbf{0}_ {\cdot\cdot}&amp;\bar{A}_ {\mu_2\times\mu_2}&amp;\cdots&amp;\mathbf{0}_ {\cdot\cdot}\\ \vdots&amp;\vdots&amp;\ddots&amp;\vdots \\ \mathbf{0}_ {\cdot\cdot}&amp;\mathbf{0}_ {\cdot\cdot}&amp;\cdots&amp;\bar{A}_ {\mu_ p\times\mu_ p} \end{bmatrix},\quad\bar{B}=\begin{bmatrix}\mathbf{0}_ {\cdot n}\\ \mathbf{0}_ {\cdot (n-1)}\\ \vdots\\ \mathbf{0}_ {\cdot 1}\end{bmatrix}$$<br>Here $\bar{A}_ {\mu_ i\times\mu_ i}$ is the same structure as in SISO, $\mathbf{0}_ {\cdot\cdot}$ is a zero matrix except the last row, $\mathbf{0}_ {\cdot i}$ is a zero matrix except for the last row, and in the last row there are $i$ non-zeros on the right with the first element being 1.</li></ul><h1 id="System-Performance">System Performance<a class="header-anchor" href="#System-Performance"> ❮</a></h1><ul><li><p><strong>System Characteristic Equation</strong>: The polynomical with the roots equal to the poles of the output that are independent of the input.</p></li><li><p><strong>System Type</strong>: A plant $G$ can always be written as $G(s)=\frac{K\prod^m_{i=1}(s-s_i)}{s^N\prod^p_{j=1}(s-s_j)},\;z_i,z_j\neq 0$ or $G(z)=\frac{K\prod^m_{i=1}(z-z_i)}{(z-1)^N\prod^p_{j=1}(z-z_j)},\;z_i,z_j\neq 1$. Here $N$ is called the system type of $G(z)$.</p></li><li><p>Properties that matters for a controller:</p><ol><li>Stability</li><li>Steady state accuracy</li><li>Transient response</li><li>Sensitivity</li><li>Exogenous disturbance rejection</li><li>Bounded control effort</li></ol></li></ul><h2 id="Stability">Stability<a class="header-anchor" href="#Stability"> ❮</a></h2><ul><li><strong>Stability</strong> means when the time goes to infinity, the system response is bounded.<ul><li>A system is <strong>stable</strong> if all its poles lies in the left half of $s$-plane or all inside the unit circle of $z$-plane.</li><li>A system is <strong>marginally stable</strong> if one of the pole is on the imaginary axis of $s$-plane or on the unit circle of $z$-plane.</li></ul></li><li>Stability of linear systems is independent of input<ul><li>The stability of a linear system can be evaluated by its characteristic equation $1-G_{op}(z)=0$, where $G_{op}$ is the open-loop transfer function (transfer function when input is eliminated, or feedback route is cut off).</li></ul></li><li>Methods to evaluate stability<ul><li><strong>Routh-Hurwitz Criterion</strong>: $s$-plane (omited here, see <a href="https://en.wikipedia.org/wiki/Routh%E2%80%93Hurwitz_stability_criterion" target="_blank" rel="noopener">Wikipedia</a>)<ul><li>For discrete system, a strategy is use bilinear transformation: $z=e^{\omega T}\approx \frac{1+\omega T/2}{1-\omega T/2}$</li></ul></li><li><strong>Jury Criterion</strong>: $z$-plane (see <a href="https://en.wikipedia.org/wiki/Jury_stability_criterion" target="_blank" rel="noopener">Wikipedia</a>)</li><li><strong>Root Locus Method</strong>: both $s$- and $z$-plane (see <a href="https://en.wikipedia.org/wiki/Root_locus" target="_blank" rel="noopener">Wikipedia</a>, <code>rlocus</code> in MATLAB)</li><li><strong>Nyquist Criterion</strong>: both $s$- and $z$-plane (see <a href="https://en.wikipedia.org/wiki/Nyquist_stability_criterion" target="_blank" rel="noopener">Wikipedia</a>, <code>nyquist</code> in MATLAB)<ul><li>It works for both continuous and discrete systems, the difference is that in $s$-plane the detour point is at $s=0$ while in $z$-plane the detour point is at $z=1$.</li></ul></li><li><strong>Bode Diagrams</strong>: draw frequency response for (pulse) transfer function, works for both $s$- and $z$-plane (see <a href="https://en.wikipedia.org/wiki/Bode_plot" target="_blank" rel="noopener">Wikipedia</a>, <code>bode</code> in MATLAB)</li></ul><blockquote><p>A review of the stability judgement method <a href="https://www.zhihu.com/question/60272694" target="_blank" rel="noopener">here at 知乎</a></p></blockquote></li></ul><h2 id="Lyaponov-Stability">Lyaponov Stability<a class="header-anchor" href="#Lyaponov-Stability"> ❮</a></h2><blockquote><p>Lyaponove Stability is only concerned with the effect of initial conditions on the response of the system (ZIR)</p></blockquote><ul><li><strong>Equilibrium Point</strong> $x_e$: Consider NLTV $\dot{x}(t)=f(x(t),u(t),t)$, equilibrium point satisfies $x(t_0)=x_e,\;u(t)\equiv 0\Rightarrow x(t)=x_e,\;\text{i.e. }f(x_e,0,t)=0,\;\forall t&gt;t_0$<ul><li>For discrete system, it’s $x(k+1)=x(k)=x_e$</li><li>For LTI system, $x_e$ can be calculated from $Ax_e=0$, so the origin $x=0$ is always an equilibrium point.</li><li>Set of equilibrium points in LTI systems are connected.</li></ul></li><li><strong>Lyapunov stability</strong>: An equilibrium point $x_e$ of the system $\dot{x}=A(t)x$ is <strong>stable (in the sense of Lyapunov)</strong> iff $\forall \epsilon&gt;0,\;\exists \delta(t_0,\epsilon)&gt;0$ s.t. $\Vert x(t_0)-x_e\Vert&lt;\delta\Rightarrow\Vert x(t)-x_e\Vert &lt;\epsilon,\;\forall t&gt;t_0$<ul><li>$x_e$ is <strong>uniformly stable</strong> if $\delta=\delta(\epsilon)$ (regardless of $t_0$)</li><li>$x_e$ <em>in LTI</em> is stable $\Rightarrow x_e$ is uniformly stable</li><li>$x_e$ is <strong>asymptotically stable</strong> if $\Vert x(t)-x_e\Vert\to 0$ as $t\to 0$</li><li>$x_e$ is <strong>exponentially stable</strong> if $\Vert x(t)-x_e\Vert \leqslant \gamma e^{-\lambda(t-t_0)}\Vert x(t_0)-x(e)\Vert$</li><li>$x_e$ <em>in LTI</em> is asymptotically stable $\Rightarrow x_e$ is exponentially stable</li><li>$x_e$ is <strong>globally stable</strong> if $\delta$ can be chosen arbitrarily large</li></ul></li><li>For LTV system, the system is stable (the zero solution is stable) iff $\Phi(t,t_0)$ is bounded by $K(t_0)$.<ul><li>If bounded by constant $K$, then the system is uniformly stable.</li><li>If bounded by constant $K$ and $\Vert\Phi(t,0)\Vert\to 0$ as $t\to 0$, then the system is asymptotically stable.</li></ul></li><li>For LTI system $\dot{x}=Ax$, it is Lyapunov stable iff $\mathrm{Re}(\lambda_i)\leqslant 0$ or $\mathrm{Re}(\lambda_i)=0,\;\eta_i=1$. ($\eta_i$ is the multiplicity of $\lambda_i$)<ul><li>If $\mathrm{Re}(\lambda_i)&lt;0$, then the system is asymptotically stable</li></ul></li><li>Internal stability: concerns the state variables</li><li>External stability: concerns the output variables</li></ul><blockquote><p>Notes for contents below:</p><ul><li>Positive definite (pd.) function: function $V$ is pd. wrt. $p$ if $V(x)&gt;0,\;x\neq p$ and $V(x)=0,\;x=p$</li><li>$C^n$ denotes the set of continuous and at least n-th differentiable functions</li></ul></blockquote><ul><li><p><strong>Lyapunov’s Direct Method</strong>: Let $\mathcal{U}$ be an open neighborhood of $p$ and let $V:\mathcal{U}-&gt;\mathbb{R}$ be a countinuous positive definite $C^1$ function wrt. $p$, we have following two conclusions:</p><ol><li>If $\dot{V}\leqslant 0$ on $\mathcal{U}\backslash\{p\}$ then $p$ is a stable fixed point of $\dot{x}=f(x)$</li><li>If $\dot{V}&lt; 0$ on $\mathcal{U}\backslash\{p\}$ then $p$ is an asymptotically stable fixed point of $\dot{x}=f(x)$</li></ol></li><li><p><strong>Lyapunov Function</strong>:</p><ul><li>A function satisfying conclusion 1 is called a Lyapunov function</li><li>A function satisfying conclusion 2 is called a <strong>strict</strong> Lyapunov function</li><li>A function that is $C^1$ and pd. is called a Lyapunov function candidate</li></ul><blockquote><p>The energy function usually can be used as Lyapunov function. If it’s only semi-positive definite, one can use <a href="https://en.wikipedia.org/wiki/LaSalle%27s_invariance_principle" target="_blank" rel="noopener">LaSalle’s Theorem</a></p></blockquote></li><li><p>For LTI system, the zero solution of $\dot{x}=Ax$ is asymptotically stable iff $\forall$ pd. hermitian matrices $Q$, equation $A^*P+PA=-Q$ has a unique hermitian solution $P$ that is positive definite.</p><ul><li>$A^*P+PA=-Q$ is called Lyapunov’s Matrix Equation</li></ul><blockquote><p>here $V(x)=x^* Px=\int^\infty_0 x^*(t)Qx(t)dt$, which can be also called cost-to-go, or generalized energy</p></blockquote></li><li><p><strong>Lyapunov’s Indirect Method</strong> (Lyapunov’s First Method / Lyapunov’s Linearization Theorem): The nonlinear system $\dot{x}=f(x)$ is (locally) asymptotically stable near the equilibrium point $x_e$ if the linearized system $\dot{x}_L=\frac{\partial f}{\partial x}(x_e)x_L$ is asymptotically stable.</p></li></ul><h2 id="Bounded-Input-Bounded-Output-Stability">Bounded-Input Bounded-Output Stability<a class="header-anchor" href="#Bounded-Input-Bounded-Output-Stability"> ❮</a></h2><blockquote><p>BIBO stability is only concerned with the response of the system to the input (ZSR).</p></blockquote><ul><li><strong>Bounded-Input Bounded-Output (BIBO) stability</strong>: The LTV system is said to be (uniformly) BIBO stable if there exists a finite constant $g$ s.t. $\forall u(\cdot)$, its forced response $y_f(\cdot)$ satisfies $$ \sup_{t\in[0,\infty)}\Vert y_f(t)\Vert \leqslant g \sup_{t\in[0,\infty)} \Vert u(t)\Vert $$<blockquote><p>The impulse response can be analyzed to assess BIBO stability<br>The LTV system is uniformly BIBO stable iff every entry of $D(t)$ is bounded and $\sup_{t\geqslant 0}\int^t_0|g_{ij}(t,\tau)|d\tau &lt;\infty$ for every entry $g_{ij}$ of the matrix $C(t)\Phi(t,\tau)B(\tau)$.</p></blockquote></li><li>BIBO stability is related with the stability descibed in classical control theory.</li><li>Exponential Lyapunov Stability $\Rightarrow$ BIBO stability</li></ul><h2 id="Steady-State-Accuracy">Steady State Accuracy<a class="header-anchor" href="#Steady-State-Accuracy"> ❮</a></h2><p>Steady state accurary can be derived from the property of Laplace/Z-transform as mentioned above (assuming stability)<br>$$\lim\limits_{t\to \infty} f(t) = \lim\limits_{z\to 1} (z-1)F(z) = \lim\limits_{s\to 0}sF(s)$$</p><h2 id="Transient-Response">Transient Response<a class="header-anchor" href="#Transient-Response"> ❮</a></h2><p>Some measurements of transient response (with step input):</p><ul><li><strong>Rise time</strong> $t_r$: time from 10% to 90% of steady state value</li><li><strong>Peak overshoot</strong>: $M_p$ for overshoot magnitude and $t_p$ for time</li><li><strong>Settling time</strong> $t_s$: time after which the magnitude fall in $1-d$ to $1-d$ final value. $d$ is usually %2~5.</li></ul><h2 id="Sensitivity">Sensitivity<a class="header-anchor" href="#Sensitivity"> ❮</a></h2><p>Given a transfer function $H(z)$ with parameter $\Theta\in\mathbb{R}$, then sensitivity is defined as $S_H=\frac{\partial H}{\partial \Theta}\cdot\frac{\Theta}{H} = \frac{\partial H/H}{\partial \Theta/\Theta}$</p><h1 id="Discretization-and-Linearization">Discretization and Linearization<a class="header-anchor" href="#Discretization-and-Linearization"> ❮</a></h1><h2 id="Discretization-Example">Discretization Example<a class="header-anchor" href="#Discretization-Example"> ❮</a></h2><p>The following image shows a minial example of sampling and hold.</p><img src="/blog/2020-06/ControlSystemNotes/snh.png" title="A minimal example of a conversion process with analog and digital signals"><h2 id="Sampling-A-D">Sampling (A/D)<a class="header-anchor" href="#Sampling-A-D"> ❮</a></h2><ul><li>Ideal sampler (a.k.a impulse modulator) converts a continuous signal $e: \mathbb{R}_+ \to \mathbb{R}$ to a discrete one $\hat{e}: \mathbb{N}\to \mathbb{R}$, such that $$ \hat{e}=e(t)\delta(t-kT)=e(t)\delta_T(t); \forall k\in \mathbb{N} $$<ul><li>Ideal sampler is actually applying starred transform.</li></ul></li><li>How to sample? A rule of thumb used to select sampling rates is chosing a rate of at least 5 samples per time constant.<ul><li>The $\tau$ appearing in the transient response term $ke^{-t/\tau}$ of a first order analog system is called the time constant.</li><li>If the sampling time is too large, it can make the system unstable.</li></ul></li></ul><h2 id="Reconstruction-Hold-D-A">Reconstruction/Hold (D/A)<a class="header-anchor" href="#Reconstruction-Hold-D-A"> ❮</a></h2><ul><li>Zero order hold (ZOH): $ZOH(\{e(k)\}_{k\in\mathbb{N}})(t) = e(k)\;for\;kT\leq t\leq (k+1)T$<ul><li>Alternative form: $ZOH(\{e(k)\})=\sum^\infty_{k=0}e(k)(H(t-kT)-H(t-(k+1)T))$</li><li>Its Laplace Transform: $G_{ZOH}(s)=\frac{1-e^{-Ts}}{s}$</li></ul></li><li>First order hold (FOH): (delayed version) $$FOH(\{e(k)\}_ {k\in\mathbb{N}})(t)=\sum_ {k\in\mathbb{N}}\left[e(kT)+\frac{t-kT}{T}(e(kT)-e((k-1)T)) \right]\left[H(t-kT)-H(t-(k+1)T) \right]$$</li></ul><h2 id="State-Space-Representation-2">State Space Representation<a class="header-anchor" href="#State-Space-Representation-2"> ❮</a></h2><p>Suppose we are given the LTI continuous system<br>$$\begin{align*} \dot{x}(t) &amp;= Ax(t)+Bu(t) \\ y(t)&amp;=Cx(t)+Du(t) \end{align*}$$<br>If the input is sampled and ZOH and the output is sampled, then<br>$$\begin{align*} x(k+1)&amp;=\bar{A}x(k)+\bar{B}u(k) \\ y(k)&amp;=\bar{C}x(k)+\bar{D}u(k)\end{align*}$$<br>where $\bar{A}=\Phi((k+1)T,kT)=e^{AkT}$<br><br>$\bar{B}=\int^{(k+1)T}_{kT}\Phi((k+1)T,\tau)B\mathrm{d}\tau=A^{-1}(e^{AT}-I)$<br><br>$\bar{C}=C$ and $\bar{D}=D$</p><blockquote><p>Steps to apply conversion:</p><ol><li>Dervice SS model for analog system</li><li>Calculate discrete representation (<code>c2d</code> in MATLAB)</li><li>Calculate pulse transfer function (<code>ss2tf</code> in MATLAB)<br>If there is a complex system with multiple sampling and holding, a general rule is</li></ol><ul><li>Each ZOH output is assumed to be an input</li><li>Each sampler input is assumed to be an output<br>and then create continuous state space from analog part of the system, then discretize them to generate discrete equations</li></ul></blockquote><h2 id="s-plane-and-z-plane">$s$-plane and $z$-plane<a class="header-anchor" href="#s-plane-and-z-plane"> ❮</a></h2><p>When converting $s$ to $z$, the complex variables are related by $z=e^{Ts}$. Suppose $s=\sigma+j\omega$, then $z=e^{T\sigma}\angle \omega T$</p><blockquote><p>Note: if frequencies differ in integer multiples of the sampling frequency $\frac{2\pi}{T}=\omega_s$, then they are sampled into the same location in the $z$-plane.</p></blockquote><p>For transient response relationship, suppose $s$-plane poles occur at $s=\sigma\pm j\omega$, then the transient response if $Ae^{\sigma t}\cos(\omega t+\varphi)$. When sampling occurs at $z$-plane poles, then the transient response if $Ae^{\sigma kT}\cos(\omega kT+\varphi)$.</p><details><summary>Example: 2nd order transfer function</summary>$$G(s)=\frac{\omega_n^2}{s^2+2\xi\omega_ns+\omega_n^2}$$ The $z$-plane poles occur at $z=r\angle\pm\theta$ where $r=e^{-\xi\omega_n T}$ and $\theta=\omega_n T\sqrt{1-\xi^2}$.<p>Then we can get the inverse relationship</p><ul><li>$\xi=-\ln( r)/\sqrt{\ln^2( r)+\theta^2}$</li><li>$\omega_n=(1/T)\sqrt{\ln^2( r)+\theta^2}$</li><li>(time constant) $\tau=-T/\ln( r)$</li></ul></details><h2 id="Linearization">Linearization<a class="header-anchor" href="#Linearization"> ❮</a></h2><ul><li><strong>Jacobian Linearization</strong>: linearize $\dot{x}=f(x,u)$ at an equilibrium $(x_e, u_e)$ is $$\frac{\mathrm{d}z}{\mathrm{d}t}=Az+Bv,\quad\text{where}\;A=\left.\frac{\mathrm{d}f}{\mathrm{d}x}\right|_ {\begin{split}x=x_e\\u=u_e\end{split}},\;B=\left.\frac{\mathrm{d}f}{\mathrm{d}u}\right|_ {\begin{split}x=x_e\\u=u_e\end{split}},\;z=(x-x_e),\;v=(u-u_e)$$<ul><li>change $(x_e, u_e)$ to a trajectory $(x_e(t), u_e(t))$ we can linearize the system about a trajectory.</li></ul></li></ul><h1 id="Controllers">Controllers<a class="header-anchor" href="#Controllers"> ❮</a></h1><h2 id="Full-State-Feedback">Full State Feedback<a class="header-anchor" href="#Full-State-Feedback"> ❮</a></h2><blockquote><p>This method can be used for both continuous and discrete systems, just make sure to use corresponding method for choosing correct closed-loop transfer function.</p></blockquote><p>For state space systems, with access to all of the state variables, we can change the $A$ matrix and thereby change the system dynamics by feedback.</p><p>Consider SISO LTI system ($u\in\mathbb{R},y\in\mathbb{R}$), we define the input as $u\equiv Kx+Ev$ where $K\in\mathbb{R}^{1\times n},\;E\in\mathbb{R}$ is an input matrix and $v(t)\in\mathbb{R}^\mathbb{R}$ is the exogeneous (externally applied) input. The new system will be $$\begin{align}\dot{x}&amp;=(A+BK)x+BEv\\y&amp;=(C+DK)x+DEv\end{align}$$<br>The mission is to find a state update matrix $A_{\mathrm{CL}}\equiv A+BK$ with desired set of eigenvalues, therefore we can construct $A_{\mathrm{CL}}$ with specific eigenvalues and then calculate $K$. This process will be quite easy if the system is already in controllable cannonical form. (which can be constructed directly from transfer function or using similarity transform)</p><p>Another way (SISO only) to calculate $K$ without controllable cannonical form is using the following formulae given the desired characteristic polynomial $\phi^{\star}(s)=s^n+\sum^{n-1}_ {i=0} a^\star_i s^i$ and original characteristic polynomial $\phi(s)=s^n+\sum^{n-1}_ {i=0} a_i s^i$</p><ul><li><strong>Ackermann’s Formula</strong>: $K=-e^\top_n\mathcal{C}^{-1}\phi^\star(A)$ (here $e_i$ is unit vector with 1 at i-th position)</li><li><strong>Bass-Gura’s Formula</strong>: $$K=-[(a^\star_{n-1}-a_{n-1}) \;\cdots\;(a^\star_0-a_0)]\begin{bmatrix}1&amp;a_{n-1}&amp;a_{n-2}&amp;\cdots&amp;a_1\\&amp;1&amp;a_{n-1}&amp;\cdots&amp;a_2\\ &amp;&amp;\ddots&amp;\ddots&amp;\vdots \\ &amp;&amp;&amp;1&amp;a_{n-1}\\ &amp;&amp;&amp;&amp;1\end{bmatrix}^{-1}\mathcal{C}^{-1}$$</li></ul><p>Note that the zeros of transfer function will not be affected by state feedback.</p><h2 id="State-Estimation-Observer-Design">State Estimation (Observer Design)<a class="header-anchor" href="#State-Estimation-Observer-Design"> ❮</a></h2><blockquote><p>Some times we don’t have the direct access to the state, we need construct an observer<br>For stochastic version, please check <a href="/blog/2019-03/StochasticSystemNotes/#Observation-Filtering">my notes for stochastic system</a></p></blockquote><p>Assume a plant $\Sigma$ and an (<strong>Luenberger</strong>) <strong>observer</strong> $\hat{\Sigma}$:<br>$$\Sigma:\begin{cases}\dot{x}=Ax+Bu\\ y=Cx\end{cases},\quad \hat{\Sigma}:\begin{cases} \dot{\hat{x}}=A\hat{x}+Bu+L(y-\hat{y})\\ y=C\hat{x}\end{cases}$$</p><p>Subtract observer dynamics from plant dynamics and define $e\equiv x-\hat{x}$, the dynamics for $e$ is $\dot{e}=(A-LC)e$ and $y-\hat{y}=Ce$. This error dynamic $A_e=A-LC$ can be easily changed with observable cannonical form. (which similarly can be constructed directly from transfer function or using similarity transform)</p><ul><li><strong>Reduced-order Observer</strong>: If the state length of the system $n$ is large while $n-p$ is small, split the system and let $x_1$ holds the states that can be measured directly while $x_2$ holds states that are to be estimated, (i.e. $y=x_1+Du$). Define $z=\hat{x}_2-Lx_1$ then the system runs like $$\begin{align*}\begin{bmatrix}x_1 \\ \hat{x}_2\end{bmatrix}&amp;=\begin{bmatrix} y-Du\\ z+Lx_1 \end{bmatrix}\qquad\begin{split}&amp;\text{measurement} \\ &amp;\text{observer}\end{split} \\ u&amp;=K\begin{bmatrix}x_1 \\ \hat{x}_2 \end{bmatrix} + v \qquad\text{control law}\end{align*}$$ And then the error we care about is only $e=x_2-\hat{x}_2$.</li><li><strong>Ackermann’s Formula</strong>: $L=\phi^\star(A)\mathcal{O}^{-1}e_n$ ($\phi^\star$ is the desired characteristic function for $A_e$)</li><li><strong>Separation Principle</strong>: If a stable observer and stable state feedback are designed for an LTI system, then the combined observer and feedback will be stable.</li></ul><blockquote><p>Errors from state estimation</p><ol><li>Inaccurate knowledge of $A$ and $B$</li><li>Initial condition uncertainty</li><li>Disturbance or sensor error<br>It’s advised to choose observer poles to be 2-4x faster than closed loop poles</li></ol></blockquote><h2 id="LQR"><a href="https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic_regulator" target="_blank" rel="noopener">LQR</a><a class="header-anchor" href="#LQR"> ❮</a></h2><blockquote><p>Motivation: handle control constraints and time varying dynamics with performance metric (ideas of optimal control)<br>Note: $x^\top Ax$ is called a <strong>quadratic form</strong>, $x^\top Ay$ is called a <strong>bilinear form</strong></p></blockquote><ul><li>A quadratic function $f(x)=x^\top Dx+C^\top x+c_0$ has one minimizer iff $D\succ 0$, or multiple minimizers iff $D\succeq 0$.</li><li>(discrete finite time) <strong>Linear Quadratic Regulator</strong> (LQR): the control problem is defined as $$\begin{align*}\min_{u\in\left(\mathbb{R}^m\right)^{\{0,\ldots,N\}}} J_{N}(u,x_0)&amp;=\frac{1}{2}\sum^{N}_{k=0}(x^\top(k)Q(k)x(k)+u^\top(k)R(k)u(k)) \\ \mathrm{s.t.}\qquad x(k+1) &amp;= A(k)x(k) + B(k)u(k)\quad \forall k\in\{0,\ldots,N-1\}\\ y(k)&amp;=C(k)x(k)\\ x(0)&amp;=x_0\end{align*}$$ where $Q(k)\succ 0$ and $R(k)\succ 0$</li><li><strong>Bellman’s Principle of Optimality</strong>: If a closed loop control $u^\star$ is optimal over the interval $0\leqslant k\leqslant N$, it’s also optimal over any subinterval $m\leqslant k\leqslant N$ where $m\in\{0,\ldots,N\}$</li><li><strong>The Minimum Principle</strong>: The optimal input to the LQR problem satisfies the following backward equations: $$\begin{align*}u^\star(k)&amp;=-K(x)x(k) \\ K(k)&amp;=\left[B^\top(k) P(k+1)B(k)+\frac{1}{2}R(k)\right]^{-1}B^\top(k)P(k+1)A(k) \\ P(k)&amp;=A^\top(k)P(k+1)[A(k)- B(k)K(k)]+\frac{1}{2}Q(k)\end{align*}$$ and $P(N)=Q(N),\;K(N)=0$. The optimal cost is $J^\star_N=x^\top(0)P(0)x(0)$</li><li>For infinite horizon, $K(k)$ start becoming constants. The optimal input for LQR problem (assuming the system became LTI when $N\to\infty$) is $u^*(k)=-Kx(k)$ where $$K=(B^\top PB+R/2)^{-1}B^\top PA$$ and $P\succ 0$ is the unique solution to the discrete-time <strong>algebraic Riccati Equation</strong>: $$P=A^\top PA-A^\top PB\left(B^\top PB+R/2\right)^{-1}B^\top PA+Q/2$$</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;&lt;ul&gt;&lt;li&gt;This note combines content from ME 564 Linear Systems and ME 561 Discrete Digital Control&lt;/li&gt;&lt;li&gt;Please read &lt;a href=&quot;/blog/2020-06/AlgebraBasicsNotes/&quot; title=&quot;the Algebra Basics notes&quot;&gt;the Algebra Basics notes&lt;/a&gt; first if you are not familiar with related concepts.&lt;/li&gt;&lt;li&gt;In this note, $f\in\mathbb{F}^\mathbb{G}$ stands for a function with domain in $\mathbb{G}$ and co-domain in $\mathbb{F}$, i.e. $f:\mathbb{F}\to\mathbb{G}$, $H(x)$ generally stands for Heaviside function (step function)&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;h1 id=&quot;Transforms&quot;&gt;Transforms&lt;a class=&quot;header-anchor&quot; href=&quot;#Transforms&quot;&gt; ❮&lt;/a&gt;&lt;/h1&gt;&lt;h2 id=&quot;Laplace-Transform&quot;&gt;Laplace Transform&lt;a class=&quot;header-anchor&quot; href=&quot;#Laplace-Transform&quot;&gt; ❮&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Definition: $F(s)=\mathcal{L}\{f(t)\}(s)=\int^\infty_0 f(t)e^{-st}\mathrm{d}t$&lt;blockquote&gt;&lt;p&gt;Note that the transform is not well defined for all functions in $\mathbb{C}^\mathbb{R}$. And the transform is only valid for $s$ in a region of convergence, which is usually separated by 0.&lt;/p&gt;&lt;/blockquote&gt;&lt;/li&gt;&lt;li&gt;Laplace Transform is a linear map from $(\mathbb{C}^\mathbb{R}, \mathbb{C})$ to $(\mathbb{C}^\mathbb{C}, \mathbb{C})$ and it’s one-to-one.&lt;/li&gt;&lt;li&gt;Properties: (see &lt;a href=&quot;https://en.wikipedia.org/wiki/Laplace_transform&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Wikipedia&lt;/a&gt; or &lt;a href=&quot;https://lpsa.swarthmore.edu/LaplaceZTable/LaplacePropTable.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;this page&lt;/a&gt; for full list)&lt;ul&gt;&lt;li&gt;Derivative: $f’(t) \xleftrightarrow{\mathcal{L}} sF(s)-f(0^-)$&lt;/li&gt;&lt;li&gt;Integration: $\int^t_0 f(\tau)d\tau \xleftrightarrow{\mathcal{L}} \frac{1}{s}F(s)$&lt;/li&gt;&lt;li&gt;Delay: $f(t-a)H(t-a) \xleftrightarrow{\mathcal{L}} e^{-as}F(s)$&lt;/li&gt;&lt;li&gt;Convolution: $\int^t_0 f(\tau)g(t-\tau)\mathrm{d}\tau \xleftrightarrow{\mathcal{L}} F(s)G(s)$&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Stationary Value: $\lim\limits_{t\to 0} f(t) = \lim\limits_{s\to \infty} sF(s), \lim\limits_{t\to \infty} f(t) = \lim\limits_{s\to 0} sF(s)$&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://zyxin.xyz/blog/categories/Notes/"/>
    
      <category term="Control" scheme="http://zyxin.xyz/blog/categories/Notes/Control/"/>
    
    
      <category term="Math" scheme="http://zyxin.xyz/blog/tags/Math/"/>
    
      <category term="Control" scheme="http://zyxin.xyz/blog/tags/Control/"/>
    
  </entry>
  
  <entry>
    <title>终端的特殊控制符</title>
    <link href="http://zyxin.xyz/blog/2020-05/TerminalControlCharacters/"/>
    <id>http://zyxin.xyz/blog/2020-05/TerminalControlCharacters/</id>
    <published>2020-05-10T22:49:54.000Z</published>
    <updated>2021-07-12T02:40:53.188Z</updated>
    
    <content type="html"><![CDATA[<p>之前碰到过很多终端工具可以显示非常好看的进度条，或者显示丰富的颜色，甚至还有的直接可以在终端通过字符绘制UI（a.k.a. <a href="https://en.wikipedia.org/wiki/Text-based_user_interface" target="_blank" rel="noopener">TUI</a>），我一直都很好奇是怎么做到的。之后又知道了curses这个Python库和它的一些高层封装（例如asciimatics），然后最终在Stack Overflow里面查到了这些都是通过特殊的终端控制符来实现的。本文就介绍这些终端控制符的使用方法，他们很适合用来写一个简单无依赖的TUI。如果需要更复杂和全面的TUI功能，还是最好使用封装好的库。</p><h1 id="ASCII-控制符">ASCII 控制符<a class="header-anchor" href="#ASCII-控制符"> ❮</a></h1><p>在最开始接触编程的时候，如果你学的是C，那你一定很熟悉<code>\n</code>，这就是一个”换行“的转义字符，代表终端光标令起一行。有时你还会碰到<code>\r</code>，这是”回车“。“回车”这个名字来源于打字机时代，在使用打字机的时候，如果你需要新起一行，那么需要的操作是：转动滚筒把纸往外抽一行，再把字车（相当与打印机的打印头）移到最左端。这两个操作的名字分别是“换行”和“回车”。因此严格来说另起一行的字符串应该是<code>\r\n</code>，这也是Windows的标准，而在Unix中则简化成<code>\n</code>会自动执行回车。</p><a id="more"></a><p>换行和回车是两个非常常用的控制字符，也是定义在了ASCII表中的控制字符。在ASCII表中还定义了其他的控制字符，列在下面了。</p><table><thead><tr><th>ASCII名字</th><th>ASCII码</th><th>printf风格转义</th><th>用途</th></tr></thead><tbody><tr><td>BEL 铃声</td><td>0x07</td><td><code>\a</code></td><td>哔一下（执不执行取决于终端）</td></tr><tr><td>BS 退格</td><td>0x08</td><td><code>\b</code></td><td>*光标回退一格</td></tr><tr><td>ESC 退出</td><td>0x1B</td><td><code>\e</code></td><td>可代表按下ESC键，不是C标准</td></tr><tr><td>FF 换页</td><td>0x0C</td><td><code>\f</code></td><td>光标移到新一页</td></tr><tr><td>LF 换行</td><td>0x0A</td><td><code>\n</code></td><td>光标下移一行</td></tr><tr><td>CR 回车</td><td>0x0D</td><td><code>\r</code></td><td>光标回到行首</td></tr><tr><td>HT 水平制表</td><td>0x09</td><td><code>\t</code></td><td>标记水平制表位（Tab键）</td></tr><tr><td>VT 垂直制表</td><td>0x0B</td><td><code>\v</code></td><td>标记垂直制表位</td></tr><tr><td>NUL 空值</td><td>0x00</td><td><code>\0</code></td><td>代表啥也没有，C里面终结字符串</td></tr><tr><td>-</td><td>-</td><td>**<code>\c</code></td><td>终止输出，基本不被支持了</td></tr></tbody></table><p>*光标这里泛指各类终端的指示当前文本位置的东西，在打字机上叫“type guide”，在显示屏上里面叫“光标 cursor”，而在有些场合也叫指针。<br>**这个用法貌似只在一些终端中有，我也不确定它是否有对应一个字符。在<a href="http://www.gnu.org/software/coreutils/manual/html_node/printf-invocation.html" target="_blank" rel="noopener">GNU的文档</a>里有简短解释。</p><h1 id="ANSI-VT100-控制符（串）">ANSI/VT100 控制符（串）<a class="header-anchor" href="#ANSI-VT100-控制符（串）"> ❮</a></h1><p>很多终端都支持彩色文字的输出，而彩色文字的表达方式通常都参考ANSI的色彩标准。而ANSI用来实现色彩显示的转义表还定义了指针控制和设备管理的功能。</p><p>这一类控制符实际上是个字符串，所以应该叫控制串？他们都由<code>&lt;ESC&gt;</code>字符开头，也就是<code>0x1B</code>。所以我推测实际上<code>ESC</code>的双关（退出/转义）也被用到了这里哈哈。以下内容大部分来自 ANSI/VT100 Terminal Control Escape Sequences 表格，详细解释可以参考这个表格以及维基的页面。链接都放在引用部分。</p><blockquote><p><code>0x1B</code>在一些终端中会用<code>^[</code>代表，因此如果你看到了<code>^[[</code>那通常也都是通过这种方法转义的字符序列。</p></blockquote><p>我把这个表中能用于bash的字符都拎出来放在下面了。以下表中的转义序列名称都是我自己翻译的，我不知道有没有统一的中文翻译hhh。</p><h2 id="终端设备相关">终端设备相关<a class="header-anchor" href="#终端设备相关"> ❮</a></h2><table><thead><tr><th>名称</th><th>转义字符串</th></tr></thead><tbody><tr><td>查询设备码</td><td><code>&lt;ESC&gt;[c</code></td></tr><tr><td>报告设备码</td><td><code>&lt;ESC&gt;[{code}0c</code></td></tr><tr><td>查询光标位置</td><td><code>&lt;ESC&gt;[6n</code></td></tr><tr><td>报告光标位置</td><td><code>&lt;ESC&gt;[{ROW};{COLUMN}R</code></td></tr><tr><td>重置设备</td><td><code>&lt;ESC&gt;c</code></td></tr></tbody></table><blockquote><p>可以在你的终端里输入<code>printf &quot;\x1b[c&quot;</code>，看看会输出什么</p></blockquote><h2 id="光标控制">光标控制<a class="header-anchor" href="#光标控制"> ❮</a></h2><table><thead><tr><th>名称</th><th>转义字符串</th></tr></thead><tbody><tr><td>设置指针位置</td><td><code>&lt;ESC&gt;[{ROW};{COLUMN}H</code></td></tr><tr><td>指针上移</td><td><code>&lt;ESC&gt;[{COUNT}A</code></td></tr><tr><td>指针下移</td><td><code>&lt;ESC&gt;[{COUNT}B</code></td></tr><tr><td>指针前移（右移）</td><td><code>&lt;ESC&gt;[{COUNT}C</code></td></tr><tr><td>指针后移（左移）</td><td><code>&lt;ESC&gt;[{COUNT}D</code></td></tr><tr><td>保存指针位置</td><td><code>&lt;ESC&gt;[s</code></td></tr><tr><td>复原指针位置（到保存位置）</td><td><code>&lt;ESC&gt;[u</code></td></tr><tr><td>保存指针位置和属性</td><td><code>&lt;ESC&gt;7</code></td></tr><tr><td>复原指针位置和属性</td><td><code>&lt;ESC&gt;8</code></td></tr></tbody></table><h2 id="滚动">滚动<a class="header-anchor" href="#滚动"> ❮</a></h2><table><thead><tr><th>名称</th><th>转义字符串</th></tr></thead><tbody><tr><td>启用滚动</td><td><code>&lt;ESC&gt;[r</code></td></tr><tr><td>启用指定行之间滚动</td><td><code>&lt;ESC&gt;[{START};{END}r</code></td></tr><tr><td>向下滚动一行</td><td><code>&lt;ESC&gt;D</code></td></tr><tr><td>向上滚动一行</td><td><code>&lt;ESC&gt;M</code></td></tr></tbody></table><h2 id="制表">制表<a class="header-anchor" href="#制表"> ❮</a></h2><table><thead><tr><th>名称</th><th>转义字符串</th></tr></thead><tbody><tr><td>设置对齐位</td><td><code>&lt;ESC&gt;H</code></td></tr><tr><td>清楚对齐位</td><td><code>&lt;ESC&gt;[g</code></td></tr><tr><td>清楚所有对齐位</td><td><code>&lt;ESC&gt;[3g</code></td></tr></tbody></table><h2 id="清除">清除<a class="header-anchor" href="#清除"> ❮</a></h2><table><thead><tr><th>名称</th><th>转义字符串</th></tr></thead><tbody><tr><td>清除文字到行末</td><td><code>&lt;ESC&gt;[K</code></td></tr><tr><td>清除文字到行首</td><td><code>&lt;ESC&gt;[1K</code></td></tr><tr><td>清除整行</td><td><code>&lt;ESC&gt;[2K</code></td></tr><tr><td>清除文字到屏幕底</td><td><code>&lt;ESC&gt;[J</code></td></tr><tr><td>清除文字到屏幕顶</td><td><code>&lt;ESC&gt;[1J</code></td></tr><tr><td>清屏</td><td><code>&lt;ESC&gt;[2J</code></td></tr></tbody></table><h2 id="定义">定义<a class="header-anchor" href="#定义"> ❮</a></h2><ul><li>设置文字绑定: <code>&lt;ESC&gt;[{key};&quot;{string}&quot;p</code></li></ul><h2 id="显示颜色属性">显示颜色属性<a class="header-anchor" href="#显示颜色属性"> ❮</a></h2><ul><li>设置光标属性: <code>&lt;ESC&gt;[{attr1};...;{attrn}m</code></li></ul><table><thead><tr><th>属性代码</th><th>属性效果</th><th>属性代码</th><th>属性效果</th><th>属性代码</th><th>属性效果</th></tr></thead><tbody><tr><td>0</td><td>重置</td><td>30</td><td>前景黑</td><td>40</td><td>背景黑</td></tr><tr><td>1</td><td>亮</td><td>31</td><td>前景红</td><td>41</td><td>背景红</td></tr><tr><td>2</td><td>暗</td><td>32</td><td>前景绿</td><td>42</td><td>背景绿</td></tr><tr><td>4</td><td>下划线</td><td>33</td><td>前景黄</td><td>43</td><td>背景黄</td></tr><tr><td>5</td><td>闪烁</td><td>34</td><td>前景蓝</td><td>44</td><td>背景蓝</td></tr><tr><td>7</td><td>反向</td><td>35</td><td>前景紫</td><td>45</td><td>背景紫</td></tr><tr><td>8</td><td>隐藏</td><td>36</td><td>前景青</td><td>46</td><td>背景青</td></tr><tr><td></td><td></td><td>37</td><td>前景白</td><td>47</td><td>背景白</td></tr></tbody></table><h1 id="Reference">Reference<a class="header-anchor" href="#Reference"> ❮</a></h1><p>ASCII转义符</p><ul><li>Wiki <a href="https://en.wikipedia.org/wiki/Escape_sequences_in_C" target="_blank" rel="noopener">Escape sequences in C</a></li><li><a href="https://www.bing.com/search?q=ascii+table" target="_blank" rel="noopener">Bing ASCII table</a></li><li><a href="https://linux.die.net/man/1/printf" target="_blank" rel="noopener"><code>printf</code> Linux man page</a></li></ul><p>ANSI转移符</p><ul><li><a href="http://www.termsys.demon.co.uk/vtansi.htm" target="_blank" rel="noopener"><code>ANSI/VT100 Terminal Control Escape Sequences</code></a></li><li>Wiki <a href="https://en.wikipedia.org/wiki/ANSI_escape_code" target="_blank" rel="noopener"><code>ANSI escape code</code></a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前碰到过很多终端工具可以显示非常好看的进度条，或者显示丰富的颜色，甚至还有的直接可以在终端通过字符绘制UI（a.k.a. &lt;a href=&quot;https://en.wikipedia.org/wiki/Text-based_user_interface&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TUI&lt;/a&gt;），我一直都很好奇是怎么做到的。之后又知道了curses这个Python库和它的一些高层封装（例如asciimatics），然后最终在Stack Overflow里面查到了这些都是通过特殊的终端控制符来实现的。本文就介绍这些终端控制符的使用方法，他们很适合用来写一个简单无依赖的TUI。如果需要更复杂和全面的TUI功能，还是最好使用封装好的库。&lt;/p&gt;&lt;h1 id=&quot;ASCII-控制符&quot;&gt;ASCII 控制符&lt;a class=&quot;header-anchor&quot; href=&quot;#ASCII-控制符&quot;&gt; ❮&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;在最开始接触编程的时候，如果你学的是C，那你一定很熟悉&lt;code&gt;\n&lt;/code&gt;，这就是一个”换行“的转义字符，代表终端光标令起一行。有时你还会碰到&lt;code&gt;\r&lt;/code&gt;，这是”回车“。“回车”这个名字来源于打字机时代，在使用打字机的时候，如果你需要新起一行，那么需要的操作是：转动滚筒把纸往外抽一行，再把字车（相当与打印机的打印头）移到最左端。这两个操作的名字分别是“换行”和“回车”。因此严格来说另起一行的字符串应该是&lt;code&gt;\r\n&lt;/code&gt;，这也是Windows的标准，而在Unix中则简化成&lt;code&gt;\n&lt;/code&gt;会自动执行回车。&lt;/p&gt;
    
    </summary>
    
      <category term="Coding" scheme="http://zyxin.xyz/blog/categories/Coding/"/>
    
    
      <category term="Shell" scheme="http://zyxin.xyz/blog/tags/Shell/"/>
    
  </entry>
  
  <entry>
    <title>进程、线程与协程 (C# vs Python)</title>
    <link href="http://zyxin.xyz/blog/2019-11/ParallelismInPythonAndCsharp/"/>
    <id>http://zyxin.xyz/blog/2019-11/ParallelismInPythonAndCsharp/</id>
    <published>2019-11-07T05:57:31.000Z</published>
    <updated>2021-07-12T02:40:53.188Z</updated>
    
    <content type="html"><![CDATA[<p>近来由于项目需要，接触了一下一直没去了解过的Python异步语法，发现和之前我熟悉的C#有很多不同。在深入Python的异步逻辑之后，由于Python在语法上保留了很多语言机制的细节（比如成员函数的<code>self</code>参数），我反而对C#的异步有了更深的了解。这里就来重新梳理一下各种并行方法的区别，以及他们在C#和Python上实现的区别。（这里只讨论单机的并行机制。）</p><p>总的来说，并行机制主要有进程(Process)、线程(Thread)和协程(Coroutine)，其并行实现的开销依次递减，但是他们对每个任务的鲁棒性也是依次递减的。进程是操作系统资源分配的最小单元，线程则是能够被CPU并行处理的最小单元，而协程则是目前实现“并行”的最简单方法。一个进程中可以有多个线程，而一个线程中可以有多个协程。他们具体在特性上有以下区别</p><table><thead><tr><th></th><th>进程</th><th>线程</th><th>协程</th></tr></thead><tbody><tr><td>独立内存堆</td><td>√</td><td>×</td><td>×</td></tr><tr><td>独立处理器（可硬件并行）</td><td>√</td><td>√</td><td>×</td></tr><tr><td>独立上下文</td><td>√</td><td>√</td><td>×</td></tr><tr><td>独立栈、寄存器状态</td><td>√</td><td>√</td><td>√</td></tr></tbody></table><a id="more"></a><h1 id="进程">进程<a class="header-anchor" href="#进程"> ❮</a></h1><p>进程是系统层面实现并行的机制了，进程管理是现代操作系统的一大核心之一。进程之间互不影响，操作系统会保证一个程序崩溃了，其他程序以及系统内核不会崩溃。操作系统还会提供其他的进程管理功能，例如<a href="https://en.wikipedia.org/wiki/Scheduling_(computing)" target="_blank" rel="noopener">进程调度</a>、设置进程优先级等等。不同语言底层对进程接口的实现实际上都是对系统接口的封装。</p><h2 id="一些概念">一些概念<a class="header-anchor" href="#一些概念"> ❮</a></h2><p>与进程相关的概念通常都是操作系统课程的必修知识哈哈：</p><ul><li>进程间通信(Inter-process communiation, IPC)：故名思意。常用手段有管道、共享内存、信号量(Semaphore)、消息队列等。</li><li>管道(Pipe)：管道大概是进程间通信的最常用方式？分命名管道和匿名管道, 进程双方均可往其中读写数据。</li><li>远程过程调用(Remote procedure call): 远程过程调用通过特定的消息序列化手段，可以实现进程间通信，其使用形式是把一个“远程”的函数在本地进行执行。</li><li>进程锁：如果为了避免多个进程访问同一个资源的冲突的话，就会用到进程锁，其实现方法有<a href="https://blog.csdn.net/luansxx/article/details/7736618" target="_blank" rel="noopener">管道、信号量</a>、以及文件锁等。</li><li>文件锁：文件锁是实现进程互斥的一种常用手段，只需要建立空文件句柄并锁上就可了~并且文件锁还能做到权限控制，非常方便~</li></ul><h2 id="C">C#<a class="header-anchor" href="#C"> ❮</a></h2><p>C#中对进程控制的模块主要通过<a href="https://docs.microsoft.com/dotnet/api/system.diagnostics.process" target="_blank" rel="noopener"><code>System.Diagnostics.Process</code></a>实现，可以实现建立进程、管理进程等，还可以指定具体的内存映射参数，如虚拟内存的页大小。而对管道的支持则是在<code>Process</code>类中有一部分，以及在<a href="https://docs.microsoft.com/en-us/dotnet/api/system.io.pipes" target="_blank" rel="noopener"><code>System.IO.Pipe</code></a>里面有更全面的接口。我觉得这样的命名空间分类是挺合理的，<code>Process</code>类的API其实只能用来进行程序调用和系统诊断，而<code>Pipe</code>则由于它和<code>Stream</code>的概念比较符合，因此归在IO空间下是合适的。</p><h2 id="Python">Python<a class="header-anchor" href="#Python"> ❮</a></h2><p>Python中对进程的控制以及通信方法的实现都在<code>multiprocess</code>包里，它的一些具体使用方法可以参考<a href="/blog/2017-12/PythonCall/" title="另一篇之前的博文">另一篇之前的博文</a>。值得一提的是，Python中还针对Unix系统提供了<code>fcntl</code>, <code>posix</code>等库专门用来调用系统底层API，这些API有部分是和进程有关的。相关内容还是查阅对应的资料会比较清楚~</p><h1 id="线程">线程<a class="header-anchor" href="#线程"> ❮</a></h1><p>线程是进程中细化的并行机制，线程的实现也需要用到操作系统的接口，不过线程的创建的管理基本都是在进程内部完成的。由于线程之间不独立内存空间，因此在C++这种能够随意操作内存的语言中，一个线程崩了，这个进程也大概率就崩了。但是在C#和Python中，由于有比较完善的Exception机制，并且没有什么机会直接操作内存，一般线程崩了主进程还是能接着跑的。多线程想必应该是大家用的最多的并行方法了~</p><h2 id="一些概念-2">一些概念<a class="header-anchor" href="#一些概念-2"> ❮</a></h2><p>在线程里面又有一些新的概念</p><ul><li>线程池(Thread pool)：线程池与内存池相似，都是为了避免频繁新建和销毁线程(or 内存)而造成额外的开销</li><li>线程锁：线程锁与进程锁相似，是为了避免线程间访问同样的资源而产生冲突（例如<a href="https://stackoverflow.com/questions/34510/what-is-a-race-condition" target="_blank" rel="noopener">race condition</a>）。线程间产生访问冲突非常常见，因此程序员掌握线程锁的使用是非常必要的。线程锁在C++中的<code>&lt;mutex&gt;</code>有非常全面的实现。这里面锁的类型具有代表性，分为条件锁、自旋锁等等，具体区别可以参考<a href="https://blog.csdn.net/bian_qing_quan11/article/details/73734157" target="_blank" rel="noopener">这篇博客</a>。C++的多线程非常令人头大…这里就不展开了。</li><li>事件(Event)：在多线程体系中，事件是一种常用于线程同步的机制，如果线程需要在运行过程中等待其他线程的运行，就可以使用事件机制。</li></ul><h2 id="C-2">C#<a class="header-anchor" href="#C-2"> ❮</a></h2><p>C#中与线程相关的模块在<a href="https://docs.microsoft.com/dotnet/api/system.threading" target="_blank" rel="noopener"><code>System.Threading</code></a>空间下。<code>System.Threading.Thread</code>提供了线程实现的类，使用delegate即可创建线程对象。这个空间底下也提供了<code>SpinLock</code>、<code>Semaphore</code>、<code>Mutex</code>等线程锁，以及<code>AutoResetEvent</code>实现了事件机制。<code>System.Threading.ThreadPool</code>则提供了线程池的实现。另外需要指出的是C#提供了<code>lock</code>关键字，只需对冲突的对象使用<code>lock</code>锁上，那么在其对应的上下文中就能够避免冲突。</p><h2 id="Python-2">Python<a class="header-anchor" href="#Python-2"> ❮</a></h2><p>Python中与线程相关的对象在<a href="https://docs.python.org/library/threading.html" target="_blank" rel="noopener"><code>threading</code></a>模块中，其中<code>Thread</code>类提供了线程实现，<code>Lock</code>, <code>Semaphore</code>提供了线程锁，<code>Event</code>实现了事件机制。Python中可以使用<code>with lock:</code>这样的块实现与C#<code>lock</code>相似的语法，但是这个地方的lock仍然需要自己声明，不如C#和Java中的<code>lock</code>用着方便。</p><p>总体而言C#和Python对多线程机制的支持都比较全面，然而CPython有一个臭名昭著的<a href="http://cenalulu.github.io/python/gil-in-python/" target="_blank" rel="noopener">全局锁GIL</a>，使得其多线程效率大幅下降。因此在很多Python库中，大家宁愿使用<code>multiprocess</code>多进程来进行并行（即便需要处理进程间通信的问题），也不愿使用<code>threading</code>来完成并行任务。这一点上不得不说Python辣鸡！</p><h1 id="协程">协程<a class="header-anchor" href="#协程"> ❮</a></h1><p>协程应该是21世纪才用的比较多的技术了，并且这个概念应该是在Go里面提的最多。在前文我提到协程是并行时打了引号，这是因为协程本质上还是同一个时刻只能干一件事，没法利用硬件并行，因此我们形容协程都是用“异步”(Asychronized)而不是“并行”(Parallel)。异步是与同步相对的，只要程序能一会干点这个，一会干点那个，不按顺序来，那就可以称作异步了。协程的广泛应用是由于近些年大型服务器的负载越来越大，并发需求越来越高（<s>同时剁手的人越来越多</s>），多任务切换的开销越来越不可忽视，因此协程这个开销最小的方法就被广泛应用了。协程实际上不是一个比线程更小的概念，而是另一类概念（并行/串行 vs 异步/同步)。协程的特点是一个任务能够跑到一半就暂停，然后把状态存起来，等到需要的东西备齐了以后再把状态复原接着跑；至于暂停之前和之后是不是在同一个线程上跑、有没有跟别的任务一块跑并不重要。因此实际上协程是回调(Callback)机制的一个封装升级。</p><h2 id="一些概念-3">一些概念<a class="header-anchor" href="#一些概念-3"> ❮</a></h2><ul><li>事件循环(Event loop)：事件循环是一种非常简单的实现异步的机制，简而言之就是维护一个队列，然后把队列里的任务挨个执行，而任务随时随地可以被添加进队列。</li><li>异步执行/等待(async/await)：这两个关键词在多个语言中都有出现。async用来修饰函数，说明这个函数可以异步执行；await用来等待异步函数的结束，如果没有结束就把当前任务搁着。</li></ul><h2 id="C-3">C#<a class="header-anchor" href="#C-3"> ❮</a></h2><p>C#中没有协程的概念，C#在5.0版本中引入的<code>async</code>/<code>await</code>关键字提供了异步执行的接口。据我所知C#应该是最早一批引入这个概念的语言了，并且C#里面async和await的使用非常顺滑~。C#的async/await调度与Go一样，都是通过线程池实现，因此性能也非常不错。C#中与async/await有关的接口在<a href="https://docs.microsoft.com/en-us/dotnet/api/system.threading.tasks%60" target="_blank" rel="noopener"><code>System.Threading.Tasks</code></a>下，里面的<code>Task</code>类型是对能够await的对象的封装。</p><p>C#中也有用到Event loop来实现异步的地方，一般是在UI相关的函数中，例如整个C#里面的<code>event</code>机制都是通过事件循环来实现的。使用事件循环来完成与UI相关的异步应该是非常标准的做法了，例如Qt里面也有<code>QEventLoop</code>来实现UI的异步回调。与Event loop相关的是Dispatcher机制，Dispatcher可以将指定任务加进事件循环中执行，例如在WPF中可以用Window的Dispatcher在其他线程中将任务加进UI主线程。</p><p>另外需要指出的是C#还可以通过<code>yield</code>关键词实现异步，<code>yield return</code>可能是C#最早的异步机制了，不过功能有限，只能与<code>IEnumerable</code>合作使用。C#中有一些协程的库（如Unity里的）就是使用<code>yield</code>机制来实现的。具体怎么使用<code>yield</code>还请去学习C#的语法~</p><h2 id="Python-3">Python<a class="header-anchor" href="#Python-3"> ❮</a></h2><p>Python对异步的支持就来的比较晚了，直到<a href="https://www.python.org/dev/peps/pep-0492/" target="_blank" rel="noopener">PEP 492</a>才正式加入了对<code>async</code>关键字的支持，放在了<code>asyncio</code>模块中。Python对这对关键词的实现又很辣鸡了，<a href="https://robertoprevato.github.io/Comparisons-of-async-await/" target="_blank" rel="noopener">采用的是Event loop机制来实现</a>（可能是因为多线程性能太差了吧= =）。最让人蛋疼是为了执行异步函数你还需要自己开event loop，如果你之前开过一个了，那你还需要把之前那个loop找回来，然后dispatch进去，这是何其难受！。。</p><p>Python中只要对象有<code>__await__</code>、<code>__aiter__</code>或者<code>__aenter__</code>就可以分别支持<code>await</code>、<code>async for</code>和<code>async with</code>的代码块。Python还设计了三个相关概念：Coroutine代表异步对象、Task代表异步执行计划、Future代表异步执行结果。。何必呢？？？像C#用一个Task代表全部不行吗？再配合event loop的接口，就产生了<code>create_task</code>、<code>run_coroutine_threadsafe</code>、<code>run_until_complete</code>、<code>run_in_executor</code>等我总是搞不清区别的函数。。。我爱C#！</p><hr>以上是我对C#和Python中异步机制的总结，我对各语言底层的了解并不深，如有错漏还请指点~]]></content>
    
    <summary type="html">
    
      &lt;p&gt;近来由于项目需要，接触了一下一直没去了解过的Python异步语法，发现和之前我熟悉的C#有很多不同。在深入Python的异步逻辑之后，由于Python在语法上保留了很多语言机制的细节（比如成员函数的&lt;code&gt;self&lt;/code&gt;参数），我反而对C#的异步有了更深的了解。这里就来重新梳理一下各种并行方法的区别，以及他们在C#和Python上实现的区别。（这里只讨论单机的并行机制。）&lt;/p&gt;&lt;p&gt;总的来说，并行机制主要有进程(Process)、线程(Thread)和协程(Coroutine)，其并行实现的开销依次递减，但是他们对每个任务的鲁棒性也是依次递减的。进程是操作系统资源分配的最小单元，线程则是能够被CPU并行处理的最小单元，而协程则是目前实现“并行”的最简单方法。一个进程中可以有多个线程，而一个线程中可以有多个协程。他们具体在特性上有以下区别&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;/th&gt;&lt;th&gt;进程&lt;/th&gt;&lt;th&gt;线程&lt;/th&gt;&lt;th&gt;协程&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;独立内存堆&lt;/td&gt;&lt;td&gt;√&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;独立处理器（可硬件并行）&lt;/td&gt;&lt;td&gt;√&lt;/td&gt;&lt;td&gt;√&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;独立上下文&lt;/td&gt;&lt;td&gt;√&lt;/td&gt;&lt;td&gt;√&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;独立栈、寄存器状态&lt;/td&gt;&lt;td&gt;√&lt;/td&gt;&lt;td&gt;√&lt;/td&gt;&lt;td&gt;√&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="Coding" scheme="http://zyxin.xyz/blog/categories/Coding/"/>
    
      <category term="Language" scheme="http://zyxin.xyz/blog/categories/Coding/Language/"/>
    
    
      <category term="Python" scheme="http://zyxin.xyz/blog/tags/Python/"/>
    
      <category term="C#" scheme="http://zyxin.xyz/blog/tags/C/"/>
    
      <category term="Parallelism" scheme="http://zyxin.xyz/blog/tags/Parallelism/"/>
    
  </entry>
  
</feed>
