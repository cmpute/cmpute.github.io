<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jacob Zhong</title>
  
  <subtitle>Blog</subtitle>
  <link href="/blog/atom.xml" rel="self"/>
  
  <link href="http://zyxin.xyz/blog/"/>
  <updated>2020-12-17T18:51:48.362Z</updated>
  <id>http://zyxin.xyz/blog/</id>
  
  <author>
    <name>Jacob Zhong</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>“音频发烧友” / “Audiophile” 入门及杂谈</title>
    <link href="http://zyxin.xyz/blog/2020-12/AudiophileIntroduction/"/>
    <id>http://zyxin.xyz/blog/2020-12/AudiophileIntroduction/</id>
    <published>2020-12-17T03:23:00.000Z</published>
    <updated>2020-12-17T18:51:48.362Z</updated>
    
    <content type="html"><![CDATA[<p>最近买了一些新的耳机，但是买完总感觉自己被收智商税了，于是就查查查了好多资料。这篇文章介绍我理解下音频发烧友的一些词汇是什么意思，如果你不烧耳机音响，但是想了解这个群体的，这个文章也能作为一个入门参考～Hifi领域有很多词汇我也没懂的，我也写在文章里了，如果有老烧路过请指教一二。我尝试用我学过的知识来客观解释音频领域的知识，我没上过信号处理，相关课程只上过自动控制和离散控制。</p><h1 id="发烧到底追求的是什么">发烧到底追求的是什么<a class="header-anchor" href="#发烧到底追求的是什么"> ❮</a></h1><p>刚好今天有看到<a href="https://www.youtube.com/watch?v=rM8sxFxmOUw" target="_blank" rel="noopener">一个Youtube视频</a>讲到，为什么音乐人并不care那些高端的音频设备。视频内容本身的观点是：一方面音乐人更关注的是音乐本身能不能打动人，另一方面是音乐人很多也没有那么多闲钱哈哈哈。以及底下的评论有很多人说自己是pro musician，然后疯狂喷audiophile追求的东西是虚无飘渺的。我承认烧音频领域有很多玄学都是脑放（脑补出来的），但是做耳机解码之类的厂家也是有很多pro audo engineer，不能否认这里面也是有很多技术门道的。根据这一帮自称pro musician的发言，我估计他们也没有多牛，他们的观点也有幸存者偏差在里面，并且本身不同级别不同类型的音乐也有不同的需求，因此这些评论也就看看就好。不过有一点我是同意的，听音乐最重要的还是音乐本身，对音质的追求不应放于对音乐本身的追求之上。</p><p>再打个比方，对音质的追求和对画质的追求其实是相似的，好的（照片）画质能让我们看清楚世界更多的细节，好的音质能让我们更真切地感受到被乐器包围的感觉。音频处理和视频处理也有很多相似的地方，因为他们都经常被看作信号来处理，后文我也会经常拿画质来打比方。</p><a id="more"></a><h2 id="什么是好的音质">什么是好的音质<a class="header-anchor" href="#什么是好的音质"> ❮</a></h2><p>在找到了自己喜欢的音乐之后，我们当然会希望手里的设备能更好的还原音乐本身，能听到每一点细节。因此对音质好的标准在我看来（我相信也是大多数人的观点），指的是<strong>耳朵听到的声音感觉和你站在录音的地方听到的声音感觉非常相似</strong>。由此可见在现场听，在乐器和人声面前听才能获得<strong>完美的音质</strong>，这是音质的金标准。当然这个也不是那么统一的，例如我并不觉得在歌手演唱会听到的音乐会比手机放出来的好听，因为演唱会非常嘈杂并且音响素质也不见得很好；但如果是在音乐厅听交响乐，我可以拍胸脯保证听到的声音远超电子设备播放出来的。又比如我还听很多电子音乐，里面很多音色都是直接合成出来的，那就无法通过这样的标准来定义了，这种情况下最好的音质可以定义成<strong>你听到的声音和音乐人以及调音师想让你听到的声音一致</strong>。</p><p>上面提到的音质是可以客观定义和测量的，但是另一部分人追求的音质则是他听到的声音符不符合他的口味，例如有些人喜欢温润的女声，有些人喜欢低沉的bass，这些其实都是主观的喜好。这才是发烧友的精髓——定制，就像搞机械键盘什么的，定制和折腾才是发烧友的精髓。不过我对这样的音质并没有什么追求，因为他们通常都可以通过简单的Eq（调整Equilizer）来解决。</p><h2 id="听到的声音的几个指标">听到的声音的几个指标<a class="header-anchor" href="#听到的声音的几个指标"> ❮</a></h2><p>通常对于音频发烧友来说，音质好不好是个比较笼统的词汇，因为音质好大抵是相似的，而音质差则各有各的差法。为了区分这些方面，audiophile们利用以及发明了很多与音质有关的词汇，我把比较常见的以及他们的意思列在下面了</p><table><thead><tr><th>声音基本概念</th><th>(声音的本质是声波)</th></tr></thead><tbody><tr><td>Loudness / 响度</td><td>声波的振幅，通常会取势能来计算平均振幅</td></tr><tr><td>Tone / 音调</td><td>（简单）声波的频率，真实的声音通常会是多个频率的叠加</td></tr><tr><td>Timbre / 音色</td><td>声波的形状，人通过音色区分声音的来源</td></tr></tbody></table><table><thead><tr><th>音频信号</th><th>(如何表征一个音频信号)</th></tr></thead><tbody><tr><td>Spectrogram / 频谱</td><td>频谱描述信号在各个频率上的幅度，一般通过Fourier变换计算，由于Fourier变换是可逆的，因此频谱可以唯一地对应一段声音</td></tr><tr><td>Frequency (Response) / 频率(响应)</td><td>频率响应描述输入信号和输出信号在频域上的差异</td></tr><tr><td>Phase / 相位</td><td>相位本身指周期信号中信号在周期的哪个位置，但是相位本身很少用，用的更多的是相位差。我们常用的是将相位差推广到非周期信号，然后用来描述多个声道之间的信号时间差</td></tr></tbody></table><table><thead><tr><th>可以量化的词汇</th><th>（客观描述音质）</th></tr></thead><tbody><tr><td>Bass / 低频</td><td>20Hz-20kHz的低频部分</td></tr><tr><td>Mid / 中频</td><td>20Hz-20kHz的中间部分</td></tr><tr><td>Treble / 高频</td><td>20-20kHz的高频部分</td></tr><tr><td>Imaging / 声像</td><td>声音的定位准不准，与信号相位有关。<a href="https://www.rtings.com/headphones/tests/sound-quality/imaging" target="_blank" rel="noopener">可参考Rtings的测量方法</a></td></tr><tr><td>Sound Stage / 声场</td><td>感受到的空间大小，听起来音源越分散，声场越大。这个主要是针对耳机还原音箱声场的感觉，具体解释参考<a href="https://site.douban.com/widget/notes/275603/note/118007253/" target="_blank" rel="noopener">豆瓣这篇文章</a>，测量方法参考<a href="https://www.rtings.com/headphones/tests/sound-quality/passive-soundstage" target="_blank" rel="noopener">Rtings的测试流程</a>。</td></tr><tr><td>Dynamic Range / 动态范围</td><td>在同一段声音里同时表现幅度很大和很小的信号的能力，可以参考图像的HDR技术。</td></tr><tr><td>Transient / 瞬态</td><td>这个词我是抱有疑问的，虽然控制器确实有响应时间这个参数，但是用在声音信号上感觉并不算很合适。好像一般通过追踪方波输入来看耳机的瞬态响应。</td></tr><tr><td>Signal to Noise Ratio (SNR) / 信噪比</td><td>字面意思，信号对噪声的比。这个噪音通常是音频电路的底噪。</td></tr><tr><td>Total Harmonic Distortion (THD) / 总谐波失真</td><td>输入一个纯净正弦信号，输出里这个信号的谐波就是谐波失真。</td></tr><tr><td>Intermodulation Distortion (IMD) / 互调失真</td><td>输出两个频率的信号，测输出信号的失真</td></tr><tr><td>Crosstalk / 串扰</td><td>多通道之间的信号干扰</td></tr></tbody></table><table><thead><tr><th>玄学词汇</th><th>（主观描述音质）</th></tr></thead><tbody><tr><td>Fidelity / Resolution / 解析力</td><td>这个词可能指的是低失真？有时候感觉也指超高频的频率响应。被各种厂家的广告用烂了，没有统一的解释</td></tr><tr><td>Punchy / 力度</td><td>通常指的是低频非常重</td></tr><tr><td>Congested / 拥挤 / Shouty</td><td>大概指的是声场小，或者是中高频gain太高</td></tr><tr><td>Sharp / 锐</td><td>一般是在频谱的某一小段中高频上有刺突</td></tr><tr><td>Clean / Clarity / 通透 / 纯净</td><td>应该都指的是中高频比较突出</td></tr><tr><td>Sound quality / 音质</td><td>虽然这里我们客观地讨论了什么是好音质，但是在audiophile社区里面这个词并不都是这么定义的</td></tr><tr><td>Tonality / 调性</td><td>这个词我着实没弄懂，本身是用来形容乐曲的谱调的，但是用来形容音质我也摸不找头脑</td></tr><tr><td>Layered / 层次感</td><td>这虽然我知道是什么意思，以及能听出来区别，但是觉得这个词很模糊。我猜测它与声像和声场都有关。</td></tr></tbody></table><p>其中低中高频的区别这里贴一张<a href="https://crinacle.com/2020/04/08/graphs-101-how-to-read-headphone-measurements/" target="_blank" rel="noopener">引自crinacle的图</a></p><img src="/blog/2020-12/AudiophileIntroduction/fr-chart.png" title="频率对应图"><h2 id="人能听出多大差别">人能听出多大差别<a class="header-anchor" href="#人能听出多大差别"> ❮</a></h2><p>在定义了什么是好音质以后，还有个问题是人能听出来多大的音质差别？首先一个基本常识是人的听力范围是在20Hz-20kHz之间（也有说16Hz-20kHz的），这是目前通用的标准，包括音频的采样率定在44100Hz也是参考了这个数据。另外在<a href="https://www.zhihu.com/question/274582289/answer/640857360" target="_blank" rel="noopener">这个知乎回答</a>里面，答主引用了这些数据：</p><blockquote><p>人耳所能感知到的纯单音变化最小幅度为0.3dB<br>人耳在最敏感的500Hz~2kHz段所能感知到的频率变化一般是0.2%<br>人耳所能感知的低次谐波失真变化最小量一般在1%上下</p></blockquote><p>以上这些数据可以作为参考，但是它们并不能作为硬性指标，例如说音频信号超过20kHz的部分就是完全没有意义的，我认为是不科学的。一是因为以上都是统计数据，不能否认现实世界有“金耳朵”的存在（不过至少我不是），二是以上数据来源于科学实验，其实验过程与我们听音乐的时候可能并不相同，有可能会导致音频敏感度的差别。不过至少这些数据让我们有一个大概的概念，如果一个音频设备带来的提升远小于这些数，那么极大概率你是听不出他们带来的区别的。</p><h1 id="发烧友字典">发烧友字典<a class="header-anchor" href="#发烧友字典"> ❮</a></h1><p>Hifi界另外一些让人摸不着头脑的地方就是，各种各样的词汇，以及这些词汇似乎指向的东西有时候也很不明确。。。这里把我自己学到的记一下。</p><h2 id="音乐播放器系统组成">音乐播放器系统组成<a class="header-anchor" href="#音乐播放器系统组成"> ❮</a></h2><p>我们这里不考虑录音室的系统组成，而只考虑用户的系统组成。</p><h3 id="音源">音源<a class="header-anchor" href="#音源"> ❮</a></h3><p>好像音源又叫<strong>前端</strong>？整那么邪乎干啥。。。音源有CD机、电脑、唱片机（俗称转盘turntable？），因为现在都是数字音乐了，因此重要的就是源文件的音质。音频格式有很多说法，首先分PCM和DSD两种，PCM是时域采样而DSD是频域采样。PCM又有很多指标，例如位深指音频每个采样的精度（通常是16bit，HiRes则有24bit以上），采样率指采样的频率，根据Nyquist-Shannon采样定律，频率高于20kHz理论上就能做到无损采样。然后音乐文件的格式又分有损和无损，有损格式如果比特率足够高还行，如果很低那就会非常严重地影响音质。然后音源会输出到数字界面。</p><h3 id="数字界面">数字界面<a class="header-anchor" href="#数字界面"> ❮</a></h3><p>虽然机油送我了一个数字界面，但是我并不能听出区别，以及我到现在也不是很清楚这个界面是干什么的。根据<a href="https://www.zhihu.com/question/30806888/answer/50247612" target="_blank" rel="noopener">我知乎看到的资料</a>，数字界面是把USB信号转换成DAC芯片能够直接读取的信号。外置数字界面的好处一个是时钟（可能）比内置更加精确，另一个是给没有USB或者火线（IEEE-1394协议）接口的DAC提供输入。这里就涉及另一个玄学的概念，叫<strong>时钟抖动（Jitter）</strong>。由于数字信号的采样（指PCM）是恒定频率的，因此如果数字线路的时钟频率不稳定，是会非常影响DAC转换结果的。抖动可以来源于时钟本身（如晶振），也可能来源于数字信号传输的接口芯片。不过就像知乎另一个回答说的，一般这种抖动都非常非常细微，我并不认为这对信号能有太多影响，并且我确实也听不出来。想感受一下多玄学的可以再看看<a href="http://www.erji.net/forum.php?mod=viewthread&amp;tid=7494&amp;extra=pageD1&amp;page=" target="_blank" rel="noopener">耳机大家庭的文章</a>。。。</p><h3 id="解码器（Digital-Analog-Converter，DAC）">解码器（Digital-Analog Converter，DAC）<a class="header-anchor" href="#解码器（Digital-Analog-Converter，DAC）"> ❮</a></h3><p>DAC就是数模转换器，用来将数字信号转成模拟信号。这个过程我觉得挺重要的，因为数模转换（模数转换）带来的信号损失还是很明显的。从控制理论里的零阶保持（ZOH）来理解的话，<a href="https://www.dummies.com/education/science/science-engineering/real-world-signals-and-systems-case-solving-the-dac-zoh-droop-problem-in-the-z-domain/" target="_blank" rel="noopener">转换过程会影响信号的相位和高频</a>。DAC的质量在整个音频管道中还是比较重要的。另外一个特性是DAC支持的格式，现在主流的hifi解码都支持高位深和DSD的音频了。</p><h3 id="放大器（Amplifier，Amp）">放大器（Amplifier，Amp）<a class="header-anchor" href="#放大器（Amplifier，Amp）"> ❮</a></h3><p>用于音箱的一般称功放（功率放大器），用于耳机的一般称耳放。放大器的作用就是把解码出来的模拟信号放大到合适的音量。很多设备如手机，甚至一些DAC都把功放集成进去了。独立的放大器设备有两个好处，一个的更好的电磁隔离，更少的底噪，另一个是可以提供更大的功率储备，用来推特别难推的耳机（如低阻低灵敏度的耳机），在极端状态下可以减少失真。</p><p>耳放还分两种：电子管耳放（胆机），晶体管耳放（石机）。我没听过胆机，但都说胆机声音温润，估计说到底就是胆机削低了高频。因此如果纯音质角度看，选一个低失真的耳放就可以了。</p><p>功放有时分前级后级，据我查到的资料说，前级是low-pass filter，用来处理低频，然后后级整体放大？这里我也不懂了，搜到各种不一样的说法，我觉得我还是别管这玩意了。（<a href="https://www.zhihu.com/question/30806888" target="_blank" rel="noopener">知乎参考在此</a>）</p><h3 id="接口">接口<a class="header-anchor" href="#接口"> ❮</a></h3><p>再讲一下不同音频设备之间的连接接口，数字的接口一般就是USB和S/PDIF了，模拟信号主要有TRS，TRRS，XLR等等，可以<a href="http://sound.zol.com.cn/512/5124960_all.html" target="_blank" rel="noopener">参考这篇文章</a>。这些接口本身没什么差别，虽然说有人对这个很在意，甚至还有人对墙上插座的接口很讲究，但是我觉得这都是玄学= =（就是不科学）</p><h3 id="回放设备">回放设备<a class="header-anchor" href="#回放设备"> ❮</a></h3><p>就是音箱或者耳机，这玩意也是有各种产品。音箱分有源音箱和无源音箱，有源就是内置了放大器的。耳机则分入耳（In-ear, IEM），和头戴式耳机（On-ear/Over-ear）。具体这就不展开了。</p><h2 id="耳机相关">耳机相关<a class="header-anchor" href="#耳机相关"> ❮</a></h2><h3 id="单元">单元<a class="header-anchor" href="#单元"> ❮</a></h3><ul><li>Balanced Armerture / BA / 动铁：平衡铁通过磁场变化，带动振膜运动。</li><li>Dynamic Driver / DD / 动圈：磁场直接驱动线圈，带动振膜运动。</li><li>Planar Magnetic / 平板：磁场直接驱动金属板运动。</li><li>Piezoelectric Ceramic / 压电陶瓷单元：压电晶体带动振膜变形发声。</li><li>Electrostatic / EST / 静电：电场带动振膜运动。<br>(可以参见<a href="https://www.youtube.com/watch?v=BKhS7X8rs74" target="_blank" rel="noopener">Linus的视频</a>)</li></ul><h3 id="线材">线材<a class="header-anchor" href="#线材"> ❮</a></h3><p>首先要说明的是，音频线材对声音的影响微乎其微。线材影响声音的原理是不同线材有不同的阻抗、容抗和感抗曲线（主要是阻抗和荣抗），因此可能会微微影响低阻耳机的频率响应。另外，说线材能提升音质的几乎就是扯淡。参见<a href="https://www.zhihu.com/question/274582289" target="_blank" rel="noopener">该知乎回答</a>。</p><ul><li>TPC: 电解铜</li><li>OFC: 无氧铜</li><li>OCC: 单结晶无氧铜</li><li>5N/6N/7N: （铜）纯度，几个N就有几个9。</li><li>Litz/Litz2: 绕线方式，参考<a href="https://www.newenglandwire.com/product/litz-wire-types-and-constructions/" target="_blank" rel="noopener">Litz官网</a>。并不知道不同绕线方法对感抗有没有什么影响。。<br>现在的线材基本都是无氧铜。个人认为为了好看和功能换线可以，为了换口味换线可以，为了增加屏蔽层减少外部信号噪音可以，但是为了“提升音质”就纯粹是智商税了。另外上面提到的都是传递模拟信号的线材，对那种audio-grade的USB线我是打死都不信有什么区别的，数字信号对这么点阻抗的变化根本不可能有什么反应。</li></ul><h3 id="耳塞-耳垫">耳塞/耳垫<a class="header-anchor" href="#耳塞-耳垫"> ❮</a></h3><p>耳塞(Tip)和耳垫(Pad)可以影响声音在进入耳朵之前的回响，因此也是会改变声音的。耳垫的影响比较大，耳塞我觉得比较小。不过同样的，我认为不同的耳塞耳垫都是相当于给耳机加了EQ，因此不必追求高音质的耳垫。有一点例外，如果耳塞耳垫有漏音的话，会严重影响音质，这种情况下就需要更换了。</p><hr><p>这大概就是我对audiophile各种知识的笔记了。在了解这么多之后，我还是觉得，选一个小巧、功能多、性能还过得去的DAC和amp，然后选个音质够用的耳机就行了，不用再换了。音质这玩意到最后音质提升的性价比实在太低了，还是找更多好听的音乐来的实在。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近买了一些新的耳机，但是买完总感觉自己被收智商税了，于是就查查查了好多资料。这篇文章介绍我理解下音频发烧友的一些词汇是什么意思，如果你不烧耳机音响，但是想了解这个群体的，这个文章也能作为一个入门参考～Hifi领域有很多词汇我也没懂的，我也写在文章里了，如果有老烧路过请指教一二。我尝试用我学过的知识来客观解释音频领域的知识，我没上过信号处理，相关课程只上过自动控制和离散控制。&lt;/p&gt;&lt;h1 id=&quot;发烧到底追求的是什么&quot;&gt;发烧到底追求的是什么&lt;a class=&quot;header-anchor&quot; href=&quot;#发烧到底追求的是什么&quot;&gt; ❮&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;刚好今天有看到&lt;a href=&quot;https://www.youtube.com/watch?v=rM8sxFxmOUw&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;一个Youtube视频&lt;/a&gt;讲到，为什么音乐人并不care那些高端的音频设备。视频内容本身的观点是：一方面音乐人更关注的是音乐本身能不能打动人，另一方面是音乐人很多也没有那么多闲钱哈哈哈。以及底下的评论有很多人说自己是pro musician，然后疯狂喷audiophile追求的东西是虚无飘渺的。我承认烧音频领域有很多玄学都是脑放（脑补出来的），但是做耳机解码之类的厂家也是有很多pro audo engineer，不能否认这里面也是有很多技术门道的。根据这一帮自称pro musician的发言，我估计他们也没有多牛，他们的观点也有幸存者偏差在里面，并且本身不同级别不同类型的音乐也有不同的需求，因此这些评论也就看看就好。不过有一点我是同意的，听音乐最重要的还是音乐本身，对音质的追求不应放于对音乐本身的追求之上。&lt;/p&gt;&lt;p&gt;再打个比方，对音质的追求和对画质的追求其实是相似的，好的（照片）画质能让我们看清楚世界更多的细节，好的音质能让我们更真切地感受到被乐器包围的感觉。音频处理和视频处理也有很多相似的地方，因为他们都经常被看作信号来处理，后文我也会经常拿画质来打比方。&lt;/p&gt;
    
    </summary>
    
      <category term="Misc" scheme="http://zyxin.xyz/blog/categories/Misc/"/>
    
    
      <category term="Audiophile" scheme="http://zyxin.xyz/blog/tags/Audiophile/"/>
    
  </entry>
  
  <entry>
    <title>Minecraft 1.12建服及侦测器BUD</title>
    <link href="http://zyxin.xyz/blog/2020-12/MCBud112/"/>
    <id>http://zyxin.xyz/blog/2020-12/MCBud112/</id>
    <published>2020-12-16T23:09:30.000Z</published>
    <updated>2020-12-18T00:22:55.059Z</updated>
    
    <content type="html"><![CDATA[<p>进来给实验室的服务器上装了个Minecraft服务器，给大家闲来无事上来种种菜，顺便体验一下新版本的特性。之前最高只玩过1.8，现在虽然更新到1.16了，但是听说很多Mod都还是只支持到1.12，所以就搭了1.12的服务器。基岩版的MC（Win10自带的那个）虽然性能很好，但是由于不购买就没法玩，所以考虑到大家肯定最开始都不想买，以及那个开服好像很麻烦，就还是搭了Java的服务器。</p><h1 id="一分钟上手Minecraft开服">一分钟上手Minecraft开服<a class="header-anchor" href="#一分钟上手Minecraft开服"> ❮</a></h1><p>以前玩Minecraft的时候都觉得开服务器好麻烦，要知道各种各样的配置方法，因此很佩服服主管理这些东西。直到有一天我搜到了这个：<a href="https://github.com/itzg/docker-minecraft-server" target="_blank" rel="noopener">docker-minecraft-server</a>，瞬间感觉一键开服不是梦了！这个repo把Minecraft的服务器版本以及Bukkit/Spigot服务器端Mod框架（可以理解成服务器上的Forge）都嵌进去了，简直不要太方便。数据也是从host的硬盘里mount进去的，因此如果你的服务器要转移或者备份也很方便。有了这个，开服只需要一行命令（假设你服务器上有docker）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 25565:25565 --name mc -e EULA=TRUE itzg/minecraft-server</span><br></pre></td></tr></table></figure><a id="more"></a><p>由于可以设置的环境变量非常多，因此我后来把配置都写到了docker-compose文件里面，这样修改设置后启动服务器就更简单了～目前我的设置如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">'3.8'</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">minecraft:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">itzg/minecraft-server</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"25565:25565"</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"/home/jacobz/Minecraft/docker-data:/data"</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="attr">MEMORY:</span> <span class="string">4G</span></span><br><span class="line">      <span class="attr">EULA:</span> <span class="string">"TRUE"</span></span><br><span class="line">      <span class="attr">VERSION:</span> <span class="number">1.12</span><span class="number">.2</span></span><br><span class="line">      <span class="attr">ENABLE_AUTOPAUSE:</span> <span class="string">"TRUE"</span></span><br><span class="line">      <span class="comment"># OVERRIDE_SERVER_PROPERTIES: "TRUE"</span></span><br><span class="line">      <span class="attr">MAX_TICK_TIME:</span> <span class="string">"-1"</span></span><br><span class="line">      <span class="attr">ONLINE_MODE:</span> <span class="string">"FALSE"</span></span><br><span class="line">      <span class="attr">TZ:</span> <span class="string">US/Eastern</span></span><br><span class="line">      <span class="attr">DIFFICULTY:</span> <span class="string">easy</span></span><br><span class="line">      <span class="attr">TYPE:</span> <span class="string">BUKKIT</span></span><br><span class="line">      <span class="attr">OPS:</span> <span class="string">cmpute</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">always</span></span><br></pre></td></tr></table></figure><h1 id="侦测器单片BUD">侦测器单片BUD<a class="header-anchor" href="#侦测器单片BUD"> ❮</a></h1><p>在服务器上玩了几天，最终还是想搭一个自动农场来解决温饱问题。再不去骗村民的情况下，最方便的食物我觉得就是南瓜饼了，它的原料（鸡蛋、糖、南瓜）都是非常好自动化的。因此我就想着顺便琢磨一下有侦测器之后自动农场有没有什么更方便的方法。甘蔗机在<a href="/blog/2017-08/MCTowerSugarcane/" title="我以前甘蔗机的博文">我以前甘蔗机的博文</a>里面有写到，侦测器搭甘蔗机的效率不如传统的BUD，因此主要可以改动的就是在南瓜机上了。感觉应该不是很难，因此我本地琢磨了一会，弄出来两种利用侦测器的单片BUD：</p><table><tr><th>上置型</th><th>下置型</th></tr><tr></tr><tr><td><link rel="stylesheet" href="/blog/css/minecraft.css" type="text/css"><p></p><div class="layered-blueprint" style="min-height:128px;width:128px"><input type="radio" id="mc_schematic_侧视图_0_活塞-沙子版本" class="layered-blueprint-radio" name="侧视图_0" checked><label for="mc_schematic_侧视图_0_活塞-沙子版本" class="layered-blueprint-tab">活塞+沙子版本</label><div class="layered-blueprint-layer"><table class="schematic" cellspacing="0" cellpadding="0" style="margin:0;line-height:0"><tbody><tr><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-128px -192px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-512px -96px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td></tr><tr><td><div><span class="sprite schematic-sprite" style="background-position:-32px -544px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-288px -352px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -224px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td></tr><tr><td><div><span class="text">O</span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-320px -352px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td></tr><tr><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td></tr></tbody></table></div><input type="radio" id="mc_schematic_侧视图_0_粘性活塞版本" class="layered-blueprint-radio" name="侧视图_0"><label for="mc_schematic_侧视图_0_粘性活塞版本" class="layered-blueprint-tab">粘性活塞版本</label><div class="layered-blueprint-layer"><table class="schematic" cellspacing="0" cellpadding="0" style="margin:0;line-height:0"><tbody><tr><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-512px -96px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td></tr><tr><td><div><span class="sprite schematic-sprite" style="background-position:-32px -544px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-288px -448px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -224px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td></tr><tr><td><div><span class="text">O</span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-320px -352px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td></tr><tr><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td></tr></tbody></table></div></div><p></p></td><td><link rel="stylesheet" href="/blog/css/minecraft.css" type="text/css"><p></p><div class="layered-blueprint" style="min-height:128px;width:128px"><input type="radio" id="mc_schematic_侧视图_12_活塞-沙子版本" class="layered-blueprint-radio" name="侧视图_12" checked><label for="mc_schematic_侧视图_12_活塞-沙子版本" class="layered-blueprint-tab">活塞+沙子版本</label><div class="layered-blueprint-layer"><table class="schematic" cellspacing="0" cellpadding="0" style="margin:0;line-height:0"><tbody><tr><td style="width:32px;height:32px"></td><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-256px -352px"><br></span></div></td><td><div><span class="text">O</span></div></td></tr><tr><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -224px"><br></span></div></td><td style="width:32px;height:32px"></td><td><div><span class="sprite schematic-sprite" style="background-position:-96px -544px"><br></span></div></td></tr><tr><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-512px -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-128px -192px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td></tr><tr><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-288px -352px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td></tr></tbody></table></div><input type="radio" id="mc_schematic_侧视图_12_粘性活塞版本" class="layered-blueprint-radio" name="侧视图_12"><label for="mc_schematic_侧视图_12_粘性活塞版本" class="layered-blueprint-tab">粘性活塞版本</label><div class="layered-blueprint-layer"><table class="schematic" cellspacing="0" cellpadding="0" style="margin:0;line-height:0"><tbody><tr><td style="width:32px;height:32px"></td><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-256px -352px"><br></span></div></td><td><div><span class="text">O</span></div></td></tr><tr><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -224px"><br></span></div></td><td style="width:32px;height:32px"></td><td><div><span class="sprite schematic-sprite" style="background-position:-96px -544px"><br></span></div></td></tr><tr><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-512px -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-352px -736px"><br></span></div></td></tr><tr><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:-288px -448px"><br></span></div></td><td><div><span class="sprite schematic-sprite" style="background-position:0 -160px"><br></span></div></td></tr></tbody></table></div></div><p></p></td></tr><tr></tr></table><p>上图中O代表检测更新的地方，可以看见上置型的结构比下置的要稍微精简一点点，并且由于南瓜只能生成在泥土上，因此我最后使用了上置型的方法搭了自动南瓜机。对比，只需要把这个结构横着堆叠一下就行，在南瓜机上面有了侦测器确实可以大大减小粘性活塞的使用。不过由于这个结构比之前的方案宽度多了一格，因此没一层可能只能容纳两排南瓜了，因此如果要更密集的堆叠可能需要考虑改进这个结构。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;进来给实验室的服务器上装了个Minecraft服务器，给大家闲来无事上来种种菜，顺便体验一下新版本的特性。之前最高只玩过1.8，现在虽然更新到1.16了，但是听说很多Mod都还是只支持到1.12，所以就搭了1.12的服务器。基岩版的MC（Win10自带的那个）虽然性能很好，但是由于不购买就没法玩，所以考虑到大家肯定最开始都不想买，以及那个开服好像很麻烦，就还是搭了Java的服务器。&lt;/p&gt;&lt;h1 id=&quot;一分钟上手Minecraft开服&quot;&gt;一分钟上手Minecraft开服&lt;a class=&quot;header-anchor&quot; href=&quot;#一分钟上手Minecraft开服&quot;&gt; ❮&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;以前玩Minecraft的时候都觉得开服务器好麻烦，要知道各种各样的配置方法，因此很佩服服主管理这些东西。直到有一天我搜到了这个：&lt;a href=&quot;https://github.com/itzg/docker-minecraft-server&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;docker-minecraft-server&lt;/a&gt;，瞬间感觉一键开服不是梦了！这个repo把Minecraft的服务器版本以及Bukkit/Spigot服务器端Mod框架（可以理解成服务器上的Forge）都嵌进去了，简直不要太方便。数据也是从host的硬盘里mount进去的，因此如果你的服务器要转移或者备份也很方便。有了这个，开服只需要一行命令（假设你服务器上有docker）&lt;/p&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;docker run -d -p 25565:25565 --name mc -e EULA=TRUE itzg/minecraft-server&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Game" scheme="http://zyxin.xyz/blog/categories/Game/"/>
    
      <category term="Minecraft" scheme="http://zyxin.xyz/blog/categories/Game/Minecraft/"/>
    
    
      <category term="Redstone" scheme="http://zyxin.xyz/blog/tags/Redstone/"/>
    
      <category term="Automation" scheme="http://zyxin.xyz/blog/tags/Automation/"/>
    
      <category term="Docker" scheme="http://zyxin.xyz/blog/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Notes for Algebra Basics</title>
    <link href="http://zyxin.xyz/blog/2020-06/AlgebraBasicsNotes/"/>
    <id>http://zyxin.xyz/blog/2020-06/AlgebraBasicsNotes/</id>
    <published>2020-06-28T01:02:13.000Z</published>
    <updated>2021-01-24T03:06:07.986Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Selected notes from <code>ROB 501</code> and <code>ME 564</code>.<br>$\{x_i\}^b_a$ denotes set $\{x_a, x_{a+1}, \ldots, x_b\}$<br>TODO: add Jordan Form</p></blockquote><h1 id="Algebraic-Structures">Algebraic Structures<a class="header-anchor" href="#Algebraic-Structures"> ❮</a></h1><h2 id="Operation">Operation<a class="header-anchor" href="#Operation"> ❮</a></h2><ul><li>Definition: an (binary, closed) <strong>operation</strong> $\ast$ on a set $S$ is a mapping of $S\times S\to S$</li><li><strong>Commutative</strong>: $x\ast y=y\ast x,\;\forall x,y\in S$</li><li><strong>Associative</strong>: $(x\ast y)\ast z=x\ast (y\ast z),\;\forall x,y,z\in S$</li></ul><h2 id="Group">Group<a class="header-anchor" href="#Group"> ❮</a></h2><ul><li>Definition: a <strong>group</strong> is a pair $(\mathcal{S},\ast)$ with following axioms<ol><li>$\ast$ is associative on $\mathcal{S}$</li><li>(Identity element) $\exists e\in \mathcal{S}\text{ s.t. }x\ast e=e\ast x=x,\;\forall x\in \mathcal{S}$</li><li>(Inverse element) $\forall x\in \mathcal{S}, \exists x’ \in \mathcal{S}\text{ s.t. }x\ast x’=x’\ast x=e$</li></ol></li><li><strong>Abelian</strong>: a group is called <strong>abelian group</strong> if $\ast$ is also commutative</li></ul><a id="more"></a><h2 id="Ring">Ring<a class="header-anchor" href="#Ring"> ❮</a></h2><ul><li>Definition: a <strong>ring</strong> is a triplet $(\mathcal{R},+,\ast)$ consisting of a set of <code>scalars</code> $\mathcal{R}$ and two operators + and $\ast$ with following axioms<ol><li>$(\mathcal{R},+)$ is an abelian group with identity denoted $0$</li><li>$\forall a,b,c \in \mathcal{R}\text{ s.t. }a\ast(b\ast c) = (a\ast b)\ast c$</li><li>$\exists 1\in\mathcal{R}, \forall a\in\mathcal{R}\text{ s.t. }a\cdot 1=a$</li><li>$\ast$ is distributive over $+$</li></ol></li></ul><h2 id="Field">Field<a class="header-anchor" href="#Field"> ❮</a></h2><ul><li>Definition: a <strong>field</strong> $(\mathcal{F},+,\ast)$ is a ring where $(\mathcal{F}\backslash\{0\},\ast)$ is also an abelian group.<blockquote><p>Difference from ring to field is that $\ast$ need to be commutative and have a multiplicative inverse</p></blockquote></li></ul><h2 id="Vector-Space">Vector Space<a class="header-anchor" href="#Vector-Space"> ❮</a></h2><ul><li>Definition: a <strong>vector space</strong> (aka. <strong>linear space</strong>) is a triplet $(\mathcal{U},\oplus,\cdot)$ defined over a field $(\mathcal{F},+,\ast)$ with following axioms, where set $\mathcal{U}$ is called <code>vectors</code>, operator $\oplus$ is called <code>vector addition</code> and mapping $\cdot$ is called <code>scalar multiplication</code>:<ol><li>(<strong>Null vector</strong>) $(\mathcal{U},+)$ is an abelian group with identity element $\emptyset$</li><li>Scalar multiplication is a mapping of $\mathcal{F}\times\mathcal{U}\to\mathcal{U}$</li><li>$\alpha\cdot(x\oplus y) = \alpha\cdot x \oplus \alpha\cdot y,\;\forall x,y\in\mathcal{U};\alpha\in\mathcal{F}$</li><li>$(\alpha+\beta)\cdot x = \alpha\cdot x\oplus\beta\cdot x,\;\forall x\in\mathcal{U};\alpha,\beta\in\mathcal{F}$</li><li>$(\alpha\ast\beta)\cdot x=\alpha\cdot(\beta\cdot x),\;\forall x\in\mathcal{U};\alpha,\beta\in\mathcal{F}$</li><li>$1_\mathcal{F}\cdot x=x$</li></ol><blockquote><p>Usually we don’t distinguish vector addition $\oplus$ and addition of scalar $+$. Juxtaposition is also commonly used for <em>both</em> scalar multiplication $\cdot$ and multiplication of scalars $\ast$</p></blockquote></li><li><strong>Subspace</strong>: a subspace $\mathcal{V}$ of a linear space $\mathcal{U}$ over field $\mathcal{F}$ is a subset of $\mathcal{U}$ which is itself a linear space over $\mathcal{F}$ under same vector addition and scalar multiplication.</li></ul><h3 id="Basis-Coordinate">Basis &amp; Coordinate<a class="header-anchor" href="#Basis-Coordinate"> ❮</a></h3><ul><li><strong>Linear Independence</strong>: Let $\mathcal{V}$ be a vector space over $\mathcal{F}$ and let $X=\{x_i\}^n_1\subset \mathcal{V}$<ul><li>X is <strong>linearly dependent</strong> if $\exists \alpha_1,\ldots,\alpha_n\in\mathcal{F}$ not all 0 s.t. $\sum^n_{i=1} \alpha_i x_i=0$.</li><li>X is <strong>linearly independent</strong> if $\sum^n_{i=1} \alpha_i x_i=0 \Rightarrow \alpha_1=\alpha_2=\ldots=\alpha_n=0$</li></ul></li><li><strong>Span</strong>: Given a set of vectors $V$, the set of linear combinations of vectors in $V$ is called the <strong>span</strong> of it, denoted $\mathrm{span}\{V\}$</li><li><strong>Basis</strong>: A set of linearly independent vectors in a linear space $\mathcal{V}$ is a <strong>basis</strong> if every vector in $\mathcal{V}$ can be expressed as a <em>unique linear combination</em> of these vectors. (see below “Coordinate”)<ul><li>Basis Expansion: Let $(X,\mathcal{F})$ be a vector space of dimension n. If $\{v_i\}^k_1,\;1\leqslant k&lt; n$ is linearly independent, then $\exists \{v_i\}^n_{k+1}$ such that $\{v_i\}_1^n$ is a basis.</li><li><strong>Reciprocal Basis</strong>: Given basis $\{v_i\}^n_1$, a set ${r_i}^1_n$ that satifies $\langle r_i,v_j \rangle=\delta_i(j)$ is a reciprocal basis. It can be generated by Gram-Schmidt Process and $\forall x\in\mathcal{X}, x=\sum^n_{i=1}\langle r_i,x\rangle v_i$.</li></ul></li><li><strong>Dimension</strong>: <em>Cardinality</em> of the basis is called the <strong>dimension</strong> of that vector space, which is equal to <em>the maximum number of linearly independent vectors</em> in the space. Denoted as $dim(\mathcal{V})$.<ul><li>In an $n$-dimensional vector space, any set of $n$ linearly independent vectors is a basis.</li></ul></li><li><strong>Coordinate</strong>: For a vector $x$ in vector space $\mathcal{V}$, given a basis $\{e_1, \ldots, e_n\}$ we can write $x$ as $x=\sum^n_{i=1}\beta_i e_i=E\beta$ where $E=\begin{bmatrix}e_1&amp;e_2&amp;\ldots&amp;e_n\end{bmatrix}$ and $\beta=\begin{bmatrix}\beta_1&amp;\beta_2&amp;\ldots&amp;\beta_n\end{bmatrix}^\top$. Here $\beta$ is called the <strong>representation</strong> (or <strong>coordinate</strong>) of $x$ given the basis $E$.</li></ul><h3 id="Norm-Inner-product">Norm &amp; Inner product<a class="header-anchor" href="#Norm-Inner-product"> ❮</a></h3><ul><li><strong>Inner Product</strong>: an operator on two vectors that produces a scalar result (i.e. $\langle\cdot,\cdot\rangle:\mathcal{V}\to\mathbb{R}\;or\;\mathbb{C}$) with following axioms:<ol><li>(Symmetry) $\langle x,y \rangle=\overline{\langle y,x\rangle},\;\forall x,y\in\mathcal{V}$</li><li>(Bilinearity) $\langle \alpha x+\beta y,z\rangle=\alpha\langle x,z\rangle+\beta\langle y,z\rangle,\;\forall x,y,z\in\mathcal{V};\alpha,\beta\in\mathbb{C}$</li><li>(Pos. definiteness) $\langle x,x\rangle\geqslant 0,\;\forall x\in\mathcal{V}$ and $\langle x,x\rangle=0\Rightarrow x=0_\mathcal{V}$</li></ol></li><li><strong>Inner Product Space</strong>: A linear space with a defined inner product</li><li><strong>Orthogonality</strong>:<ul><li>Perpedicularity of vectors ($x\perp y$): $\langle x,y\rangle=0$</li><li>Perpedicularity of a vector to a set ($y\perp\mathcal{S},\mathcal{S}\subset\mathcal{V}$): $y\perp x,\;\forall x\in\mathcal{S}$</li><li><strong>Orthogonal Set</strong>: set $\mathcal{S}\subset(\mathcal{U},\langle\cdot,\cdot\rangle)$ is orthogonal $\Leftrightarrow x\perp y,\;\forall x,y\in\mathcal{S},x\neq y$</li><li><strong>Orthonormal Set</strong>: set $\mathcal{S}$ is orthonormal iff $\mathcal{S}$ is orthogonal and $\Vert x\Vert=1,\;\forall x\in\mathcal{S}$</li><li>Orthogonality of sets ($\mathcal{X}\perp\mathcal{Y}$): $\langle x,y\rangle=0,\;\forall x\in\mathcal{X};y\in\mathcal{Y}$</li><li><strong>Orthogonal Complement</strong>: Let $(\mathcal{V},\langle\cdot,\cdot\rangle)$ be an inner product space and let $\mathcal{U}\subset\mathcal{V}$ be a subspace of $\mathcal{V}$, the orthogonal complement of $\mathcal{U}$ is $\mathcal{U}^\perp=\left\{v\in\mathcal{V}\middle|\langle v,u\rangle=0,\;\forall u\in\mathcal{U}\right\}$.<ul><li>$\mathcal{U}^\perp\subset\mathcal{V}$ is a subspace</li><li>$\mathcal{V}=\mathcal{U}\overset{\perp}{\oplus}\mathcal{U}^\perp$ ($\oplus$: direct sum, $\overset{\perp}{\oplus}$: orthogonal sum)</li></ul></li></ul></li><li><strong>Norm</strong>: A <strong>norm</strong> on a linear space $\mathcal{V}$ is mapping $\Vert\cdot\Vert:\;\mathcal{V}\to\mathbb{R}$ such that:<ol><li>(Positive definiteness) $\Vert x\Vert\geqslant 0\;\forall x\in \mathcal{V}$ and $\Vert x\Vert =0\Rightarrow x=0_\mathcal{V}$</li><li>(Homogeneous) $\Vert \alpha x\Vert=|\alpha|\cdot\Vert x\Vert,\;\forall x\in\mathcal{V},\alpha\in\mathbb{R}$</li><li>(Triangle inequality) $\Vert x+y\Vert\leqslant\Vert x\Vert+\Vert y\Vert$</li></ol></li><li><strong>Distance</strong>: Norm can be used to measure distance between two vectors. Meanwhile, distance from a vector to a (sub)space is defined as $d(x,\mathcal{S})=\inf_{y\in\mathcal{S}} d(x,y)=\inf_{y\in\mathcal{S}} \Vert x-y\Vert$<ul><li><strong>Projection Point</strong>: $x^* =\arg\min_{y\in\mathcal{S}}\Vert x-y\Vert$ is the projection point of $x$ on linear space $\mathcal{S}$.</li><li><strong>Projection Theorem</strong>: $\exists !x^* \in\mathcal{S}$ s.t. $\Vert x-x^* \Vert=d(x,\mathcal{S})$ and we have $(x-x^*) \perp\mathcal{S}$</li><li><strong>Orthogonal Projection</strong>: $P(x)=x^*:\mathcal{X}\to\mathcal{M}$ is called the orthogonal projection of $\mathcal{X}$ onto $\mathcal{M}$</li></ul></li><li><strong>Normed Space</strong>: A linear space with a defined norm $\Vert\cdot\Vert$, denoted $(\mathcal{V},\mathcal{F},\Vert\cdot\Vert)$<blockquote><p>A inner product space is always a normed space because we can define $\Vert x\Vert=\sqrt{\langle x,x\rangle}$</p></blockquote></li><li>Common $\mathbb{R}^n$ Norms:<ul><li>Euclidean norm (2-norm): $\Vert x\Vert_2=\left(\sum^n_{i=1}|x_i|^2\right)^{1/2}=\left\langle x,x\right\rangle^{1/2}=\left(x^\top x\right)^{1/2}$</li><li>$l_p$ norm (p-norm): $\Vert x\Vert_p=\left(\sum^n_{i=1}|x_i|^p\right)^{1/p}$</li><li>$l_1$ norm: $\Vert x\Vert_1=\sum^n_{i=1}|x_i|$</li><li>$l_\infty$ norm: $\Vert x\Vert_\infty=\max_{i}\{x_i\}$</li></ul></li><li>Common matrix norms:<blockquote><p>Matrix norms are also called <strong>operator norms</strong>, can measure how much a linear operator “magnifies” what it operates on.</p></blockquote><ul><li>A general form induced from $\mathbb{R}^n$ norm: $$\Vert A\Vert=\sup_{x\neq 0}\frac{\Vert Ax\Vert}{\Vert x\Vert}=\sup_{\Vert x\Vert=1}\Vert Ax\Vert$$</li><li>$\Vert A\Vert_1=\max_j\left(\sum^n_{i=1}|a_{ij}|\right)$</li><li>$\Vert A\Vert_2=\left[ \max_{\Vert x\Vert=1}\left\{(Ax)^* (Ax)\right\}\right]^{1/2}=\left[ \lambda_{max}(A^ *A)\right]^{1/2}$ ($\lambda_{max}$: largest eigenvalue)</li><li>$\Vert A\Vert_\infty=\max_i\left(\sum^n_{j=1}|a_{ij}|\right)$</li><li>(Frobenius Norm) $\Vert A\Vert_F=\left[ \sum^m_{i=1}\sum^n_{j=1}\left|a_{ij}\right|^2\right]^{1/2}=\left[ tr(A^*A)\right]^{1/2}$</li></ul></li><li>Useful inequations:<ul><li><strong>Cauchy-Schwarz</strong>: $|\langle x,y\rangle|\leqslant\left\langle x,x\right\rangle^{1/2}\cdot\left\langle y,y\right\rangle^{1/2}$</li><li><strong>Triangle</strong> (aka. $\Delta$): $\Vert x+y\Vert\leqslant\Vert x\Vert+\Vert y\Vert$<blockquote><p>Lemma: $\Vert x-y\Vert \geqslant \left| \Vert x\Vert-\Vert y\Vert \right|$</p></blockquote></li><li><strong>Pythagorean</strong>: $x\perp y \Leftrightarrow \Vert x+y\Vert=\Vert x\Vert+\Vert y\Vert$</li></ul></li></ul><h3 id="Gramian">Gramian<a class="header-anchor" href="#Gramian"> ❮</a></h3><ul><li><strong>Gram-Schmidt Process</strong>: A method to find orthogonal basis $\{v_i\}^n_1$ given an ordinary basis $\{y_i\}^n_1$. It’s done by perform $v_k=y_k-\sum^{k-1}_{j=1}\frac{\langle y_k,v_j\rangle}{\langle v_j,v_j \rangle}\cdot v_j$ iteratively from 1 to $n$. To get an orthonormal basis, just normalize these vectors.</li><li><a href="https://en.wikipedia.org/wiki/Gramian_matrix" target="_blank" rel="noopener"><strong>Gram Matrix</strong></a>: The Gram matrix generated from vectors $\{y_i\}_ 1^k$ is denoted $G(y_ 1,y_ 2,\ldots,y_ k)$. Its element $G_{ij}=\langle y_i,y_j\rangle$<ul><li><strong>Gram Determinant</strong>: $g(y_1,y_2,\ldots,y_n)=\det G$</li><li><strong>Normal Equations</strong>: Given subspace $\mathcal{M}$ and its basis $\{y_i\}^n_1$, the projection point of $\forall x\in\mathcal{M}$ can be represented by $$x^*=\alpha y=\begin{bmatrix}\alpha_1&amp;\alpha_2&amp;\ldots&amp;\alpha_n\end{bmatrix}\begin{bmatrix}y_1\\y_2\\ \vdots \\y_n\end{bmatrix},\;\beta=\begin{bmatrix}\langle x,y_1\rangle\\ \langle x,y_2\rangle\\ \vdots\\ \langle x,y_n\rangle\end{bmatrix} where\;G^\top\alpha=\beta$$<blockquote><p>For least-squares problem $Ax=b$, consider $\mathcal{M}$ to be the column space of $A$, then $G=A^\top A,\;\beta=A^\top b,\;G^\top\alpha=\beta\Rightarrow\alpha=(A^\top A)^{-1}A^\top b$. Similarly for weighted least-squares problem ($\Vert x\Vert=x^\top Mx$), let $G=A^\top MA, \beta=A^\top Mb$, we can get $\alpha=(A^\top MA)^{-1}A^\top Mb$</p></blockquote></li></ul></li></ul><h1 id="Linear-Algebra">Linear Algebra<a class="header-anchor" href="#Linear-Algebra"> ❮</a></h1><h2 id="Linear-Operator">Linear Operator<a class="header-anchor" href="#Linear-Operator"> ❮</a></h2><ul><li><p>Definition: a linear operator $\mathcal{A}$ (aka. linear transformation, linear mapping) is a function $f: V\to U$ that operate on a linear space $(\mathcal{V},\mathcal{F})$ to produce elements in another linear space $(\mathcal{U},\mathcal{F})$ and obey $$\mathcal{A}(\alpha_1 x_1+\alpha_2 x_2) = \alpha_1\mathcal{A}(x_1) + \alpha_2\mathcal{A}(x_2),\;\forall x_1,x_2\in V;\alpha_1, \alpha_2\in\mathcal{F}$$</p></li><li><p><strong>Range (Space)</strong>: $\mathcal{R}(\mathcal{A})=\left\{u\in U\middle|\mathcal{A}(v)=u,\;\forall v\in V\right\}$</p></li><li><p><strong>Null Space</strong> (aka. <strong>kernel</strong>): $\mathcal{N}(\mathcal{A})=\left\{v\in V\middle|\mathcal{A}(v)=\emptyset_U\right\}$</p></li><li><p><strong>$\mathcal{A}$-invariant subspace</strong>: Given vector space $(\mathcal{V},\mathcal{F})$ and linear operator $\mathcal{A}:\mathcal{V}\rightarrow \mathcal{V}$, $\mathcal{W}\subseteq\mathcal{V}$ is $A$-invariant if $\forall x\in\mathcal{W}$, $\mathcal{A}x\in\mathcal{W}$.</p><ul><li>Both $\mathcal{R}(\mathcal{A})$ and $\mathcal{N}(\mathcal{A})$ are $\mathcal{A}$-invariant</li></ul></li><li><p>Matrix Representation: Given bases for both $V$ and $U$ (respectively $\{v_i\}^n_1$ and $\{u_j\}^m_1$), matrix representation $A$ satisfies $\mathcal{A}(v_i)=\sum^m_{j=0}A_{ji}u_j$ so that $\beta=A\alpha$ where $\alpha$ and $\beta$ is the representation of a vector under $\{v_i\}$ and $\{u_j\}$ respectively.</p><img src="/blog/2020-06/AlgebraBasicsNotes/linear_map_relations.png" title="Relation between a linear map and its matrix representations"><ul><li>$P$ and $Q$ are change of basis matrices, $A=Q^{-1}\tilde{A}P,\;\tilde{A}=QAP^{-1}$</li><li>The i-th column of $A$ is the coordinates of $\mathcal{A}(v_i)$ represented by the basis $\{u_j\}$, similarly i-th column of $\tilde{A}$ is $\mathcal{A}(\tilde{v}_i)$ represented in $\{\tilde{u}_j\}$</li><li>The i-th column of $P$ is the coordinates of $v_i$ represented by the basis $\{\tilde{v}\}$, similarly i-th column of $Q$ is $u_j$ represented in $\{\tilde{u}\}$</li></ul></li><li><p>Matrix Similarity ($A\sim B$): Two (square) matrix representations ($A,B$) of the same linear operator are called <strong>similar</strong> (or <strong>conjugate</strong>) and they satisfies $\exists P$ s.t. $B=PAP^{-1}$.</p><blockquote><p>From now on we don’t distinguish between linear operator $\mathcal{A}$ and its matrix representation where choice of basis doesn’t matter.</p></blockquote></li><li><p><strong>Rank</strong>: $rank(A)=\rho(A)\equiv dim(\mathcal{R}(A))$</p><ul><li><strong>Sylvester’s Inequality</strong>: $\rho(A)+\rho(B)-n\leqslant \rho(AB)\leqslant \min\{\rho(A), \rho(B)\}$</li><li><strong>Singularity</strong>: $\rho(A)&lt; n$</li></ul></li><li><p><strong>Nullity</strong>: $null(A)=\nu(A)\equiv dim(\mathcal{N}(A))$</p><ul><li>$\rho(A)+\nu(A)=n$ ($n$ is the dimensionality of domain space)</li></ul></li><li><p><strong>Adjoint</strong>: The adjoint of the linear map $\mathcal{A}: \mathcal{V}\to\mathcal{W}$ is the linear map $\mathcal{A}^*: \mathcal{W}\to\mathcal{V}$ such that $\langle y,\mathcal{A}(x)\rangle_\mathcal{W}=\langle \mathcal{A}^ *(y),x\rangle_\mathcal{V}$</p><blockquote><p>For its matrix representation, adjoint of $A$ is $A^ *$, which is $A^\top$ for real numbers.<br><br>Properties of $\mathcal{A}^ *$ is similar to matrix $A^ *$</p></blockquote><ul><li>$\mathcal{U}=\mathcal{R}(A)\overset{\perp}{\oplus}\mathcal{N}(A^ *),\;\mathcal{V}=\mathcal{R}(A^ *)\overset{\perp}{\oplus}\mathcal{N}(A)$</li><li>$\mathcal{N}(A^* )=\mathcal{N}(AA^* )\subseteq\mathcal{U},\;\mathcal{R}(A)=\mathcal{R}(AA^*)\subseteq\mathcal{U}$</li></ul></li><li><p><strong>Self-adjoint</strong>: $\mathcal{A}$ is self-adjoint iff $\mathcal{A}^*=\mathcal{A}$.</p><ul><li>For self-adjoint $\mathcal{A}$, if $\mathcal{V}=\mathbb{C}^{n\times n}$ then $A$ is <strong>hermitian</strong>; if $\mathcal{V}=\mathbb{R}^{n\times n}$ then $A$ is <strong>symmetric</strong>.</li><li>Self-adjoint matrices have real eigenvalues and orthogonal eigenvectors</li><li><strong>Skew symmetric</strong>: $A^*=-A$<blockquote><p>For quadratic form $x^\top Ax=x^\top(\frac{A+A^\top}{2}+\frac{A-A^\top}{2})x$, since $A-A^\top$ is skew symmetric, scalar $x^\top (A-A^\top) x=-x^\top (A-A^\top)x$, so the skew-symmetric part is zero. Therefore for quadratic form $x^\top Ax$ we can always assume $A$ is symmetric.</p></blockquote></li></ul></li><li><p><strong>Definiteness</strong>: (for symmetric matrix $P$)</p><ul><li>Positive definite ($P\succ 0$): $\forall x\in\mathbb{R}^n\neq 0,\; x^\top Px&gt;0 \Leftrightarrow$ all eigenvalues of $P$ are positive.</li><li>Semi-positive definite ($P\succcurlyeq 0$): $x^\top Px\geqslant 0 \Leftrightarrow$ all eigenvalues of $P$ are non-negative.</li><li>Negative definite ($P\prec 0$): $x^\top Px &lt; 0 \Leftrightarrow$ all eigenvalues of $P$ are negative.</li></ul></li><li><p><strong>Orthogonal Matrix</strong>: $Q$ is orthogonal iff $Q^\top Q=I$, iff columns of $Q$ are orthonormal.</p><ul><li>If $A\in\mathbb{R}^{n\times b}$ is symmetric, then $\exists$ orthogonal $Q$ s.t. $Q^\top AQ=\Lambda=\mathrm{diag}\{\lambda_1,\ldots,\lambda_n\}$ (see <a href="#Eigendecomposition-and-Jordan-Form">Eigen-decomposition</a> section below)</li></ul></li><li><p><strong>Orthogonal Projection</strong>: Given linear space $\mathcal{X}$ and subspace $\mathcal{M}$, $P(x)=x^*:\mathcal{X}\to\mathcal{M}$ ($x^ *$ is the projection point) is called orthogonal projection. If $\{v_i\}$ is a orthonormal basis of $\mathcal{M}$, then $P(x)=\sum_i \langle x,v_i\rangle v_i$</p></li></ul><h2 id="Eigenvalue-and-Canonical-Forms">Eigenvalue and Canonical Forms<a class="header-anchor" href="#Eigenvalue-and-Canonical-Forms"> ❮</a></h2><ul><li><strong>Eigenvalue</strong> and <strong>Eigenvector</strong>: Given mapping $\mathcal{A}:\mathcal{V}\rightarrow\mathcal{V}$, if $\exists \lambda\in\mathcal{F}, v\neq \emptyset_{\mathcal{V}}\in\mathcal{V}$ s.t. $\mathcal{A}(v) = \lambda v$, then $\lambda$ is the <strong>eigenvalue</strong>, $v$ is the <strong>eigenvector</strong> (aka. <strong>spectrum</strong>).<ul><li>If eigenvalues are all distinct, then the associated eigenvectors form a basis.</li></ul></li><li><strong>Eigenspace</strong>: $\mathcal{N}_\lambda = \mathcal{N}(\mathcal{A}-\lambda \mathcal{I})$.<ul><li>$q=dim(\mathcal{N}_\lambda)$ is called the <strong>geometric multiplicity</strong> (几何重度)</li><li>$\mathcal{N}_\lambda$ is an $\mathcal{A}$-invariant subspace.</li></ul></li><li><strong>Characteristic Polynomial</strong>: $\phi(s)\equiv\mathcal{det}(A-s I)$ is a polynomial of degree $n$ in $s$<ul><li>Its solutions are the eigenvalues of $A$.</li><li>The multiplicity $m_i$ of root term $(s-\lambda_i)$ here is called <strong>algebraic multiplicity</strong> (代数重度) of $\lambda_i$.</li></ul></li><li><strong>Cayley-Hamilton Theorem</strong>: $\phi(A)=\mathbf{0}$<blockquote><p>Proof needs the eigendecomposition or Jordan decomposition descibed below</p></blockquote></li><li><strong>Minimal Polynomial</strong>: $\psi(s)$ is the minimal polynomial of $A$ iff $\psi(s)$ is the polynomial of least degree for which $\psi(A)=0$ and $\psi$ is monic (coefficient of highest order term is 1)<ul><li>The multiplicity $\eta_i$ of root term $(s-\lambda_i)$ here is called the <strong>index</strong> of $\lambda_i$</li></ul></li><li><strong>Eigendecomposition</strong> (aka. <strong>Spectral Decomposition</strong>) is directly derived from the definition of eigenvalues: $$A=Q\Lambda Q^{-1}, \Lambda=\mathrm{diag}\left\{\lambda_1,\lambda_2,\ldots,\lambda_n\right\}$$<br>where $Q$ is a square matrix whose $i$-th column is the eigenvector $q_i$ corresponding to eigenvalue $\lambda_i$.<ul><li>Feasibility: $A$ can be diagonalized (using eigendecomposition) iff. $q_i=m_i$ for all $\lambda_i$.</li><li>If $A$ has $n$ distinct eigenvalues, then $A$ can be diagonalized.</li></ul></li><li><strong>Generalized eigenvector</strong>: A vector $v$ is a generalized eigenvector of rank $k$ associated with eigenvalue $\lambda$ iff $v\in\mathcal{N}\left((A-\lambda I)^k\right)$ but $v\notin\mathcal{N}\left((A-\lambda I)^{k-1}\right)$<ul><li>If $v$ is a generalized eigenvector of rank $k$, $(A-\lambda I)v$ is a generalized eigenvector of rank $k-1$. This creates a chain of generalized eigenvectors (called <strong>Jordan Chain</strong>) from rank $k$ to $1$, and they are linearly independent.</li><li>$\eta$ (index, 幂零指数) of $\lambda$ is the smallest integer s.t. $dim\left(\mathcal{N}\left((A-\lambda I)^\eta\right)\right)$</li><li>The space spanned by the chain of generalized eigenvectors from rank $\eta$ is called the <strong>generalized eigenspace</strong> (with dimension $\eta$).</li><li>Different generalized eigenspaces associated with the same and with different eigenvalues are orthogonal.</li></ul></li><li><strong>Jordan Decomposition</strong>: Similar to eigendecomposition, but works for all square matrices. $A=PJP^{-1}$ where $J=\mathrm{diag}\{J_1,J_2,\ldots,J_p\}$ is the <strong>Jordan Form</strong> of A consisting of Jordan Blocks.<ul><li><strong>Jordan Block</strong>: $J_i=\begin{bmatrix} \lambda &amp; 1 &amp;&amp;&amp; \\&amp;\lambda&amp;1&amp;&amp;\\&amp;&amp;\lambda&amp;\ddots&amp;\\&amp;&amp;&amp;\ddots&amp;1\\&amp;&amp;&amp;&amp;\lambda\end{bmatrix}$</li><li>Each Jordan block corresponds to a generalized eigenspace</li><li>$q_i$ = the count of Jordan blocks associated with $\lambda_i$</li><li>$m_i$ = the count of $\lambda_i$ on diagonal of $J$</li><li>$\eta_i$ = the dimension of the largest Jordan block associated with $\lambda_i$</li></ul></li></ul><blockquote><p>$\Lambda$ in eigendecomposition, $J$ in Jordan Form and $\Sigma$ in SVD (see below) are three kinds of <strong><a href="https://en.wikipedia.org/wiki/Canonical_form#Linear_algebra" target="_blank" rel="noopener">Canonical Forms</a></strong> of a matrix $A$</p></blockquote><ul><li><strong>Function of matrics</strong>: Let $f(\cdot)$ be an analytic function and $\lambda_i$ be an eigenvalue of $A$. If $p(\cdot)$ is a polynomial that satisfies $p(\lambda_i)=f(\lambda_i)$ and $\frac{\mathrm{d}^k}{\mathrm{d}s^k} p(\lambda_i)=\frac{\mathrm{d}^k}{\mathrm{d}s^k} f(\lambda_i)$ for $k=1,\ldots,\eta_i-1$, then $f(A)\equiv p(A)$.<blockquote><ul><li>This extends the functions applicable to matrics from polynomials (trivial) to any analytical functions</li><li>By Cayley-Hamilton, we can always choose $p$ to be order $n-1$</li></ul></blockquote></li><li><strong>Sylvester’s Formula</strong>: $f(A)=\sum^k_{i=1}f(\lambda_i)A_i$ ($f$ being analytic)</li></ul><h2 id="SVD-and-Linear-Equations">SVD and Linear Equations<a class="header-anchor" href="#SVD-and-Linear-Equations"> ❮</a></h2><p>SVD Decomposition is useful in various fields and teached by a lot of courses, its complete version is formulated as $$A=U\Sigma V^*, \Sigma=\begin{bmatrix}\mathbf{\sigma}&amp;\mathbf{0}\\ \mathbf{0}&amp;\mathbf{0}\end{bmatrix}, \mathbf{\sigma}=\mathrm{diag}\left\{\sqrt{\lambda_1},\sqrt{\lambda_2},\ldots,\sqrt{\lambda_r}\right\},V=\begin{bmatrix}V_1&amp;V_2\end{bmatrix},U=\begin{bmatrix}U_1&amp;U_2\end{bmatrix}$$<br>where</p><ul><li>$r=\rho(A)$ is the rank of matrix $A$</li><li>$\sigma_i$ are called <strong>sigular values</strong>, $\lambda_i$ are eigenvalues of $A^* A$</li><li>Columns of $V_1$ span $\mathcal{R}(A^ *A)=\mathcal{R}(A^ *)$, columns of $V_2$ span $\mathcal{N}(A^ *A)=\mathcal{N}(A)$</li><li>Columns of $U_1=AV_1\sigma^{-1}$ span $\mathcal{R}(A)$, columns of $U_2$ span $\mathcal{N}(A^*)$</li></ul><blockquote><p>SVD can be derived by doing eigenvalue decomposition on $A^* A$</p></blockquote><p>With SVD introduced, we can efficiently solve general linear equation $Ax=b$ as $x=x_r+x_n$ where $x_r\in\mathcal{R}(A^\top)$ and $x_n\in\mathcal{N}(A)$.</p><table><thead><tr><th></th><th>$Ax=b$</th><th>tall $A$ ($m&gt;n$)</th><th>fat $A$ ($m&lt; n$)</th></tr></thead><tbody><tr><td></td><td></td><td>Overdetermined,<br>Least Squares,<br>use Normal Equations</td><td>Underdetermined,<br>Quadratic Programming,<br>use Lagrange Multiplies</td></tr><tr><td>I.$b\in\mathcal{R}(A)$</td><td></td><td></td><td></td></tr><tr><td>1.$\mathcal{N}(A)={0}$</td><td>$x$ exist &amp; is unique</td><td>$x=(A^\top A)^{-1}A^\top b=A^+b$</td><td>$x=A^\top(AA^\top)^{-1}b=A^+b$</td></tr><tr><td>2.$\mathcal{N}(A)\neq{0}$</td><td>$x$ exist &amp; not unique</td><td>$x_r=(A^\top A)^{-1}A^\top b=A^+b$</td><td>$x_r=A^\top(AA^\top)^{-1}b=A^+b$</td></tr><tr><td>II.$b\notin\mathcal{R}(A)$</td><td></td><td></td><td></td></tr><tr><td>1.$\mathcal{N}(A)={0}$</td><td>$x$ not exists, $x_r$ exist &amp; is unique</td><td>$x_r=(A^\top A)^{-1}A^\top b=A^+b$</td><td>$x_r=A^\top(AA^\top)^{-1}b=A^+b$</td></tr><tr><td>2.$\mathcal{N}(A)\neq{0}$</td><td>$x$ not exists, $x_r$ not exist</td><td>$(A^\top A)^{-1}$ invertible</td><td>$(AA^\top)^{-1}$ invertible</td></tr></tbody></table><ul><li>$A^+=(A^\top A)^{-1}A^\top$ is left pseudo-inverse, $A^+=A^\top (AA^\top)^{-1}$ is right pseudo-inverse.</li><li>$A^+$ can be unified by the name <strong>Moore-Penrose Inverse</strong> and calculated using SVD by $A^+=V\Sigma^+ U^\top$ where $\Sigma^+$ take inverse of non-zeros.</li></ul><h2 id="Miscellaneous">Miscellaneous<a class="header-anchor" href="#Miscellaneous"> ❮</a></h2><blockquote><p>Selected theorems and lemmas useful in Linear Algebra. For more matrix properties see <a href="/blog/2019-06/MatrixAlgebra/" title="my post about Matrix Algebra">my post about Matrix Algebra</a></p></blockquote><ul><li>Matrix Square Root: $N^\top N=P$, then $N$ is the square root of $P$<blockquote><p>Square root is not unique. Cholesky decomposition is often used as square root.</p></blockquote></li><li><strong>Schur Complement</strong>: Given matrices $A_{n\times n}, B_{n\times m}, C_{m\times m}$, the matrix $M=\begin{bmatrix}A&amp;B\\ B^\top&amp;C\end{bmatrix}$ is symmetric. Then the following are equivalent (TFAE)<ol><li>$M\succ 0$</li><li>$A\succ 0$ and $C-B^\top A^{-1}B\succ 0$ (LHS called Schur complement of $A$ in $M$)</li><li>$C\succ 0$ and $A-B C^{-1}B^\top\succ 0$ (LHS called Schur complement of $C$ in $M$)</li></ol></li><li>Matrix Inverse Lemma: $(A+BCD)^{-1}=A^{-1}-A^{-1}B\left(C^{-1}+DA^{-1}B\right)^{-1}DA$</li><li>Properties of $A^\top A$<ul><li>$A^\top A \succeq 0$ and $A^\top A \succ 0 \Leftrightarrow A$ has full rank.</li><li>$A^\top A$ and $AA^\top$ have same non-zero eigenvalues, but different eigenvectors.</li><li>If $v$ is eigenvector of $A^\top A$ about $\lambda$, then $Av$ is eigenvector of $AA^\top$ about $\lambda$.</li><li>If $v$ is eigenvector of $AA^\top$ about $\lambda$, then $A^\top v$ is eigenvector of $A^\top A$ about $\lambda$.</li><li>$tr(A^\top A)=tr(AA^\top)=\sum_i\sum_j\left|A_{ij}\right|^2$</li><li>$det(A)=\prod_i\lambda_i, tr(A)=\sum_i\lambda_i$</li></ul></li></ul><h1 id="Real-Analysis">Real Analysis<a class="header-anchor" href="#Real-Analysis"> ❮</a></h1><h2 id="Set-theory">Set theory<a class="header-anchor" href="#Set-theory"> ❮</a></h2><blockquote><p>$\text{~}S$ stands for complement of set $S$ in following contents. These concepts are discussed under normed space $(\mathcal{X}, \Vert\cdot\Vert)$</p></blockquote><ul><li><strong>Open Ball</strong>: Let $x_0\in\mathcal{X}$ and let $a\in\mathbb{R}, a&gt;0$, then the open ball of radius $a$ about $x_0$ is $B_a(x_0)=\left\{x\in\mathcal{X}\middle| \Vert x-x_0\Vert &lt; a\right\}$<ul><li>Given subset $S\subset \mathcal{X}$, $d(x,S)=0\Leftrightarrow \forall\epsilon &gt;0, B_\epsilon(x)\cap S\neq\emptyset$</li><li>Given subset $S\subset \mathcal{X}$, $d(x,S)&gt;0\Leftrightarrow \exists\epsilon &gt;0, B_\epsilon(x)\cap S=\emptyset$</li></ul></li><li><strong>Interior Point</strong>: Given subset $S\subset\mathcal{X}$, $x\in S$ is an interior point of $S$ iff $\exists\epsilon &gt;0, B_\epsilon(x)\subset S$<ul><li><strong>Interior</strong>: $\mathring{S}=\{x\in \mathcal{X}|x\text{ is an interior point of }S\}=\{x\in\mathcal{X}|d(x,\text{~}S)&gt;0\}$</li></ul></li><li><strong>Open Set</strong>: $S$ is open if $\mathring{S}=S$</li><li><strong>Closure Point</strong>: Given subset $S\subset\mathcal{X}$, $x\in S$ is a closure point of $S$ iff $\forall\epsilon &gt;0, B_\epsilon(x)\cap S\neq\emptyset$.<ul><li><strong>Closure</strong>: $\bar{S}=\{x\in\mathcal{X}|x\text{ is a closure point of }S\}=\{x\in\mathcal{X}|d(x,S)=0\}$<blockquote><p>Note that $\partial\mathcal{X}=\emptyset$</p></blockquote></li></ul></li><li><strong>Closed Set</strong>: $S$ is closed if $\bar{S}=S$<blockquote><p>$S$ is open $\Leftrightarrow$ $\text{~}S$ is closed, $S$ is closed $\Leftrightarrow$ $\text{~}S$ is open. Set being both open and closed is called <strong>clopen</strong>(e.g. the whole set $\mathcal{X}$), empty set is clopen by convention.</p></blockquote></li><li><strong>Set Boundary</strong>: $\partial S=\bar{S}\cap\overline{\text{~}S}=\bar{S}\backslash\mathring{S}$</li></ul><h2 id="Sequences">Sequences<a class="header-anchor" href="#Sequences"> ❮</a></h2><ul><li><strong>Sequence</strong>($\{x_n\}$): a set of vectors indexed by the counting numbers<ul><li><strong>Subsequence</strong>: Let $1\leqslant n_1&lt; n_2&lt;\ldots$ be an infinite set of increasing integers, then $\{x_{n_i}\}$ is a subsequence of $\{x_n\}$</li></ul></li><li><strong>Convergence</strong>($\{x_n\}\to x\in\mathcal{X}$): $\forall \epsilon&gt;0,\exists N(\epsilon)&lt;\infty\text{ s.t. }\forall n\geqslant N, \Vert x_n-x\Vert &lt;\epsilon$<ul><li>If $x_n \to x$ and $x_n \to y$, then $x=y$</li><li>If $x_n \to x_0$ and $\{x_{n_i}\}$ is a subsequence of $\{x_n\}$, then $\{x_{n_i}\} \to x_0$</li><li><strong>Cauchy Convergence</strong> (necessary condition for convergence): $\{x_n\}$ is cauchy if $\forall \epsilon&gt;0,\exists N(\epsilon)&lt;\infty$ s.t. $\forall n,m\geqslant N, \Vert x_n-x_m\Vert &lt;\epsilon$</li><li>If $\mathcal{X}$ is finite dimensional, $\{x_n\}$ is cauchy $\Rightarrow$ $\{x_n\}$ has a limit in $\mathcal{X}$</li></ul></li><li><strong>Limit Point</strong>: Given subset $S\subset\mathcal{X}$, $x$ is a limit point of $S$ if $\exists \{x_n\}$ s.t. $\forall n\geqslant 1, x_n\in S$ and $x_n\to x$<ul><li>$x$ is a limit point of $S$ iff $x\in\bar{S}$</li><li>$S$ is closed iff $S$ contains its limit points</li></ul></li><li><strong>Complete Space</strong>: a normed space is <strong>complete</strong> if every Cauchy sequence has a limit. A complete normed space $(\mathcal{X}, \Vert\cdot\Vert)$ is called a <strong>Banach space</strong>.<ul><li>$S\subset \mathcal{X}$ is complete if every Cauchy sequence with elements from $S$ has a limit in $S$</li><li>$S\subset \mathcal{X}$ is complete $\Rightarrow S$ is closed</li><li>$\mathcal{X}$ is complete and $S\subset\mathcal{X} \Rightarrow S$ is complete</li><li>All finite dimensional subspaces of $X$ are complete</li></ul></li><li><strong>Completion of Normed Space</strong>: $\mathcal{Y}=\bar{\mathcal{X}}=\mathcal{X}+\{$all limit points of Cauchy sequences in $\mathcal{X}\}$<blockquote><p>E.g. $C[a,b]$ contains continuous functions over $[a,b]$. $(C[a,b], \Vert\cdot\Vert_1)$ is not complete, $(C[a,b], \Vert\cdot\Vert_\infty)$ is complete. Completion of $(C[a,b], \Vert\cdot\Vert_1)$ requires Lebesque integration.</p></blockquote></li><li><strong>Contraction Mapping</strong>: Let $S\subset\mathcal{X}$ be a subset and $T:S\to S$ is a contraction mapping if $\exists 0\leqslant c\leqslant 1$ such that, $\forall x,y \in S, \Vert T(x)-T(y)\Vert\leqslant c\Vert x-y\Vert$<ul><li><strong>Fixed Point</strong>: $x^* \in\mathcal{X}$ is a fixed point of $T$ if $T(x^ *)=x^ *$</li><li><a href="https://en.wikipedia.org/wiki/Banach_fixed-point_theorem" target="_blank" rel="noopener"><strong>Contraction Mapping Theorem</strong> (不动点定理)</a>: If $T:S\to S$ is a contraction mapping in a complete subset $S$, then $\exists! x^ *\in\mathcal{X}\text{ s.t. }T(x^ *)=x^ *$. Moreover, $\forall x_0\in S$, the sequence $x_{k+1}=T(x_k),k\geqslant 0$ is Cauchy and converges to $x^ *$.<blockquote><p>E.g. Newton Method: $x_{k+1}=x_k-\epsilon\left[\frac{\partial h}{\partial x}(x_k)\right]^{-1}\left(h(x_k)-y\right)$</p></blockquote></li></ul></li></ul><h2 id="Continuity-and-Compactness">Continuity and Compactness<a class="header-anchor" href="#Continuity-and-Compactness"> ❮</a></h2><ul><li><strong>Continuous</strong>: Let $(\mathcal{X},\Vert\cdot\Vert_\mathcal{X})$ and $(\mathcal{Y},\Vert\cdot\Vert_\mathcal{Y})$ be two normed spaces. A function $f:\mathcal{X}\to\mathcal{Y}$ is continuous at $x_0\in\mathcal{X}$ if $\forall\epsilon &gt;0,\exists \delta(\epsilon,x_0)&gt;0\text{ s.t. }\Vert x-x_0\Vert_\mathcal{X}&lt;\delta \Rightarrow\Vert f(x)-f(x_0)\Vert_\mathcal{Y} &lt;\epsilon$<ul><li>$f$ is continuous on $S\subset\mathcal{X}$ if $f$ is continuous at $\forall x_0\in S$</li><li>If $f$ in continuous at $x_0$ and $\{x_n\}$ is a sequence s.t. $x_n\to x_0$, then the sequence $\{f(x_n)\}$ in $\mathcal{Y}$ converges to $f(x_0)$</li><li>If $f$ is discontinuous at $x_0$, then $\exists \{x_n\}\in\mathcal{X}$ s.t. $x_n\to x_0$ but $f(x_n)\nrightarrow f(x_0)$</li></ul></li><li><strong>Compact</strong>: $S\subset\mathcal{X}$ is (sequentially) compact if every sequence in $S$ has a convergent subsequence with limit in $S$</li><li><strong>Bounded</strong>: $S\subset\mathcal{S}$ is bounded if $\exists r&lt;\infty$ such that $S\subset B_r(0)$<ul><li>$S$ is compact $\Rightarrow$ $S$ is closed and bounded</li><li><strong>Bolzano-Weierstrass Theorem</strong>: In a finite-dimensional normed space, $C$ is closed and bounded $\Leftrightarrow$ for $C$ is compact</li></ul></li><li><strong>Weierstrass Theorem</strong>: If $C\subset\mathcal{X}$ is a compact subset and $f:C\to\mathbb{R}$ is continuous at each point of $C$, then $f$ achieves its extreme values, i.e. $\exists \bar{x}\in C\text{ s.t. }f(\bar{x})=\sup_{x\in C} f(x)$ and $\exists \underline{x}\in C\text{ s.t. }f(\underline{x})=\inf_{x\in C} f(x)$<ul><li>$f:C\to\mathbb{R}$ continuous and $C$ compact $\Rightarrow$ $\sup_{x\in C}f(x)&lt;\infty$</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;&lt;p&gt;Selected notes from &lt;code&gt;ROB 501&lt;/code&gt; and &lt;code&gt;ME 564&lt;/code&gt;.&lt;br&gt;$\{x_i\}^b_a$ denotes set $\{x_a, x_{a+1}, \ldots, x_b\}$&lt;br&gt;TODO: add Jordan Form&lt;/p&gt;&lt;/blockquote&gt;&lt;h1 id=&quot;Algebraic-Structures&quot;&gt;Algebraic Structures&lt;a class=&quot;header-anchor&quot; href=&quot;#Algebraic-Structures&quot;&gt; ❮&lt;/a&gt;&lt;/h1&gt;&lt;h2 id=&quot;Operation&quot;&gt;Operation&lt;a class=&quot;header-anchor&quot; href=&quot;#Operation&quot;&gt; ❮&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Definition: an (binary, closed) &lt;strong&gt;operation&lt;/strong&gt; $\ast$ on a set $S$ is a mapping of $S\times S\to S$&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Commutative&lt;/strong&gt;: $x\ast y=y\ast x,\;\forall x,y\in S$&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Associative&lt;/strong&gt;: $(x\ast y)\ast z=x\ast (y\ast z),\;\forall x,y,z\in S$&lt;/li&gt;&lt;/ul&gt;&lt;h2 id=&quot;Group&quot;&gt;Group&lt;a class=&quot;header-anchor&quot; href=&quot;#Group&quot;&gt; ❮&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Definition: a &lt;strong&gt;group&lt;/strong&gt; is a pair $(\mathcal{S},\ast)$ with following axioms&lt;ol&gt;&lt;li&gt;$\ast$ is associative on $\mathcal{S}$&lt;/li&gt;&lt;li&gt;(Identity element) $\exists e\in \mathcal{S}\text{ s.t. }x\ast e=e\ast x=x,\;\forall x\in \mathcal{S}$&lt;/li&gt;&lt;li&gt;(Inverse element) $\forall x\in \mathcal{S}, \exists x’ \in \mathcal{S}\text{ s.t. }x\ast x’=x’\ast x=e$&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Abelian&lt;/strong&gt;: a group is called &lt;strong&gt;abelian group&lt;/strong&gt; if $\ast$ is also commutative&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://zyxin.xyz/blog/categories/Notes/"/>
    
      <category term="Math" scheme="http://zyxin.xyz/blog/categories/Notes/Math/"/>
    
    
      <category term="Math" scheme="http://zyxin.xyz/blog/tags/Math/"/>
    
      <category term="Algebra" scheme="http://zyxin.xyz/blog/tags/Algebra/"/>
    
  </entry>
  
  <entry>
    <title>Notes for Control System</title>
    <link href="http://zyxin.xyz/blog/2020-06/ControlSystemNotes/"/>
    <id>http://zyxin.xyz/blog/2020-06/ControlSystemNotes/</id>
    <published>2020-06-10T19:25:34.000Z</published>
    <updated>2021-01-24T03:46:02.361Z</updated>
    
    <content type="html"><![CDATA[<blockquote><ul><li>This note combines content from ME 564 Linear Systems and ME 561 Discrete Digital Control</li><li>Please read <a href="/blog/2020-06/AlgebraBasicsNotes/" title="the Algebra Basics notes">the Algebra Basics notes</a> first if you are not familiar with related concepts.</li><li>In this note, $f\in\mathbb{F}^\mathbb{G}$ stands for a function with domain in $\mathbb{G}$ and co-domain in $\mathbb{F}$, i.e. $f:\mathbb{F}\to\mathbb{G}$, $H(x)$ generally stands for Heaviside function (step function)</li></ul></blockquote><h1 id="Transforms">Transforms<a class="header-anchor" href="#Transforms"> ❮</a></h1><h2 id="Laplace-Transform">Laplace Transform<a class="header-anchor" href="#Laplace-Transform"> ❮</a></h2><ul><li>Definition: $F(s)=\mathcal{L}\{f(t)\}(s)=\int^\infty_0 f(t)e^{-st}\mathrm{d}t$<blockquote><p>Note that the transform is not well defined for all functions in $\mathbb{C}^\mathbb{R}$. And the transform is only valid for $s$ in a region of convergence, which is usually separated by 0.</p></blockquote></li><li>Laplace Transform is a linear map from $(\mathbb{C}^\mathbb{R}, \mathbb{C})$ to $(\mathbb{C}^\mathbb{C}, \mathbb{C})$ and it’s one-to-one.</li><li>Properties: (see <a href="https://en.wikipedia.org/wiki/Laplace_transform" target="_blank" rel="noopener">Wikipedia</a> or <a href="https://lpsa.swarthmore.edu/LaplaceZTable/LaplacePropTable.html" target="_blank" rel="noopener">this page</a> for full list)<ul><li>Derivative: $f’(t) \xleftrightarrow{\mathcal{L}} sF(s)-f(0^-)$</li><li>Integration: $\int^t_0 f(\tau)d\tau \xleftrightarrow{\mathcal{L}} \frac{1}{s}F(s)$</li><li>Delay: $f(t-a)H(t-a) \xleftrightarrow{\mathcal{L}} e^{-as}F(s)$</li><li>Convolution: $\int^t_0 f(\tau)g(t-\tau)\mathrm{d}\tau \xleftrightarrow{\mathcal{L}} F(s)G(s)$</li></ul></li><li>Stationary Value: $\lim\limits_{t\to 0} f(t) = \lim\limits_{s\to \infty} sF(s), \lim\limits_{t\to \infty} f(t) = \lim\limits_{s\to 0} sF(s)$</li></ul><a id="more"></a><h3 id="Inverse-Laplace-Transform">Inverse Laplace Transform<a class="header-anchor" href="#Inverse-Laplace-Transform"> ❮</a></h3><blockquote><p>Laplace transform is one-to-one, so we can apply inverse transform on functions in s-space</p></blockquote><p>There are several ways to calculate Laplace transform, the first one is directly evaluating integration while the latter two are converting the function into certain formats that are convenient for table lookup:</p><ol><li>(Mellin’s) Inverse formula: $f(t)=\mathcal{L}^{-1}\{F(s)\}(t)=\frac{1}{2\pi j}\lim\limits_{T\to\infty} \int ^{\gamma+iT}_{\gamma-iT} e^{st}F(s)\mathrm{d}s$ where the integration is done along the vertical line $Re(s)=\gamma$ in the convex s-plane such that $\gamma$ is greater than the real part of all poles of $F(s)$.</li><li>Power Series: $F(s) = \sum^\infty_{n=0} \frac{n!a_n}{s^{n+1}}\xleftrightarrow{\mathcal{L}} f(t) = \sum ^\infty_{n=0} a_n t^n $</li><li>Partial Fractions: $F(s)=\frac{k_1}{s+a}+\frac{k_2}{s+b}+\ldots \xleftrightarrow{\mathcal{L}} f(t)=k_1 e^{-at} + k_2 e^{-bt} + \ldots$<ul><li>To calculate partial fractions, one can use <a href="http://tutorial.math.lamar.edu/Classes/Alg/DividingPolynomials.aspx" target="_blank" rel="noopener">Polynomial Division</a> or following lemma:</li><li>Suppose $F(s)=\frac{N(s)}{D(s)}=\frac{N(s)}{\prod^n_{i=1} (s-p_i)^{r_i}}$ where $\mathrm{deg}(N(s)) &lt; \mathrm{deg(D(s))}$ and each $p_i$ is a distinct root of $D(s)$ (i.e. pole) with multiplicity $r_i$, then $F(s)=\sum^n_{i=1}\sum^{r_i}_ {j=1} \frac{k_{ij}}{(s-p_i)j}$ where $k_{ij}=\frac{1}{(r_i-j)!}\left.\frac{\mathrm{d}^{r_i-j}}{\mathrm{d}s^{r_i-j}}(s-p_i)^{r_i}F(s)\right\vert_{s=p_i}$</li></ul></li></ol><h2 id="Z-Transfrom">Z-Transfrom<a class="header-anchor" href="#Z-Transfrom"> ❮</a></h2><ul><li>Definition: $F(z)=\mathcal{Z}\{f(k)_ {k\in\mathbb{N}}\}(z)=\sum^\infty_{k=0} f(k)z^{-k}$</li></ul><blockquote><p>Notice that $f$ is defined on natural numbers. In time domain, it’s usually corresponding to $f(kT)$. Z-transform is also only valid for $z$ in certain region (usually separated by 1)</p></blockquote><ul><li>Laplace Transform is a linear map from $(\mathbb{C}^\mathbb{N}, \mathbb{C})$ to $(\mathbb{C}^\mathbb{C}, \mathbb{C})$ and it’s one-to-one.</li><li>Properties: (see <a href="https://en.wikipedia.org/wiki/Z-transform" target="_blank" rel="noopener">Wikipedia</a> or <a href="https://lpsa.swarthmore.edu/LaplaceZTable/LaplacePropTable.html" target="_blank" rel="noopener">this page</a> for full list)<ul><li>Accumulation: $\sum^n_{k=-\infty} f(k) \xleftrightarrow{\mathcal{Z}} \frac{1}{1-z^{-1}}F(z)$</li><li>Delay: $f(k-m) \xleftrightarrow{\mathcal{Z}} z^{-m}F(z)$</li><li>Convolution: $\sum^k_{n=0}f_1(n)f_2(k-n) \xleftrightarrow{\mathcal{Z}} F_1(z)F_2(z)$</li></ul></li><li>Stationary Value: $\lim\limits_{t\to 0} f(t) = \lim\limits_{z\to \infty} F(z), \lim\limits_{t\to \infty} f(t) = \lim\limits_{z\to 1} (z-1)F(z)$</li></ul><details><summary>Example: Z-Transform of PID controller</summary>Assume the close-loop error input of the controller is $e(t)$, and $e(kT)$ after sampling. PID controller action in analog is $$m(t)=K\left(e(t)+\frac{1}{T_i}\int^t_0e(t)\mathrm{d}t+T_d\frac{\mathrm{d}e(t)}{\mathrm{d}t}\right)$$ We can approximate by trapezoidal rule with two point difference: $$m(kT)=K\left(e(kT)+\frac{T}{T_i}\sum^k_{h=1}\frac{e((h-1)T)+e(hT)}{2}+T_d\left(\frac{e(kT)-e((k-1)T)}{T}\right)\right)$$ Lets define $f(hT) = \frac{1}{2}\left(e((h-1)T)+e(hT)\right),\;f(0)=0$ Then $$\begin{split}\mathcal{Z}\left(\left\{\sum^k_{h=1}\frac{e((h-1)T)+e(hT)}{2}\right\}_k\right)(z)=\mathcal{Z}\left(\left\{\sum^k_{h=1}f(hT)\right\}_k\right)(z) \\ =\frac{1}{1-z^{-1}}(F(z)-F(0))=\frac{1}{1-z^{-1}}F(z)\end{split}$$ Notice that $$F(z)=\mathcal{Z}\left({f(hT)}_h\right)(z)=\frac{1+z^{-1}}{2}E(z)$$ so we can calculate the Z-transform of $m(kT)$ $$\begin{split} M(z)&=K\left(1+\frac{T}{2T_i}\left(\frac{1+z^{-1}}{1-z^{-1}}\right)+\frac{T_d}{T}(1-z^{-1})\right)E(z)\\&=K\left(1-\frac{T}{2T_i}+\frac{T}{T_i}\frac{1}{1-z^{-1}}+\frac{T_d}{T}(1-z^{-1})\right)E(z)\\&=\left(K_p+K_i\left(\frac{1}{1-z^-1}\right)+K_d(1-z^{-1})\right)E(z) \end{split}$$<p>Here we have</p><ul><li>Proportional Gain $K_p=K-\frac{KT}{2T_i}$</li><li>Integral Gain $K_I=\frac{KT}{T_i}$</li><li>Derivative Gain $K_d=\frac{KT_d}{T}$</li></ul></details><h3 id="Inverse-Z-Transform">Inverse Z-Transform<a class="header-anchor" href="#Inverse-Z-Transform"> ❮</a></h3><ol><li>Inverse formula: $f(k)=\mathcal{Z}^{-1}\{F(z)\}(k)=\frac{1}{2\pi j}\oint _\Gamma z^{k-1}F(z)\mathrm{d}z$ where the integration is done along any closed path $\Gamma$ that encloses all finite poles of $z^{k-1}X(z)$ in the z-plane.<ul><li>According to residual theorem, we can write it as $f(k)=\sum_{p_i}Res(z^{k-1}f(z), pi)$ where $p_i$ are poles of $z^{k-1}f(k)$ and residual $Res(g(z),p)=\frac{1}{(m-1)!}\left.\frac{\mathrm{d}^{m-1}}{\mathrm{d}z^{m-1}}\left((z-p)^mg(z)\right)\right\vert_{z=p}$ with $m$ being the multiplicity of the pole $p$ in $g$.</li></ul></li><li>Power Series: same as inverse laplace.</li><li>Partial Fractions: same as inverse laplace.</li></ol><h3 id="Modified-Z-Transfrom">Modified Z-Transfrom<a class="header-anchor" href="#Modified-Z-Transfrom"> ❮</a></h3><ul><li>Definition: $F(z,m)=\mathcal{Z}_m(f,m)=\mathcal{Z}(\left\{f(kT-(1-m)T)\right\} _{k\in\mathbb{N}^+})(z)$</li><li>We denote corresponding continuous form $\mathcal{L}(f(t-(1-m)T)\delta_ T(t))$ as $F^*(s,m)$</li><li>Residual Theorem: $\mathcal{Z}_m(f,m)=z^{-1}\sum _{p_i} Res(\frac{F(s)e^{mTs}}{1-z^{-1}e^{Ts}}, p_i)$</li><li>ModZ Transform is usually used when there’s delay in the system, use this transform to shift the signal with proper $m$ value.</li></ul><h2 id="Starred-Transform">Starred Transform<a class="header-anchor" href="#Starred-Transform"> ❮</a></h2><ul><li>Definition: $F^* (s)=\sum^\infty_{n=0}f(n*T)e^{-nTs}$</li></ul><blockquote><p>Starred Transform is defined in continuous s-domain, but it only aggregates on discrete s values defined periodically by sampling time T, like Z-Transform. Starred Transform is usually exchangeable with Z-Transform with $z=e^{Ts}$.</p></blockquote><ul><li>Sometimes we also see <code>*</code> as an operator to sample a continuous signal. It converts a continuous signal to discrete delta functions. (See the “Sampler” section below)</li><li>Calculation from Laplace Transform<ul><li>$F^*(s)=\sum_{p_i\in\{poles\;of\;F(\lambda)\}} Res\left(F(\lambda)\frac{1}{1-e^{-T(s-\lambda)}}, p_i\right)$</li><li>$F^*(s)=\frac{1}{T}\sum^\infty_{n=-\infty}F(s+jn\omega_s)+\frac{e(0)}{2}$ where $\omega_s=\frac{2\pi}{T}$</li></ul></li><li>Properties:<ul><li>$F^*(s)$ is periodic in s plane with period $j\omega_s=\frac{2\pi j}{T}$</li><li>If $F(s)$ has a pole at $s=s_0$, then $F^*(s)$ must have poles at $s=s_0+jn\omega_s$ for $m\in\mathbb{Z}$</li><li>$A(s)=B(s)F^* (s) \Rightarrow A^* (s)=B^* (s)F^* (s)$, while usually $A(s)=B(s)F(s) \nRightarrow A^* (s)=B^* (s)F^* (s)$</li></ul></li></ul><h2 id="Fourier-Transform">Fourier Transform<a class="header-anchor" href="#Fourier-Transform"> ❮</a></h2><blockquote><p>Fourier transform is basically to substitute $s=j\omega$ into Laplace transform. Additional properties are not discussed here.</p><ul><li>One important theorem (Shannon-Nyquist Sampling Theorem): Suppose $e:\mathbb{R}_+\to\mathbb{R}$ has a Fourier Transform with no frequency components greater than $f_0$, then $e$ is uniquely determined by the signal $e_s$ generated by ideally sampling $e$ with period $\frac{1}{2}f_0$.</li></ul></blockquote><h1 id="State-Space-Representation">State Space Representation<a class="header-anchor" href="#State-Space-Representation"> ❮</a></h1><h2 id="Continuous-State-Space-Representation">Continuous State Space Representation<a class="header-anchor" href="#Continuous-State-Space-Representation"> ❮</a></h2><h3 id="Definition">Definition<a class="header-anchor" href="#Definition"> ❮</a></h3><p>A continuous-time linear state-space system can be described by following two equations:<br>\begin{align}&amp;\text{State equation}:\;&amp;\dot{x}(t)&amp;=A(t)x(t)+B(t)u(t),&amp;\;x(t)\in\mathbb{R}^n,\;u(t)&amp;\in\mathbb{R}^m \\&amp;\text{Output equation}:\;&amp;y(t)&amp;=C(t)x(t)+D(t)u(t),&amp;\;y(t)&amp;\in\mathbb{R}^p\end{align}</p><p>The input $u:[0,\infty)\to\mathbb{R}^m$, state $x:[0,\infty)\to\mathbb{R}^n$, and output $y:[0,\infty)\to\mathbb{R}^p$ are all <em>signals</em>, i.e. functions of continuous time $t\in[0,\infty)$. The coefficients $A\in\mathbb{R}^{n\times n}$,$B\in\mathbb{R}^{n\times m}$,$C\in\mathbb{R}^{p\times n}$,$D\in\mathbb{R}^{p\times m}$</p><p>This linear time-varying (LTV) system can be written compactly as<br>\begin{align*} \dot{x}&amp;=A(t)x+B(t)u \\ y&amp;=C(t)x+D(t)u\end{align*}<br>Similarly, linear time-invariant (LTI) system can be written as<br>\begin{align} \dot{x}&amp;=Ax+Bu \\ y&amp;=Cx+Du\end{align}</p><p>For non-linear system, the equation will be written as</p><table><tr><th style="text-align:center">time-varying (NLTV)</th><th style="text-align:center">time-invariant (NTLI)</th><th style="text-align:center">time-invariant autonomous</th></tr><tr><td><p>\begin{align*}\dot{x}&amp;=f(x,u,t)\\y&amp;=g(x,u,t)\end{align*}</p></td><td><p>\begin{align*}\dot{x}&amp;=f(x,u)\\y&amp;=g(x,u)\end{align*}</p></td><td><p>\begin{align*}\dot{x}&amp;=f(x)\\y&amp;=g(x)\end{align*}</p></td></tr></table><h3 id="Solution">Solution<a class="header-anchor" href="#Solution"> ❮</a></h3><blockquote><p><em><strong>Math prerequisites here:</strong></em></p><ul><li>For definition of function on matrix, see <a href="/blog/2020-06/AlgebraBasicsNotes/#Eigenvalue-and-Canonical-Forms">my notes for algebra basics</a></li><li>$e^A$ is matrix exponential, <code>expm</code> in MATLAB<ol><li>$\frac{\mathrm{d}}{\mathrm{d}t}e^{At}=Ae^{At}=e^{At}A$</li><li>$e^{(A+B)t}\Leftrightarrow AB=BA$ <strong>(be careful when commute matrices)</strong></li><li>$\mathcal{L}\{e^{At}\}=(sI-A)^{-1}$ (can be derived from property 1 and laplace derivative)</li></ol></li><li>To calculate $e^A$<ol><li>Eigenvalue decomposition</li><li>Jordan form decomposition</li><li>Directly evaluate infinite power series (converges quickly)</li><li>Inverse Laplace transform</li></ol></li><li>For more properties of the matrix function, see <a href="/blog/2019-06/MatrixAlgebra/" title="Matrix Algebra">Matrix Algebra</a></li></ul></blockquote><ul><li><p>For homogeneous LTI system: $$\begin{align}x(t)=e^{A(t-t_0)}x_0\end{align}$$</p><ul><li>“<em>homogeneous</em>” = zero-input, Eq.5 is also called <strong>zero input response</strong> (ZIR).</li><li>“<em>homogeneous equation</em>” = 齐次方程</li></ul></li><li><p>For LTI system:<br>$$\begin{align}x(t)=e^{A(t-t_0)}x(t_0)+\int^t_{t_0}e^{A(t-\tau)}Bu(\tau)d\tau\end{align}$$<br>This result requires $A$ to be time-invariant, $B,C,D$ can be time varying.</p><ul><li>The solution consists of two parts: ZIR and ZSR (<strong>zero state response</strong>, $x(t_0)=0$), which are homogenenous solution (通解) and particular solution (特解) of the ODE.</li><li>ZIR and ZSR are both linear mapping</li></ul></li><li><p>For homogeneous LTV system: $$\begin{align}x(t)=\Phi(t,t_0)x_0\end{align}$$</p><ul><li>Matrix $\Phi$ is called the <strong>state transition matrix</strong>, defined as $$\begin{equation}\begin{split}\Phi(t,t_0)\equiv I+\int^t_{t_0}A(s_1)\mathrm{d}s_1+\int^t_ {t_0}A(s_1)\int^{s_1}_ {t_0}A(s_2)\mathrm{d}s_2\mathrm{d}s_1+\\ \int^t_ {t_0}A(s_1)\int^{s_1}_ {t_0}A(s_2)\int^{s_2}_ {t_0}A(s_3)\mathrm{d}s_3\mathrm{d}s_2\mathrm{d}s_1+\cdots\end{split}\end{equation}$$</li><li>Properties of $\Phi$:<ol><li>$\Phi(t,t)=I$</li><li>$\frac{\mathrm{d}}{\mathrm{d}t}\Phi(t,t_0)=A(t)\Phi(t,t_0)$</li><li>(semigroup property) $\Phi(t,s)\Phi(s,\tau)=\Phi(t,\tau)$</li><li>$\forall t,\tau\geqslant 0,\;[\Phi(t,\tau)]^{-1}=\Phi(\tau,t)$</li></ol></li><li>Eq.6 can be directly derived by evaluating Eq.8</li></ul></li><li><p>For LTV system:<br>$$\begin{align}x(t)=\Phi(t,t_0)x_0+\int^t_{t_0}\Phi(t,\tau)B(\tau)u(\tau)d\tau\end{align}$$</p></li><li><p>Some conclusions:</p><ul><li>The solution given by Eq.9 is unique</li><li>The set of all solutions to ZIR system forms a vector space of dimension $n$</li><li>If $A(t)A(s)=A(s)A(t)$, then $\Phi(t,t_0)=e^{\int^t_{t_0}A(\tau)\mathrm{d}\tau}$</li></ul></li><li><p><strong>Phase Portraits</strong>: A phase portrait is a graph of several zero-input responses on the phase plane ($\dot{x}(t)$ and $x(t)$ are phase variables)</p><blockquote><p>Usually in phase portraits, there are two straight lines corresponding to the eigenvector of A, other lines are growing in or opposite to the direction of the lines.</p></blockquote></li></ul><h3 id="Transfer-function">Transfer function<a class="header-anchor" href="#Transfer-function"> ❮</a></h3><ul><li>For LTI case, $\frac{Y(s)}{U(s)} = C(sI-A)^{-1}B+D$<blockquote><p>This can be derived by take laplace transform of both sides of state equations</p></blockquote></li></ul><h2 id="Discrete-State-Space-Representation">Discrete State Space Representation<a class="header-anchor" href="#Discrete-State-Space-Representation"> ❮</a></h2><h3 id="Definition-2">Definition<a class="header-anchor" href="#Definition-2"> ❮</a></h3><p>A discrete-time linear state-space system can be described by following two equations:<br>$$\begin{align}&amp;\text{State eq.}:\;&amp;x(k+1)&amp;=A(k)x(k)+B(k)u(k),&amp;\;x\in\mathbb{R}^n,\;u&amp;\in\mathbb{R}^m \\ &amp;\text{Output eq.}:\;&amp;y(k)&amp;=C(k)x(k)+D(k)u(k),&amp;\;y&amp;\in\mathbb{R}^p\end{align}$$</p><p>The input $u:\mathbb{N}\to\mathbb{R}^m$, state $x:\mathbb{N}\to\mathbb{R}^n$, and output $y:\mathbb{N}\to\mathbb{R}^p$ are all <em>signals</em>, i.e. functions of continuous time $t\in\mathbb{N}$.</p><p>Discrete LTI system is sometimes written compactly as $$\begin{align} x_{k+1}&amp;=Ax_k+Bu_k \\ y_k&amp;=Cx_k+Du_k \end{align}$$</p><h3 id="Transfer-function-2">Transfer function<a class="header-anchor" href="#Transfer-function-2"> ❮</a></h3><ul><li>For LTI case, $H(z)=C(zI-A)^{-1}B+D$ (pulse tranfer function)</li></ul><h2 id="Controllability-Reachability">Controllability &amp; Reachability<a class="header-anchor" href="#Controllability-Reachability"> ❮</a></h2><blockquote><p>Note: hereafter $\mathfrak{R}$ denotes <a href="/blog/2020-06/AlgebraBasicsNotes/#Linear-Operator">range space</a>, $\mathfrak{N}$ denotes <a href="/blog/2020-06/AlgebraBasicsNotes/#Linear-Operator">null space</a>.</p></blockquote><ul><li><strong>Controllability</strong>: $\exists u$ that drives any initial state $x(t_0)=x_0$ to $x(t_1)=0$</li><li><strong>Reachability</strong>: $\exists u$ that drives initial state $x(t_0)=0$ to any $x(t_1)=x_1$</li></ul><p>Consider the continuous LTV system $\dot{x}=A(t)x+B(t)u,\;x\in\mathbb{R}^n,u\in\mathbb{R}^m$.</p><ul><li><p><strong>Reachable Subspace</strong>: Given $t_0$ &amp; $t_1$, the reachable subspace $\mathcal{R}[t_0, t_1]$ consists of all states $x_1$ for which there exists and input $u:[t_0, t_1]\to\mathbb{R}^m$ that transfers the state from $x(t_0)=0$ to $x(t_1)=x_1$.</p><ul><li>$\mathcal{R}[t_0, t_1]\equiv\left\{x_1\in\mathbb{R}^n\middle|\exists u(\cdot),\;x_1=\int^{t_1}_{t_0}\Phi(t_1,\tau)B(\tau)u(\tau)\mathrm{d}\tau\right\}$</li></ul></li><li><p><strong>Controllable Subspace</strong>: Given $t_0$ &amp; $t_1$, the controllable subspace $\mathcal{C}[t_0, t_1]$ consists of all states $x_0$ for which there exists an input $u:[t_0, t_1]\to\mathbb{R}^m$ that transfers the state from $x(t_0)=x_0$ to $x(t_1)=0$</p><ul><li>$\mathcal{C}[t_0, t_1]\equiv\left\{x_0\in\mathbb{R}^m\middle|\exists u(\cdot),\;0=\Phi(t_1,t_0)x_0+\int^{t_1}_{t_0}\Phi(t_1,\tau)B(\tau)u(\tau)\mathrm{d}\tau\right\}$</li><li>or $\mathcal{C}[t_0, t_1]\equiv\left\{x_0\in\mathbb{R}^m\middle|\exists u(\cdot),\;x_0=\int^{t_1}_{t_0}\Phi(t_0,\tau)B(\tau)\left[-u(\tau)\right]\mathrm{d}\tau\right\}$</li></ul></li><li><p><strong>Reachability Grammian</strong>: $W_\mathcal{R}(t_0, t_1)\equiv\int^{t_1}_{t_0}\Phi(t_1,\tau)B(\tau)B(\tau)^\top\Phi^\top(t_1,\tau)\mathrm{d}\tau$ given times $t_1&gt;t_0\geqslant0$</p><ul><li>The system is reachable at time $t_0$ iff $\exists t_1$ s.t. $W_\mathcal{R}(t_0,t_1)$ is non-singular.</li></ul><blockquote><p>non-singular for some $t_1$ $\Rightarrow$ non-singular for any $t_1$</p></blockquote><ul><li>$\mathcal{R}[t_0,t_1]=\mathfrak{R}(W_\mathcal{R}(t_0,t_1))$</li><li>if $x_1=W_\mathcal{R}(t_0,t_1)\eta_1\in\mathfrak{R}(W_\mathcal{R}(t_0,t_1))$, the control $u(t)=B^\top(t)\Phi^T(t_1,t)\eta_1$,$t\in[t_0,t_1]$ can be used to transfer the system from $x(t_0)=0$ to $x(t_1)=x_1$ (w/ minimum energy)</li></ul><blockquote><p>minimum energy = minimum $\int^T_0\Vert u(\tau)\Vert^2\mathrm{d}\tau$</p></blockquote><ul><li>For LTI system $W_\mathcal{R}(t_0,t_1)=\int^{t_1}_ {t_ 0}e^{A(t_1-\tau)}BB^\top e^{A^{\top} (t_1-\tau)}\mathrm{d}\tau=\int^{t_1-t_ 0}_ {0}e^{At}BB^\top e^{A^{\top}t}$</li></ul></li><li><p><a href="https://en.wikipedia.org/wiki/Controllability_Gramian" target="_blank" rel="noopener"><strong>Controllability Grammian</strong></a>: $W_\mathcal{C}(t_0, t_1)\equiv\int^{t_1}_{t_0}\Phi(t_0,\tau)B(\tau)B(\tau)^\top\Phi^\top(t_0,\tau)\mathrm{d}\tau$ given times $t_1&gt;t_0\geqslant0$</p><ul><li><p>The system is reachable at time $t_0$ iff $\exists t_1$ s.t. $W_\mathcal{C}(t_0,t_1)$ is non-singular.</p></li><li><p>$\mathcal{C}[t_0,t_1]=\mathfrak{R}(W_\mathcal{C}(t_0,t_1))$</p></li><li><p>if $x_0=W_\mathcal{C}(t_0,t_1)\eta_0\in\mathfrak{R}(W_\mathcal{C}(t_0,t_1))$, control $u(t)=-B^\top(t)\Phi^\top(t_0,t)\eta_0$,$t\in[t_0,t_1]$ can be used to transfer the state from $x(t_0)=x_0$ to $x(t_1)=0$ (w/ minimum energy)</p></li><li><p>For LTI system $W_\mathcal{C}(t_0,t_1)=\int^{t_1}_ {t_ 0}e^{A(t_0-\tau)}BB^\top e^{A^{\top} (t_0-\tau)}\mathrm{d}\tau=\int^{t_1-t_ 0}_ {0}e^{-At}BB^\top e^{-A^{\top}t}$</p></li></ul></li><li><p><strong>Controllability Matrix</strong>: For LTI system, controllability matrix $\mathcal{C}=[B\;|\;AB\;|\;A^2B\;\cdots\;A^{n-1}B]$</p><blockquote><p>The controllability matrix works for both continuous and discrete system, and it’s easier to be derived from discrete LTI equations:<br>In discrete LTI, $\mathcal{C}\mathbf{u}=-A^k x_0$ where $\mathbf{u}=\begin{bmatrix}u_{k-1} &amp; u_{k-2} &amp; \ldots &amp; u_0\end{bmatrix}^\top$</p></blockquote><ul><li>For LTI, $\mathcal{R}[t_0,t_1]=\mathfrak{R}(W_\mathcal{R}[t_0,t_1])=\mathfrak{R}(\mathcal{C})=\mathfrak{R}(W_\mathcal{C}[t_0,t_1])=\mathcal{C}[t_0,t_1]$</li></ul><blockquote><p>This implies Controllability $\Leftrightarrow$ Reachability for LTI systems.</p></blockquote><ul><li>The controllable subspace $\mathfrak{\mathcal{C}}$ is the smallest A-invariant subspace that contains $\mathfrak{\mathcal{B}}$</li><li>If the controllability matrix has full rank, the LTI system (or the pair $(A,B)$) is <strong>completely controllable</strong></li></ul></li><li><p><strong>PBH-Eigenvector Test</strong>: An LTI system is not controllable iff there exists a nonzero <em>left</em> eigenvector $v$ of $A$ such that $vB=0$</p></li><li><p><strong>PBH-Rank Test</strong>: An LTI system will be controllable iff $[\lambda I-A \;| \;B]$ has full row rank for all eigenvalue $\lambda$</p></li><li><p>For LTI system, there exists an input $u(\cdot)$ that transfer the state from $x_0$ ito $x_1$ in finite time $T$ iff $x_1-e^{AT}x_0\in\mathfrak{R}(\mathcal{C})$</p><ul><li>The input that transfers any state $x_0$ to any other state $x_1$ in some finite time $T$ is $u(t)=B^\top e^{A^{\top}(T-t)}W_\mathcal{R}^{-1}(0,T)[x_1 -e^{AT}x_0]$, for $t\in[0,T]$ (w/ minimum energy)</li></ul></li></ul><h2 id="Observability">Observability<a class="header-anchor" href="#Observability"> ❮</a></h2><ul><li><p><strong>Observability</strong>: Given any input $u(t)$ and output $y(t)$ over $t\in[t_0,t_1]$, it’s sufficient to determine a unique initial state $\exists !x(t_0)$.</p></li><li><p><a href="https://en.wikipedia.org/wiki/Observability_Gramian" target="_blank" rel="noopener"><strong>Observability Grammian</strong></a>: $W_\mathcal{O}(t_0,t_1)\equiv\int^{t_1}_{t_0}\Phi^\top(t_1,\tau)C^\top(\tau)C(\tau)\Phi(t_1,\tau)\mathrm{d}\tau$</p><ul><li><p>The system is observable at time $t_0$ iff $\exists t_1$ s.t. $W_{\mathcal{O}}(0,t)$ is nonsingular.</p></li><li><p>For LTI system $W_{\mathcal{O}}(t_0,t_1)=\int^{t_1}_{t_0} e^{A^{\top}(t_1-\tau)}C^\top Ce^{A(t_1-\tau)}\mathrm{d}\tau=\int^{t_1-t_0}_0 e^{A^{\top}\tau}C^\top Ce^{A\tau}\mathrm{d}\tau$</p></li></ul></li><li><p><strong>Observability Matrix</strong>: For LTI system, observability $\mathcal{O}=\begin{bmatrix}C\\CA\\CA^2\\ \vdots\\CA^{k-1}\end{bmatrix}$</p><blockquote><p>The controllability matrix works for both continuous and discrete system, and it’s easier to be derived from discrete LTI equations:<br>In discrete LTI, $\Psi_{k-1}=\mathcal{O}x_0$ where $$\Psi_k\equiv\begin{bmatrix}y_0\\y_1\\y_2\\ \vdots\\ y_{k-1}\end{bmatrix}-\begin{bmatrix} D &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\ CB &amp; D &amp; 0 &amp; \cdots &amp; 0 \\ CAB &amp; CB &amp; D &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ CA^{k-2}B &amp; CA^{k-3}B &amp; CA^{k-4}B &amp; \cdots &amp; 0 \end{bmatrix}\begin{bmatrix}u_0\\u_1\\u_2\\ \vdots \\ u_{k-1}\end{bmatrix}$$</p></blockquote><ul><li>If the controllability matrix has full rank, the LTI system (or the pair $(A,C)$) is <strong>completely observable</strong>.</li></ul></li><li><p><strong>PBH-Rank Test</strong>: An LTI system will be observable iff $\begin{bmatrix}A-\lambda I \\C\end{bmatrix}$ has full column rank for all eigenvalue $\lambda$</p></li></ul><h2 id="Duality">Duality<a class="header-anchor" href="#Duality"> ❮</a></h2><ul><li><strong>Duality Theorem</strong>: The pair $(A,B)$ is controllable iff the pair $(A^\top, B^\top)$ is observable.<ul><li><em>Controllability</em> only depends on matrix $A$ and $B$ while the <em>Observability</em> only depends on matrix $A$ and $C$</li><li>Duality theorem is useful for proof of observability conclusions from controllability</li></ul></li><li><strong>Adjoint System</strong>:</li></ul><table><tr><th></th><th>Original System</th><th>Adjoint System</th></tr><tr><td>Equations</td><td>$$\begin{align*} \dot{x}&=A(t)x+B(t)u \\ y&=C(t)x \end{align*}$$</td><td>$$\begin{align*} \dot{p}&=-A^*(t)p-C^*(t)v \\ z&=B^*(t)p\end{align*}$$</td></tr><tr><td>Initial Condition</td><td>$x(t_0)=x_0$</td><td>$p(t_1)=p_1$</td></tr><tr><td>State Trasition Matrix</td><td>$\Phi(t,t_0)$</td><td>$\Phi^*(t_1,t)=\left(\Phi^*(t,t_1)\right)^{-1}$</td></tr><tr><td>Zero-State Response</td><td>$$\begin{split}L_u:\;&u(\cdot)\to x(t_1)\\=&\int^{t_1}_{t_0}\Phi(t_1,\tau)B(\tau)u(\tau)\mathrm{d}\tau\end{split}$$</td><td>$$\begin{split}P_u:\;&v(\cdot)\to p(t_0)\\=&\int^{t_1}_{t_0}\Phi^*(\tau,t_0)C^*(\tau)v(\tau)\mathrm{d}\tau\end{split}$$</td></tr><tr><td>Zero-Input Response</td><td>$$\begin{split}L_0:\;&x_0\to y(\cdot)\\=&C(\cdot)\Phi(\cdot,t_0)x_0\end{split}$$</td><td>$$\begin{split}P_0:\;&p_1\to z(\cdot)\\=&B^*(\cdot)\Phi^*(t_1,\cdot)p_1\end{split}$$</td></tr><tr><td rowspan="3">Duality Theorem</td><td>Controllable ($\rho(L_u)=n$)</td><td>Observable ($\rho(P_0^*)=n$)</td></tr><tr><td>Observable ($\rho(L_0^*)=n$)</td><td>Controllable ($\rho(P_u)=n$)</td></tr><tr><td>A state is reachable ($x\in\mathfrak{R}(L_u)$)</td><td>A state is unobservable ($x\in\mathfrak{N}(L_0)$)</td></tr></table>Note that ZIR and ZSR are both linear mappings and $L_u^*=P_0,\;L_0^*=P_u$<h2 id="Decomposition-and-Realizations">Decomposition and Realizations<a class="header-anchor" href="#Decomposition-and-Realizations"> ❮</a></h2><ul><li><p>Similarity Transform of a (LTI) system: Based on Eq.3 and Eq.4, define $x=P\bar{x}$, then we have $$\begin{align}\dot{\bar{x}}&amp;=P^{-1}AP\bar{x}+P^{-1}Bu&amp;=\bar{A}\bar{x}+\bar{B}u \\ y&amp;=CP\bar{x}+Du&amp;=\bar{C}\bar{x}+Du\end{align}$$</p><ul><li>Similarity transform doesn’t affect transfer function.</li></ul></li><li><p><strong>Controllability Decomposition</strong>: For an uncontrollable LTI system, define matrix $V=[V_1\;V_2]$ where $V_1$ is a basis for $\mathfrak{R}(\mathcal{C})$ and $V_2$ complete a basis for $\mathbb{R}^n$, then after similarity transform with $\bar{x}=V^{-1}x$, we can partition the system like following:<br>$$\begin{align*}\dot{\bar{x}}&amp;=\bar{A}\bar{x}+\bar{B}u&amp;=&amp;\begin{bmatrix}\bar{A}_ {11}&amp;\bar{A}_ {12} \\ \mathbf{0} &amp; \bar{A} _{22}\end{bmatrix}\begin{bmatrix}\bar{x} _1 \\ \bar{x} _2\end{bmatrix}+\begin{bmatrix}\bar{B} _1 \\ \mathbf{0}\end{bmatrix}u \\ y&amp;=\bar{C}\bar{x}+Du&amp;=&amp; \begin{bmatrix}\bar{C} _1 &amp; \bar{C} _2\end{bmatrix}\begin{bmatrix}\bar{x} _1 \\ \bar{x} _2\end{bmatrix} + Du\end{align*}$$<br>Here $\bar{x}_2$ is uncontrollable, and ZSR of the system with or without $\bar{x}_2$ is the same.</p></li><li><p><strong>Observability Decomposition</strong>: For an unobservable LTI system, define matrix $U=\begin{bmatrix}U_1\\U_2\end{bmatrix}$ where $U_1$ is a basis for $\mathfrak{R}(\mathcal{O}^\top)$ and $U_2$ complete a basis for $\mathbb{R}^n$, then after similarity transform with $\hat{x}=Ux$, we can partition the system like following:<br>$$\begin{align*}\dot{\hat{x}}&amp;=\hat{A}\hat{x}+\hat{B}u&amp;=&amp;\begin{bmatrix}\hat{A}_ {11}&amp;\mathbf{0} \\ \hat{A}_ {21} &amp; \hat{A} _{22}\end{bmatrix}\begin{bmatrix}\hat{x} _1 \\ \hat{x} _2\end{bmatrix}+\begin{bmatrix}\hat{B} _1 \\ \hat{B} _2\end{bmatrix}u \\ y&amp;=\hat{C}\hat{x}+Du&amp;=&amp; \begin{bmatrix}\hat{C} _1 &amp; \mathbf{0}\end{bmatrix}\begin{bmatrix}\hat{x} _1 \\ \hat{x} _2\end{bmatrix} + Du\end{align*}$$</p></li><li><p><strong>Realization</strong>: $\Sigma$ (system with Eq.3 and Eq.4) is a realization of $H(s)$ iff $H(s)=C(sI-A)^{-1}B+D$.</p><ul><li><strong>Equivalent</strong>: Two realizations are said to be equivalent if they have the same transfer function</li><li><strong>Algebraically Equivalent</strong>: Two realizations have same transfer function and $n$ (dimension of states). (this implies a similarity transform between them)</li><li><strong>Minimal Realization</strong>: $\Sigma$ is a minimal realization of $H(s)$ iff there doesn’t exists an equivalent realization $\bar{\Sigma}$ with $\bar{n}&lt; n$</li></ul></li><li><p>$\Sigma$ is a minial realization iff $\Sigma$ is completely controllable and observable.</p></li><li><p><strong>Kalman Cannonical Structure Theorem</strong> (aka. Kalman Decomposition): suppose $\rho(\mathcal{C})&lt; n$ and $\rho(\mathcal{O})&lt; n$, $\mathfrak{R}(\mathcal{C})$ are the controllable states, $\mathfrak{N}(\mathcal{O})$ are the unobservable states, define subspaces:</p></li></ul><table><thead><tr><th style="text-align:left">Subspaces</th><th style="text-align:center">Controllable</th><th style="text-align:center">Observable</th></tr></thead><tbody><tr><td style="text-align:left">$V_2\equiv\mathfrak{R}(\mathcal{C})\cup\mathfrak{N}(\mathcal{O})$</td><td style="text-align:center">Yes</td><td style="text-align:center">No</td></tr><tr><td style="text-align:left">$V_1$ s.t. $V_1\oplus V_2=\mathfrak{R}(\mathcal{C})$</td><td style="text-align:center">Yes</td><td style="text-align:center">Yes</td></tr><tr><td style="text-align:left">$V_4$ s.t. $V_4\oplus V_2=\mathfrak{N}(\mathcal{O})$</td><td style="text-align:center">No</td><td style="text-align:center">No</td></tr><tr><td style="text-align:left">$V_3$ s.t. $V_1\oplus V_2\oplus V_3\oplus V_4=\mathbb{C}^n$</td><td style="text-align:center">No</td><td style="text-align:center">Yes</td></tr></tbody></table><p>Then let $$\begin{align*}\tilde{x}=\begin{bmatrix}\tilde{x}_ 1\\ \tilde{x}_ 2\\ \tilde{x}_ 3\\ \tilde{x}_ 4\end{bmatrix},\;\tilde{A}=&amp;\begin{bmatrix}A_ {\mathrm{co}} &amp;&amp;A_{13}&amp;\\A_{21}&amp;A_{\mathrm{c\bar{o}}}&amp;A_{23}&amp;A_{24}\\&amp;&amp;A_{\mathrm{\bar{c}o}}&amp;\\&amp;&amp;A_{43}&amp;A_{\mathrm{\bar{c}\bar{o}}} \end{bmatrix},\;\tilde{B}=\begin{bmatrix}B_{\mathrm{co}}\\ B_{\mathrm{c\bar{o}}} \\ \mathbf{0} \\ \mathbf{0} \end{bmatrix} \\ \tilde{C}=&amp;\begin{bmatrix}C_{\mathrm{co}}&amp;\mathbf{0}\quad&amp;C_{\mathrm{\bar{c}o}}&amp;\mathbf{0}\quad\end{bmatrix}\end{align*}$$<br>$$\tilde{\Sigma}:\begin{cases} \dot{\tilde{x}}=A_{\mathrm{co}}\tilde{x}_ 1+B_{\mathrm{co}}u_1\\ y=C_{\mathrm{co}}\tilde{x}_1\end{cases}$$</p><p>$\tilde{\Sigma}$ is completely controllable and completely observable.</p><hr>Consider SISO systems $$H(s)=\frac{b(s)}{a(s)}=\frac{b_{n-1}s^{n-1}+\ldots+b_1s+b_0}{s^n+a_{n-1}s^{n-1}+\ldots+a_1s+a_0}=\frac{\sum^{n-1}_{j=0} b_js^j}{s^n+\sum^{n-1}_{i=0} a_is^i}=\frac{Y(s)}{U(s)}$$<ul><li><strong>Controllable Cannonical Form</strong>: $$\begin{align}\dot{x}&amp;=\begin{bmatrix} 0&amp;1&amp;&amp;\\ \vdots &amp; &amp; \ddots &amp; \\ 0&amp;&amp;&amp;1 \\-a_0&amp;-a_1&amp;\cdots&amp;-a_{n-1}\end{bmatrix}x+\begin{bmatrix}0\\ \vdots \\ 0 \\ 1\end{bmatrix}u&amp;=&amp;A_cx+B_cu\\ y&amp;=\begin{bmatrix}\quad b_0 &amp;\quad b_1 &amp;\cdots &amp; \quad b_{n-1}\end{bmatrix}x&amp;=&amp;C_cx \end{align}$$<ul><li>$(A_c, B_c)$ is controllable</li><li>$(A_c, C_c)$ is observable if $a(s)$ and $b(s)$ have no common factors</li></ul></li><li><strong>Observable Cannonical Form</strong>: $$\begin{align}\dot{x}&amp;=\begin{bmatrix} 0&amp;&amp;&amp;&amp;-a_0\\ 1 &amp; \ddots &amp;&amp;&amp;-a_1 &amp; \\ &amp;\ddots&amp;\ddots&amp;&amp;\vdots \\&amp;&amp;\ddots&amp;0&amp;-a_{n-2} \\ &amp;&amp;&amp;1&amp;-a_{n-1}\end{bmatrix}x+\begin{bmatrix}b_0\\ b_1 \\ \vdots \\ b_{n-2} \\ b_{n-1}\end{bmatrix}u&amp;=&amp;A_ox+B_ou\\ y&amp;=\begin{bmatrix}0 &amp; \;\cdots &amp;\;\cdots &amp; 0 &amp; \quad 1\qquad \end{bmatrix}x&amp;=&amp;C_ox \end{align}$$<ul><li>$(A_o, C_o)$ is observable</li><li>$(A_o, B_o)$ is controllable if $a(s)$ and $b(s)$ have no common factors</li></ul></li><li><strong>Model Cannonical Forms</strong>: Do <strong>Spectral Decomposition</strong> (eigen-decomposition) or Jordan Decomposition, and then use the modal matrix (matrix of eigenvectors) to do similarity transform.</li><li><strong>The Gilbert Realization</strong>: Let $G(s)$ be a $p\times m$ rational transfer function with simple poles (nonrepeated) at $\lambda_i,\;i=1,2,\ldots,k$. Calculate partial fraction expansion $$G(s)=\sum^k_{i=1}\frac{R_i}{s-\lambda_i},\qquad \text{Residue}\;R_i=\lim_{s\to\lambda_i}(s-\lambda_i)G(s)$$ Let $r_i=\rho(R_i)$, now write $R_i=C_iB_i$ where $C_ i\in\mathbb{R}^ {p\times r_ i},\;B_ i\in\mathbb{R}^ {r_ i\times p}$, then write $$A=\mathrm{blkdiag}\{\lambda_i I_{r_i}\},\;B^\top=[B_1^\top \;\cdots\; B^\top_k],\;C=[C_1\; \cdots\;C_k]$$, then $(A,B,C)$ is a realization of $G(s)$ with order $n=\sum^k_1 r_i$</li></ul><p>For MIMO system the cannonical forms with be quite complex:</p><ul><li><strong>Controllable Cannonical Form (for MIMO)</strong>: Here we provide a way to convert from controllable LTI system to controllable. The collection of independent columns of $\mathcal{C}$ may be expressed as $$M=[b_1\;Ab_1\; \cdots\;A^{\mu_1-1}b_1\;|\;b_2\;Ab_2\;\cdots\;A^{\mu_2-1}b_2\;|\;\cdots\;|\;b_p\;Ab_p\;\cdots\;A^{\mu_p-1}b_p]$$ Construct $M^{-1}$ and then $T$:$$M^{-1}=\left[m_{11}^\top\;m_{12}^\top\;\cdots\;m_{1\mu_1}^\top\;\middle|\;\cdots\;\middle|\;m_{p1}^\top\;m_{p2}^\top\;\cdots\;m_{p\mu_p}^\top \right]^\top$$ $$T=\left[m_{1\mu_1}^\top\;(m_{1\mu_1}A)^\top\;\cdots\;\left(m_{1\mu_1}A^{\mu_1-1}\right)^\top\;\middle|\;\cdots\;\middle|\; m_{p\mu_p}^\top\;(m_{p\mu_p}A)^\top\;\cdots\;\left(m_{p\mu_p}A^{\mu_p-1}\right)^\top\right]^\top$$<br>Perform similarity transform with $\bar{x}=Tx$ and the canonical form will be obtained like following:<br>$$\bar{A}=\begin{bmatrix}\bar{A}_ {\mu_1\times\mu_1}&amp;\mathbf{0}_ {\cdot\cdot}&amp;\cdots&amp;\mathbf{0}_ {\cdot\cdot} \\ \mathbf{0}_ {\cdot\cdot}&amp;\bar{A}_ {\mu_2\times\mu_2}&amp;\cdots&amp;\mathbf{0}_ {\cdot\cdot}\\ \vdots&amp;\vdots&amp;\ddots&amp;\vdots \\ \mathbf{0}_ {\cdot\cdot}&amp;\mathbf{0}_ {\cdot\cdot}&amp;\cdots&amp;\bar{A}_ {\mu_ p\times\mu_ p} \end{bmatrix},\quad\bar{B}=\begin{bmatrix}\mathbf{0}_ {\cdot n}\\ \mathbf{0}_ {\cdot (n-1)}\\ \vdots\\ \mathbf{0}_ {\cdot 1}\end{bmatrix}$$<br>Here $\bar{A}_ {\mu_ i\times\mu_ i}$ is the same structure as in SISO, $\mathbf{0}_ {\cdot\cdot}$ is a zero matrix except the last row, $\mathbf{0}_ {\cdot i}$ is a zero matrix except for the last row, and in the last row there are $i$ non-zeros on the right with the first element being 1.</li></ul><h1 id="System-Performance">System Performance<a class="header-anchor" href="#System-Performance"> ❮</a></h1><ul><li><p><strong>System Characteristic Equation</strong>: The polynomical with the roots equal to the poles of the output that are independent of the input.</p></li><li><p><strong>System Type</strong>: A plant $G$ can always be written as $G(s)=\frac{K\prod^m_{i=1}(s-s_i)}{s^N\prod^p_{j=1}(s-s_j)},\;z_i,z_j\neq 0$ or $G(z)=\frac{K\prod^m_{i=1}(z-z_i)}{(z-1)^N\prod^p_{j=1}(z-z_j)},\;z_i,z_j\neq 1$. Here $N$ is called the system type of $G(z)$.</p></li><li><p>Properties that matters for a controller:</p><ol><li>Stability</li><li>Steady state accuracy</li><li>Transient response</li><li>Sensitivity</li><li>Exogenous disturbance rejection</li><li>Bounded control effort</li></ol></li></ul><h2 id="Stability">Stability<a class="header-anchor" href="#Stability"> ❮</a></h2><ul><li><strong>Stability</strong> means when the time goes to infinity, the system response is bounded.<ul><li>A system is <strong>stable</strong> if all its poles lies in the left half of $s$-plane or all inside the unit circle of $z$-plane.</li><li>A system is <strong>marginally stable</strong> if one of the pole is on the imaginary axis of $s$-plane or on the unit circle of $z$-plane.</li></ul></li><li>Stability of linear systems is independent of input<ul><li>The stability of a linear system can be evaluated by its characteristic equation $1-G_{op}(z)=0$, where $G_{op}$ is the open-loop transfer function (transfer function when input is eliminated, or feedback route is cut off).</li></ul></li><li>Methods to evaluate stability<ul><li><strong>Routh-Hurwitz Criterion</strong>: $s$-plane (omited here, see <a href="https://en.wikipedia.org/wiki/Routh%E2%80%93Hurwitz_stability_criterion" target="_blank" rel="noopener">Wikipedia</a>)<ul><li>For discrete system, a strategy is use bilinear transformation: $z=e^{\omega T}\approx \frac{1+\omega T/2}{1-\omega T/2}$</li></ul></li><li><strong>Jury Criterion</strong>: $z$-plane (see <a href="https://en.wikipedia.org/wiki/Jury_stability_criterion" target="_blank" rel="noopener">Wikipedia</a>)</li><li><strong>Root Locus Method</strong>: both $s$- and $z$-plane (see <a href="https://en.wikipedia.org/wiki/Root_locus" target="_blank" rel="noopener">Wikipedia</a>, <code>rlocus</code> in MATLAB)</li><li><strong>Nyquist Criterion</strong>: both $s$- and $z$-plane (see <a href="https://en.wikipedia.org/wiki/Nyquist_stability_criterion" target="_blank" rel="noopener">Wikipedia</a>, <code>nyquist</code> in MATLAB)<ul><li>It works for both continuous and discrete systems, the difference is that in $s$-plane the detour point is at $s=0$ while in $z$-plane the detour point is at $z=1$.</li></ul></li><li><strong>Bode Diagrams</strong>: draw frequency response for (pulse) transfer function, works for both $s$- and $z$-plane (see <a href="https://en.wikipedia.org/wiki/Bode_plot" target="_blank" rel="noopener">Wikipedia</a>, <code>bode</code> in MATLAB)</li></ul><blockquote><p>A review of the stability judgement method <a href="https://www.zhihu.com/question/60272694" target="_blank" rel="noopener">here at 知乎</a></p></blockquote></li></ul><h2 id="Lyaponov-Stability">Lyaponov Stability<a class="header-anchor" href="#Lyaponov-Stability"> ❮</a></h2><blockquote><p>Lyaponove Stability is only concerned with the effect of initial conditions on the response of the system (ZIR)</p></blockquote><ul><li><strong>Equilibrium Point</strong> $x_e$: Consider NLTV $\dot{x}(t)=f(x(t),u(t),t)$, equilibrium point satisfies $x(t_0)=x_e,\;u(t)\equiv 0\Rightarrow x(t)=x_e,\;\text{i.e. }f(x_e,0,t)=0,\;\forall t&gt;t_0$<ul><li>For discrete system, it’s $x(k+1)=x(k)=x_e$</li><li>For LTI system, $x_e$ can be calculated from $Ax_e=0$, so the origin $x=0$ is always an equilibrium point.</li><li>Set of equilibrium points in LTI systems are connected.</li></ul></li><li><strong>Lyapunov stability</strong>: An equilibrium point $x_e$ of the system $\dot{x}=A(t)x$ is <strong>stable (in the sense of Lyapunov)</strong> iff $\forall \epsilon&gt;0,\;\exists \delta(t_0,\epsilon)&gt;0$ s.t. $\Vert x(t_0)-x_e\Vert&lt;\delta\Rightarrow\Vert x(t)-x_e\Vert &lt;\epsilon,\;\forall t&gt;t_0$<ul><li>$x_e$ is <strong>uniformly stable</strong> if $\delta=\delta(\epsilon)$ (regardless of $t_0$)</li><li>$x_e$ <em>in LTI</em> is stable $\Rightarrow x_e$ is uniformly stable</li><li>$x_e$ is <strong>asymptotically stable</strong> if $\Vert x(t)-x_e\Vert\to 0$ as $t\to 0$</li><li>$x_e$ is <strong>exponentially stable</strong> if $\Vert x(t)-x_e\Vert \leqslant \gamma e^{-\lambda(t-t_0)}\Vert x(t_0)-x(e)\Vert$</li><li>$x_e$ <em>in LTI</em> is asymptotically stable $\Rightarrow x_e$ is exponentially stable</li><li>$x_e$ is <strong>globally stable</strong> if $\delta$ can be chosen arbitrarily large</li></ul></li><li>For LTV system, the system is stable (the zero solution is stable) iff $\Phi(t,t_0)$ is bounded by $K(t_0)$.<ul><li>If bounded by constant $K$, then the system is uniformly stable.</li><li>If bounded by constant $K$ and $\Vert\Phi(t,0)\Vert\to 0$ as $t\to 0$, then the system is asymptotically stable.</li></ul></li><li>For LTI system $\dot{x}=Ax$, it is Lyapunov stable iff $\mathrm{Re}(\lambda_i)\leqslant 0$ or $\mathrm{Re}(\lambda_i)=0,\;\eta_i=1$. ($\eta_i$ is the multiplicity of $\lambda_i$)<ul><li>If $\mathrm{Re}(\lambda_i)&lt;0$, then the system is asymptotically stable</li></ul></li><li>Internal stability: concerns the state variables</li><li>External stability: concerns the output variables</li></ul><blockquote><p>Notes for contents below:</p><ul><li>Positive definite (pd.) function: function $V$ is pd. wrt. $p$ if $V(x)&gt;0,\;x\neq p$ and $V(x)=0,\;x=p$</li><li>$C^n$ denotes the set of continuous and at least n-th differentiable functions</li></ul></blockquote><ul><li><p><strong>Lyapunov’s Direct Method</strong>: Let $\mathcal{U}$ be an open neighborhood of $p$ and let $V:\mathcal{U}-&gt;\mathbb{R}$ be a countinuous positive definite $C^1$ function wrt. $p$, we have following two conclusions:</p><ol><li>If $\dot{V}\leqslant 0$ on $\mathcal{U}\backslash\{p\}$ then $p$ is a stable fixed point of $\dot{x}=f(x)$</li><li>If $\dot{V}&lt; 0$ on $\mathcal{U}\backslash\{p\}$ then $p$ is an asymptotically stable fixed point of $\dot{x}=f(x)$</li></ol></li><li><p><strong>Lyapunov Function</strong>:</p><ul><li>A function satisfying conclusion 1 is called a Lyapunov function</li><li>A function satisfying conclusion 2 is called a <strong>strict</strong> Lyapunov function</li><li>A function that is $C^1$ and pd. is called a Lyapunov function candidate</li></ul><blockquote><p>The energy function usually can be used as Lyapunov function. If it’s only semi-positive definite, one can use <a href="https://en.wikipedia.org/wiki/LaSalle%27s_invariance_principle" target="_blank" rel="noopener">LaSalle’s Theorem</a></p></blockquote></li><li><p>For LTI system, the zero solution of $\dot{x}=Ax$ is asymptotically stable iff $\forall$ pd. hermitian matrices $Q$, equation $A^*P+PA=-Q$ has a unique hermitian solution $P$ that is positive definite.</p><ul><li>$A^*P+PA=-Q$ is called Lyapunov’s Matrix Equation</li></ul><blockquote><p>here $V(x)=x^* Px=\int^\infty_0 x^*(t)Qx(t)dt$, which can be also called cost-to-go, or generalized energy</p></blockquote></li><li><p><strong>Lyapunov’s Indirect Method</strong> (Lyapunov’s First Method / Lyapunov’s Linearization Theorem): The nonlinear system $\dot{x}=f(x)$ is (locally) asymptotically stable near the equilibrium point $x_e$ if the linearized system $\dot{x}_L=\frac{\partial f}{\partial x}(x_e)x_L$ is asymptotically stable.</p></li></ul><h2 id="Bounded-Input-Bounded-Output-Stability">Bounded-Input Bounded-Output Stability<a class="header-anchor" href="#Bounded-Input-Bounded-Output-Stability"> ❮</a></h2><blockquote><p>BIBO stability is only concerned with the response of the system to the input (ZSR).</p></blockquote><ul><li><strong>Bounded-Input Bounded-Output (BIBO) stability</strong>: The LTV system is said to be (uniformly) BIBO stable if there exists a finite constant $g$ s.t. $\forall u(\cdot)$, its forced response $y_f(\cdot)$ satisfies $$ \sup_{t\in[0,\infty)}\Vert y_f(t)\Vert \leqslant g \sup_{t\in[0,\infty)} \Vert u(t)\Vert $$<blockquote><p>The impulse response can be analyzed to assess BIBO stability<br>The LTV system is uniformly BIBO stable iff every entry of $D(t)$ is bounded and $\sup_{t\geqslant 0}\int^t_0|g_{ij}(t,\tau)|d\tau &lt;\infty$ for every entry $g_{ij}$ of the matrix $C(t)\Phi(t,\tau)B(\tau)$.</p></blockquote></li><li>BIBO stability is related with the stability descibed in classical control theory.</li><li>Exponential Lyapunov Stability $\Rightarrow$ BIBO stability</li></ul><h2 id="Steady-State-Accuracy">Steady State Accuracy<a class="header-anchor" href="#Steady-State-Accuracy"> ❮</a></h2><p>Steady state accurary can be derived from the property of Laplace/Z-transform as mentioned above (assuming stability)<br>$$\lim\limits_{t\to \infty} f(t) = \lim\limits_{z\to 1} (z-1)F(z) = \lim\limits_{s\to 0}sF(s)$$</p><h2 id="Transient-Response">Transient Response<a class="header-anchor" href="#Transient-Response"> ❮</a></h2><p>Some measurements of transient response (with step input):</p><ul><li><strong>Rise time</strong> $t_r$: time from 10% to 90% of steady state value</li><li><strong>Peak overshoot</strong>: $M_p$ for overshoot magnitude and $t_p$ for time</li><li><strong>Settling time</strong> $t_s$: time after which the magnitude fall in $1-d$ to $1-d$ final value. $d$ is usually %2~5.</li></ul><h2 id="Sensitivity">Sensitivity<a class="header-anchor" href="#Sensitivity"> ❮</a></h2><p>Given a transfer function $H(z)$ with parameter $\Theta\in\mathbb{R}$, then sensitivity is defined as $S_H=\frac{\partial H}{\partial \Theta}\cdot\frac{\Theta}{H} = \frac{\partial H/H}{\partial \Theta/\Theta}$</p><h1 id="Discretization-and-Linearization">Discretization and Linearization<a class="header-anchor" href="#Discretization-and-Linearization"> ❮</a></h1><h2 id="Discretization-Example">Discretization Example<a class="header-anchor" href="#Discretization-Example"> ❮</a></h2><p>The following image shows a minial example of sampling and hold.</p><img src="/blog/2020-06/ControlSystemNotes/snh.png" title="A minimal example of a conversion process with analog and digital signals"><h2 id="Sampling-A-D">Sampling (A/D)<a class="header-anchor" href="#Sampling-A-D"> ❮</a></h2><ul><li>Ideal sampler (a.k.a impulse modulator) converts a continuous signal $e: \mathbb{R}_+ \to \mathbb{R}$ to a discrete one $\hat{e}: \mathbb{N}\to \mathbb{R}$, such that $$ \hat{e}=e(t)\delta(t-kT)=e(t)\delta_T(t); \forall k\in \mathbb{N} $$<ul><li>Ideal sampler is actually applying starred transform.</li></ul></li><li>How to sample? A rule of thumb used to select sampling rates is chosing a rate of at least 5 samples per time constant.<ul><li>The $\tau$ appearing in the transient response term $ke^{-t/\tau}$ of a first order analog system is called the time constant.</li><li>If the sampling time is too large, it can make the system unstable.</li></ul></li></ul><h2 id="Reconstruction-Hold-D-A">Reconstruction/Hold (D/A)<a class="header-anchor" href="#Reconstruction-Hold-D-A"> ❮</a></h2><ul><li>Zero order hold (ZOH): $ZOH(\{e(k)\}_{k\in\mathbb{N}})(t) = e(k)\;for\;kT\leq t\leq (k+1)T$<ul><li>Alternative form: $ZOH(\{e(k)\})=\sum^\infty_{k=0}e(k)(H(t-kT)-H(t-(k+1)T))$</li><li>Its Laplace Transform: $G_{ZOH}(s)=\frac{1-e^{-Ts}}{s}$</li></ul></li><li>First order hold (FOH): (delayed version) $$FOH(\{e(k)\}_ {k\in\mathbb{N}})(t)=\sum_ {k\in\mathbb{N}}\left[e(kT)+\frac{t-kT}{T}(e(kT)-e((k-1)T)) \right]\left[H(t-kT)-H(t-(k+1)T) \right]$$</li></ul><h2 id="State-Space-Representation-2">State Space Representation<a class="header-anchor" href="#State-Space-Representation-2"> ❮</a></h2><p>Suppose we are given the LTI continuous system<br>$$\begin{align*} \dot{x}(t) &amp;= Ax(t)+Bu(t) \\ y(t)&amp;=Cx(t)+Du(t) \end{align*}$$<br>If the input is sampled and ZOH and the output is sampled, then<br>$$\begin{align*} x(k+1)&amp;=\bar{A}x(k)+\bar{B}u(k) \\ y(k)&amp;=\bar{C}x(k)+\bar{D}u(k)\end{align*}$$<br>where $\bar{A}=\Phi((k+1)T,kT)=e^{AkT}$<br><br>$\bar{B}=\int^{(k+1)T}_{kT}\Phi((k+1)T,\tau)B\mathrm{d}\tau=A^{-1}(e^{AT}-I)$<br><br>$\bar{C}=C$ and $\bar{D}=D$</p><blockquote><p>Steps to apply conversion:</p><ol><li>Dervice SS model for analog system</li><li>Calculate discrete representation (<code>c2d</code> in MATLAB)</li><li>Calculate pulse transfer function (<code>ss2tf</code> in MATLAB)<br>If there is a complex system with multiple sampling and holding, a general rule is</li></ol><ul><li>Each ZOH output is assumed to be an input</li><li>Each sampler input is assumed to be an output<br>and then create continuous state space from analog part of the system, then discretize them to generate discrete equations</li></ul></blockquote><h2 id="s-plane-and-z-plane">$s$-plane and $z$-plane<a class="header-anchor" href="#s-plane-and-z-plane"> ❮</a></h2><p>When converting $s$ to $z$, the complex variables are related by $z=e^{Ts}$. Suppose $s=\sigma+j\omega$, then $z=e^{T\sigma}\angle \omega T$</p><blockquote><p>Note: if frequencies differ in integer multiples of the sampling frequency $\frac{2\pi}{T}=\omega_s$, then they are sampled into the same location in the $z$-plane.</p></blockquote><p>For transient response relationship, suppose $s$-plane poles occur at $s=\sigma\pm j\omega$, then the transient response if $Ae^{\sigma t}\cos(\omega t+\varphi)$. When sampling occurs at $z$-plane poles, then the transient response if $Ae^{\sigma kT}\cos(\omega kT+\varphi)$.</p><details><summary>Example: 2nd order transfer function</summary>$$G(s)=\frac{\omega_n^2}{s^2+2\xi\omega_ns+\omega_n^2}$$ The $z$-plane poles occur at $z=r\angle\pm\theta$ where $r=e^{-\xi\omega_n T}$ and $\theta=\omega_n T\sqrt{1-\xi^2}$.<p>Then we can get the inverse relationship</p><ul><li>$\xi=-\ln( r)/\sqrt{\ln^2( r)+\theta^2}$</li><li>$\omega_n=(1/T)\sqrt{\ln^2( r)+\theta^2}$</li><li>(time constant) $\tau=-T/\ln( r)$</li></ul></details><h2 id="Linearization">Linearization<a class="header-anchor" href="#Linearization"> ❮</a></h2><ul><li><strong>Jacobian Linearization</strong>: linearize $\dot{x}=f(x,u)$ at an equilibrium $(x_e, u_e)$ is $$\frac{\mathrm{d}z}{\mathrm{d}t}=Az+Bv,\quad\text{where}\;A=\left.\frac{\mathrm{d}f}{\mathrm{d}x}\right|_ {\begin{split}x=x_e\\u=u_e\end{split}},\;B=\left.\frac{\mathrm{d}f}{\mathrm{d}u}\right|_ {\begin{split}x=x_e\\u=u_e\end{split}},\;z=(x-x_e),\;v=(u-u_e)$$<ul><li>change $(x_e, u_e)$ to a trajectory $(x_e(t), u_e(t))$ we can linearize the system about a trajectory.</li></ul></li></ul><h1 id="Controllers">Controllers<a class="header-anchor" href="#Controllers"> ❮</a></h1><h2 id="Full-State-Feedback">Full State Feedback<a class="header-anchor" href="#Full-State-Feedback"> ❮</a></h2><blockquote><p>This method can be used for both continuous and discrete systems, just make sure to use corresponding method for choosing correct closed-loop transfer function.</p></blockquote><p>For state space systems, with access to all of the state variables, we can change the $A$ matrix and thereby change the system dynamics by feedback.</p><p>Consider SISO LTI system ($u\in\mathbb{R},y\in\mathbb{R}$), we define the input as $u\equiv Kx+Ev$ where $K\in\mathbb{R}^{1\times n},\;E\in\mathbb{R}$ is an input matrix and $v(t)\in\mathbb{R}^\mathbb{R}$ is the exogeneous (externally applied) input. The new system will be $$\begin{align}\dot{x}&amp;=(A+BK)x+BEv\\y&amp;=(C+DK)x+DEv\end{align}$$<br>The mission is to find a state update matrix $A_{\mathrm{CL}}\equiv A+BK$ with desired set of eigenvalues, therefore we can construct $A_{\mathrm{CL}}$ with specific eigenvalues and then calculate $K$. This process will be quite easy if the system is already in controllable cannonical form. (which can be constructed directly from transfer function or using similarity transform)</p><p>Another way (SISO only) to calculate $K$ without controllable cannonical form is using the following formulae given the desired characteristic polynomial $\phi^{\star}(s)=s^n+\sum^{n-1}_ {i=0} a^\star_i s^i$ and original characteristic polynomial $\phi(s)=s^n+\sum^{n-1}_ {i=0} a_i s^i$</p><ul><li><strong>Ackermann’s Formula</strong>: $K=-e^\top_n\mathcal{C}^{-1}\phi^\star(A)$ (here $e_i$ is unit vector with 1 at i-th position)</li><li><strong>Bass-Gura’s Formula</strong>: $$K=-[(a^\star_{n-1}-a_{n-1}) \;\cdots\;(a^\star_0-a_0)]\begin{bmatrix}1&amp;a_{n-1}&amp;a_{n-2}&amp;\cdots&amp;a_1\\&amp;1&amp;a_{n-1}&amp;\cdots&amp;a_2\\ &amp;&amp;\ddots&amp;\ddots&amp;\vdots \\ &amp;&amp;&amp;1&amp;a_{n-1}\\ &amp;&amp;&amp;&amp;1\end{bmatrix}^{-1}\mathcal{C}^{-1}$$</li></ul><p>Note that the zeros of transfer function will not be affected by state feedback.</p><h2 id="State-Estimation-Observer-Design">State Estimation (Observer Design)<a class="header-anchor" href="#State-Estimation-Observer-Design"> ❮</a></h2><blockquote><p>Some times we don’t have the direct access to the state, we need construct an observer<br>For stochastic version, please check <a href="/blog/2019-03/StochasticSystemNotes/#Observation-Filtering">my notes for stochastic system</a></p></blockquote><p>Assume a plant $\Sigma$ and an (<strong>Luenberger</strong>) <strong>observer</strong> $\hat{\Sigma}$:<br>$$\Sigma:\begin{cases}\dot{x}=Ax+Bu\\ y=Cx\end{cases},\quad \hat{\Sigma}:\begin{cases} \dot{\hat{x}}=A\hat{x}+Bu+L(y-\hat{y})\\ y=C\hat{x}\end{cases}$$</p><p>Subtract observer dynamics from plant dynamics and define $e\equiv x-\hat{x}$, the dynamics for $e$ is $\dot{e}=(A-LC)e$ and $y-\hat{y}=Ce$. This error dynamic $A_e=A-LC$ can be easily changed with observable cannonical form. (which similarly can be constructed directly from transfer function or using similarity transform)</p><ul><li><strong>Reduced-order Observer</strong>: If the state length of the system $n$ is large while $n-p$ is small, split the system and let $x_1$ holds the states that can be measured directly while $x_2$ holds states that are to be estimated, (i.e. $y=x_1+Du$). Define $z=\hat{x}_2-Lx_1$ then the system runs like $$\begin{align*}\begin{bmatrix}x_1 \\ \hat{x}_2\end{bmatrix}&amp;=\begin{bmatrix} y-Du\\ z+Lx_1 \end{bmatrix}\qquad\begin{split}&amp;\text{measurement} \\ &amp;\text{observer}\end{split} \\ u&amp;=K\begin{bmatrix}x_1 \\ \hat{x}_2 \end{bmatrix} + v \qquad\text{control law}\end{align*}$$ And then the error we care about is only $e=x_2-\hat{x}_2$.</li><li><strong>Ackermann’s Formula</strong>: $L=\phi^\star(A)\mathcal{O}^{-1}e_n$ ($\phi^\star$ is the desired characteristic function for $A_e$)</li><li><strong>Separation Principle</strong>: If a stable observer and stable state feedback are designed for an LTI system, then the combined observer and feedback will be stable.</li></ul><blockquote><p>Errors from state estimation</p><ol><li>Inaccurate knowledge of $A$ and $B$</li><li>Initial condition uncertainty</li><li>Disturbance or sensor error<br>It’s advised to choose observer poles to be 2-4x faster than closed loop poles</li></ol></blockquote><h2 id="LQR"><a href="https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic_regulator" target="_blank" rel="noopener">LQR</a><a class="header-anchor" href="#LQR"> ❮</a></h2><blockquote><p>Motivation: handle control constraints and time varying dynamics with performance metric (ideas of optimal control)<br>Note: $x^\top Ax$ is called a <strong>quadratic form</strong>, $x^\top Ay$ is called a <strong>bilinear form</strong></p></blockquote><ul><li>A quadratic function $f(x)=x^\top Dx+C^\top x+c_0$ has one minimizer iff $D\succ 0$, or multiple minimizers iff $D\succeq 0$.</li><li>(discrete finite time) <strong>Linear Quadratic Regulator</strong> (LQR): the control problem is defined as $$\begin{align*}\min_{u\in\left(\mathbb{R}^m\right)^{\{0,\ldots,N\}}} J_{N}(u,x_0)&amp;=\frac{1}{2}\sum^{N}_{k=0}(x^\top(k)Q(k)x(k)+u^\top(k)R(k)u(k)) \\ \mathrm{s.t.}\qquad x(k+1) &amp;= A(k)x(k) + B(k)u(k)\quad \forall k\in\{0,\ldots,N-1\}\\ y(k)&amp;=C(k)x(k)\\ x(0)&amp;=x_0\end{align*}$$ where $Q(k)\succ 0$ and $R(k)\succ 0$</li><li><strong>Bellman’s Principle of Optimality</strong>: If a closed loop control $u^\star$ is optimal over the interval $0\leqslant k\leqslant N$, it’s also optimal over any subinterval $m\leqslant k\leqslant N$ where $m\in\{0,\ldots,N\}$</li><li><strong>The Minimum Principle</strong>: The optimal input to the LQR problem satisfies the following backward equations: $$\begin{align*}u^\star(k)&amp;=-K(x)x(k) \\ K(k)&amp;=\left[B^\top(k) P(k+1)B(k)+\frac{1}{2}R(k)\right]^{-1}B^\top(k)P(k+1)A(k) \\ P(k)&amp;=A^\top(k)P(k+1)[A(k)- B(k)K(k)]+\frac{1}{2}Q(k)\end{align*}$$ and $P(N)=Q(N),\;K(N)=0$. The optimal cost is $J^\star_N=x^\top(0)P(0)x(0)$</li><li>For infinite horizon, $K(k)$ start becoming constants. The optimal input for LQR problem (assuming the system became LTI when $N\to\infty$) is $u^*(k)=-Kx(k)$ where $$K=(B^\top PB+R/2)^{-1}B^\top PA$$ and $P\succ 0$ is the unique solution to the discrete-time <strong>algebraic Riccati Equation</strong>: $$P=A^\top PA-A^\top PB\left(B^\top PB+R/2\right)^{-1}B^\top PA+Q/2$$</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;&lt;ul&gt;&lt;li&gt;This note combines content from ME 564 Linear Systems and ME 561 Discrete Digital Control&lt;/li&gt;&lt;li&gt;Please read &lt;a href=&quot;/blog/2020-06/AlgebraBasicsNotes/&quot; title=&quot;the Algebra Basics notes&quot;&gt;the Algebra Basics notes&lt;/a&gt; first if you are not familiar with related concepts.&lt;/li&gt;&lt;li&gt;In this note, $f\in\mathbb{F}^\mathbb{G}$ stands for a function with domain in $\mathbb{G}$ and co-domain in $\mathbb{F}$, i.e. $f:\mathbb{F}\to\mathbb{G}$, $H(x)$ generally stands for Heaviside function (step function)&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;h1 id=&quot;Transforms&quot;&gt;Transforms&lt;a class=&quot;header-anchor&quot; href=&quot;#Transforms&quot;&gt; ❮&lt;/a&gt;&lt;/h1&gt;&lt;h2 id=&quot;Laplace-Transform&quot;&gt;Laplace Transform&lt;a class=&quot;header-anchor&quot; href=&quot;#Laplace-Transform&quot;&gt; ❮&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Definition: $F(s)=\mathcal{L}\{f(t)\}(s)=\int^\infty_0 f(t)e^{-st}\mathrm{d}t$&lt;blockquote&gt;&lt;p&gt;Note that the transform is not well defined for all functions in $\mathbb{C}^\mathbb{R}$. And the transform is only valid for $s$ in a region of convergence, which is usually separated by 0.&lt;/p&gt;&lt;/blockquote&gt;&lt;/li&gt;&lt;li&gt;Laplace Transform is a linear map from $(\mathbb{C}^\mathbb{R}, \mathbb{C})$ to $(\mathbb{C}^\mathbb{C}, \mathbb{C})$ and it’s one-to-one.&lt;/li&gt;&lt;li&gt;Properties: (see &lt;a href=&quot;https://en.wikipedia.org/wiki/Laplace_transform&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Wikipedia&lt;/a&gt; or &lt;a href=&quot;https://lpsa.swarthmore.edu/LaplaceZTable/LaplacePropTable.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;this page&lt;/a&gt; for full list)&lt;ul&gt;&lt;li&gt;Derivative: $f’(t) \xleftrightarrow{\mathcal{L}} sF(s)-f(0^-)$&lt;/li&gt;&lt;li&gt;Integration: $\int^t_0 f(\tau)d\tau \xleftrightarrow{\mathcal{L}} \frac{1}{s}F(s)$&lt;/li&gt;&lt;li&gt;Delay: $f(t-a)H(t-a) \xleftrightarrow{\mathcal{L}} e^{-as}F(s)$&lt;/li&gt;&lt;li&gt;Convolution: $\int^t_0 f(\tau)g(t-\tau)\mathrm{d}\tau \xleftrightarrow{\mathcal{L}} F(s)G(s)$&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Stationary Value: $\lim\limits_{t\to 0} f(t) = \lim\limits_{s\to \infty} sF(s), \lim\limits_{t\to \infty} f(t) = \lim\limits_{s\to 0} sF(s)$&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://zyxin.xyz/blog/categories/Notes/"/>
    
      <category term="Control" scheme="http://zyxin.xyz/blog/categories/Notes/Control/"/>
    
    
      <category term="Math" scheme="http://zyxin.xyz/blog/tags/Math/"/>
    
      <category term="Control" scheme="http://zyxin.xyz/blog/tags/Control/"/>
    
  </entry>
  
  <entry>
    <title>终端的特殊控制符</title>
    <link href="http://zyxin.xyz/blog/2020-05/TerminalControlCharacters/"/>
    <id>http://zyxin.xyz/blog/2020-05/TerminalControlCharacters/</id>
    <published>2020-05-10T22:49:54.000Z</published>
    <updated>2020-05-10T22:50:05.182Z</updated>
    
    <content type="html"><![CDATA[<p>之前碰到过很多终端工具可以显示非常好看的进度条，或者显示丰富的颜色，甚至还有的直接可以在终端通过字符绘制UI（a.k.a. <a href="https://en.wikipedia.org/wiki/Text-based_user_interface" target="_blank" rel="noopener">TUI</a>），我一直都很好奇是怎么做到的。之后又知道了curses这个Python库和它的一些高层封装（例如asciimatics），然后最终在Stack Overflow里面查到了这些都是通过特殊的终端控制符来实现的。本文就介绍这些终端控制符的使用方法，他们很适合用来写一个简单无依赖的TUI。如果需要更复杂和全面的TUI功能，还是最好使用封装好的库。</p><h1 id="ASCII-控制符">ASCII 控制符<a class="header-anchor" href="#ASCII-控制符"> ❮</a></h1><p>在最开始接触编程的时候，如果你学的是C，那你一定很熟悉<code>\n</code>，这就是一个”换行“的转义字符，代表终端光标令起一行。有时你还会碰到<code>\r</code>，这是”回车“。“回车”这个名字来源于打字机时代，在使用打字机的时候，如果你需要新起一行，那么需要的操作是：转动滚筒把纸往外抽一行，再把字车（相当与打印机的打印头）移到最左端。这两个操作的名字分别是“换行”和“回车”。因此严格来说另起一行的字符串应该是<code>\r\n</code>，这也是Windows的标准，而在Unix中则简化成<code>\n</code>会自动执行回车。</p><a id="more"></a><p>换行和回车是两个非常常用的控制字符，也是定义在了ASCII表中的控制字符。在ASCII表中还定义了其他的控制字符，列在下面了。</p><table><thead><tr><th>ASCII名字</th><th>ASCII码</th><th>printf风格转义</th><th>用途</th></tr></thead><tbody><tr><td>BEL 铃声</td><td>0x07</td><td><code>\a</code></td><td>哔一下（执不执行取决于终端）</td></tr><tr><td>BS 退格</td><td>0x08</td><td><code>\b</code></td><td>*光标回退一格</td></tr><tr><td>ESC 退出</td><td>0x1B</td><td><code>\e</code></td><td>可代表按下ESC键，不是C标准</td></tr><tr><td>FF 换页</td><td>0x0C</td><td><code>\f</code></td><td>光标移到新一页</td></tr><tr><td>LF 换行</td><td>0x0A</td><td><code>\n</code></td><td>光标下移一行</td></tr><tr><td>CR 回车</td><td>0x0D</td><td><code>\r</code></td><td>光标回到行首</td></tr><tr><td>HT 水平制表</td><td>0x09</td><td><code>\t</code></td><td>标记水平制表位（Tab键）</td></tr><tr><td>VT 垂直制表</td><td>0x0B</td><td><code>\v</code></td><td>标记垂直制表位</td></tr><tr><td>NUL 空值</td><td>0x00</td><td><code>\0</code></td><td>代表啥也没有，C里面终结字符串</td></tr><tr><td>-</td><td>-</td><td>**<code>\c</code></td><td>终止输出，基本不被支持了</td></tr></tbody></table><p>*光标这里泛指各类终端的指示当前文本位置的东西，在打字机上叫“type guide”，在显示屏上里面叫“光标 cursor”，而在有些场合也叫指针。<br>**这个用法貌似只在一些终端中有，我也不确定它是否有对应一个字符。在<a href="http://www.gnu.org/software/coreutils/manual/html_node/printf-invocation.html" target="_blank" rel="noopener">GNU的文档</a>里有简短解释。</p><h1 id="ANSI-VT100-控制符（串）">ANSI/VT100 控制符（串）<a class="header-anchor" href="#ANSI-VT100-控制符（串）"> ❮</a></h1><p>很多终端都支持彩色文字的输出，而彩色文字的表达方式通常都参考ANSI的色彩标准。而ANSI用来实现色彩显示的转义表还定义了指针控制和设备管理的功能。</p><p>这一类控制符实际上是个字符串，所以应该叫控制串？他们都由<code>&lt;ESC&gt;</code>字符开头，也就是<code>0x1B</code>。所以我推测实际上<code>ESC</code>的双关（退出/转义）也被用到了这里哈哈。以下内容大部分来自 ANSI/VT100 Terminal Control Escape Sequences 表格，详细解释可以参考这个表格以及维基的页面。链接都放在引用部分。</p><blockquote><p><code>0x1B</code>在一些终端中会用<code>^[</code>代表，因此如果你看到了<code>^[[</code>那通常也都是通过这种方法转义的字符序列。</p></blockquote><p>我把这个表中能用于bash的字符都拎出来放在下面了。以下表中的转义序列名称都是我自己翻译的，我不知道有没有统一的中文翻译hhh。</p><h2 id="终端设备相关">终端设备相关<a class="header-anchor" href="#终端设备相关"> ❮</a></h2><table><thead><tr><th>名称</th><th>转义字符串</th></tr></thead><tbody><tr><td>查询设备码</td><td><code>&lt;ESC&gt;[c</code></td></tr><tr><td>报告设备码</td><td><code>&lt;ESC&gt;[{code}0c</code></td></tr><tr><td>查询光标位置</td><td><code>&lt;ESC&gt;[6n</code></td></tr><tr><td>报告光标位置</td><td><code>&lt;ESC&gt;[{ROW};{COLUMN}R</code></td></tr><tr><td>重置设备</td><td><code>&lt;ESC&gt;c</code></td></tr></tbody></table><blockquote><p>可以在你的终端里输入<code>printf &quot;\x1b[c&quot;</code>，看看会输出什么</p></blockquote><h2 id="光标控制">光标控制<a class="header-anchor" href="#光标控制"> ❮</a></h2><table><thead><tr><th>名称</th><th>转义字符串</th></tr></thead><tbody><tr><td>设置指针位置</td><td><code>&lt;ESC&gt;[{ROW};{COLUMN}H</code></td></tr><tr><td>指针上移</td><td><code>&lt;ESC&gt;[{COUNT}A</code></td></tr><tr><td>指针下移</td><td><code>&lt;ESC&gt;[{COUNT}B</code></td></tr><tr><td>指针前移（右移）</td><td><code>&lt;ESC&gt;[{COUNT}C</code></td></tr><tr><td>指针后移（左移）</td><td><code>&lt;ESC&gt;[{COUNT}D</code></td></tr><tr><td>保存指针位置</td><td><code>&lt;ESC&gt;[s</code></td></tr><tr><td>复原指针位置（到保存位置）</td><td><code>&lt;ESC&gt;[u</code></td></tr><tr><td>保存指针位置和属性</td><td><code>&lt;ESC&gt;7</code></td></tr><tr><td>复原指针位置和属性</td><td><code>&lt;ESC&gt;8</code></td></tr></tbody></table><h2 id="滚动">滚动<a class="header-anchor" href="#滚动"> ❮</a></h2><table><thead><tr><th>名称</th><th>转义字符串</th></tr></thead><tbody><tr><td>启用滚动</td><td><code>&lt;ESC&gt;[r</code></td></tr><tr><td>启用指定行之间滚动</td><td><code>&lt;ESC&gt;[{START};{END}r</code></td></tr><tr><td>向下滚动一行</td><td><code>&lt;ESC&gt;D</code></td></tr><tr><td>向上滚动一行</td><td><code>&lt;ESC&gt;M</code></td></tr></tbody></table><h2 id="制表">制表<a class="header-anchor" href="#制表"> ❮</a></h2><table><thead><tr><th>名称</th><th>转义字符串</th></tr></thead><tbody><tr><td>设置对齐位</td><td><code>&lt;ESC&gt;H</code></td></tr><tr><td>清楚对齐位</td><td><code>&lt;ESC&gt;[g</code></td></tr><tr><td>清楚所有对齐位</td><td><code>&lt;ESC&gt;[3g</code></td></tr></tbody></table><h2 id="清除">清除<a class="header-anchor" href="#清除"> ❮</a></h2><table><thead><tr><th>名称</th><th>转义字符串</th></tr></thead><tbody><tr><td>清除文字到行末</td><td><code>&lt;ESC&gt;[K</code></td></tr><tr><td>清除文字到行首</td><td><code>&lt;ESC&gt;[1K</code></td></tr><tr><td>清除整行</td><td><code>&lt;ESC&gt;[2K</code></td></tr><tr><td>清除文字到屏幕底</td><td><code>&lt;ESC&gt;[J</code></td></tr><tr><td>清除文字到屏幕顶</td><td><code>&lt;ESC&gt;[1J</code></td></tr><tr><td>清屏</td><td><code>&lt;ESC&gt;[2J</code></td></tr></tbody></table><h2 id="定义">定义<a class="header-anchor" href="#定义"> ❮</a></h2><ul><li>设置文字绑定: <code>&lt;ESC&gt;[{key};&quot;{string}&quot;p</code></li></ul><h2 id="显示颜色属性">显示颜色属性<a class="header-anchor" href="#显示颜色属性"> ❮</a></h2><ul><li>设置光标属性: <code>&lt;ESC&gt;[{attr1};...;{attrn}m</code></li></ul><table><thead><tr><th>属性代码</th><th>属性效果</th><th>属性代码</th><th>属性效果</th><th>属性代码</th><th>属性效果</th></tr></thead><tbody><tr><td>0</td><td>重置</td><td>30</td><td>前景黑</td><td>40</td><td>背景黑</td></tr><tr><td>1</td><td>亮</td><td>31</td><td>前景红</td><td>41</td><td>背景红</td></tr><tr><td>2</td><td>暗</td><td>32</td><td>前景绿</td><td>42</td><td>背景绿</td></tr><tr><td>4</td><td>下划线</td><td>33</td><td>前景黄</td><td>43</td><td>背景黄</td></tr><tr><td>5</td><td>闪烁</td><td>34</td><td>前景蓝</td><td>44</td><td>背景蓝</td></tr><tr><td>7</td><td>反向</td><td>35</td><td>前景紫</td><td>45</td><td>背景紫</td></tr><tr><td>8</td><td>隐藏</td><td>36</td><td>前景青</td><td>46</td><td>背景青</td></tr><tr><td></td><td></td><td>37</td><td>前景白</td><td>47</td><td>背景白</td></tr></tbody></table><h1 id="Reference">Reference<a class="header-anchor" href="#Reference"> ❮</a></h1><p>ASCII转义符</p><ul><li>Wiki <a href="https://en.wikipedia.org/wiki/Escape_sequences_in_C" target="_blank" rel="noopener">Escape sequences in C</a></li><li><a href="https://www.bing.com/search?q=ascii+table" target="_blank" rel="noopener">Bing ASCII table</a></li><li><a href="https://linux.die.net/man/1/printf" target="_blank" rel="noopener"><code>printf</code> Linux man page</a></li></ul><p>ANSI转移符</p><ul><li><a href="http://www.termsys.demon.co.uk/vtansi.htm" target="_blank" rel="noopener"><code>ANSI/VT100 Terminal Control Escape Sequences</code></a></li><li>Wiki <a href="https://en.wikipedia.org/wiki/ANSI_escape_code" target="_blank" rel="noopener"><code>ANSI escape code</code></a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前碰到过很多终端工具可以显示非常好看的进度条，或者显示丰富的颜色，甚至还有的直接可以在终端通过字符绘制UI（a.k.a. &lt;a href=&quot;https://en.wikipedia.org/wiki/Text-based_user_interface&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TUI&lt;/a&gt;），我一直都很好奇是怎么做到的。之后又知道了curses这个Python库和它的一些高层封装（例如asciimatics），然后最终在Stack Overflow里面查到了这些都是通过特殊的终端控制符来实现的。本文就介绍这些终端控制符的使用方法，他们很适合用来写一个简单无依赖的TUI。如果需要更复杂和全面的TUI功能，还是最好使用封装好的库。&lt;/p&gt;&lt;h1 id=&quot;ASCII-控制符&quot;&gt;ASCII 控制符&lt;a class=&quot;header-anchor&quot; href=&quot;#ASCII-控制符&quot;&gt; ❮&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;在最开始接触编程的时候，如果你学的是C，那你一定很熟悉&lt;code&gt;\n&lt;/code&gt;，这就是一个”换行“的转义字符，代表终端光标令起一行。有时你还会碰到&lt;code&gt;\r&lt;/code&gt;，这是”回车“。“回车”这个名字来源于打字机时代，在使用打字机的时候，如果你需要新起一行，那么需要的操作是：转动滚筒把纸往外抽一行，再把字车（相当与打印机的打印头）移到最左端。这两个操作的名字分别是“换行”和“回车”。因此严格来说另起一行的字符串应该是&lt;code&gt;\r\n&lt;/code&gt;，这也是Windows的标准，而在Unix中则简化成&lt;code&gt;\n&lt;/code&gt;会自动执行回车。&lt;/p&gt;
    
    </summary>
    
      <category term="Coding" scheme="http://zyxin.xyz/blog/categories/Coding/"/>
    
    
      <category term="Shell" scheme="http://zyxin.xyz/blog/tags/Shell/"/>
    
  </entry>
  
  <entry>
    <title>进程、线程与协程 (C# vs Python)</title>
    <link href="http://zyxin.xyz/blog/2019-11/ParallelismInPythonAndCsharp/"/>
    <id>http://zyxin.xyz/blog/2019-11/ParallelismInPythonAndCsharp/</id>
    <published>2019-11-07T05:57:31.000Z</published>
    <updated>2020-12-16T22:04:37.843Z</updated>
    
    <content type="html"><![CDATA[<p>近来由于项目需要，接触了一下一直没去了解过的Python异步语法，发现和之前我熟悉的C#有很多不同。在深入Python的异步逻辑之后，由于Python在语法上保留了很多语言机制的细节（比如成员函数的<code>self</code>参数），我反而对C#的异步有了更深的了解。这里就来重新梳理一下各种并行方法的区别，以及他们在C#和Python上实现的区别。（这里只讨论单机的并行机制。）</p><p>总的来说，并行机制主要有进程(Process)、线程(Thread)和协程(Coroutine)，其并行实现的开销依次递减，但是他们对每个任务的鲁棒性也是依次递减的。进程是操作系统资源分配的最小单元，线程则是能够被CPU并行处理的最小单元，而协程则是目前实现“并行”的最简单方法。一个进程中可以有多个线程，而一个线程中可以有多个协程。他们具体在特性上有以下区别</p><table><thead><tr><th></th><th>进程</th><th>线程</th><th>协程</th></tr></thead><tbody><tr><td>独立内存堆</td><td>√</td><td>×</td><td>×</td></tr><tr><td>独立处理器（可硬件并行）</td><td>√</td><td>√</td><td>×</td></tr><tr><td>独立上下文</td><td>√</td><td>√</td><td>×</td></tr><tr><td>独立栈、寄存器状态</td><td>√</td><td>√</td><td>√</td></tr></tbody></table><a id="more"></a><h1 id="进程">进程<a class="header-anchor" href="#进程"> ❮</a></h1><p>进程是系统层面实现并行的机制了，进程管理是现代操作系统的一大核心之一。进程之间互不影响，操作系统会保证一个程序崩溃了，其他程序以及系统内核不会崩溃。操作系统还会提供其他的进程管理功能，例如<a href="https://en.wikipedia.org/wiki/Scheduling_(computing)" target="_blank" rel="noopener">进程调度</a>、设置进程优先级等等。不同语言底层对进程接口的实现实际上都是对系统接口的封装。</p><h2 id="一些概念">一些概念<a class="header-anchor" href="#一些概念"> ❮</a></h2><p>与进程相关的概念通常都是操作系统课程的必修知识哈哈：</p><ul><li>进程间通信(Inter-process communiation, IPC)：故名思意。常用手段有管道、共享内存、信号量(Semaphore)、消息队列等。</li><li>管道(Pipe)：管道大概是进程间通信的最常用方式？分命名管道和匿名管道, 进程双方均可往其中读写数据。</li><li>远程过程调用(Remote procedure call): 远程过程调用通过特定的消息序列化手段，可以实现进程间通信，其使用形式是把一个“远程”的函数在本地进行执行。</li><li>进程锁：如果为了避免多个进程访问同一个资源的冲突的话，就会用到进程锁，其实现方法有<a href="https://blog.csdn.net/luansxx/article/details/7736618" target="_blank" rel="noopener">管道、信号量</a>、以及文件锁等。</li><li>文件锁：文件锁是实现进程互斥的一种常用手段，只需要建立空文件句柄并锁上就可了~并且文件锁还能做到权限控制，非常方便~</li></ul><h2 id="C">C#<a class="header-anchor" href="#C"> ❮</a></h2><p>C#中对进程控制的模块主要通过<a href="https://docs.microsoft.com/dotnet/api/system.diagnostics.process" target="_blank" rel="noopener"><code>System.Diagnostics.Process</code></a>实现，可以实现建立进程、管理进程等，还可以指定具体的内存映射参数，如虚拟内存的页大小。而对管道的支持则是在<code>Process</code>类中有一部分，以及在<a href="https://docs.microsoft.com/en-us/dotnet/api/system.io.pipes" target="_blank" rel="noopener"><code>System.IO.Pipe</code></a>里面有更全面的接口。我觉得这样的命名空间分类是挺合理的，<code>Process</code>类的API其实只能用来进行程序调用和系统诊断，而<code>Pipe</code>则由于它和<code>Stream</code>的概念比较符合，因此归在IO空间下是合适的。</p><h2 id="Python">Python<a class="header-anchor" href="#Python"> ❮</a></h2><p>Python中对进程的控制以及通信方法的实现都在<code>multiprocess</code>包里，它的一些具体使用方法可以参考<a href="/blog/2017-12/PythonCall/" title="另一篇之前的博文">另一篇之前的博文</a>。值得一提的是，Python中还针对Unix系统提供了<code>fcntl</code>, <code>posix</code>等库专门用来调用系统底层API，这些API有部分是和进程有关的。相关内容还是查阅对应的资料会比较清楚~</p><h1 id="线程">线程<a class="header-anchor" href="#线程"> ❮</a></h1><p>线程是进程中细化的并行机制，线程的实现也需要用到操作系统的接口，不过线程的创建的管理基本都是在进程内部完成的。由于线程之间不独立内存空间，因此在C++这种能够随意操作内存的语言中，一个线程崩了，这个进程也大概率就崩了。但是在C#和Python中，由于有比较完善的Exception机制，并且没有什么机会直接操作内存，一般线程崩了主进程还是能接着跑的。多线程想必应该是大家用的最多的并行方法了~</p><h2 id="一些概念-2">一些概念<a class="header-anchor" href="#一些概念-2"> ❮</a></h2><p>在线程里面又有一些新的概念</p><ul><li>线程池(Thread pool)：线程池与内存池相似，都是为了避免频繁新建和销毁线程(or 内存)而造成额外的开销</li><li>线程锁：线程锁与进程锁相似，是为了避免线程间访问同样的资源而产生冲突（例如<a href="https://stackoverflow.com/questions/34510/what-is-a-race-condition" target="_blank" rel="noopener">race condition</a>）。线程间产生访问冲突非常常见，因此程序员掌握线程锁的使用是非常必要的。线程锁在C++中的<code>&lt;mutex&gt;</code>有非常全面的实现。这里面锁的类型具有代表性，分为条件锁、自旋锁等等，具体区别可以参考<a href="https://blog.csdn.net/bian_qing_quan11/article/details/73734157" target="_blank" rel="noopener">这篇博客</a>。C++的多线程非常令人头大…这里就不展开了。</li><li>事件(Event)：在多线程体系中，事件是一种常用于线程同步的机制，如果线程需要在运行过程中等待其他线程的运行，就可以使用事件机制。</li></ul><h2 id="C-2">C#<a class="header-anchor" href="#C-2"> ❮</a></h2><p>C#中与线程相关的模块在<a href="https://docs.microsoft.com/dotnet/api/system.threading" target="_blank" rel="noopener"><code>System.Threading</code></a>空间下。<code>System.Threading.Thread</code>提供了线程实现的类，使用delegate即可创建线程对象。这个空间底下也提供了<code>SpinLock</code>、<code>Semaphore</code>、<code>Mutex</code>等线程锁，以及<code>AutoResetEvent</code>实现了事件机制。<code>System.Threading.ThreadPool</code>则提供了线程池的实现。另外需要指出的是C#提供了<code>lock</code>关键字，只需对冲突的对象使用<code>lock</code>锁上，那么在其对应的上下文中就能够避免冲突。</p><h2 id="Python-2">Python<a class="header-anchor" href="#Python-2"> ❮</a></h2><p>Python中与线程相关的对象在<a href="https://docs.python.org/library/threading.html" target="_blank" rel="noopener"><code>threading</code></a>模块中，其中<code>Thread</code>类提供了线程实现，<code>Lock</code>, <code>Semaphore</code>提供了线程锁，<code>Event</code>实现了事件机制。Python中可以使用<code>with lock:</code>这样的块实现与C#<code>lock</code>相似的语法，但是这个地方的lock仍然需要自己声明，不如C#和Java中的<code>lock</code>用着方便。</p><p>总体而言C#和Python对多线程机制的支持都比较全面，然而CPython有一个臭名昭著的<a href="http://cenalulu.github.io/python/gil-in-python/" target="_blank" rel="noopener">全局锁GIL</a>，使得其多线程效率大幅下降。因此在很多Python库中，大家宁愿使用<code>multiprocess</code>多进程来进行并行（即便需要处理进程间通信的问题），也不愿使用<code>threading</code>来完成并行任务。这一点上不得不说Python辣鸡！</p><h1 id="协程">协程<a class="header-anchor" href="#协程"> ❮</a></h1><p>协程应该是21世纪才用的比较多的技术了，并且这个概念应该是在Go里面提的最多。在前文我提到协程是并行时打了引号，这是因为协程本质上还是同一个时刻只能干一件事，没法利用硬件并行，因此我们形容协程都是用“异步”(Asychronized)而不是“并行”(Parallel)。异步是与同步相对的，只要程序能一会干点这个，一会干点那个，不按顺序来，那就可以称作异步了。协程的广泛应用是由于近些年大型服务器的负载越来越大，并发需求越来越高（<s>同时剁手的人越来越多</s>），多任务切换的开销越来越不可忽视，因此协程这个开销最小的方法就被广泛应用了。协程实际上不是一个比线程更小的概念，而是另一类概念（并行/串行 vs 异步/同步)。协程的特点是一个任务能够跑到一半就暂停，然后把状态存起来，等到需要的东西备齐了以后再把状态复原接着跑；至于暂停之前和之后是不是在同一个线程上跑、有没有跟别的任务一块跑并不重要。因此实际上协程是回调(Callback)机制的一个封装升级。</p><h2 id="一些概念-3">一些概念<a class="header-anchor" href="#一些概念-3"> ❮</a></h2><ul><li>事件循环(Event loop)：事件循环是一种非常简单的实现异步的机制，简而言之就是维护一个队列，然后把队列里的任务挨个执行，而任务随时随地可以被添加进队列。</li><li>异步执行/等待(async/await)：这两个关键词在多个语言中都有出现。async用来修饰函数，说明这个函数可以异步执行；await用来等待异步函数的结束，如果没有结束就把当前任务搁着。</li></ul><h2 id="C-3">C#<a class="header-anchor" href="#C-3"> ❮</a></h2><p>C#中没有协程的概念，C#在5.0版本中引入的<code>async</code>/<code>await</code>关键字提供了异步执行的接口。据我所知C#应该是最早一批引入这个概念的语言了，并且C#里面async和await的使用非常顺滑~。C#的async/await调度与Go一样，都是通过线程池实现，因此性能也非常不错。C#中与async/await有关的接口在<a href="https://docs.microsoft.com/en-us/dotnet/api/system.threading.tasks%60" target="_blank" rel="noopener"><code>System.Threading.Tasks</code></a>下，里面的<code>Task</code>类型是对能够await的对象的封装。</p><p>C#中也有用到Event loop来实现异步的地方，一般是在UI相关的函数中，例如整个C#里面的<code>event</code>机制都是通过事件循环来实现的。使用事件循环来完成与UI相关的异步应该是非常标准的做法了，例如Qt里面也有<code>QEventLoop</code>来实现UI的异步回调。与Event loop相关的是Dispatcher机制，Dispatcher可以将指定任务加进事件循环中执行，例如在WPF中可以用Window的Dispatcher在其他线程中将任务加进UI主线程。</p><p>另外需要指出的是C#还可以通过<code>yield</code>关键词实现异步，<code>yield return</code>可能是C#最早的异步机制了，不过功能有限，只能与<code>IEnumerable</code>合作使用。C#中有一些协程的库（如Unity里的）就是使用<code>yield</code>机制来实现的。具体怎么使用<code>yield</code>还请去学习C#的语法~</p><h2 id="Python-3">Python<a class="header-anchor" href="#Python-3"> ❮</a></h2><p>Python对异步的支持就来的比较晚了，直到<a href="https://www.python.org/dev/peps/pep-0492/" target="_blank" rel="noopener">PEP 492</a>才正式加入了对<code>async</code>关键字的支持，放在了<code>asyncio</code>模块中。Python对这对关键词的实现又很辣鸡了，<a href="https://robertoprevato.github.io/Comparisons-of-async-await/" target="_blank" rel="noopener">采用的是Event loop机制来实现</a>（可能是因为多线程性能太差了吧= =）。最让人蛋疼是为了执行异步函数你还需要自己开event loop，如果你之前开过一个了，那你还需要把之前那个loop找回来，然后dispatch进去，这是何其难受！。。</p><p>Python中只要对象有<code>__await__</code>、<code>__aiter__</code>或者<code>__aenter__</code>就可以分别支持<code>await</code>、<code>async for</code>和<code>async with</code>的代码块。Python还设计了三个相关概念：Coroutine代表异步对象、Task代表异步执行计划、Future代表异步执行结果。。何必呢？？？像C#用一个Task代表全部不行吗？再配合event loop的接口，就产生了<code>create_task</code>、<code>run_coroutine_threadsafe</code>、<code>run_until_complete</code>、<code>run_in_executor</code>等我总是搞不清区别的函数。。。我爱C#！</p><hr>以上是我对C#和Python中异步机制的总结，我对各语言底层的了解并不深，如有错漏还请指点~]]></content>
    
    <summary type="html">
    
      &lt;p&gt;近来由于项目需要，接触了一下一直没去了解过的Python异步语法，发现和之前我熟悉的C#有很多不同。在深入Python的异步逻辑之后，由于Python在语法上保留了很多语言机制的细节（比如成员函数的&lt;code&gt;self&lt;/code&gt;参数），我反而对C#的异步有了更深的了解。这里就来重新梳理一下各种并行方法的区别，以及他们在C#和Python上实现的区别。（这里只讨论单机的并行机制。）&lt;/p&gt;&lt;p&gt;总的来说，并行机制主要有进程(Process)、线程(Thread)和协程(Coroutine)，其并行实现的开销依次递减，但是他们对每个任务的鲁棒性也是依次递减的。进程是操作系统资源分配的最小单元，线程则是能够被CPU并行处理的最小单元，而协程则是目前实现“并行”的最简单方法。一个进程中可以有多个线程，而一个线程中可以有多个协程。他们具体在特性上有以下区别&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;/th&gt;&lt;th&gt;进程&lt;/th&gt;&lt;th&gt;线程&lt;/th&gt;&lt;th&gt;协程&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;独立内存堆&lt;/td&gt;&lt;td&gt;√&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;独立处理器（可硬件并行）&lt;/td&gt;&lt;td&gt;√&lt;/td&gt;&lt;td&gt;√&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;独立上下文&lt;/td&gt;&lt;td&gt;√&lt;/td&gt;&lt;td&gt;√&lt;/td&gt;&lt;td&gt;×&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;独立栈、寄存器状态&lt;/td&gt;&lt;td&gt;√&lt;/td&gt;&lt;td&gt;√&lt;/td&gt;&lt;td&gt;√&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
    
    </summary>
    
      <category term="Coding" scheme="http://zyxin.xyz/blog/categories/Coding/"/>
    
      <category term="Language" scheme="http://zyxin.xyz/blog/categories/Coding/Language/"/>
    
    
      <category term="Python" scheme="http://zyxin.xyz/blog/tags/Python/"/>
    
      <category term="C#" scheme="http://zyxin.xyz/blog/tags/C/"/>
    
      <category term="Parallelism" scheme="http://zyxin.xyz/blog/tags/Parallelism/"/>
    
  </entry>
  
  <entry>
    <title>在Cython中操作数组</title>
    <link href="http://zyxin.xyz/blog/2019-08/CythonArray/"/>
    <id>http://zyxin.xyz/blog/2019-08/CythonArray/</id>
    <published>2019-08-28T21:19:01.000Z</published>
    <updated>2019-11-12T03:02:41.977Z</updated>
    
    <content type="html"><![CDATA[<p>Cython提供了很多方法来搭建C/C++内存和Python对象间的桥梁，但是官方的教程只介绍了一些基础的方法。这篇文章就介绍一下我在各个场合学到和用到的Cython封装（多维）数组的技巧。一般而言这个桥梁会分为两部分，Python与Cython和Cython与C/C++。其中Python中的数组主要形式是<code>list</code>、<code>array.array</code>和<code>numpy.ndarray</code>；Cython中的数组形式有<code>[:,:,:]</code>（<a href="https://cython.readthedocs.io/en/latest/src/userguide/memoryviews.html?highlight=pointer#view-cython-arrays" target="_blank" rel="noopener">Memoryview</a>/<a href="https://www.python.org/dev/peps/pep-3118/" target="_blank" rel="noopener">Buffer</a>）和<code>cython.view.array</code>；C/C++的数组形式有<code>**</code>（指针）、<code>vector</code>和<code>Eigen::Vector/Matrix</code>。</p><blockquote><p>本篇介绍的主要内容也来自于Cython的文档：<a href="http://cython.readthedocs.io/en/latest/src/userguide/memoryviews.html" target="_blank" rel="noopener">Typed Memoryviews</a>。</p></blockquote><a id="more"></a><p>在这里也先介绍一下Cython中的这几个概念：</p><ul><li><strong>Memoryview</strong>：这是cython提供的一种语法糖，相当于提供了C中<code>int[][][]</code>形式数组的类型。由于Memoryview可以兼容Python的Buffer协议，因此我把他们放在了一起。Memoryview需要指定元素的类型，这个类型必须是内置数值类型或者<strong>C结构体</strong>。</li><li><strong><code>cython.view.array</code></strong>：这是Cython提供的一个多维数组类型，与<code>numpy.ndarray</code>非常相似了。<br>这两个东西也是可以相互转换的，例如</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> cython.view cimport array <span class="keyword">as</span> cvarray</span><br><span class="line"></span><br><span class="line"><span class="comment"># Cython array to Memoryview</span></span><br><span class="line">cyarr = cvarray(shape=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), itemsize=sizeof(int), format=<span class="string">"i"</span>)</span><br><span class="line">cdef int [:, :, :] cyarr_view = cyarr</span><br><span class="line"></span><br><span class="line"><span class="comment"># Memoryview to Cython array</span></span><br><span class="line">cdef cvarray back = cyarr_view</span><br></pre></td></tr></table></figure><h1 id="Python与Cython数组相互转换">Python与Cython数组相互转换<a class="header-anchor" href="#Python与Cython数组相互转换"> ❮</a></h1><p>Python与Cython之间的转换基本上都由Cython的Memoryview提供了接口，实际上直接赋值就可以。例如官方给出的这段例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> cpython cimport array <span class="keyword">as</span> cparray</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Memoryview on a NumPy array</span></span><br><span class="line">narr = np.arange(<span class="number">27</span>, dtype=np.dtype(<span class="string">"i"</span>)).reshape((<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">cdef int [:, :, :] narr_view = narr</span><br><span class="line"></span><br><span class="line"><span class="comment"># Memoryview on a CPython array</span></span><br><span class="line">parr = cparray.array(<span class="string">'i'</span>, range(<span class="number">3</span>))</span><br><span class="line">cdef int [:] parr_view = parr</span><br></pre></td></tr></table></figure><p>顺带一提，<code>list</code>对象由于本身不代表一段连续内存，因此需要先转换为<code>array</code>或<code>ndarray</code>再赋值给Memoryview。反过来由于Numpy支持Buffer协议，因此Memoryview和Cython的<code>cython.view.array</code>都可以直接转换为<code>numpy.ndarray</code>，然后转换为<code>array</code>和<code>list</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> cpython cimport array <span class="keyword">as</span> cparray</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">parr = cparray.array(<span class="string">'i'</span>, range(<span class="number">3</span>))</span><br><span class="line">cdef int [:] parr_view = parr</span><br><span class="line">narr = np.array(parr_view) <span class="comment"># explicit version: np.array(parr_view, copy=False)</span></span><br></pre></td></tr></table></figure><blockquote><p>以上这些代码中的等式都没有发生内存拷贝。</p></blockquote><h1 id="Cython数组与C-C-数组相互转换">Cython数组与C/C++数组相互转换<a class="header-anchor" href="#Cython数组与C-C-数组相互转换"> ❮</a></h1><p>Cython的Memoryview同样承担了大量与C/C++数组进行转换的功能，不过Memoryview只支持一种转换方法，就是与raw指针的相互转换：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> libc.stdlib cimport malloc</span><br><span class="line">cdef double* data = &lt;double*&gt;malloc(sizeof(double) * <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert pointer to Memoryview</span></span><br><span class="line">cdef double[:] view = &lt;double[:<span class="number">2</span>,:<span class="number">2</span>]&gt;data</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert Memoryview to pointer</span></span><br><span class="line">data = &amp;view[<span class="number">0</span>,<span class="number">0</span>]</span><br></pre></td></tr></table></figure><blockquote><p>以上代码的等式中也没有发生内存拷贝。</p></blockquote><p>这里需要指出的是，由于指针本身只是一段内存的代表，因此在转换时制定类型和长度（如<code>&lt;double[4]&gt;</code>），并且需要保证指针指向的数组是C型连续的（多维数组中最后一维的内存是连续的）。如果要将<code>vector</code>和<code>Eigen::Matrix</code>转换为Memoryview，那么也同样需要获取其内存指针（<code>vector::data</code>和<code>Eigen::Matrix::data</code>）。另外，通过<strong>指针转换出来的Memoryview没有引用计数</strong>，因此如果你的指针是某个Cython类的成员，那么不要使用指针转换，而使用Buffer协议的方式进行传递。</p><h1 id="其他直接转换的方法">其他直接转换的方法<a class="header-anchor" href="#其他直接转换的方法"> ❮</a></h1><p>除了上面提到的方法之外还有一些直接转换的方法，但是这些方法往往不会做类型和尺寸检查，以及很重要的内存连续性检查（Memoryview会区分C型内存和Fortran型内存），因此使用时需要谨慎。</p><ul><li><code>cdef vector[int] data; cdef list view = data</code>：Cython提供了list和vector直接转换的接口</li><li><code>cdef np.ndarray[double] data; cdef double* view = &lt;double*&gt; data.data</code></li><li><code>cdef np.ndarray[double, ndim=2] data; cdef double* view = &amp;data[0,0]</code></li><li><code>cdef array.array data; cdef double* view = data.data.as_doubles[0]</code>：利用了<a href="https://cython.readthedocs.io/en/latest/src/tutorial/array.html#zero-overhead-unsafe-access-to-raw-c-pointer" target="_blank" rel="noopener">Cython中的API</a></li></ul><h1 id="非内置类型的转换">非内置类型的转换<a class="header-anchor" href="#非内置类型的转换"> ❮</a></h1><p>在实际应用过程中还会碰到由复杂元素构成的数组（例如PCL里面的PointXYZ、SLAM里会用到的Quaternion），这时就有将复杂类型（通常是自定义struct）在Python和C/C++之间转换的需求。这时可以选择利用Cython提供的MemoryView，也可以利用Python的Buffer协议直接将C++对象传递给Python。</p><p>使用Buffer协议的方法请直接参考<a href="https://cython.readthedocs.io/en/latest/src/userguide/buffer.html" target="_blank" rel="noopener">Cython文档</a>，使用Memoryview的例子如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> libc.stdlib cimport malloc</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">cdef struct buf: </span><br><span class="line">    int size </span><br><span class="line">    int count</span><br><span class="line">cdef buf[:] data = &lt;buf[:<span class="number">2</span>]&gt;malloc(sizeof(buf)*<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(np.array(data).dtype)</span><br><span class="line"><span class="comment"># [('size', '&lt;i4'), ('count', '&lt;i4')]</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Cython提供了很多方法来搭建C/C++内存和Python对象间的桥梁，但是官方的教程只介绍了一些基础的方法。这篇文章就介绍一下我在各个场合学到和用到的Cython封装（多维）数组的技巧。一般而言这个桥梁会分为两部分，Python与Cython和Cython与C/C++。其中Python中的数组主要形式是&lt;code&gt;list&lt;/code&gt;、&lt;code&gt;array.array&lt;/code&gt;和&lt;code&gt;numpy.ndarray&lt;/code&gt;；Cython中的数组形式有&lt;code&gt;[:,:,:]&lt;/code&gt;（&lt;a href=&quot;https://cython.readthedocs.io/en/latest/src/userguide/memoryviews.html?highlight=pointer#view-cython-arrays&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Memoryview&lt;/a&gt;/&lt;a href=&quot;https://www.python.org/dev/peps/pep-3118/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Buffer&lt;/a&gt;）和&lt;code&gt;cython.view.array&lt;/code&gt;；C/C++的数组形式有&lt;code&gt;**&lt;/code&gt;（指针）、&lt;code&gt;vector&lt;/code&gt;和&lt;code&gt;Eigen::Vector/Matrix&lt;/code&gt;。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;本篇介绍的主要内容也来自于Cython的文档：&lt;a href=&quot;http://cython.readthedocs.io/en/latest/src/userguide/memoryviews.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Typed Memoryviews&lt;/a&gt;。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Coding" scheme="http://zyxin.xyz/blog/categories/Coding/"/>
    
      <category term="Language" scheme="http://zyxin.xyz/blog/categories/Coding/Language/"/>
    
    
      <category term="Cython" scheme="http://zyxin.xyz/blog/tags/Cython/"/>
    
      <category term="Python" scheme="http://zyxin.xyz/blog/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Cython中的特殊函数</title>
    <link href="http://zyxin.xyz/blog/2019-08/CythonFunctions/"/>
    <id>http://zyxin.xyz/blog/2019-08/CythonFunctions/</id>
    <published>2019-08-28T18:23:37.000Z</published>
    <updated>2019-11-12T03:02:41.977Z</updated>
    
    <content type="html"><![CDATA[<p>这次来介绍一下Cython中的特殊函数定义，Cython相比Python本身的特殊函数之外还增加了一些新的函数，用来满足对C特性的支持，其中有些内容还经常令人混淆。关于Python中特殊变量和特殊函数名的内容，<a href="https://docs.python.org/3/reference/datamodel.html" target="_blank" rel="noopener">请参考Python官方文档</a>。</p><h1 id="def-cdef和cpdef"><code>def</code>, <code>cdef</code>和<code>cpdef</code><a class="header-anchor" href="#def-cdef和cpdef"> ❮</a></h1><p>首先最开始需要分清的便是Cython中的三种函数类型。<code>def</code>定义的对象（包括变量、函数、类型）都是普通的Python对象，是Python可以直接调用的，因此其参数都只能是Python类型或对象；<code>cdef</code>定义的对象则是C/C++层面的，可以直接用C/C++对象作为参数，因此不能被普通Python代码调用，这样减少了很多overhead因此可以提高运行效率。另外尽管<code>cdef</code>的函数不是Python对象，无法当作变量使用，但还是可以获取函数指针的。而<code>cpdef</code>则是同时兼具两方面特性，其本质是用<code>cdef</code>定义函数后再用<code>def</code>定义一个函数封装，使得在Cython中调用时可以调用高效的<code>cdef</code>版本，而在Python中调用的是与Python兼容的<code>def</code>版本。</p><a id="more"></a><h1 id="init-和-cinit"><code>__init__</code>和<code>__cinit__</code><a class="header-anchor" href="#init-和-cinit"> ❮</a></h1><p>在理清了上面几个关键字后另一个经常令人疑惑的点便是<code>__init__</code>和<code>__cinit__</code>的区别。<code>__cinit__</code>和<code>__dealloc__</code>都是Cython特有的特殊函数。<a href="https://cython.readthedocs.io/en/latest/src/userguide/special_methods.html#initialisation-methods-cinit-and-init" target="_blank" rel="noopener">官方文档在其用法上解释</a>的并不清楚，只是说<code>__cinit__</code>可以用来进行C/C++级别的初始化。实际上，使用<code>__cinit__</code>的重要原因是源于其特性：<code>__cinit__</code>会像C++一样自动执行基类的<code>__cinit__</code>，因此它保证会在构造时被执行一次（且只被执行一次）。由于Python中的<code>__init__</code>函数默认不会调用基类的<code>__init__</code>，因此如果想保证类型中的<code>cdef</code>成员被初始化，避免可能的堆栈问题（如指针没有初始化），那么就可以选择使用<code>__cinit__</code>。如果理解了这一点就可以知道，什么时候需要使用<code>__cinit__</code>了。</p><p>但是使用<code>__cinit__</code>的时候有很多限制需要了解：</p><ol><li><code>__cinit__</code>有时会带来额外的开销，<a href="https://kaushikghose.wordpress.com/2015/03/08/cython-__cinit__/" target="_blank" rel="noopener">这篇博客中有一些分析</a>。</li><li><code>__cinit__</code>的参数声明和<code>__init__</code>必须一致，因为会同时被调用。因此通常<code>__cinit__</code>的参数中会留下<code>*kargs</code>和<code>**kvargs</code>。<a href="https://stackoverflow.com/a/33091422" target="_blank" rel="noopener">Stackoverflow上也有人问过这个情况</a>。</li><li><code>__cinit__</code>中如果要用<code>malloc</code>分配内存，记得在<code>__dealloc__</code>中销毁。<code>__dealloc__</code>相当于C++版本的<code>__del__</code></li><li><code>__cinit__</code>和<code>__init__</code>一样也只能使用<code>def</code>声明，不能用<code>__cdef__</code>和<code>__cpdef__</code>。具体原因我并不清楚。</li></ol><h1 id="运算符重载">运算符重载<a class="header-anchor" href="#运算符重载"> ❮</a></h1><p>其他大多数的特殊函数定义和用法几乎和Python相同，但是需要特别指出的是运算符重载的部分。以加法为例，在Python中加法<code>a + b</code>的实现方式是：</p><ol><li>如果<code>a</code>中定义了<code>__add__</code>，那么调用<code>a.__add__(b)</code></li><li>如果<code>a</code>中没有定义，而<code>b</code>中定义了<code>__radd__</code>，那么调用<code>b.__radd(a)</code></li></ol><p>而在Python的C扩展类里（包含Cython和pybind11的实现），其实现方式是寻找接受<code>a</code>和<code>b</code>类型的<code>__add__</code>重载，也就是说本质上在C扩展类中定义的<code>__add__</code>都是<code>__add__</code>的重载，这也是与C++的<code>operator</code>重载理念一致，只不过这个<code>__add__</code>仍然需要定义在类里。在<a href="https://cython.readthedocs.io/en/latest/src/userguide/special_methods.html#arithmetic-operators" target="_blank" rel="noopener">Cython文档中给出的运算符列表</a>里，参数里带<code>self</code>的函数都是按照Python中的方法实现的，<code>self</code>不能指定类型；而以<code>x, y</code>这种形式为参数的则是按照C扩展类执行方式的函数，<code>x</code>和<code>y</code>都可以指定类型。</p><p>另外Cython还定义了一个特殊的运算符函数<code>__richcmp__</code>，这个是Python中没有的，不过其功能只是把比较符号（&gt;,&lt;,=）的实现合并了，与Python的<code>__eq__</code>、<code>__lt__</code>等函数没有本质区别。<a href="https://cython.readthedocs.io/en/latest/src/userguide/special_methods.html#rich-comparison-operators" target="_blank" rel="noopener">这在官方文档中也有说明</a></p><h1 id="getbuffer"><code>__getbuffer__</code><a class="header-anchor" href="#getbuffer"> ❮</a></h1><p><a href="https://cython.readthedocs.io/en/latest/src/userguide/special_methods.html#buffer-interface-pep-3118-no-python-equivalents-see-note-1" target="_blank" rel="noopener">Cython中有两个版本的Buffer协议</a>，一种是提案PEP-3118定义的，另一种是Python官方定义之前Cython自己的定义方式。其中前者在<a href="/blog/2019-08/CythonInterop/" title="之前介绍Cython封装的文章">之前介绍Cython封装的文章</a>中已有介绍，就不多赘述。其相关的特殊函数是<code>__getbuffer__</code>和<code>__releasebuffer__</code>，这两个函数也都是Cython特有的。而后者比较难用，已经被标记为depricated废弃了，也不介绍了。</p><h1 id="属性（property）">属性（property）<a class="header-anchor" href="#属性（property）"> ❮</a></h1><p>Cython中还提供了一套非常方便的属性定义方法。原本在Python中定义属性非常但疼，例如下面的代码定义了名为<code>length</code>的属性，使得你可以通过<code>square.length</code>的方法访问它</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Square</span>:</span></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">length</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._length</span><br><span class="line"><span class="meta">    @length.setter</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">length</span><span class="params">(self, value)</span>:</span></span><br><span class="line">        self._length = value</span><br><span class="line"><span class="meta">    @length.deleter</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">length</span><span class="params">(self)</span>:</span></span><br><span class="line">        self._length = <span class="number">0</span></span><br></pre></td></tr></table></figure><p>而在Cython中定义属性就更简单了，它除了支持上面的方法外还有另一种更加直观的定义方式（虽然这个方式也已经被标记为depricated了）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cdef <span class="class"><span class="keyword">class</span> <span class="title">Square</span>:</span></span><br><span class="line">    property length:</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__get__</span><span class="params">(self)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> self._length</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__set__</span><span class="params">(self, value)</span>:</span></span><br><span class="line">            self._length = value</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__del__</span><span class="params">(self)</span>:</span></span><br><span class="line">            self._length = <span class="number">0</span></span><br></pre></td></tr></table></figure><hr><p>Cython的类型还有各种其他的奇奇怪怪的小特性，在Cython的这两篇文档里有详细介绍：<a href="https://cython.readthedocs.io/en/latest/src/userguide/extension_types.html" target="_blank" rel="noopener">Extension Types</a>, <a href="https://cython.readthedocs.io/en/latest/src/userguide/special_methods.html#buffer-interface-pep-3118-no-python-equivalents-see-note-1" target="_blank" rel="noopener">Special Methods of Extension Types</a>，仅供参考～</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这次来介绍一下Cython中的特殊函数定义，Cython相比Python本身的特殊函数之外还增加了一些新的函数，用来满足对C特性的支持，其中有些内容还经常令人混淆。关于Python中特殊变量和特殊函数名的内容，&lt;a href=&quot;https://docs.python.org/3/reference/datamodel.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;请参考Python官方文档&lt;/a&gt;。&lt;/p&gt;&lt;h1 id=&quot;def-cdef和cpdef&quot;&gt;&lt;code&gt;def&lt;/code&gt;, &lt;code&gt;cdef&lt;/code&gt;和&lt;code&gt;cpdef&lt;/code&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#def-cdef和cpdef&quot;&gt; ❮&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;首先最开始需要分清的便是Cython中的三种函数类型。&lt;code&gt;def&lt;/code&gt;定义的对象（包括变量、函数、类型）都是普通的Python对象，是Python可以直接调用的，因此其参数都只能是Python类型或对象；&lt;code&gt;cdef&lt;/code&gt;定义的对象则是C/C++层面的，可以直接用C/C++对象作为参数，因此不能被普通Python代码调用，这样减少了很多overhead因此可以提高运行效率。另外尽管&lt;code&gt;cdef&lt;/code&gt;的函数不是Python对象，无法当作变量使用，但还是可以获取函数指针的。而&lt;code&gt;cpdef&lt;/code&gt;则是同时兼具两方面特性，其本质是用&lt;code&gt;cdef&lt;/code&gt;定义函数后再用&lt;code&gt;def&lt;/code&gt;定义一个函数封装，使得在Cython中调用时可以调用高效的&lt;code&gt;cdef&lt;/code&gt;版本，而在Python中调用的是与Python兼容的&lt;code&gt;def&lt;/code&gt;版本。&lt;/p&gt;
    
    </summary>
    
      <category term="Coding" scheme="http://zyxin.xyz/blog/categories/Coding/"/>
    
      <category term="Language" scheme="http://zyxin.xyz/blog/categories/Coding/Language/"/>
    
    
      <category term="Cython" scheme="http://zyxin.xyz/blog/tags/Cython/"/>
    
      <category term="Python" scheme="http://zyxin.xyz/blog/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Cython与C/C++的交互</title>
    <link href="http://zyxin.xyz/blog/2019-08/CythonInterop/"/>
    <id>http://zyxin.xyz/blog/2019-08/CythonInterop/</id>
    <published>2019-08-28T03:47:47.000Z</published>
    <updated>2019-11-12T03:02:41.977Z</updated>
    
    <content type="html"><![CDATA[<p>用Cython也用了很有一段时间了，这次就介绍一下它的最重要功能——使用Cython来封装C/C++代码。最基本的封装方法可以参见Cython文档中的相关页面：<a href="https://cython.readthedocs.io/en/latest/src/userguide/external_C_code.html" target="_blank" rel="noopener">Interfacing with External C Code</a>和<a href="https://cython.readthedocs.io/en/latest/src/userguide/wrapping_CPlusPlus.html" target="_blank" rel="noopener">Using C++ in Cython</a>，本文介绍主要是比较重要和常用的Cython/C++交互特性，而自定义Python拓展类（而不是封装现有C++）的一些操作可以<a href="https://cython.readthedocs.io/en/latest/src/tutorial/cdef_classes.html" target="_blank" rel="noopener">参考官方教程</a>。</p><p>封装C++代码时，最重要的关键词就是<code>extern</code>，在定义函数时使用这个关键字就说明该声明是外部的，而使用<code>cdef extern from</code>语句就能指定声明对应的头文件。例如如果要封装函数<code>func</code>，对应的Cython语句是</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cdef extern <span class="keyword">from</span> <span class="string">"func.c"</span>:</span><br><span class="line">    void func(int arg)</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="文件结构">文件结构<a class="header-anchor" href="#文件结构"> ❮</a></h1><p>首先讲一下Cython的文件结构。如果你之有一个小模块需要封装的话你可以把所有代码写到同一个<code>pyx</code>里进行编译，否则的话你就可以利用Cython的目录结构来管理多个层次的代码。Cython的文件一共有三种：<code>pyx</code>，<a href="https://cython.readthedocs.io/en/latest/src/userguide/language_basics.html#the-include-statement-and-include-files" target="_blank" rel="noopener"><code>pxi</code></a>（<a href="https://stackoverflow.com/a/45440199" target="_blank" rel="noopener">注意与<code>pyi</code>区分</a>）和<code>pxd</code>（<a href="https://stackabuse.com/differences-between-pyc-pyd-and-pyo-python-files/" target="_blank" rel="noopener">注意与<code>pyd</code>区分</a>）。</p><p><code>.pyx</code>是Cython的源文件，类似于<code>.cpp</code>文件在C++中的地位，而对应<code>.h</code>头文件地位的则是<code>pyi</code>。在Cython中添加<code>import 'header.pyi'</code>的语句就会将<code>header.pyi</code>文件中的内容原封不动地直接插入当前位置，这与C++的<code>#include</code>语句的作用是相同的。而<code>pxd</code>则是另一套符号化的逻辑，<code>.pxd</code>文件中只能声明函数、声明类型、不能有函数和类型的定义内容（除了<code>inline</code>函数外），而在<code>cimport</code>了<code>pxd</code>的定义之后当前代码便引入了对应的函数或者类型签名。这个工作方式则更符合C++中头文件的实际用途。定义了<code>pxd</code>后就可以在多个Cython文件之间共享同一个类型了。</p><p>不过既然涉及了<code>include</code>语法，就必然要指定类似于C++的引用路径了。<code>pxi</code>和<code>pxd</code>文件的引用路径可以<a href="https://cython.readthedocs.io/en/latest/src/userguide/sharing_declarations.html#search-paths-for-definition-files" target="_blank" rel="noopener">在cythonize过程中手动指定</a>，而<code>pxd</code>由于是符号化的还可以通过新建<code>__init__.pxd</code>的方式来实现类似于Python的引用方法。只要在Cython搜索目录下的文件夹中包含<code>__init__.pxd</code>文件，Cython就会认为这是一个Cython库，之后就可以用<code>cimport</code>语句通过与Python中<code>import</code>相类似的语法将对应模块文件（<code>.pxd</code>文件）引用进来。当然，<code>pxd</code>文件<a href="https://cython.readthedocs.io/en/latest/src/userguide/source_files_and_compilation.html#compiling-with-the-cythonize-command" target="_blank" rel="noopener">也可以通过命令参数直接导入</a>。关于如何组织这些文件以及头文件之间的关系，读者可以参考<a href="https://github.com/cmpute/pcl.py" target="_blank" rel="noopener">我写的PCL封装库</a>和<a href="https://cython.readthedocs.io/en/latest/src/userguide/sharing_declarations.html" target="_blank" rel="noopener">Cython的相关文档</a>。</p><blockquote><p>函数在pxd中的定义不能显式指定默认参数，而是必须用<code>*</code>代替，例如<code>cdef void func(a=0)</code>在<code>pxd</code>中声明的话需要改为<code>cdef void func(a=*)</code>。</p></blockquote><h1 id="类型封装">类型封装<a class="header-anchor" href="#类型封装"> ❮</a></h1><p>Cython对C++的类型提供了基本可用的封装语法。为什么说基本可用，是因为Cython目前对模板的支持还非常有限，因此实际上可以说Cython只支持到C++98的程度。不过尽管如此，Cython已经能够完成大多数代码的封装需求了。Cython对<code>class</code>的支持通过<code>cdef cppclass &lt;class-name&gt;</code>来实现，这里<code>cppclass</code>关键词是为了和Cython的<code>class</code>关键词进行区分。Cython中<code>class</code>关键词代表的是和Python一致的<code>PyObject</code>对象，代表的是Python类型，而<code>cppclass</code>则指代C++原生类型，由于Cython文件中无法直接编写C++代码，因此<code>cdef cppclass</code>语句通常在<code>cdef extern from</code>的语法块中，用来封装现有的C++类型。另外一点需要注意的地方是Cython<a href="https://cython.readthedocs.io/en/latest/src/userguide/language_basics.html#c-variable-and-type-definitions" target="_blank" rel="noopener">提供封装<code>enum</code>和<code>struct</code>的语法</a>，但是针对的是C中的<code>enum</code>和<code>struct</code>，而非C++中的<code>enum class</code>和<code>struct</code>（C++中<code>struct</code>和<code>class</code>几乎没有区别）。如果要封装C++版本的<code>enum</code>和<code>struct</code>可以直接使用<code>cppclass</code>关键词。以下是封装C++类型的一个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cdef extern <span class="keyword">from</span> <span class="string">"test.h"</span>:</span><br><span class="line">    cdef cppclass Test:</span><br><span class="line">        void print()</span><br></pre></td></tr></table></figure><h2 id="别名与-namespace-关键字">别名与 namespace 关键字<a class="header-anchor" href="#别名与-namespace-关键字"> ❮</a></h2><p>由于Cython最后生成的是全局的C代码，因此在引用C++类时需要明确声明类型含命名空间的全称，这里就需要用到别名的机制。Cython允许从<code>.h</code>文件中导入声明的时候给类型和方法改名字，具体用法如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cdef extern <span class="keyword">from</span> <span class="string">"&lt;header-name&gt;"</span>:</span><br><span class="line">    cdef void &lt;new-function-name&gt; <span class="string">"&lt;origin-function-name&gt;"</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    cdef cppclass &lt;new-<span class="class"><span class="keyword">class</span>-<span class="title">name</span>&gt; "&lt;<span class="title">origin</span>-<span class="title">class</span>-<span class="title">name</span>&gt;":</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>简而言之就是在方法或者类型名称后添加引号，引号里写上原本C++中的名字。这个机制有很多tricky的用法，它可以用来声明带命名空间的方法和类型、可以用来<a href="https://stackoverflow.com/a/25955546" target="_blank" rel="noopener">重命名C++中的运算符</a>、可以用来直接声明实例化的模板类型、甚至可以用来把C++常量声明成类型用于模板参数（这种操作可以<a href="https://github.com/wouterboomsma/eigency/blob/master/eigency/core.pxd" target="_blank" rel="noopener">参考eigency库中的代码</a>）。</p><p>其中针对第一种用法，为了简化带有命名空间对象的声明，Cython加入了<code>namespace</code>关键字。在<code>cdef</code>语句中添加<code>namespace</code>从句可以使得Cython编译器默认给其包含的语句块中所有的类型加上对应的命名空间，例如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cdef extern <span class="keyword">from</span> <span class="string">"test.h"</span> namespace <span class="string">"ns"</span>:</span><br><span class="line">    cdef cppclass Test:</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>与以下代码是等价的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cdef extern <span class="keyword">from</span> <span class="string">"test.h"</span>:</span><br><span class="line">    cdef cppclass Test <span class="string">"ns::Test"</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h2 id="模板支持">模板支持<a class="header-anchor" href="#模板支持"> ❮</a></h2><p>这个特性在<a href="/blog/2018-12/CythonTypes/" title="之前介绍Cython类型的文章中">之前介绍Cython类型的文章中</a>也有提到过，这里补充一下它的一些特性。Cython对C++模板的支持通过<code>[]</code>符号实现，以下是Cython中对<code>vector</code>的封装代码可供参考</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cdef extern <span class="keyword">from</span> <span class="string">"&lt;vector&gt;"</span> namespace <span class="string">"std"</span> nogil:</span><br><span class="line">    cdef cppclass vector[T,ALLOCATOR=*]:</span><br><span class="line">        ctypedef T value_type</span><br><span class="line">        ctypedef ALLOCATOR allocator_type</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><p>其中<code>vector[T,ALLOCATOR=*]</code>对应的就是C++中的<code>vector&lt;T, ALLOCATOR&gt;</code>符号。模板参数在Cython中同样可以有复数个，也可以有默认值，<a href="https://gist.github.com/bjodah/3cc42d9c5f70a321af29" target="_blank" rel="noopener">似乎现在也支持常数作为模板参数</a>，不过我没有尝试过，而据说老版本是不支持常数模板参数的。</p><p>之前有提到Cython中对模板的支持是阉割过的，主要特征有以下几点：</p><ul><li>Cython不支持模板参数的类型声明访问。例如上面的<code>vector</code>类型声明中不能使用<code>ctypedef allocator_type.size_type size_type</code>这样的语法，而这样的类型推断在C++中是有很多的。</li><li>Cython不支持模板构造函数中包含新的模板参数<br>不过Cython一直在改进对模板的支持，因此以后也很有可能会得到改进。</li></ul><h1 id="Buffer协议">Buffer协议<a class="header-anchor" href="#Buffer协议"> ❮</a></h1><p>Cython还针对性地支持了<a href="https://docs.python.org/3/c-api/buffer.html" target="_blank" rel="noopener">Python的Buffer协议</a>，用来传递一块结构化的内存，这个协议的标准被记录在了<a href="https://www.python.org/dev/peps/pep-3118/" target="_blank" rel="noopener">提案PEP-3118</a>中。这个协议通过<a href="https://cython.readthedocs.io/en/latest/src/userguide/buffer.html" target="_blank" rel="noopener"><code>__getbuffer__</code>和<code>__releasebuffer__</code></a>两个Cython自定义的特殊函数实现，通过这个方式Cython代码就可以将C++内存转化为Python识别的内存。因为Numpy支持将支持Buffer协议的对象转换为ndarray，因此这个Buffer协议的通常用法是将一个C++对象变成Numpy的矩阵。具体的使用案例也可以<a href="https://github.com/cmpute/pcl.py/blob/master/pcl/PointCloud.pyx#L565" target="_blank" rel="noopener">参照我的pcl封装库中的对应代码</a>。</p><hr>本文介绍了Cython中操作C/C++对象的方法，不过仅仅介绍了一些进阶用法。如果是新手的话还是先参照之前提到两篇文档学习基本的函数、类型封装方法吧～]]></content>
    
    <summary type="html">
    
      &lt;p&gt;用Cython也用了很有一段时间了，这次就介绍一下它的最重要功能——使用Cython来封装C/C++代码。最基本的封装方法可以参见Cython文档中的相关页面：&lt;a href=&quot;https://cython.readthedocs.io/en/latest/src/userguide/external_C_code.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Interfacing with External C Code&lt;/a&gt;和&lt;a href=&quot;https://cython.readthedocs.io/en/latest/src/userguide/wrapping_CPlusPlus.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Using C++ in Cython&lt;/a&gt;，本文介绍主要是比较重要和常用的Cython/C++交互特性，而自定义Python拓展类（而不是封装现有C++）的一些操作可以&lt;a href=&quot;https://cython.readthedocs.io/en/latest/src/tutorial/cdef_classes.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;参考官方教程&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;封装C++代码时，最重要的关键词就是&lt;code&gt;extern&lt;/code&gt;，在定义函数时使用这个关键字就说明该声明是外部的，而使用&lt;code&gt;cdef extern from&lt;/code&gt;语句就能指定声明对应的头文件。例如如果要封装函数&lt;code&gt;func&lt;/code&gt;，对应的Cython语句是&lt;/p&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;cdef extern &lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;func.c&quot;&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    void func(int arg)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Coding" scheme="http://zyxin.xyz/blog/categories/Coding/"/>
    
      <category term="Language" scheme="http://zyxin.xyz/blog/categories/Coding/Language/"/>
    
    
      <category term="Cython" scheme="http://zyxin.xyz/blog/tags/Cython/"/>
    
      <category term="Python" scheme="http://zyxin.xyz/blog/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>如何选择Python与C++之间的胶水</title>
    <link href="http://zyxin.xyz/blog/2019-08/GluePythonCpp/"/>
    <id>http://zyxin.xyz/blog/2019-08/GluePythonCpp/</id>
    <published>2019-08-11T21:05:40.000Z</published>
    <updated>2020-01-05T04:31:12.905Z</updated>
    
    <content type="html"><![CDATA[<p>Python作为一门胶水语言，它与C/C++之间的兼容性（Interoperability）我认为是它相比其他动态语言脱颖而出的最大原因。Python原生支持的是与C语言的接口，Python的发行版自带有<code>Python.h</code>头文件，里面提供了在C中调用Python和反过来在Python中调用C的接口定义。但是C++就不一样了，虽然C++ ⇔ C ⇔ Python的通道是可行的，但是想要完整兼容C++的特性的话需要很多额外的重复代码（boilerplate）。因此相应针对Python/C++绑定的库也就应运而生了，我所了解的库主要有四个：<a href="https://www.boost.org/doc/libs/1_70_0/libs/python/doc/html/index.html" target="_blank" rel="noopener">Boost.Python</a>，<a href="https://cython.org/" target="_blank" rel="noopener">Cython</a>，<a href="https://pybind11.readthedocs.io/en/stable/" target="_blank" rel="noopener">pybind11</a>，<a href="http://www.swig.org/" target="_blank" rel="noopener">SWIG</a>。虽然网上也有不少比较三者的页面，但是我觉得都不够详细，这篇博客就介绍一下我基于使用这几个库的经验比较。</p><p>上面说到的这些库我基本都有接触过，其中用过的有pybind11和Cython，分别用在了我正在写的<a href="https://github.com/cmpute/cgal.py" target="_blank" rel="noopener">CGAL</a>和<a href="https://github.com/cmpute/pcl.py" target="_blank" rel="noopener">PCL</a>的绑定上。另外二者则是在其他库的代码中有读过（如Caffe和CGAL的官方绑定）。总的来说，Boost.Python和pybind11主要用于给现有C++代码提供Python绑定，并且不用学习新的语法;SWIG提供一个给C++代码编写多种语言绑定的框架，它本质上是一种代码生成器，基于SWIG自定义的语法;Cython则是基于Python的C/C++代码封装器，其本质也是代码生成器，但是Cython的语法是Python的超集，也就是说Python的代码可以零成本移植到Cython中。</p><a id="more"></a><h1 id="Boost-Python-vs-pybind11">Boost.Python vs pybind11<a class="header-anchor" href="#Boost-Python-vs-pybind11"> ❮</a></h1><p>Boost.Python是一个Boost框架中封装C++代码的工具，通过宏定义和元编程来简化Python的API调用，消灭bolierplate。Boost.Python还提供对Numpy底层API的封装，因此适用性很强，能满足Python绑定的绝大多数需求。而pybind11则是受Boost.Python启发的一套类似的API，其目标是提供Header-only的易用的Python接口。由于pybind11脱胎于Boost，因此它们的接口非常相似，例如最简单的封装一个函数，Boost.Python代码如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;boost/python.hpp&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> i + j;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">BOOST_PYTHON_MODULE(example)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">using</span> <span class="keyword">namespace</span> boost::python;</span><br><span class="line">    def(<span class="string">"add"</span>, add);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>而对应的pybind11代码则是</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;pybind11/pybind11.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> i + j;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PYBIND11_MODULE(example, m) &#123;</span><br><span class="line">    m.def(<span class="string">"add"</span>, &amp;add);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>因此熟练掌握这两者之一的开发者能很快上手另一个库的使用。他们的编译方式也是相似的，只需添加一个工程，写好对应的封装代码，然后利用他们的CMake模块进行编译，生成的动态链接库只要文件名正确就可以直接从Python进行import了。他们二者的区别主要有以下几个方面：</p><ol><li>pybind11是Header-only的，因此只需把它的头文件添加到include目录就算安装好了。而Boost.Python则是需要先编译安装才能使用，需要处理其依赖。</li><li>pybind11的社区更加活跃，Boost.Python则受限于Boost的更新周期，回应反馈可能会比较慢。</li><li>pybind11的易用性更好，文档齐全且友善，由于没有依赖问题，编译方便上手也快。</li><li>Boost.Python兼容旧特性的C++，也兼容Boost自定义的类型（如smartptr），因此如果需要封装的代码是基于Boost的，那可能Boost.Python会比pybind11合适。pybind11针对的环境则是C++1x，并且只支持标准C++库。</li><li>Boost.Python对Numpy的支持比较完备，例如Boost.Python支持自定义<code>numpy.dtype</code>，而pybind11对Numpy的支持主要基于Python的buffer协议。<br>因此基本上如果封装不基于Boost的库的话可以先考虑pybind11，而如果是封装基于Boost的库（如PCL），或者深度操作Numpy，那还是直接上Boost.Python吧～</li></ol><h1 id="Boost-Python-pybind11-vs-Cython">Boost.Python/pybind11 vs Cython<a class="header-anchor" href="#Boost-Python-pybind11-vs-Cython"> ❮</a></h1><p>这两者的选用其实差别非常大，因为他们的代码逻辑都是不同的。而具体选择哪个库就纯粹是根据需求出发了。他们的区别如下（以下pybind11同时也代表了Boost.Python）</p><ol><li>pybind11基于C++，更适合C++工程师。Cython则是基于Python，写习惯的Python的人上手更快，并且能同时方便地兼容Python和C++。</li><li>Cython相比pybind11的环境配置更加简单，用户只需通过pip安装Cython就可以利用Cython的功能了，也无需配置路径。</li><li>Cython封装C++类会比Boost.Python更加繁杂，你需要先定义C++类，再封装成Python类。相当于Cython还多一步翻译头文件的工作。</li><li>Cython支持模板（虽然是阉割版本）！这是Cython独家的一个killer特性，不过是与第3点相关联的。如果你已经翻译好了现有的模板代码，那么用户就可以用Python的语法来自行展开模板了！pybind11需要在编译的时候实例化模板，因此一般只封装常用的实例，或者穷举所有实例化可能（这会导致生成的封装库尺寸爆炸）</li><li>pybind11封装重载函数比Cython要方便太多！Cython封装重载函数的话一般需要定义大量的可选参数和类型判断。</li><li>Cython封装继承类就更加麻烦了，不仅要处理方法重载，还要复制继承关系，十分繁复。</li><li>Cython无法利用上C++的宏定义，这对支持条件编译非常不利，很多时候还需要自己利用<a href="https://cython.readthedocs.io/en/latest/src/userguide/language_basics.html#conditional-statements" target="_blank" rel="noopener">Cython的条件语句</a>翻译一套条件编译的逻辑。</li></ol><p>以前很多人使用Cython的原因是Cython可以很方便地加速Python代码，但是<code>numba.jit</code>的出现则让这个功能实际上成了鸡肋，因此Cython最近的使用率也是越来越低了。如果没有很强的对保留模板灵活性的需求，或者不是封装目标不是基于C语言的，那还是选择pybind11来的方便。如果封装接口只是一小部分需求的话也还是用Cython会更加一致，我在自己的PCL绑定项目中使用Cython的原因是有大量基于Python的扩展代码，因此使用Cython还是能更方便。</p><h1 id="SWIG">SWIG<a class="header-anchor" href="#SWIG"> ❮</a></h1><p>SWIG是个很神奇的东西，他能够将C++代码封装成Python/C#/Java/Ruby等多种语言，但是也正因为这个灵活性，它对C++的高级特性的支持就比较辣鸡了。在<a href="https://github.com/sciencectn/cgal-bindings" target="_blank" rel="noopener">CGAL官方的绑定库</a>中可以看到有不少代码需要针对Python和Java打补丁，因此如果没有多语言的需求的话SWIG应该是下下策了。这应该也是SWIG一直没啥发展的原因吧～</p><hr>本文介绍了Boost.Python/pybin11/Cython/SWIG之间的特性与区别，而具体用法则是一笔带过。如果大家对其中的某工具感兴趣的话可以直接去官网看教程～]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Python作为一门胶水语言，它与C/C++之间的兼容性（Interoperability）我认为是它相比其他动态语言脱颖而出的最大原因。Python原生支持的是与C语言的接口，Python的发行版自带有&lt;code&gt;Python.h&lt;/code&gt;头文件，里面提供了在C中调用Python和反过来在Python中调用C的接口定义。但是C++就不一样了，虽然C++ ⇔ C ⇔ Python的通道是可行的，但是想要完整兼容C++的特性的话需要很多额外的重复代码（boilerplate）。因此相应针对Python/C++绑定的库也就应运而生了，我所了解的库主要有四个：&lt;a href=&quot;https://www.boost.org/doc/libs/1_70_0/libs/python/doc/html/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Boost.Python&lt;/a&gt;，&lt;a href=&quot;https://cython.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Cython&lt;/a&gt;，&lt;a href=&quot;https://pybind11.readthedocs.io/en/stable/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pybind11&lt;/a&gt;，&lt;a href=&quot;http://www.swig.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SWIG&lt;/a&gt;。虽然网上也有不少比较三者的页面，但是我觉得都不够详细，这篇博客就介绍一下我基于使用这几个库的经验比较。&lt;/p&gt;&lt;p&gt;上面说到的这些库我基本都有接触过，其中用过的有pybind11和Cython，分别用在了我正在写的&lt;a href=&quot;https://github.com/cmpute/cgal.py&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CGAL&lt;/a&gt;和&lt;a href=&quot;https://github.com/cmpute/pcl.py&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;PCL&lt;/a&gt;的绑定上。另外二者则是在其他库的代码中有读过（如Caffe和CGAL的官方绑定）。总的来说，Boost.Python和pybind11主要用于给现有C++代码提供Python绑定，并且不用学习新的语法;SWIG提供一个给C++代码编写多种语言绑定的框架，它本质上是一种代码生成器，基于SWIG自定义的语法;Cython则是基于Python的C/C++代码封装器，其本质也是代码生成器，但是Cython的语法是Python的超集，也就是说Python的代码可以零成本移植到Cython中。&lt;/p&gt;
    
    </summary>
    
      <category term="Coding" scheme="http://zyxin.xyz/blog/categories/Coding/"/>
    
      <category term="Language" scheme="http://zyxin.xyz/blog/categories/Coding/Language/"/>
    
    
      <category term="Python" scheme="http://zyxin.xyz/blog/tags/Python/"/>
    
      <category term="C++" scheme="http://zyxin.xyz/blog/tags/C/"/>
    
  </entry>
  
</feed>
