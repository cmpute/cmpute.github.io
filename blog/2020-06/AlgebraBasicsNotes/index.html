<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.0"><link rel="apple-touch-icon" sizes="180x180" href="/blog/uploads/assets/icon200.png"><link rel="icon" type="image/png" sizes="32x32" href="/blog/uploads/assets/icon32.png"><link rel="icon" type="image/png" sizes="16x16" href="/blog/uploads/assets/icon16.png"><link rel="mask-icon" href="/blog/images/logo.svg" color="#222"><link rel="stylesheet" href="/blog/css/main.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/blue/pace-theme-minimal.css"><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script><script class="next-config" data-name="main" type="application/json">{"hostname":"zyxin.xyz","root":"/blog/","images":"/blog/images","scheme":"Muse","darkmode":false,"version":"8.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":false,"nav":{"disqus":{"text":"Disqus Comments"},"livere":{"text":"LiveRe Comments"}},"store":true,"activeClass":"disqus"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/blog/js/config.js"></script><meta name="description" content="本文是用英文写的，且尚未翻译成中文   Selected notes from ROB 501 and ME 564. $\{x_i\}^b_a$ denotes set $\{x_a, x_{a+1}, \ldots, x_b\}$ TODO: add Jordan Form  Algebraic Structures ❮ Operation ❮  Definition: an (binary,"><meta property="og:type" content="article"><meta property="og:title" content="Notes for Algebra Basics"><meta property="og:url" content="http://zyxin.xyz/blog/2020-06/AlgebraBasicsNotes/index.html"><meta property="og:site_name" content="Jacob Zhong"><meta property="og:description" content="本文是用英文写的，且尚未翻译成中文   Selected notes from ROB 501 and ME 564. $\{x_i\}^b_a$ denotes set $\{x_a, x_{a+1}, \ldots, x_b\}$ TODO: add Jordan Form  Algebraic Structures ❮ Operation ❮  Definition: an (binary,"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://zyxin.xyz/blog/2020-06/AlgebraBasicsNotes/linear_map_relations.png"><meta property="article:published_time" content="2020-06-28T01:02:13.000Z"><meta property="article:modified_time" content="2023-01-01T05:17:43.911Z"><meta property="article:author" content="Jacob Zhong"><meta property="article:tag" content="Math"><meta property="article:tag" content="Algebra"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://zyxin.xyz/blog/2020-06/AlgebraBasicsNotes/linear_map_relations.png"><link rel="canonical" href="http://zyxin.xyz/blog/2020-06/AlgebraBasicsNotes/"><script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://zyxin.xyz/blog/2020-06/AlgebraBasicsNotes/","path":"2020-06/AlgebraBasicsNotes/","title":"Notes for Algebra Basics"}</script><script class="next-config" data-name="calendar" type="application/json">""</script><title>Notes for Algebra Basics | Jacob Zhong</title><noscript><link rel="stylesheet" href="/blog/css/noscript.css"></noscript><link rel="alternate" href="/blog/atom.xml" title="Jacob Zhong" type="application/atom+xml">
</head><body itemscope itemtype="http://schema.org/WebPage" class="use-motion"><div class="headband"></div><main class="main"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="Toggle navigation bar" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div></div><div class="site-meta"><a href="/blog/" class="brand" rel="start"><i class="logo-line"></i><h1 class="site-title">Jacob Zhong</h1><i class="logo-line"></i></a><p class="site-subtitle" itemprop="description">b.zyxin.xyz</p></div><div class="site-nav-right"><div class="toggle popup-trigger"></div></div></div><nav class="site-nav"><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/blog/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/blog/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/blog/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/blog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-sitemap"><a href="/blog/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li></ul></nav></div><div class="toggle sidebar-toggle" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div><aside class="sidebar"><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc">Table of Contents</li><li class="sidebar-nav-overview">Overview</li></ul><div class="sidebar-panel-container"><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Algebraic-Structures"><span class="nav-number">1.</span> <span class="nav-text">Algebraic Structures</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Operation"><span class="nav-number">1.1.</span> <span class="nav-text">Operation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Group"><span class="nav-number">1.2.</span> <span class="nav-text">Group</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ring"><span class="nav-number">1.3.</span> <span class="nav-text">Ring</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Field"><span class="nav-number">1.4.</span> <span class="nav-text">Field</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Vector-Space"><span class="nav-number">1.5.</span> <span class="nav-text">Vector Space</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Basis-Coordinate"><span class="nav-number">1.5.1.</span> <span class="nav-text">Basis &amp; Coordinate</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Norm-Inner-product"><span class="nav-number">1.5.2.</span> <span class="nav-text">Norm &amp; Inner product</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gramian"><span class="nav-number">1.5.3.</span> <span class="nav-text">Gramian</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Linear-Algebra"><span class="nav-number">2.</span> <span class="nav-text">Linear Algebra</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Linear-Operator"><span class="nav-number">2.1.</span> <span class="nav-text">Linear Operator</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Eigenvalue-and-Canonical-Forms"><span class="nav-number">2.2.</span> <span class="nav-text">Eigenvalue and Canonical Forms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SVD-and-Linear-Equations"><span class="nav-number">2.3.</span> <span class="nav-text">SVD and Linear Equations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Miscellaneous"><span class="nav-number">2.4.</span> <span class="nav-text">Miscellaneous</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Real-Analysis"><span class="nav-number">3.</span> <span class="nav-text">Real Analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Set-theory"><span class="nav-number">3.1.</span> <span class="nav-text">Set theory</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sequences"><span class="nav-number">3.2.</span> <span class="nav-text">Sequences</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Continuity-and-Compactness"><span class="nav-number">3.3.</span> <span class="nav-text">Continuity and Compactness</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Jacob Zhong" src="/blog/uploads/assets/avatar.png"><p class="site-author-name" itemprop="name">Jacob Zhong</p><div class="site-description" itemprop="description">Blog of Jacob Zhong</div></div><div class="site-state-wrap site-overview-item animated"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/blog/archives/"><span class="site-state-item-count">62</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/blog/categories/"><span class="site-state-item-count">16</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/blog/tags/"><span class="site-state-item-count">50</span> <span class="site-state-item-name">tags</span></a></div></nav></div><div class="links-of-author site-overview-item animated"><span class="links-of-author-item"><a href="https://github.com/cmpute" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;cmpute" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://www.linkedin.com/in/jacobzhong/" title="LinkedIn → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;jacobzhong&#x2F;" rel="noopener" target="_blank"><i class="linkedin-square fa-fw"></i>LinkedIn</a> </span><span class="links-of-author-item"><a href="mailto:cmpute@foxmail.com" title="E-Mail → mailto:cmpute@foxmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="https://www.last.fm/user/cmpute" title="Last.fm → https:&#x2F;&#x2F;www.last.fm&#x2F;user&#x2F;cmpute" rel="noopener" target="_blank"><i class="lastfm-square fa-fw"></i>Last.fm</a></span></div><div class="links-of-blogroll site-overview-item animated"><div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i> Friend Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="https://hanyuz1996.github.io/" title="https:&#x2F;&#x2F;hanyuz1996.github.io&#x2F;" rel="noopener" target="_blank">Hanyuz</a></li><li class="links-of-blogroll-item"><a href="https://xingminw.github.io/" title="https:&#x2F;&#x2F;xingminw.github.io&#x2F;" rel="noopener" target="_blank">Xingminw</a></li></ul></div></div></div></div></aside><div class="sidebar-dimmer"></div></header><div class="back-to-top" role="button" aria-label="Back to top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><noscript><div class="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class="post-block"><article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en"><link itemprop="mainEntityOfPage" href="http://zyxin.xyz/blog/2020-06/AlgebraBasicsNotes/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/blog/uploads/assets/avatar.png"><meta itemprop="name" content="Jacob Zhong"><meta itemprop="description" content="Blog of Jacob Zhong"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Jacob Zhong"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Notes for Algebra Basics</h1><div class="post-meta-container"><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2020-06-27 21:02:13" itemprop="dateCreated datePublished" datetime="2020-06-27T21:02:13-04:00">2020-06-27</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">Edited on</span> <time title="Modified: 2023-01-01 00:17:43" itemprop="dateModified" datetime="2023-01-01T00:17:43-05:00">2023-01-01</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/blog/categories/Notes/" itemprop="url" rel="index"><span itemprop="name">Notes</span></a> </span>, <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/blog/categories/Notes/Math/" itemprop="url" rel="index"><span itemprop="name">Math</span></a> </span></span><span class="post-meta-item" title="Views" id="busuanzi_container_page_pv"><span class="post-meta-item-icon"><i class="far fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span id="busuanzi_value_page_pv"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Disqus: </span><a title="disqus" href="/blog/2020-06/AlgebraBasicsNotes/#disqus_thread" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020-06/AlgebraBasicsNotes/" itemprop="commentCount"></span></a></span></div></div></header><div class="post-body" itemprop="articleBody"><div class="note warning"><p><strong>本文是用英文写的，且尚未翻译成中文</strong></p></div><blockquote><p>Selected notes from <code>ROB 501</code> and <code>ME 564</code>.<br>$\{x_i\}^b_a$ denotes set $\{x_a, x_{a+1}, \ldots, x_b\}$<br>TODO: add Jordan Form</p></blockquote><h1 id="Algebraic-Structures">Algebraic Structures<a class="header-anchor" href="#Algebraic-Structures"> ❮</a></h1><h2 id="Operation">Operation<a class="header-anchor" href="#Operation"> ❮</a></h2><ul><li>Definition: an (binary, closed) <strong>operation</strong> $\ast$ on a set $S$ is a mapping of $S\times S\to S$</li><li><strong>Commutative</strong>: $x\ast y=y\ast x,\;\forall x,y\in S$</li><li><strong>Associative</strong>: $(x\ast y)\ast z=x\ast (y\ast z),\;\forall x,y,z\in S$</li></ul><h2 id="Group">Group<a class="header-anchor" href="#Group"> ❮</a></h2><ul><li>Definition: a <strong>group</strong> is a pair $(\mathcal{S},\ast)$ with following axioms<ol><li>$\ast$ is associative on $\mathcal{S}$</li><li>(Identity element) $\exists e\in \mathcal{S}\text{ s.t. }x\ast e=e\ast x=x,\;\forall x\in \mathcal{S}$</li><li>(Inverse element) $\forall x\in \mathcal{S}, \exists x' \in \mathcal{S}\text{ s.t. }x\ast x'=x'\ast x=e$</li></ol></li><li><strong>Abelian</strong>: a group is called <strong>abelian group</strong> if $\ast$ is also commutative</li></ul><span id="more"></span><h2 id="Ring">Ring<a class="header-anchor" href="#Ring"> ❮</a></h2><ul><li>Definition: a <strong>ring</strong> is a triplet $(\mathcal{R},+,\ast)$ consisting of a set of <code>scalars</code> $\mathcal{R}$ and two operators + and $\ast$ with following axioms<ol><li>$(\mathcal{R},+)$ is an abelian group with identity denoted $0$</li><li>$\forall a,b,c \in \mathcal{R}\text{ s.t. }a\ast(b\ast c) = (a\ast b)\ast c$</li><li>$\exists 1\in\mathcal{R}, \forall a\in\mathcal{R}\text{ s.t. }a\cdot 1=a$</li><li>$\ast$ is distributive over $+$</li></ol></li></ul><h2 id="Field">Field<a class="header-anchor" href="#Field"> ❮</a></h2><ul><li>Definition: a <strong>field</strong> $(\mathcal{F},+,\ast)$ is a ring where $(\mathcal{F}\backslash\{0\},\ast)$ is also an abelian group.<blockquote><p>Difference from ring to field is that $\ast$ need to be commutative and have a multiplicative inverse</p></blockquote></li></ul><h2 id="Vector-Space">Vector Space<a class="header-anchor" href="#Vector-Space"> ❮</a></h2><ul><li>Definition: a <strong>vector space</strong> (aka. <strong>linear space</strong>) is a triplet $(\mathcal{U},\oplus,\cdot)$ defined over a field $(\mathcal{F},+,\ast)$ with following axioms, where set $\mathcal{U}$ is called <code>vectors</code>, operator $\oplus$ is called <code>vector addition</code> and mapping $\cdot$ is called <code>scalar multiplication</code>:<ol><li>(<strong>Null vector</strong>) $(\mathcal{U},+)$ is an abelian group with identity element $\emptyset$</li><li>Scalar multiplication is a mapping of $\mathcal{F}\times\mathcal{U}\to\mathcal{U}$</li><li>$\alpha\cdot(x\oplus y) = \alpha\cdot x \oplus \alpha\cdot y,\;\forall x,y\in\mathcal{U};\alpha\in\mathcal{F}$</li><li>$(\alpha+\beta)\cdot x = \alpha\cdot x\oplus\beta\cdot x,\;\forall x\in\mathcal{U};\alpha,\beta\in\mathcal{F}$</li><li>$(\alpha\ast\beta)\cdot x=\alpha\cdot(\beta\cdot x),\;\forall x\in\mathcal{U};\alpha,\beta\in\mathcal{F}$</li><li>$1_\mathcal{F}\cdot x=x$</li></ol><blockquote><p>Usually we don't distinguish vector addition $\oplus$ and addition of scalar $+$. Juxtaposition is also commonly used for <em>both</em> scalar multiplication $\cdot$ and multiplication of scalars $\ast$</p></blockquote></li><li><strong>Subspace</strong>: a subspace $\mathcal{V}$ of a linear space $\mathcal{U}$ over field $\mathcal{F}$ is a subset of $\mathcal{U}$ which is itself a linear space over $\mathcal{F}$ under same vector addition and scalar multiplication.</li></ul><h3 id="Basis-Coordinate">Basis &amp; Coordinate<a class="header-anchor" href="#Basis-Coordinate"> ❮</a></h3><ul><li><strong>Linear Independence</strong>: Let $\mathcal{V}$ be a vector space over $\mathcal{F}$ and let $X=\{x_i\}^n_1\subset \mathcal{V}$<ul><li>X is <strong>linearly dependent</strong> if $\exists \alpha_1,\ldots,\alpha_n\in\mathcal{F}$ not all 0 s.t. $\sum^n_{i=1} \alpha_i x_i=0$.</li><li>X is <strong>linearly independent</strong> if $\sum^n_{i=1} \alpha_i x_i=0 \Rightarrow \alpha_1=\alpha_2=\ldots=\alpha_n=0$</li></ul></li><li><strong>Span</strong>: Given a set of vectors $V$, the set of linear combinations of vectors in $V$ is called the <strong>span</strong> of it, denoted $\mathrm{span}\{V\}$</li><li><strong>Basis</strong>: A set of linearly independent vectors in a linear space $\mathcal{V}$ is a <strong>basis</strong> if every vector in $\mathcal{V}$ can be expressed as a <em>unique linear combination</em> of these vectors. (see below "Coordinate")<ul><li>Basis Expansion: Let $(X,\mathcal{F})$ be a vector space of dimension n. If $\{v_i\}^k_1,\;1\leqslant k&lt; n$ is linearly independent, then $\exists \{v_i\}^n_{k+1}$ such that $\{v_i\}_1^n$ is a basis.</li><li><strong>Reciprocal Basis</strong>: Given basis $\{v_i\}^n_1$, a set ${r_i}^1_n$ that satifies $\langle r_i,v_j \rangle=\delta_i(j)$ is a reciprocal basis. It can be generated by Gram-Schmidt Process and $\forall x\in\mathcal{X}, x=\sum^n_{i=1}\langle r_i,x\rangle v_i$.</li></ul></li><li><strong>Dimension</strong>: <em>Cardinality</em> of the basis is called the <strong>dimension</strong> of that vector space, which is equal to <em>the maximum number of linearly independent vectors</em> in the space. Denoted as $dim(\mathcal{V})$.<ul><li>In an $n$-dimensional vector space, any set of $n$ linearly independent vectors is a basis.</li></ul></li><li><strong>Coordinate</strong>: For a vector $x$ in vector space $\mathcal{V}$, given a basis $\{e_1, \ldots, e_n\}$ we can write $x$ as $x=\sum^n_{i=1}\beta_i e_i=E\beta$ where $E=\begin{bmatrix}e_1&amp;e_2&amp;\ldots&amp;e_n\end{bmatrix}$ and $\beta=\begin{bmatrix}\beta_1&amp;\beta_2&amp;\ldots&amp;\beta_n\end{bmatrix}^\top$. Here $\beta$ is called the <strong>representation</strong> (or <strong>coordinate</strong>) of $x$ given the basis $E$.</li></ul><h3 id="Norm-Inner-product">Norm &amp; Inner product<a class="header-anchor" href="#Norm-Inner-product"> ❮</a></h3><ul><li><strong>Inner Product</strong>: an operator on two vectors that produces a scalar result (i.e. $\langle\cdot,\cdot\rangle:\mathcal{V}\to\mathbb{R}\;or\;\mathbb{C}$) with following axioms:<ol><li>(Symmetry) $\langle x,y \rangle=\overline{\langle y,x\rangle},\;\forall x,y\in\mathcal{V}$</li><li>(Bilinearity) $\langle \alpha x+\beta y,z\rangle=\alpha\langle x,z\rangle+\beta\langle y,z\rangle,\;\forall x,y,z\in\mathcal{V};\alpha,\beta\in\mathbb{C}$</li><li>(Pos. definiteness) $\langle x,x\rangle\geqslant 0,\;\forall x\in\mathcal{V}$ and $\langle x,x\rangle=0\Rightarrow x=0_\mathcal{V}$</li></ol></li><li><strong>Inner Product Space</strong>: A linear space with a defined inner product</li><li><strong>Orthogonality</strong>:<ul><li>Perpedicularity of vectors ($x\perp y$): $\langle x,y\rangle=0$</li><li>Perpedicularity of a vector to a set ($y\perp\mathcal{S},\mathcal{S}\subset\mathcal{V}$): $y\perp x,\;\forall x\in\mathcal{S}$</li><li><strong>Orthogonal Set</strong>: set $\mathcal{S}\subset(\mathcal{U},\langle\cdot,\cdot\rangle)$ is orthogonal $\Leftrightarrow x\perp y,\;\forall x,y\in\mathcal{S},x\neq y$</li><li><strong>Orthonormal Set</strong>: set $\mathcal{S}$ is orthonormal iff $\mathcal{S}$ is orthogonal and $\Vert x\Vert=1,\;\forall x\in\mathcal{S}$</li><li>Orthogonality of sets ($\mathcal{X}\perp\mathcal{Y}$): $\langle x,y\rangle=0,\;\forall x\in\mathcal{X};y\in\mathcal{Y}$</li><li><strong>Orthogonal Complement</strong>: Let $(\mathcal{V},\langle\cdot,\cdot\rangle)$ be an inner product space and let $\mathcal{U}\subset\mathcal{V}$ be a subspace of $\mathcal{V}$, the orthogonal complement of $\mathcal{U}$ is $\mathcal{U}^\perp=\left\{v\in\mathcal{V}\middle|\langle v,u\rangle=0,\;\forall u\in\mathcal{U}\right\}$.<ul><li>$\mathcal{U}^\perp\subset\mathcal{V}$ is a subspace</li><li>$\mathcal{V}=\mathcal{U}\overset{\perp}{\oplus}\mathcal{U}^\perp$ ($\oplus$: direct sum, $\overset{\perp}{\oplus}$: orthogonal sum)</li></ul></li></ul></li><li><strong>Norm</strong>: A <strong>norm</strong> on a linear space $\mathcal{V}$ is mapping $\Vert\cdot\Vert:\;\mathcal{V}\to\mathbb{R}$ such that:<ol><li>(Positive definiteness) $\Vert x\Vert\geqslant 0\;\forall x\in \mathcal{V}$ and $\Vert x\Vert =0\Rightarrow x=0_\mathcal{V}$</li><li>(Homogeneous) $\Vert \alpha x\Vert=|\alpha|\cdot\Vert x\Vert,\;\forall x\in\mathcal{V},\alpha\in\mathbb{R}$</li><li>(Triangle inequality) $\Vert x+y\Vert\leqslant\Vert x\Vert+\Vert y\Vert$</li></ol></li><li><strong>Distance</strong>: Norm can be used to measure distance between two vectors. Meanwhile, distance from a vector to a (sub)space is defined as $d(x,\mathcal{S})=\inf_{y\in\mathcal{S}} d(x,y)=\inf_{y\in\mathcal{S}} \Vert x-y\Vert$<ul><li><strong>Projection Point</strong>: $x^* =\arg\min_{y\in\mathcal{S}}\Vert x-y\Vert$ is the projection point of $x$ on linear space $\mathcal{S}$.</li><li><strong>Projection Theorem</strong>: $\exists !x^* \in\mathcal{S}$ s.t. $\Vert x-x^* \Vert=d(x,\mathcal{S})$ and we have $(x-x^*) \perp\mathcal{S}$</li><li><strong>Orthogonal Projection</strong>: $P(x)=x^*:\mathcal{X}\to\mathcal{M}$ is called the orthogonal projection of $\mathcal{X}$ onto $\mathcal{M}$</li></ul></li><li><strong>Normed Space</strong>: A linear space with a defined norm $\Vert\cdot\Vert$, denoted $(\mathcal{V},\mathcal{F},\Vert\cdot\Vert)$<blockquote><p>A inner product space is always a normed space because we can define $\Vert x\Vert=\sqrt{\langle x,x\rangle}$</p></blockquote></li><li>Common $\mathbb{R}^n$ Norms:<ul><li>Euclidean norm (2-norm): $\Vert x\Vert_2=\left(\sum^n_{i=1}|x_i|^2\right)^{1/2}=\left\langle x,x\right\rangle^{1/2}=\left(x^\top x\right)^{1/2}$</li><li>$l_p$ norm (p-norm): $\Vert x\Vert_p=\left(\sum^n_{i=1}|x_i|^p\right)^{1/p}$</li><li>$l_1$ norm: $\Vert x\Vert_1=\sum^n_{i=1}|x_i|$</li><li>$l_\infty$ norm: $\Vert x\Vert_\infty=\max_{i}\{x_i\}$</li></ul></li><li>Common matrix norms:<blockquote><p>Matrix norms are also called <strong>operator norms</strong>, can measure how much a linear operator "magnifies" what it operates on.</p></blockquote><ul><li>A general form induced from $\mathbb{R}^n$ norm: $$\Vert A\Vert=\sup_{x\neq 0}\frac{\Vert Ax\Vert}{\Vert x\Vert}=\sup_{\Vert x\Vert=1}\Vert Ax\Vert$$</li><li>$\Vert A\Vert_1=\max_j\left(\sum^n_{i=1}|a_{ij}|\right)$</li><li>$\Vert A\Vert_2=\left[ \max_{\Vert x\Vert=1}\left\{(Ax)^* (Ax)\right\}\right]^{1/2}=\left[ \lambda_{max}(A^ *A)\right]^{1/2}$ ($\lambda_{max}$: largest eigenvalue)</li><li>$\Vert A\Vert_\infty=\max_i\left(\sum^n_{j=1}|a_{ij}|\right)$</li><li>(Frobenius Norm) $\Vert A\Vert_F=\left[ \sum^m_{i=1}\sum^n_{j=1}\left|a_{ij}\right|^2\right]^{1/2}=\left[ tr(A^*A)\right]^{1/2}$</li></ul></li><li>Useful inequations:<ul><li><strong>Cauchy-Schwarz</strong>: $|\langle x,y\rangle|\leqslant\left\langle x,x\right\rangle^{1/2}\cdot\left\langle y,y\right\rangle^{1/2}$</li><li><strong>Triangle</strong> (aka. $\Delta$): $\Vert x+y\Vert\leqslant\Vert x\Vert+\Vert y\Vert$<blockquote><p>Lemma: $\Vert x-y\Vert \geqslant \left| \Vert x\Vert-\Vert y\Vert \right|$</p></blockquote></li><li><strong>Pythagorean</strong>: $x\perp y \Leftrightarrow \Vert x+y\Vert=\Vert x\Vert+\Vert y\Vert$</li></ul></li></ul><h3 id="Gramian">Gramian<a class="header-anchor" href="#Gramian"> ❮</a></h3><ul><li><strong>Gram-Schmidt Process</strong>: A method to find orthogonal basis $\{v_i\}^n_1$ given an ordinary basis $\{y_i\}^n_1$. It's done by perform $v_k=y_k-\sum^{k-1}_{j=1}\frac{\langle y_k,v_j\rangle}{\langle v_j,v_j \rangle}\cdot v_j$ iteratively from 1 to $n$. To get an orthonormal basis, just normalize these vectors.</li><li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Gramian_matrix"><strong>Gram Matrix</strong></a>: The Gram matrix generated from vectors $\{y_i\}_ 1^k$ is denoted $G(y_ 1,y_ 2,\ldots,y_ k)$. Its element $G_{ij}=\langle y_i,y_j\rangle$<ul><li><strong>Gram Determinant</strong>: $g(y_1,y_2,\ldots,y_n)=\det G$</li><li><strong>Normal Equations</strong>: Given subspace $\mathcal{M}$ and its basis $\{y_i\}^n_1$, the projection point of $\forall x\in\mathcal{M}$ can be represented by $$x^*=\alpha y=\begin{bmatrix}\alpha_1&amp;\alpha_2&amp;\ldots&amp;\alpha_n\end{bmatrix}\begin{bmatrix}y_1\\y_2\\ \vdots \\y_n\end{bmatrix},\;\beta=\begin{bmatrix}\langle x,y_1\rangle\\ \langle x,y_2\rangle\\ \vdots\\ \langle x,y_n\rangle\end{bmatrix} where\;G^\top\alpha=\beta$$<blockquote><p>For least-squares problem $Ax=b$, consider $\mathcal{M}$ to be the column space of $A$, then $G=A^\top A,\;\beta=A^\top b,\;G^\top\alpha=\beta\Rightarrow\alpha=(A^\top A)^{-1}A^\top b$. Similarly for weighted least-squares problem ($\Vert x\Vert=x^\top Mx$), let $G=A^\top MA, \beta=A^\top Mb$, we can get $\alpha=(A^\top MA)^{-1}A^\top Mb$</p></blockquote></li></ul></li></ul><h1 id="Linear-Algebra">Linear Algebra<a class="header-anchor" href="#Linear-Algebra"> ❮</a></h1><h2 id="Linear-Operator">Linear Operator<a class="header-anchor" href="#Linear-Operator"> ❮</a></h2><ul><li><p>Definition: a linear operator $\mathcal{A}$ (aka. linear transformation, linear mapping) is a function $f: V\to U$ that operate on a linear space $(\mathcal{V},\mathcal{F})$ to produce elements in another linear space $(\mathcal{U},\mathcal{F})$ and obey $$\mathcal{A}(\alpha_1 x_1+\alpha_2 x_2) = \alpha_1\mathcal{A}(x_1) + \alpha_2\mathcal{A}(x_2),\;\forall x_1,x_2\in V;\alpha_1, \alpha_2\in\mathcal{F}$$</p></li><li><p><strong>Range (Space)</strong>: $\mathcal{R}(\mathcal{A})=\left\{u\in U\middle|\mathcal{A}(v)=u,\;\forall v\in V\right\}$</p></li><li><p><strong>Null Space</strong> (aka. <strong>kernel</strong>): $\mathcal{N}(\mathcal{A})=\left\{v\in V\middle|\mathcal{A}(v)=\emptyset_U\right\}$</p></li><li><p><strong>$\mathcal{A}$-invariant subspace</strong>: Given vector space $(\mathcal{V},\mathcal{F})$ and linear operator $\mathcal{A}:\mathcal{V}\rightarrow \mathcal{V}$, $\mathcal{W}\subseteq\mathcal{V}$ is $A$-invariant if $\forall x\in\mathcal{W}$, $\mathcal{A}x\in\mathcal{W}$.</p><ul><li>Both $\mathcal{R}(\mathcal{A})$ and $\mathcal{N}(\mathcal{A})$ are $\mathcal{A}$-invariant</li></ul></li><li><p>Matrix Representation: Given bases for both $V$ and $U$ (respectively $\{v_i\}^n_1$ and $\{u_j\}^m_1$), matrix representation $A$ satisfies $\mathcal{A}(v_i)=\sum^m_{j=0}A_{ji}u_j$ so that $\beta=A\alpha$ where $\alpha$ and $\beta$ is the representation of a vector under $\{v_i\}$ and $\{u_j\}$ respectively.</p><img src="/blog/2020-06/AlgebraBasicsNotes/linear_map_relations.png" title="Relation between a linear map and its matrix representations"><ul><li>$P$ and $Q$ are change of basis matrices, $A=Q^{-1}\tilde{A}P,\;\tilde{A}=QAP^{-1}$</li><li>The i-th column of $A$ is the coordinates of $\mathcal{A}(v_i)$ represented by the basis $\{u_j\}$, similarly i-th column of $\tilde{A}$ is $\mathcal{A}(\tilde{v}_i)$ represented in $\{\tilde{u}_j\}$</li><li>The i-th column of $P$ is the coordinates of $v_i$ represented by the basis $\{\tilde{v}\}$, similarly i-th column of $Q$ is $u_j$ represented in $\{\tilde{u}\}$</li></ul></li><li><p>Matrix Similarity ($A\sim B$): Two (square) matrix representations ($A,B$) of the same linear operator are called <strong>similar</strong> (or <strong>conjugate</strong>) and they satisfies $\exists P$ s.t. $B=PAP^{-1}$.</p><blockquote><p>From now on we don't distinguish between linear operator $\mathcal{A}$ and its matrix representation where choice of basis doesn't matter.</p></blockquote></li><li><p><strong>Rank</strong>: $rank(A)=\rho(A)\equiv dim(\mathcal{R}(A))$</p><ul><li><strong>Sylvester's Inequality</strong>: $\rho(A)+\rho(B)-n\leqslant \rho(AB)\leqslant \min\{\rho(A), \rho(B)\}$</li><li><strong>Singularity</strong>: $\rho(A)&lt; n$</li></ul></li><li><p><strong>Nullity</strong>: $null(A)=\nu(A)\equiv dim(\mathcal{N}(A))$</p><ul><li>$\rho(A)+\nu(A)=n$ ($n$ is the dimensionality of domain space)</li></ul></li><li><p><strong>Adjoint</strong>: The adjoint of the linear map $\mathcal{A}: \mathcal{V}\to\mathcal{W}$ is the linear map $\mathcal{A}^*: \mathcal{W}\to\mathcal{V}$ such that $\langle y,\mathcal{A}(x)\rangle_\mathcal{W}=\langle \mathcal{A}^ *(y),x\rangle_\mathcal{V}$</p><blockquote><p>For its matrix representation, adjoint of $A$ is $A^ *$, which is $A^\top$ for real numbers.<br><br>Properties of $\mathcal{A}^ *$ is similar to matrix $A^ *$</p></blockquote><ul><li>$\mathcal{U}=\mathcal{R}(A)\overset{\perp}{\oplus}\mathcal{N}(A^ *),\;\mathcal{V}=\mathcal{R}(A^ *)\overset{\perp}{\oplus}\mathcal{N}(A)$</li><li>$\mathcal{N}(A^* )=\mathcal{N}(AA^* )\subseteq\mathcal{U},\;\mathcal{R}(A)=\mathcal{R}(AA^*)\subseteq\mathcal{U}$</li></ul></li><li><p><strong>Self-adjoint</strong>: $\mathcal{A}$ is self-adjoint iff $\mathcal{A}^*=\mathcal{A}$.</p><ul><li>For self-adjoint $\mathcal{A}$, if $\mathcal{V}=\mathbb{C}^{n\times n}$ then $A$ is <strong>hermitian</strong>; if $\mathcal{V}=\mathbb{R}^{n\times n}$ then $A$ is <strong>symmetric</strong>.</li><li>Self-adjoint matrices have real eigenvalues and orthogonal eigenvectors</li><li><strong>Skew symmetric</strong>: $A^*=-A$<blockquote><p>For quadratic form $x^\top Ax=x^\top(\frac{A+A^\top}{2}+\frac{A-A^\top}{2})x$, since $A-A^\top$ is skew symmetric, scalar $x^\top (A-A^\top) x=-x^\top (A-A^\top)x$, so the skew-symmetric part is zero. Therefore for quadratic form $x^\top Ax$ we can always assume $A$ is symmetric.</p></blockquote></li></ul></li><li><p><strong>Definiteness</strong>: (for symmetric matrix $P$)</p><ul><li>Positive definite ($P\succ 0$): $\forall x\in\mathbb{R}^n\neq 0,\; x^\top Px&gt;0 \Leftrightarrow$ all eigenvalues of $P$ are positive.</li><li>Semi-positive definite ($P\succcurlyeq 0$): $x^\top Px\geqslant 0 \Leftrightarrow$ all eigenvalues of $P$ are non-negative.</li><li>Negative definite ($P\prec 0$): $x^\top Px &lt; 0 \Leftrightarrow$ all eigenvalues of $P$ are negative.</li></ul></li><li><p><strong>Orthogonal Matrix</strong>: $Q$ is orthogonal iff $Q^\top Q=I$, iff columns of $Q$ are orthonormal.</p><ul><li>If $A\in\mathbb{R}^{n\times b}$ is symmetric, then $\exists$ orthogonal $Q$ s.t. $Q^\top AQ=\Lambda=\mathrm{diag}\{\lambda_1,\ldots,\lambda_n\}$ (see <a href="#Eigendecomposition-and-Jordan-Form">Eigen-decomposition</a> section below)</li></ul></li><li><p><strong>Orthogonal Projection</strong>: Given linear space $\mathcal{X}$ and subspace $\mathcal{M}$, $P(x)=x^*:\mathcal{X}\to\mathcal{M}$ ($x^ *$ is the projection point) is called orthogonal projection. If $\{v_i\}$ is a orthonormal basis of $\mathcal{M}$, then $P(x)=\sum_i \langle x,v_i\rangle v_i$</p></li></ul><h2 id="Eigenvalue-and-Canonical-Forms">Eigenvalue and Canonical Forms<a class="header-anchor" href="#Eigenvalue-and-Canonical-Forms"> ❮</a></h2><ul><li><strong>Eigenvalue</strong> and <strong>Eigenvector</strong>: Given mapping $\mathcal{A}:\mathcal{V}\rightarrow\mathcal{V}$, if $\exists \lambda\in\mathcal{F}, v\neq \emptyset_{\mathcal{V}}\in\mathcal{V}$ s.t. $\mathcal{A}(v) = \lambda v$, then $\lambda$ is the <strong>eigenvalue</strong>, $v$ is the <strong>eigenvector</strong> (aka. <strong>spectrum</strong>).<ul><li>If eigenvalues are all distinct, then the associated eigenvectors form a basis.</li></ul></li><li><strong>Eigenspace</strong>: $\mathcal{N}_\lambda = \mathcal{N}(\mathcal{A}-\lambda \mathcal{I})$.<ul><li>$q=dim(\mathcal{N}_\lambda)$ is called the <strong>geometric multiplicity</strong> (几何重度)</li><li>$\mathcal{N}_\lambda$ is an $\mathcal{A}$-invariant subspace.</li></ul></li><li><strong>Characteristic Polynomial</strong>: $\phi(s)\equiv\mathcal{det}(A-s I)$ is a polynomial of degree $n$ in $s$<ul><li>Its solutions are the eigenvalues of $A$.</li><li>The multiplicity $m_i$ of root term $(s-\lambda_i)$ here is called <strong>algebraic multiplicity</strong> (代数重度) of $\lambda_i$.</li></ul></li><li><strong>Cayley-Hamilton Theorem</strong>: $\phi(A)=\mathbf{0}$<blockquote><p>Proof needs the eigendecomposition or Jordan decomposition descibed below</p></blockquote></li><li><strong>Minimal Polynomial</strong>: $\psi(s)$ is the minimal polynomial of $A$ iff $\psi(s)$ is the polynomial of least degree for which $\psi(A)=0$ and $\psi$ is monic (coefficient of highest order term is 1)<ul><li>The multiplicity $\eta_i$ of root term $(s-\lambda_i)$ here is called the <strong>index</strong> of $\lambda_i$</li></ul></li><li><strong>Eigendecomposition</strong> (aka. <strong>Spectral Decomposition</strong>) is directly derived from the definition of eigenvalues: $$A=Q\Lambda Q^{-1}, \Lambda=\mathrm{diag}\left\{\lambda_1,\lambda_2,\ldots,\lambda_n\right\}$$<br>where $Q$ is a square matrix whose $i$-th column is the eigenvector $q_i$ corresponding to eigenvalue $\lambda_i$.<ul><li>Feasibility: $A$ can be diagonalized (using eigendecomposition) iff. $q_i=m_i$ for all $\lambda_i$.</li><li>If $A$ has $n$ distinct eigenvalues, then $A$ can be diagonalized.</li></ul></li><li><strong>Generalized eigenvector</strong>: A vector $v$ is a generalized eigenvector of rank $k$ associated with eigenvalue $\lambda$ iff $v\in\mathcal{N}\left((A-\lambda I)^k\right)$ but $v\notin\mathcal{N}\left((A-\lambda I)^{k-1}\right)$<ul><li>If $v$ is a generalized eigenvector of rank $k$, $(A-\lambda I)v$ is a generalized eigenvector of rank $k-1$. This creates a chain of generalized eigenvectors (called <strong>Jordan Chain</strong>) from rank $k$ to $1$, and they are linearly independent.</li><li>$\eta$ (index, 幂零指数) of $\lambda$ is the smallest integer s.t. $dim\left (\mathcal {N}\left ((A-\lambda I)^\eta\right)\right)$</li><li>The space spanned by the chain of generalized eigenvectors from rank $\eta$ is called the <strong>generalized eigenspace</strong> (with dimension $\eta$).</li><li>Different generalized eigenspaces associated with the same and with different eigenvalues are orthogonal.</li></ul></li><li><strong>Jordan Decomposition</strong>: Similar to eigendecomposition, but works for all square matrices. $A=PJP^{-1}$ where $J=\mathrm{diag}\{J_1,J_2,\ldots,J_p\}$ is the <strong>Jordan Form</strong> of A consisting of Jordan Blocks.<ul><li><strong>Jordan Block</strong>: $J_i=\begin{bmatrix} \lambda &amp; 1 &amp;&amp;&amp; \\&amp;\lambda&amp;1&amp;&amp;\\&amp;&amp;\lambda&amp;\ddots&amp;\\&amp;&amp;&amp;\ddots&amp;1\\&amp;&amp;&amp;&amp;\lambda\end{bmatrix}$</li><li>Each Jordan block corresponds to a generalized eigenspace</li><li>$q_i$ = the count of Jordan blocks associated with $\lambda_i$</li><li>$m_i$ = the count of $\lambda_i$ on diagonal of $J$</li><li>$\eta_i$ = the dimension of the largest Jordan block associated with $\lambda_i$</li></ul></li></ul><blockquote><p>$\Lambda$ in eigendecomposition, $J$ in Jordan Form and $\Sigma$ in SVD (see below) are three kinds of <strong><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Canonical_form#Linear_algebra">Canonical Forms</a></strong> of a matrix $A$</p></blockquote><ul><li><strong>Function of matrics</strong>: Let $f(\cdot)$ be an analytic function and $\lambda_i$ be an eigenvalue of $A$. If $p(\cdot)$ is a polynomial that satisfies $p(\lambda_i)=f(\lambda_i)$ and $\frac{\mathrm{d}^k}{\mathrm{d}s^k} p(\lambda_i)=\frac{\mathrm{d}^k}{\mathrm{d}s^k} f(\lambda_i)$ for $k=1,\ldots,\eta_i-1$, then $f(A)\equiv p(A)$.<blockquote><ul><li>This extends the functions applicable to matrics from polynomials (trivial) to any analytical functions</li><li>By Cayley-Hamilton, we can always choose $p$ to be order $n-1$</li></ul></blockquote></li><li><strong>Sylvester's Formula</strong>: $f(A)=\sum^k_{i=1}f(\lambda_i)A_i$ ($f$ being analytic)</li></ul><h2 id="SVD-and-Linear-Equations">SVD and Linear Equations<a class="header-anchor" href="#SVD-and-Linear-Equations"> ❮</a></h2><p>SVD Decomposition is useful in various fields and teached by a lot of courses, its complete version is formulated as $$A=U\Sigma V^*, \Sigma=\begin{bmatrix}\mathbf{\sigma}&amp;\mathbf{0}\\ \mathbf{0}&amp;\mathbf{0}\end{bmatrix}, \mathbf{\sigma}=\mathrm{diag}\left\{\sqrt{\lambda_1},\sqrt{\lambda_2},\ldots,\sqrt{\lambda_r}\right\},V=\begin{bmatrix}V_1&amp;V_2\end{bmatrix},U=\begin{bmatrix}U_1&amp;U_2\end{bmatrix}$$<br>where</p><ul><li>$r=\rho(A)$ is the rank of matrix $A$</li><li>$\sigma_i$ are called <strong>sigular values</strong>, $\lambda_i$ are eigenvalues of $A^* A$</li><li>Columns of $V_1$ span $\mathcal{R}(A^ *A)=\mathcal{R}(A^ *)$, columns of $V_2$ span $\mathcal{N}(A^ *A)=\mathcal{N}(A)$</li><li>Columns of $U_1=AV_1\sigma^{-1}$ span $\mathcal{R}(A)$, columns of $U_2$ span $\mathcal{N}(A^*)$</li></ul><blockquote><p>SVD can be derived by doing eigenvalue decomposition on $A^* A$</p></blockquote><p>With SVD introduced, we can efficiently solve general linear equation $Ax=b$ as $x=x_r+x_n$ where $x_r\in\mathcal{R}(A^\top)$ and $x_n\in\mathcal{N}(A)$.</p><table><thead><tr><th></th><th>$Ax=b$</th><th>tall $A$ ($m&gt;n$)</th><th>fat $A$ ($m&lt; n$)</th></tr></thead><tbody><tr><td></td><td></td><td>Overdetermined,<br>Least Squares,<br>use Normal Equations</td><td>Underdetermined,<br>Quadratic Programming,<br>use Lagrange Multiplies</td></tr><tr><td>I.$b\in\mathcal{R}(A)$</td><td></td><td></td><td></td></tr><tr><td>1.$\mathcal{N}(A)={0}$</td><td>$x$ exist &amp; is unique</td><td>$x=(A^\top A)^{-1}A^\top b=A^+b$</td><td>$x=A^\top(AA^\top)^{-1}b=A^+b$</td></tr><tr><td>2.$\mathcal{N}(A)\neq{0}$</td><td>$x$ exist &amp; not unique</td><td>$x_r=(A^\top A)^{-1}A^\top b=A^+b$</td><td>$x_r=A^\top(AA^\top)^{-1}b=A^+b$</td></tr><tr><td>II.$b\notin\mathcal{R}(A)$</td><td></td><td></td><td></td></tr><tr><td>1.$\mathcal{N}(A)={0}$</td><td>$x$ not exists, $x_r$ exist &amp; is unique</td><td>$x_r=(A^\top A)^{-1}A^\top b=A^+b$</td><td>$x_r=A^\top(AA^\top)^{-1}b=A^+b$</td></tr><tr><td>2.$\mathcal{N}(A)\neq{0}$</td><td>$x$ not exists, $x_r$ not exist</td><td>$(A^\top A)^{-1}$ invertible</td><td>$(AA^\top)^{-1}$ invertible</td></tr></tbody></table><ul><li>$A^+=(A^\top A)^{-1}A^\top$ is left pseudo-inverse, $A^+=A^\top (AA^\top)^{-1}$ is right pseudo-inverse.</li><li>$A^+$ can be unified by the name <strong>Moore-Penrose Inverse</strong> and calculated using SVD by $A^+=V\Sigma^+ U^\top$ where $\Sigma^+$ take inverse of non-zeros.</li></ul><h2 id="Miscellaneous">Miscellaneous<a class="header-anchor" href="#Miscellaneous"> ❮</a></h2><blockquote><p>Selected theorems and lemmas useful in Linear Algebra. For more matrix properties see <a href="/blog/2019-06/MatrixAlgebra/" title="my post about Matrix Algebra">my post about Matrix Algebra</a></p></blockquote><ul><li>Matrix Square Root: $N^\top N=P$, then $N$ is the square root of $P$<blockquote><p>Square root is not unique. Cholesky decomposition is often used as square root.</p></blockquote></li><li><strong>Schur Complement</strong>: Given matrices $A_{n\times n}, B_{n\times m}, C_{m\times m}$, the matrix $M=\begin{bmatrix}A&amp;B\\ B^\top&amp;C\end{bmatrix}$ is symmetric. Then the following are equivalent (TFAE)<ol><li>$M\succ 0$</li><li>$A\succ 0$ and $C-B^\top A^{-1}B\succ 0$ (LHS called Schur complement of $A$ in $M$)</li><li>$C\succ 0$ and $A-B C^{-1}B^\top\succ 0$ (LHS called Schur complement of $C$ in $M$)</li></ol></li><li>Matrix Inverse Lemma: $(A+BCD)^{-1}=A^{-1}-A^{-1}B\left(C^{-1}+DA^{-1}B\right)^{-1}DA$</li><li>Properties of $A^\top A$<ul><li>$A^\top A \succeq 0$ and $A^\top A \succ 0 \Leftrightarrow A$ has full rank.</li><li>$A^\top A$ and $AA^\top$ have same non-zero eigenvalues, but different eigenvectors.</li><li>If $v$ is eigenvector of $A^\top A$ about $\lambda$, then $Av$ is eigenvector of $AA^\top$ about $\lambda$.</li><li>If $v$ is eigenvector of $AA^\top$ about $\lambda$, then $A^\top v$ is eigenvector of $A^\top A$ about $\lambda$.</li><li>$tr(A^\top A)=tr(AA^\top)=\sum_i\sum_j\left|A_{ij}\right|^2$</li><li>$det(A)=\prod_i\lambda_i, tr(A)=\sum_i\lambda_i$</li></ul></li></ul><h1 id="Real-Analysis">Real Analysis<a class="header-anchor" href="#Real-Analysis"> ❮</a></h1><h2 id="Set-theory">Set theory<a class="header-anchor" href="#Set-theory"> ❮</a></h2><blockquote><p>$\text{~}S$ stands for complement of set $S$ in following contents. These concepts are discussed under normed space $(\mathcal{X}, \Vert\cdot\Vert)$</p></blockquote><ul><li><strong>Open Ball</strong>: Let $x_0\in\mathcal{X}$ and let $a\in\mathbb{R}, a&gt;0$, then the open ball of radius $a$ about $x_0$ is $B_a(x_0)=\left\{x\in\mathcal{X}\middle| \Vert x-x_0\Vert &lt; a\right\}$<ul><li>Given subset $S\subset \mathcal{X}$, $d(x,S)=0\Leftrightarrow \forall\epsilon &gt;0, B_\epsilon(x)\cap S\neq\emptyset$</li><li>Given subset $S\subset \mathcal{X}$, $d(x,S)&gt;0\Leftrightarrow \exists\epsilon &gt;0, B_\epsilon(x)\cap S=\emptyset$</li></ul></li><li><strong>Interior Point</strong>: Given subset $S\subset\mathcal{X}$, $x\in S$ is an interior point of $S$ iff $\exists\epsilon &gt;0, B_\epsilon(x)\subset S$<ul><li><strong>Interior</strong>: $\mathring{S}=\{x\in \mathcal{X}|x\text{ is an interior point of }S\}=\{x\in\mathcal{X}|d(x,\text{~}S)&gt;0\}$</li></ul></li><li><strong>Open Set</strong>: $S$ is open if $\mathring{S}=S$</li><li><strong>Closure Point</strong>: Given subset $S\subset\mathcal{X}$, $x\in S$ is a closure point of $S$ iff $\forall\epsilon &gt;0, B_\epsilon(x)\cap S\neq\emptyset$.<ul><li><strong>Closure</strong>: $\bar{S}=\{x\in\mathcal{X}|x\text{ is a closure point of }S\}=\{x\in\mathcal{X}|d(x,S)=0\}$<blockquote><p>Note that $\partial\mathcal{X}=\emptyset$</p></blockquote></li></ul></li><li><strong>Closed Set</strong>: $S$ is closed if $\bar{S}=S$<blockquote><p>$S$ is open $\Leftrightarrow$ $\text{~}S$ is closed, $S$ is closed $\Leftrightarrow$ $\text{~}S$ is open. Set being both open and closed is called <strong>clopen</strong>(e.g. the whole set $\mathcal{X}$), empty set is clopen by convention.</p></blockquote></li><li><strong>Set Boundary</strong>: $\partial S=\bar{S}\cap\overline{\text{~}S}=\bar{S}\backslash\mathring{S}$</li></ul><h2 id="Sequences">Sequences<a class="header-anchor" href="#Sequences"> ❮</a></h2><ul><li><strong>Sequence</strong>($\{x_n\}$): a set of vectors indexed by the counting numbers<ul><li><strong>Subsequence</strong>: Let $1\leqslant n_1&lt; n_2&lt;\ldots$ be an infinite set of increasing integers, then $\{x_{n_i}\}$ is a subsequence of $\{x_n\}$</li></ul></li><li><strong>Convergence</strong>($\{x_n\}\to x\in\mathcal{X}$): $\forall \epsilon&gt;0,\exists N(\epsilon)&lt;\infty\text{ s.t. }\forall n\geqslant N, \Vert x_n-x\Vert &lt;\epsilon$<ul><li>If $x_n \to x$ and $x_n \to y$, then $x=y$</li><li>If $x_n \to x_0$ and $\{x_{n_i}\}$ is a subsequence of $\{x_n\}$, then $\{x_{n_i}\} \to x_0$</li><li><strong>Cauchy Convergence</strong> (necessary condition for convergence): $\{x_n\}$ is cauchy if $\forall \epsilon&gt;0,\exists N(\epsilon)&lt;\infty$ s.t. $\forall n,m\geqslant N, \Vert x_n-x_m\Vert &lt;\epsilon$</li><li>If $\mathcal{X}$ is finite dimensional, $\{x_n\}$ is cauchy $\Rightarrow$ $\{x_n\}$ has a limit in $\mathcal{X}$</li></ul></li><li><strong>Limit Point</strong>: Given subset $S\subset\mathcal{X}$, $x$ is a limit point of $S$ if $\exists \{x_n\}$ s.t. $\forall n\geqslant 1, x_n\in S$ and $x_n\to x$<ul><li>$x$ is a limit point of $S$ iff $x\in\bar{S}$</li><li>$S$ is closed iff $S$ contains its limit points</li></ul></li><li><strong>Complete Space</strong>: a normed space is <strong>complete</strong> if every Cauchy sequence has a limit. A complete normed space $(\mathcal{X}, \Vert\cdot\Vert)$ is called a <strong>Banach space</strong>.<ul><li>$S\subset \mathcal{X}$ is complete if every Cauchy sequence with elements from $S$ has a limit in $S$</li><li>$S\subset \mathcal{X}$ is complete $\Rightarrow S$ is closed</li><li>$\mathcal{X}$ is complete and $S\subset\mathcal{X} \Rightarrow S$ is complete</li><li>All finite dimensional subspaces of $X$ are complete</li></ul></li><li><strong>Completion of Normed Space</strong>: $\mathcal{Y}=\bar{\mathcal{X}}=\mathcal{X}+\{$all limit points of Cauchy sequences in $\mathcal{X}\}$<blockquote><p>E.g. $C[a,b]$ contains continuous functions over $[a,b]$. $(C[a,b], \Vert\cdot\Vert_1)$ is not complete, $(C[a,b], \Vert\cdot\Vert_\infty)$ is complete. Completion of $(C[a,b], \Vert\cdot\Vert_1)$ requires Lebesque integration.</p></blockquote></li><li><strong>Contraction Mapping</strong>: Let $S\subset\mathcal{X}$ be a subset and $T:S\to S$ is a contraction mapping if $\exists 0\leqslant c\leqslant 1$ such that, $\forall x,y \in S, \Vert T(x)-T(y)\Vert\leqslant c\Vert x-y\Vert$<ul><li><strong>Fixed Point</strong>: $x^* \in\mathcal{X}$ is a fixed point of $T$ if $T(x^ *)=x^ *$</li><li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Banach_fixed-point_theorem"><strong>Contraction Mapping Theorem</strong> (不动点定理)</a>: If $T:S\to S$ is a contraction mapping in a complete subset $S$, then $\exists! x^ *\in\mathcal{X}\text{ s.t. }T(x^ *)=x^ *$. Moreover, $\forall x_0\in S$, the sequence $x_{k+1}=T(x_k),k\geqslant 0$ is Cauchy and converges to $x^ *$.<blockquote><p>E.g. Newton Method: $x_{k+1}=x_k-\epsilon\left[\frac{\partial h}{\partial x}(x_k)\right]^{-1}\left(h(x_k)-y\right)$</p></blockquote></li></ul></li></ul><h2 id="Continuity-and-Compactness">Continuity and Compactness<a class="header-anchor" href="#Continuity-and-Compactness"> ❮</a></h2><ul><li><strong>Continuous</strong>: Let $(\mathcal{X},\Vert\cdot\Vert_\mathcal{X})$ and $(\mathcal{Y},\Vert\cdot\Vert_\mathcal{Y})$ be two normed spaces. A function $f:\mathcal{X}\to\mathcal{Y}$ is continuous at $x_0\in\mathcal{X}$ if $\forall\epsilon &gt;0,\exists \delta(\epsilon,x_0)&gt;0\text{ s.t. }\Vert x-x_0\Vert_\mathcal{X}&lt;\delta \Rightarrow\Vert f(x)-f(x_0)\Vert_\mathcal{Y} &lt;\epsilon$<ul><li>$f$ is continuous on $S\subset\mathcal{X}$ if $f$ is continuous at $\forall x_0\in S$</li><li>If $f$ in continuous at $x_0$ and $\{x_n\}$ is a sequence s.t. $x_n\to x_0$, then the sequence $\{f(x_n)\}$ in $\mathcal{Y}$ converges to $f(x_0)$</li><li>If $f$ is discontinuous at $x_0$, then $\exists \{x_n\}\in\mathcal{X}$ s.t. $x_n\to x_0$ but $f(x_n)\nrightarrow f(x_0)$</li></ul></li><li><strong>Compact</strong>: $S\subset\mathcal{X}$ is (sequentially) compact if every sequence in $S$ has a convergent subsequence with limit in $S$</li><li><strong>Bounded</strong>: $S\subset\mathcal{S}$ is bounded if $\exists r&lt;\infty$ such that $S\subset B_r(0)$<ul><li>$S$ is compact $\Rightarrow$ $S$ is closed and bounded</li><li><strong>Bolzano-Weierstrass Theorem</strong>: In a finite-dimensional normed space, $C$ is closed and bounded $\Leftrightarrow$ for $C$ is compact</li></ul></li><li><strong>Weierstrass Theorem</strong>: If $C\subset\mathcal{X}$ is a compact subset and $f:C\to\mathbb{R}$ is continuous at each point of $C$, then $f$ achieves its extreme values, i.e. $\exists \bar{x}\in C\text{ s.t. }f(\bar{x})=\sup_{x\in C} f(x)$ and $\exists \underline{x}\in C\text{ s.t. }f(\underline{x})=\inf_{x\in C} f(x)$<ul><li>$f:C\to\mathbb{R}$ continuous and $C$ compact $\Rightarrow$ $\sup_{x\in C}f(x)&lt;\infty$</li></ul></li></ul></div><footer class="post-footer"><div class="reward-container"><div>Treat me some coffee XD</div><button>Donate</button><div class="post-reward"><div><img src="/blog/uploads/assets/wechatpay.jpg" alt="Jacob Zhong WeChat Pay"> <span>WeChat Pay</span></div><div><img src="/blog/uploads/assets/alipay.jpg" alt="Jacob Zhong Alipay"> <span>Alipay</span></div></div></div><div class="post-copyright"><ul><li class="post-copyright-author"><strong>Post author: </strong>Jacob Zhong</li><li class="post-copyright-link"><strong>Post link: </strong><a href="http://zyxin.xyz/blog/2020-06/AlgebraBasicsNotes/" title="Notes for Algebra Basics">http://zyxin.xyz/blog/2020-06/AlgebraBasicsNotes/</a></li><li class="post-copyright-license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.</li></ul></div><div class="post-tags"><a href="/blog/tags/Math/" rel="tag"># Math</a> <a href="/blog/tags/Algebra/" rel="tag"># Algebra</a></div><div class="post-nav"><div class="post-nav-item"><a href="/blog/2020-06/ControlSystemNotes/" rel="prev" title="Notes for Control System"><i class="fa fa-chevron-left"></i> Notes for Control System</a></div><div class="post-nav-item"><a href="/blog/2020-12/MCBud112/" rel="next" title="Minecraft 1.12 建服及侦测器 BUD">Minecraft 1.12 建服及侦测器 BUD <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="tabs tabs-comment"><ul class="nav-tabs"><li class="tab"><a href="#comment-disqus">Disqus Comments</a></li><li class="tab"><a href="#comment-livere">LiveRe Comments</a></li></ul><div class="tab-content"><div class="tab-pane disqus" id="comment-disqus"><div class="comments" id="disqus_thread"><noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript></div></div><div class="tab-pane livere" id="comment-livere"><div class="comments" id="lv-container" data-id="city" data-uid="MTAyMC81NDQ2Ni8zMDkzNw=="></div></div></div></div></div></main><footer class="footer"><div class="footer-inner"><div class="languages"><label class="lang-select-label"><i class="fa fa-language"></i> <span>English</span> <i class="fa fa-angle-up" aria-hidden="true"></i></label> <select class="lang-select" data-canonical="" aria-label="Select language"><option value="zh-CN" data-href="/blog/2020-06/AlgebraBasicsNotes/" selected>简体中文</option><option value="en" data-href="/blog/en/2020-06/AlgebraBasicsNotes/" selected>English</option></select></div><div class="copyright">&copy; 2017 – <span itemprop="copyrightYear">2023</span> <span class="with-love"><i class="fire"></i> </span><span class="author" itemprop="copyrightHolder">Jacob Zhong</span></div><div class="busuanzi-count"><span class="post-meta-item" id="busuanzi_container_site_uv"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="Total Visitors"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-item" id="busuanzi_container_site_pv"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="Total Views"><span id="busuanzi_value_site_pv"></span></span></span></div><div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/@next-theme/pjax@0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script><script src="/blog/js/comments.js"></script><script src="/blog/js/utils.js"></script><script src="/blog/js/motion.js"></script><script src="/blog/js/schemes/muse.js"></script><script src="/blog/js/next-boot.js"></script><script src="/blog/js/pjax.js"></script><script src="/blog/js/third-party/pace.js"></script><script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script><script src="/blog/js/third-party/math/mathjax.js"></script><script class="next-config" data-name="disqus" type="application/json">{"enable":true,"shortname":"jacob-zhongs-blog","count":true,"i18n":{"disqus":"disqus"}}</script><script src="/blog/js/third-party/comments/disqus.js"></script><script src="/blog/js/third-party/comments/livere.js"></script></body></html>