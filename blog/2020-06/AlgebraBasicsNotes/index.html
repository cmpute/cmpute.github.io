<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 3.9.0"><link rel="apple-touch-icon" sizes="180x180" href="/blog/uploads/assets/icon200.png"><link rel="icon" type="image/png" sizes="32x32" href="/blog/uploads/assets/icon32.png"><link rel="icon" type="image/png" sizes="16x16" href="/blog/uploads/assets/icon16.png"><link rel="mask-icon" href="/blog/images/logo.svg" color="#222"><link rel="stylesheet" href="/blog/css/main.css"><link rel="stylesheet" href="/blog/lib/font-awesome/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"zyxin.xyz",root:"/blog/",scheme:"Muse",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!0},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}}}</script><meta name="description" content="Selected notes from ROB 501 and ME 564.$\{x_i\}^b_a$ denotes set $\{x_a, x_{a+1}, \ldots, x_b\}$Algebraic StructuresOperationDefinition: an (binary, closed) operation $\ast$ on a set $S$ is a mapping"><meta name="keywords" content="Math,Algebra"><meta property="og:type" content="article"><meta property="og:title" content="Notes for Algebra Basics"><meta property="og:url" content="http://zyxin.xyz/blog/2020-06/AlgebraBasicsNotes/index.html"><meta property="og:site_name" content="Jacob Zhong"><meta property="og:description" content="Selected notes from ROB 501 and ME 564.$\{x_i\}^b_a$ denotes set $\{x_a, x_{a+1}, \ldots, x_b\}$Algebraic StructuresOperationDefinition: an (binary, closed) operation $\ast$ on a set $S$ is a mapping"><meta property="og:locale" content="en"><meta property="og:image" content="http://zyxin.xyz/blog/2020-06/AlgebraBasicsNotes/linear_map_relations.png"><meta property="og:updated_time" content="2020-06-28T01:02:13.161Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Notes for Algebra Basics"><meta name="twitter:description" content="Selected notes from ROB 501 and ME 564.$\{x_i\}^b_a$ denotes set $\{x_a, x_{a+1}, \ldots, x_b\}$Algebraic StructuresOperationDefinition: an (binary, closed) operation $\ast$ on a set $S$ is a mapping"><meta name="twitter:image" content="http://zyxin.xyz/blog/2020-06/AlgebraBasicsNotes/linear_map_relations.png"><link rel="canonical" href="http://zyxin.xyz/blog/2020-06/AlgebraBasicsNotes/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"en"}</script><title>Notes for Algebra Basics | Jacob Zhong</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="Toggle navigation bar"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/blog/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Jacob Zhong</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">Blog</p></div><div class="site-nav-right"><div class="toggle popup-trigger"></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/blog/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/blog/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/blog/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/blog/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a></li><li class="menu-item menu-item-sitemap"><a href="/blog/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>Sitemap</a></li></ul></nav></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en"><link itemprop="mainEntityOfPage" href="http://zyxin.xyz/blog/2020-06/AlgebraBasicsNotes/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/blog/uploads/assets/avatar.png"><meta itemprop="name" content="Jacob Zhong"><meta itemprop="description" content="Blog of Jacob Zhong"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Jacob Zhong"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Notes for Algebra Basics</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2020-06-27 21:02:13" itemprop="dateCreated datePublished" datetime="2020-06-27T21:02:13-04:00">2020-06-27</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/blog/categories/Notes/" itemprop="url" rel="index"><span itemprop="name">Notes</span></a> </span>, <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/blog/categories/Notes/Math/" itemprop="url" rel="index"><span itemprop="name">Math</span></a> </span></span><span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span id="busuanzi_value_page_pv"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">Disqus: </span><a title="disqus" href="/blog/2020-06/AlgebraBasicsNotes/#disqus_thread" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020-06/AlgebraBasicsNotes/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><blockquote><p>Selected notes from <code>ROB 501</code> and <code>ME 564</code>.<br>$\{x_i\}^b_a$ denotes set $\{x_a, x_{a+1}, \ldots, x_b\}$</p></blockquote><h1>Algebraic Structures</h1><h2 id="Operation">Operation</h2><ul><li>Definition: an (binary, closed) <strong>operation</strong> $\ast$ on a set $S$ is a mapping of $S\times S\rightarrow S$</li><li><strong>Commutative</strong>: $x\ast y=y\ast x,\;\forall x,y\in S$</li><li><strong>Associative</strong>: $(x\ast y)\ast z=x\ast (y\ast z),\;\forall x,y,z\in S$</li></ul><h2 id="Group">Group</h2><ul><li>Definition: a <strong>group</strong> is a pair $(\mathcal{S},\ast)$ with following axioms<ol><li>$\ast$ is associative on $\mathcal{S}$</li><li>(Identity element) $\exists e\in \mathcal{S}\text{ s.t. }x\ast e=e\ast x=x,\;\forall x\in \mathcal{S}$</li><li>(Inverse element) $\forall x\in \mathcal{S}, \exists x’ \in \mathcal{S}\text{ s.t. }x\ast x’=x’\ast x=e$</li></ol></li><li><strong>Abelian</strong>: a group is called <strong>abelian group</strong> if $\ast$ is also commutative</li></ul><a id="more"></a><h2 id="Ring">Ring</h2><ul><li>Definition: a <strong>ring</strong> is a triplet $(\mathcal{R},+,\ast)$ consisting of a set of <code>scalars</code> $\mathcal{R}$ and two operators + and $\ast$ with following axioms<ol><li>$(\mathcal{R},+)$ is an abelian group with identity denoted $0$</li><li>$\forall a,b,c \in \mathcal{R}\text{ s.t. }a\ast(b\ast c) = (a\ast b)\ast c$</li><li>$\exists 1\in\mathcal{R}, \forall a\in\mathcal{R}\text{ s.t. }a\cdot 1=a$</li><li>$\ast$ is distributive over $+$</li></ol></li></ul><h2 id="Field">Field</h2><ul><li>Definition: a <strong>field</strong> $(\mathcal{F},+,\ast)$ is a ring where $(\mathcal{F}\backslash\{0\},\ast)$ is also an abelian group.<blockquote><p>Difference from ring to field is that $\ast$ need to be commutative and have a multiplicative inverse</p></blockquote></li></ul><h2 id="Vector-Space">Vector Space</h2><ul><li>Definition: a <strong>vector space</strong> (aka. <strong>linear space</strong>) is a triplet $(\mathcal{U},\oplus,\cdot)$ defined over a field $(\mathcal{F},+,\ast)$ with following axioms, where set $\mathcal{U}$ is called <code>vectors</code>, operator $\oplus$ is called <code>vector addition</code> and mapping $\cdot$ is called <code>scalar multiplication</code>:<ol><li>(<strong>Null vector</strong>) $(\mathcal{U},+)$ is an abelian group with identity element $\emptyset$</li><li>Scalar multiplication is a mapping of $\mathcal{F}\times\mathcal{U}\rightarrow\mathcal{U}$</li><li>$\alpha\cdot(x\oplus y) = \alpha\cdot x \oplus \alpha\cdot y,\;\forall x,y\in\mathcal{U};\alpha\in\mathcal{F}$</li><li>$(\alpha+\beta)\cdot x = \alpha\cdot x\oplus\beta\cdot x,\;\forall x\in\mathcal{U};\alpha,\beta\in\mathcal{F}$</li><li>$(\alpha\ast\beta)\cdot x=\alpha\cdot(\beta\cdot x),\;\forall x\in\mathcal{U};\alpha,\beta\in\mathcal{F}$</li><li>$1_\mathcal{F}\cdot x=x$</li></ol><blockquote><p>Usually we don’t distinguish vector addition $\oplus$ and addition of scalar $+$. Juxtaposition is also commonly used for <em>both</em> scalar multiplication $\cdot$ and multiplication of scalars $\ast$</p></blockquote></li><li><strong>Subspace</strong>: a subspace $\mathcal{V}$ of a linear space $\mathcal{U}$ over field $\mathcal{F}$ is a subset of $\mathcal{U}$ which is itself a linear space over $\mathcal{F}$ under same vector addition and scalar multiplication.</li></ul><h3 id="Basis-Coordinate">Basis &amp; Coordinate</h3><ul><li><strong>Linear Independence</strong>: Let $\mathcal{V}$ be a vector space over $\mathcal{F}$ and let $X=\{x_i\}^n_1\subset \mathcal{V}$<ul><li>X is <strong>linearly dependent</strong> if $\exists \alpha_1,\ldots,\alpha_n\in\mathcal{F}$ not all 0 s.t. $\sum^n_{i=1} \alpha_i x_i=0$.</li><li>X is <strong>linearly independent</strong> if $\sum^n_{i=1} \alpha_i x_i=0 \Rightarrow \alpha_1=\alpha_2=\ldots=\alpha_n=0$</li></ul></li><li><strong>Span</strong>: Given a set of vectors $V$, the set of linear combinations of vectors in $V$ is called the <strong>span</strong> of it, denoted $Span\{V\}$</li><li><strong>Basis</strong>: A set of linearly independent vectors in a linear space $\mathcal{V}$ is a <strong>basis</strong> if every vector in $\mathcal{V}$ can be expressed as a <em>unique linear combination</em> of these vectors. (see below “Coordinate”)<ul><li>Basis Expansion: Let $(X,\mathcal{F})$ be a vector space of dimension n. If $\{v_i\}^k_1,\;1\leqslant k&lt; n$ is linearly independent, then $\exists \{v_i\}^n_{k+1}$ such that $\{v_i\}_1^n$ is a basis.</li><li><strong>Reciprocal Basis</strong>: Given basis $\{v_i\}^n_1$, a set ${r_i}^1_n$ that satifies $\langle r_i,v_j \rangle=\delta_i(j)$ is a reciprocal basis. It can be generated by Gram-Schmidt Process and $\forall x\in\mathcal{X}, x=\sum^n_{i=1}\langle r_i,x\rangle v_i$.</li></ul></li><li><strong>Dimension</strong>: <em>Cardinality</em> of the basis is called the <strong>dimension</strong> of that vector space, which is equal to <em>the maximum number of linearly independent vectors</em> in the space. Denoted as $dim(\mathcal{V})$.<ul><li>In an $n$-dimensional vector space, any set of $n$ linearly independent vectors is a basis.</li></ul></li><li><strong>Coordinate</strong>: For a vector $x$ in vector space $\mathcal{V}$, given a basis $\{e_1, \ldots, e_n\}$ we can write $x$ as $x=\sum^n_{i=1}\beta_i e_i=E\beta$ where $E=\begin{bmatrix}e_1&amp;e_2&amp;\ldots&amp;e_n\end{bmatrix}$ and $\beta=\begin{bmatrix}\beta_1&amp;\beta_2&amp;\ldots&amp;\beta_n\end{bmatrix}^\top$. Here $\beta$ is called the <strong>representation</strong> (or <strong>coordinate</strong>) of $x$ given the basis $E$.</li></ul><h3 id="Norm-Inner-product">Norm &amp; Inner product</h3><ul><li><strong>Inner Product</strong>: an operator on two vectors that produces a scalar result (i.e. $\langle\cdot,\cdot\rangle:\mathcal{V}\rightarrow\mathbb{R}\;or\;\mathbb{C}$) with following axioms:<ol><li>(Symmetry) $\langle x,y \rangle=\overline{\langle y,x\rangle},\;\forall x,y\in\mathcal{V}$</li><li>(Bilinearity) $\langle \alpha x+\beta y,z\rangle=\alpha\langle x,z\rangle+\beta\langle y,z\rangle,\;\forall x,y,z\in\mathcal{V};\alpha,\beta\in\mathbb{C}$</li><li>(Pos. definiteness) $\langle x,x\rangle\geqslant 0,\;\forall x\in\mathcal{V}$ and $\langle x,x\rangle=0\Rightarrow x=0_\mathcal{V}$</li></ol></li><li><strong>Inner Product Space</strong>: A linear space with a defined inner product</li><li><strong>Orthogonality</strong>:<ul><li>Perpedicularity of vectors ($x\perp y$): $\langle x,y\rangle=0$</li><li>Perpedicularity of a vector to a set ($y\perp\mathcal{S},\mathcal{S}\subset\mathcal{V}$): $y\perp x,\;\forall x\in\mathcal{S}$</li><li><strong>Orthogonal Set</strong>: set $\mathcal{S}\subset(\mathcal{U},\langle\cdot,\cdot\rangle)$ is orthogonal $\Leftrightarrow x\perp y,\;\forall x,y\in\mathcal{S},x\neq y$</li><li><strong>Orthonormal Set</strong>: set $\mathcal{S}$ is orthonormal iff $\mathcal{S}$ is orthogonal and $\Vert x\Vert=1,\;\forall x\in\mathcal{S}$</li><li>Orthogonality of sets ($\mathcal{X}\perp\mathcal{Y}$): $\langle x,y\rangle=0,\;\forall x\in\mathcal{X};y\in\mathcal{Y}$</li><li><strong>Orthogonal Complement</strong>: Let $(\mathcal{V},\langle\cdot,\cdot\rangle)$ be an inner product space and let $\mathcal{U}\subset\mathcal{V}$ be a subspace of $\mathcal{V}$, the orthogonal complement of $\mathcal{U}$ is $\mathcal{U}^\perp=\left\{v\in\mathcal{V}\middle|\langle v,u\rangle=0,\;\forall u\in\mathcal{U}\right\}$.<ul><li>$\mathcal{U}^\perp\subset\mathcal{V}$ is a subspace</li><li>$\mathcal{V}=\mathcal{U}\overset{\perp}{\oplus}\mathcal{U}^\perp$ ($\oplus$: direct sum, $\overset{\perp}{\oplus}$: orthogonal sum)</li></ul></li></ul></li><li><strong>Norm</strong>: A <strong>norm</strong> on a linear space $\mathcal{V}$ is mapping $\Vert\cdot\Vert:\;\mathcal{V}\rightarrow\mathbb{R}$ such that:<ol><li>(Positive definiteness) $\Vert x\Vert\geqslant 0\;\forall x\in \mathcal{V}$ and $\Vert x\Vert =0\Rightarrow x=0_\mathcal{V}$</li><li>(Homogeneous) $\Vert \alpha x\Vert=|\alpha|\cdot\Vert x\Vert,\;\forall x\in\mathcal{V},\alpha\in\mathbb{R}$</li><li>(Triangle inequality) $\Vert x+y\Vert\leqslant\Vert x\Vert+\Vert y\Vert$</li></ol></li><li><strong>Distance</strong>: Norm can be used to measure distance between two vectors. Meanwhile, distance from a vector to a (sub)space is defined as $d(x,\mathcal{S})=\inf_{y\in\mathcal{S}} d(x,y)=\inf_{y\in\mathcal{S}} \Vert x-y\Vert$<ul><li><strong>Projection Point</strong>: $x^* =\arg\min_{y\in\mathcal{S}}\Vert x-y\Vert$ is the projection point of $x$ on linear space $\mathcal{S}$.</li><li><strong>Projection Theorem</strong>: $\exists !x^* \in\mathcal{S}$ s.t. $\Vert x-x^* \Vert=d(x,\mathcal{S})$ and we have $(x-x^*) \perp\mathcal{S}$</li><li><strong>Orthogonal Projection</strong>: $P(x)=x^*:\mathcal{X}\rightarrow\mathcal{M}$ is called the orthogonal projection of $\mathcal{X}$ onto $\mathcal{M}$</li></ul></li><li><strong>Normed Space</strong>: A linear space with a defined norm $\Vert\cdot\Vert$, denoted $(\mathcal{V},\mathcal{F},\Vert\cdot\Vert)$<blockquote><p>A inner product space is always a normed space because we can define $\Vert x\Vert=\sqrt{\langle x,x\rangle}$</p></blockquote></li><li>Common $\mathbb{R}^n$ Norms:<ul><li>Euclidean norm (2-norm): $\Vert x\Vert_2=\left(\sum^n_{i=1}|x_i|^2\right)^{1/2}=\left\langle x,x\right\rangle^{1/2}=\left(x^\top x\right)^{1/2}$</li><li>$l_p$ norm (p-norm): $\Vert x\Vert_p=\left(\sum^n_{i=1}|x_i|^p\right)^{1/p}$</li><li>$l_1$ norm: $\Vert x\Vert_1=\sum^n_{i=1}|x_i|$</li><li>$l_\infty$ norm: $\Vert x\Vert_\infty=\max_{i}\{x_i\}$</li></ul></li><li>Common matrix norms:<blockquote><p>Matrix norms are also called <strong>operator norms</strong>, can measure how much a linear operator “magnifies” what it operates on.</p></blockquote><ul><li>A general form induced from $\mathbb{R}^n$ norm: $$\Vert A\Vert=\sup_{x\neq 0}\frac{\Vert Ax\Vert}{\Vert x\Vert}=\sup_{\Vert x\Vert=1}\Vert Ax\Vert$$</li><li>$\Vert A\Vert_1=\max_j\left(\sum^n_{i=1}|a_{ij}|\right)$</li><li>$\Vert A\Vert_2=\left[ \max_{\Vert x\Vert=1}\left\{(Ax)^* (Ax)\right\}\right]^{1/2}=\left[ \lambda_{max}(A^ *A)\right]^{1/2}$ ($\lambda_{max}$: largest eigenvalue)</li><li>$\Vert A\Vert_\infty=\max_i\left(\sum^n_{j=1}|a_{ij}|\right)$</li><li>(Frobenius Norm) $\Vert A\Vert_F=\left[ \sum^m_{i=1}\sum^n_{j=1}\left|a_{ij}\right|^2\right]^{1/2}=\left[ tr(A^*A)\right]^{1/2}$</li></ul></li><li>Useful inequations:<ul><li><strong>Cauchy-Schwarz</strong>: $|\langle x,y\rangle|\leqslant\left\langle x,x\right\rangle^{1/2}\cdot\left\langle y,y\right\rangle^{1/2}$</li><li><strong>Triangle</strong> (aka. $\Delta$): $\Vert x+y\Vert\leqslant\Vert x\Vert+\Vert y\Vert$<blockquote><p>Lemma: $\Vert x-y\Vert \geqslant \left| \Vert x\Vert-\Vert y\Vert \right|$</p></blockquote></li><li><strong>Pythagorean</strong>: $x\perp y \Leftrightarrow \Vert x+y\Vert=\Vert x\Vert+\Vert y\Vert$</li></ul></li></ul><h3 id="Gramian">Gramian</h3><ul><li><strong>Gram-Schmidt Process</strong>: A method to find orthogonal basis $\{v_i\}^n_1$ given an ordinary basis $\{y_i\}^n_1$. It’s done by perform $v_k=y_k-\sum^{k-1}_{j=1}\frac{\langle y_k,v_j\rangle}{\langle v_j,v_j \rangle}\cdot v_j$ iteratively from 1 to $n$. To get an orthonormal basis, just normalize these vectors.</li><li><a href="https://en.wikipedia.org/wiki/Gramian_matrix" target="_blank" rel="noopener"><strong>Gram Matrix</strong></a>: The Gram matrix generated from vectors $\{y_i\}_ 1^k$ is denoted $G(y_ 1,y_ 2,\ldots,y_ k)$. Its element $G_{ij}=\langle y_i,y_j\rangle$<ul><li><strong>Gram Determinant</strong>: $g(y_1,y_2,\ldots,y_n)=\det G$</li><li><strong>Normal Equations</strong>: Given subspace $\mathcal{M}$ and its basis $\{y_i\}^n_1$, the projection point of $\forall x\in\mathcal{M}$ can be represented by $$x^*=\alpha y=\begin{bmatrix}\alpha_1&amp;\alpha_2&amp;\ldots&amp;\alpha_n\end{bmatrix}\begin{bmatrix}y_1\\y_2\\ \vdots \\y_n\end{bmatrix},\;\beta=\begin{bmatrix}\langle x,y_1\rangle\\ \langle x,y_2\rangle\\ \vdots\\ \langle x,y_n\rangle\end{bmatrix} where\;G^\top\alpha=\beta$$<blockquote><p>For least-squares problem $Ax=b$, consider $\mathcal{M}$ to be the column space of $A$, then $G=A^\top A,\;\beta=A^\top b,\;G^\top\alpha=\beta\Rightarrow\alpha=(A^\top A)^{-1}A^\top b$. Similarly for weighted least-squares problem ($\Vert x\Vert=x^\top Mx$), let $G=A^\top MA, \beta=A^\top Mb$, we can get $\alpha=(A^\top MA)^{-1}A^\top Mb$</p></blockquote></li></ul></li></ul><h1>Linear Algebra</h1><h2 id="Linear-Operator">Linear Operator</h2><ul><li><p>Definition: a linear operator $\mathcal{A}$ (aka. linear transformation, linear mapping) is a function $f: V\rightarrow U$ that operate on a linear space $(V,\mathcal{F})$ to produce elements in another linear space $(U,\mathcal{F})$ and obey $$\mathcal{A}(\alpha_1 x_1+\alpha_2 x_2) = \alpha_1\mathcal{A}(x_1) + \alpha_2\mathcal{A}(x_2),\;\forall x_1,x_2\in V;\alpha_1, \alpha_2\in\mathcal{F}$$</p></li><li><p><strong>Range (Space)</strong>: $\mathcal{R}(\mathcal{A})=\left\{u\in U\middle|\mathcal{A}(v)=u,\;\forall v\in V\right\}$</p></li><li><p><strong>Null Space</strong> (aka. <strong>kernel</strong>): $\mathcal{N}(\mathcal{A})=\left\{v\in V\middle|\mathcal{A}(v)=\emptyset_U\right\}$</p></li><li><p>Matrix Representation: Given bases for both $V$ and $U$ (respectively $\{v_i\}^n_1$ and $\{u_j\}^m_1$), matrix representation $A$ satisfies $\mathcal{A}(v_i)=\sum^m_{j=0}A_{ji}u_j$ so that $\beta=A\alpha$ where $\alpha$ and $\beta$ is the representation of a vector under $\{v_i\}$ and $\{u_j\}$ respectively.</p><img src="/blog/2020-06/AlgebraBasicsNotes/linear_map_relations.png" title="Relation between a linear map and its matrix representations"><ul><li>$P$ and $Q$ are change of basis matrices, $A=Q^{-1}\tilde{A}P,\;\tilde{A}=QAP^{-1}$</li><li>The i-th column of $A$ is the coordinates of $\mathcal{A}(v_i)$ represented by the basis $\{u_j\}$, similarly i-th column of $\tilde{A}$ is $\mathcal{A}(\tilde{v}_i)$ represented in $\{\tilde{u}_j\}$</li><li>The i-th column of $P$ is the coordinates of $v_i$ represented by the basis $\{\tilde{v}\}$, similarly i-th column of $Q$ is $u_j$ represented in $\{\tilde{u}\}$</li></ul></li><li><p>Matrix Similarity ($A\sim B$): Two (square) matrix representations ($A,B$) of the same linear operator are called <strong>similar</strong> (or <strong>conjugate</strong>) and they satisfies $\exists P$ s.t. $B=PAP^{-1}$.</p><blockquote><p>From now on we don’t distinguish between linear operator $\mathcal{A}$ and its matrix representation where choice of basis doesn’t matter.</p></blockquote></li><li><p><strong>Rank</strong>: $rank(A)=\rho(A)\equiv dim(\mathcal{R}(A))$</p></li><li><p><strong>Nullity</strong>: $null(A)=\nu(A)\equiv dim(\mathcal{N}(A))$</p><ul><li>$\rho(A)+\nu(A)=n$ ($n$ is the dimensionality of domain space)</li></ul></li><li><p><strong>Adjoint</strong>: The adjoint of the linear map $\mathcal{A}: \mathcal{V}\rightarrow\mathcal{W}$ is the linear map $\mathcal{A}^*: \mathcal{W}\rightarrow\mathcal{V}$ such that $\langle y,\mathcal{A}(x)\rangle_\mathcal{W}=\langle \mathcal{A}^ *(y),x\rangle_\mathcal{V}$</p><blockquote><p>For its matrix representation, adjoint of $A$ is $A^ *$, which is $A^\top$ for real numbers.<br><br>Properties of $\mathcal{A}^ *$ is similar to matrix $A^ *$</p></blockquote><ul><li>$\mathcal{U}=\mathcal{R}(A)\overset{\perp}{\oplus}\mathcal{N}(A^ *),\;\mathcal{V}=\mathcal{R}(A^ *)\overset{\perp}{\oplus}\mathcal{N}(A)$</li><li>$\mathcal{N}(A^* )=\mathcal{N}(AA^* )\subseteq\mathcal{U},\;\mathcal{R}(A)=\mathcal{R}(AA^*)\subseteq\mathcal{U}$</li></ul></li><li><p><strong>Self-adjoint</strong>: $\mathcal{A}$ is self-adjoint iff $\mathcal{A}^*=\mathcal{A}$.</p><ul><li>For self-adjoint $\mathcal{A}$, if $\mathcal{V}=\mathbb{C}^{n\times n}$ then $A$ is <strong>hermitian</strong>; if $\mathcal{V}=\mathbb{R}^{n\times n}$ then $A$ is <strong>symmetric</strong>.</li><li>Self-adjoint matrices have real eigenvalues and orthogonal eigenvectors</li><li><strong>Skew symmetric</strong>: $A^*=-A$<blockquote><p>For quadratic form $x^\top Ax=x^\top(\frac{A+A^\top}{2}+\frac{A-A^\top}{2})x$, since $A-A^\top$ is skew symmetric, scalar $x^\top (A-A^\top) x=-x^\top (A-A^\top)x$, so the skew-symmetric part is zero. Therefore for quadratic form $x^\top Ax$ we can always assume $A$ is symmetric.</p></blockquote></li></ul></li><li><p><strong>Definiteness</strong>: (for symmetric matrix $P$)</p><ul><li>Positive definite ($P\succ 0$): $\forall x\in\mathbb{R}^n\neq 0,\; x^\top Px&gt;0 \Leftrightarrow$ all eigenvalues of $P$ are positive.</li><li>Semi-positive definite ($P\succcurlyeq 0$): $x^\top Px\geqslant 0 \Leftrightarrow$ all eigenvalues of $P$ are non-negative.</li><li>Negative definite ($P\prec 0$): $x^\top Px &lt; 0 \Leftrightarrow$ all eigenvalues of $P$ are negative.</li></ul></li><li><p><strong>Orthogonal Matrix</strong>: $Q$ is orthogonal iff $Q^\top Q=I$, iff columns of $Q$ are orthonormal.</p><ul><li>If $A\in\mathbb{R}^{n\times b}$ is symmetric, then $\exists$ orthogonal $Q$ s.t. $Q^\top AQ=\Lambda=diag(\lambda_1,\ldots,\lambda_n)$</li></ul></li><li><p><strong>Orthogonal Projection</strong>: Given linear space $\mathcal{X}$ and subspace $\mathcal{M}$, $P(x)=x^*:\mathcal{X}\rightarrow\mathcal{M}$ ($x^ *$ is the projection point) is called orthogonal projection. If $\{v_i\}$ is a orthonormal basis of \mathcal{M}, then $P(x)=\sum_i \langle x,v_i\rangle v_i$</p></li></ul><h2 id="SVD-and-Linear-Equations">SVD and Linear Equations</h2><p>SVD Decomposition is useful in various fields and teached by a lot of courses, its complete version is formulated as $$A=U\Sigma V^*, \Sigma=\begin{bmatrix}\mathbf{\sigma}&amp;\mathbf{0}\\ \mathbf{0}&amp;\mathbf{0}\end{bmatrix}, \mathbf{\sigma}=\begin{bmatrix}\sqrt{\lambda_1}&amp;&amp;&amp;\\ &amp;\sqrt{\lambda_2}&amp;&amp;\\ &amp;&amp;\ddots&amp;\\ &amp;&amp;&amp;\sqrt{\lambda_r}\end{bmatrix},V=\begin{bmatrix}V_1&amp;V_2\end{bmatrix},U=\begin{bmatrix}U_1&amp;U_2\end{bmatrix}$$<br>where</p><ul><li>$\sigma_i$ are called <strong>sigular values</strong>, $\lambda_i$ are eigenvalues of $A^* A$</li><li>Columns of $V_1$ span $\mathcal{R}(A^ *A)=\mathcal{R}(A^ *)$, columns of $V_2$ span $\mathcal{N}(A^ *A)=\mathcal{N}(A)$</li><li>Columns of $U_1=AV_1\sigma^{-1}$ span $\mathcal{R}(A)$, columns of $U_2$ span $\mathcal{N}(A^*)$</li></ul><blockquote><p>SVD can be derived by doing eigenvalue decomposition on $A^* A$</p></blockquote><p>With SVD introduced, we can efficiently solve general linear equation $Ax=b$ as $x=x_r+x_n$ where $x_r\in\mathcal{R}(A^\top)$ and $x_n\in\mathcal{N}(A)$.</p><table><thead><tr><th></th><th>$Ax=b$</th><th>tall $A$ ($m&gt;n$)</th><th>fat $A$ ($m&lt; n$)</th></tr></thead><tbody><tr><td></td><td></td><td>Overdetermined,<br>Least Squares,<br>use Normal Equations</td><td>Underdetermined,<br>Quadratic Programming,<br>use Lagrange Multiplies</td></tr><tr><td>I.$b\in\mathcal{R}(A)$</td><td></td><td></td><td></td></tr><tr><td>1.$\mathcal{N}(A)={0}$</td><td>$x$ exist &amp; is unique</td><td>$x=(A^\top A)^{-1}A^\top b=A^+b$</td><td>$x=A^\top(AA^\top)^{-1}b=A^+b$</td></tr><tr><td>2.$\mathcal{N}(A)\neq{0}$</td><td>$x$ exist &amp; not unique</td><td>$x_r=(A^\top A)^{-1}A^\top b=A^+b$</td><td>$x_r=A^\top(AA^\top)^{-1}b=A^+b$</td></tr><tr><td>II.$b\notin\mathcal{R}(A)$</td><td></td><td></td><td></td></tr><tr><td>1.$\mathcal{N}(A)={0}$</td><td>$x$ not exists, $x_r$ exist &amp; is unique</td><td>$x_r=(A^\top A)^{-1}A^\top b=A^+b$</td><td>$x_r=A^\top(AA^\top)^{-1}b=A^+b$</td></tr><tr><td>2.$\mathcal{N}(A)\neq{0}$</td><td>$x$ not exists, $x_r$ not exist</td><td>$(A^\top A)^{-1}$ invertible</td><td>$(AA^\top)^{-1}$ invertible</td></tr></tbody></table><ul><li>$A^+=(A^\top A)^{-1}A^\top$ is left pseudo-inverse, $A^+=A^\top (AA^\top)^{-1}$ is right pseudo-inverse.</li><li>$A^+$ can be unified by the name <strong>Moore-Penrose Inverse</strong> and calculated using SVD by $A^+=V\Sigma^+ U^\top$ where $\Sigma^+$ take inverse of non-zeros.</li></ul><h2 id="Miscellaneous">Miscellaneous</h2><blockquote><p>Selected theorems and lemmas useful in Linear Algebra. For more matrix properties see <a href="/blog/2019-06/MatrixAlgebra/" title="my post about Matrix Algebra">my post about Matrix Algebra</a></p></blockquote><ul><li>Matrix Square Root: $N^\top N=P$, then $N$ is the square root of $P$<blockquote><p>Square root is not unique. Cholesky decomposition is often used as square root.</p></blockquote></li><li><strong>Schur Complement</strong>: Given matrices $A_{n\times n}, B_{n\times m}, C_{m\times m}$, the matrix $M=\begin{bmatrix}A&amp;B\\ B^\top&amp;C\end{bmatrix}$ is symmetric. Then the following are equivalent (TFAE)<ol><li>$M\succ 0$</li><li>$A\succ 0$ and $C-B^\top A^{-1}B\succ 0$ (LHS called Schur complement of $A$ in $M$)</li><li>$C\succ 0$ and $A-B C^{-1}B^\top\succ 0$ (LHS called Schur complement of $C$ in $M$)</li></ol></li><li>Matrix Inverse Lemma: $(A+BCD)^{-1}=A^{-1}-A^{-1}B\left(C^{-1}+DA^{-1}B\right)^{-1}DA$</li><li>Properties of $A^\top A$<ul><li>$A^\top A \succeq 0$ and $A^\top A \succ 0 \Leftrightarrow A$ has full rank.</li><li>$A^\top A$ and $AA^\top$ have same non-zero eigenvalues, but different eigenvectors.</li><li>If $v$ is eigenvector of $A^\top A$ about $\lambda$, then $Av$ is eigenvector of $AA^\top$ about $\lambda$.</li><li>If $v$ is eigenvector of $AA^\top$ about $\lambda$, then $A^\top v$ is eigenvector of $A^\top A$ about $\lambda$.</li><li>$tr(A^\top A)=tr(AA^\top)=\sum_i\sum_j\left|A_{ij}\right|^2$</li><li>$det(A)=\prod_i\lambda_i, tr(A)=\sum_i\lambda_i$</li></ul></li></ul><h1>Real Analysis</h1><h2 id="Set-theory">Set theory</h2><blockquote><p>$\text{~}S$ stands for complement of set $S$ in following contents. These concepts are discussed under normed space $(\mathcal{X}, \Vert\cdot\Vert)$</p></blockquote><ul><li><strong>Open Ball</strong>: Let $x_0\in\mathcal{X}$ and let $a\in\mathbb{R}, a&gt;0$, then the open ball of radius $a$ about $x_0$ is $B_a(x_0)=\left\{x\in\mathcal{X}\middle| \Vert x-x_0\Vert &lt; a\right\}$<ul><li>Given subset $S\subset \mathcal{X}$, $d(x,S)=0\Leftrightarrow \forall\epsilon &gt;0, B_\epsilon(x)\cap S\neq\emptyset$</li><li>Given subset $S\subset \mathcal{X}$, $d(x,S)&gt;0\Leftrightarrow \exists\epsilon &gt;0, B_\epsilon(x)\cap S=\emptyset$</li></ul></li><li><strong>Interior Point</strong>: Given subset $S\subset\mathcal{X}$, $x\in S$ is an interior point of $S$ iff $\exists\epsilon &gt;0, B_\epsilon(x)\subset S$<ul><li><strong>Interior</strong>: $\mathring{S}=\{x\in \mathcal{X}|x\text{ is an interior point of }S\}=\{x\in\mathcal{X}|d(x,\text{~}S)&gt;0\}$</li></ul></li><li><strong>Open Set</strong>: $S$ is open if $\mathring{S}=S$</li><li><strong>Closure Point</strong>: Given subset $S\subset\mathcal{X}$, $x\in S$ is a closure point of $S$ iff $\forall\epsilon &gt;0, B_\epsilon(x)\cap S\neq\emptyset$.<ul><li><strong>Closure</strong>: $\bar{S}=\{x\in\mathcal{X}|x\text{ is a closure point of }S\}=\{x\in\mathcal{X}|d(x,S)=0\}$<blockquote><p>Note that $\partial\mathcal{X}=\emptyset$</p></blockquote></li></ul></li><li><strong>Closed Set</strong>: $S$ is closed if $\bar{S}=S$<blockquote><p>$S$ is open $\Leftrightarrow$ $\text{~}S$ is closed, $S$ is closed $\Leftrightarrow$ $\text{~}S$ is open. Set being both open and closed is called <strong>clopen</strong>(e.g. the whole set $\mathcal{X}$), empty set is clopen by convention.</p></blockquote></li><li><strong>Set Boundary</strong>: $\partial S=\bar{S}\cap\overline{\text{~}S}=\bar{S}\backslash\mathring{S}$</li></ul><h2 id="Sequences">Sequences</h2><ul><li><strong>Sequence</strong>($\{x_n\}$): a set of vectors indexed by the counting numbers<ul><li><strong>Subsequence</strong>: Let $1\leqslant n_1&lt; n_2&lt;\ldots$ be an infinite set of increasing integers, then $\{x_{n_i}\}$ is a subsequence of $\{x_n\}$</li></ul></li><li><strong>Convergence</strong>($\{x_n\}\rightarrow x\in\mathcal{X}$): $\forall \epsilon&gt;0,\exists N(\epsilon)&lt;\infty\text{ s.t. }\forall n\geqslant N, \Vert x_n-x\Vert &lt;\epsilon$<ul><li>If $x_n \rightarrow x$ and $x_n \rightarrow y$, then $x=y$</li><li>If $x_n \rightarrow x_0$ and $\{x_{n_i}\}$ is a subsequence of $\{x_n\}$, then $\{x_{n_i}\} \rightarrow x_0$</li><li><strong>Cauchy Convergence</strong> (necessary condition for convergence): $\{x_n\}$ is cauchy if $\forall \epsilon&gt;0,\exists N(\epsilon)&lt;\infty$ s.t. $\forall n,m\geqslant N, \Vert x_n-x_m\Vert &lt;\epsilon$</li><li>If $\mathcal{X}$ is finite dimensional, $\{x_n\}$ is cauchy $\Rightarrow$ $\{x_n\}$ has a limit in $\mathcal{X}$</li></ul></li><li><strong>Limit Point</strong>: Given subset $S\subset\mathcal{X}$, $x$ is a limit point of $S$ if $\exists \{x_n\}$ s.t. $\forall n\geqslant 1, x_n\in S$ and $x_n\rightarrow x$<ul><li>$x$ is a limit point of $S$ iff $x\in\bar{S}$</li><li>$S$ is closed iff $S$ contains its limit points</li></ul></li><li><strong>Complete Space</strong>: a normed space is <strong>complete</strong> if every Cauchy sequence has a limit. A complete normed space $(\mathcal{X}, \Vert\cdot\Vert)$ is called a <strong>Banach space</strong>.<ul><li>$S\subset \mathcal{X}$ is complete if every Cauchy sequence with elements from $S$ has a limit in $S$</li><li>$S\subset \mathcal{X}$ is complete $\Rightarrow S$ is closed</li><li>$\mathcal{X}$ is complete and $S\subset\mathcal{X} \Rightarrow S$ is complete</li><li>All finite dimensional subspaces of $X$ are complete</li></ul></li><li><strong>Completion of Normed Space</strong>: $\mathcal{Y}=\bar{\mathcal{X}}=\mathcal{X}+\{$all limit points of Cauchy sequences in $\mathcal{X}\}$<blockquote><p>E.g. $C[a,b]$ contains continuous functions over $[a,b]$. $(C[a,b], \Vert\cdot\Vert_1)$ is not complete, $(C[a,b], \Vert\cdot\Vert_\infty)$ is complete. Completion of $(C[a,b], \Vert\cdot\Vert_1)$ requires Lebesque integration.</p></blockquote></li><li><strong>Contraction Mapping</strong>: Let $S\subset\mathcal{X}$ be a subset and $T:S\rightarrow S$ is a contraction mapping if $\exists 0\leqslant c\leqslant 1$ such that, $\forall x,y \in S, \Vert T(x)-T(y)\Vert\leqslant c\Vert x-y\Vert$<ul><li><strong>Fixed Point</strong>: $x^* \in\mathcal{X}$ is a fixed point of $T$ if $T(x^ *)=x^ *$</li><li><a href="https://en.wikipedia.org/wiki/Banach_fixed-point_theorem" target="_blank" rel="noopener"><strong>Contraction Mapping Theorem</strong>(不动点定理)</a>: If $T:S\rightarrow S$ is a contraction mapping in a complete subset $S$, then $\exists! x^ *\in\mathcal{X}\text{ s.t. }T(x^ *)=x^ *$. Moreover, $\forall x_0\in S$, the sequence $x_{k+1}=T(x_k),k\geqslant 0$ is Cauchy and converges to $x^ *$.<blockquote><p>E.g. Newton Method: $x_{k+1}=x_k-\epsilon\left[\frac{\partial h}{\partial x}(x_k)\right]^{-1}\left(h(x_k)-y\right)$</p></blockquote></li></ul></li></ul><h2 id="Continuity-and-Compactness">Continuity and Compactness</h2><ul><li><strong>Continuous</strong>: Let $(\mathcal{X},\Vert\cdot\Vert_\mathcal{X})$ and $(\mathcal{Y},\Vert\cdot\Vert_\mathcal{Y})$ be two normed spaces. A function $f:\mathcal{X}\rightarrow\mathcal{Y}$ is continuous at $x_0\in\mathcal{X}$ if $\forall\epsilon &gt;0,\exists \delta(\epsilon,x_0)&gt;0\text{ s.t. }\Vert x-x_0\Vert_\mathcal{X}&lt;\delta \Rightarrow\Vert f(x)-f(x_0)\Vert_\mathcal{Y} &lt;\epsilon$<ul><li>$f$ is continuous on $S\subset\mathcal{X}$ if $f$ is continuous at $\forall x_0\in S$</li><li>If $f$ in continuous at $x_0$ and $\{x_n\}$ is a sequence s.t. $x_n\rightarrow x_0$, then the sequence $\{f(x_n)\}$ in $\mathcal{Y}$ converges to $f(x_0)$</li><li>If $f$ is discontinuous at $x_0$, then $\exists \{x_n\}\in\mathcal{X}$ s.t. $x_n\rightarrow x_0$ but $f(x_n)\nrightarrow f(x_0)$</li></ul></li><li><strong>Compact</strong>: $S\subset\mathcal{X}$ is (sequentially) compact if every sequence in $S$ has a convergent subsequence with limit in $S$</li><li><strong>Bounded</strong>: $S\subset\mathcal{S}$ is bounded if $\exists r&lt;\infty$ such that $S\subset B_r(0)$<ul><li>$S$ is compact $\Rightarrow$ $S$ is closed and bounded</li><li><strong>Bolzano-Weierstrass Theorem</strong>: In a finite-dimensional normed space, $C$ is closed and bounded $\Leftrightarrow$ for $C$ is compact</li></ul></li><li><strong>Weierstrass Theorem</strong>: If $C\subset\mathcal{X}$ is a compact subset and $f:C\rightarrow\mathbb{R}$ is continuous at each point of $C$, then $f$ achieves its extreme values, i.e. $\exists \bar{x}\in C\text{ s.t. }f(\bar{x})=\sup_{x\in C} f(x)$ and $\exists \underline{x}\in C\text{ s.t. }f(\underline{x})=\inf_{x\in C} f(x)$<ul><li>$f:C\rightarrow\mathbb{R}$ continuous and $C$ compact $\Rightarrow$ $\sup_{x\in C}f(x)&lt;\infty$</li></ul></li></ul></div><div class="reward-container"><div>Treat me some coffee XD</div><button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'>Donate</button><div id="qr" style="display:none"><div style="display:inline-block"><img src="/blog/uploads/assets/wechatpay.jpg" alt="Jacob Zhong WeChat Pay"><p>WeChat Pay</p></div><div style="display:inline-block"><img src="/blog/uploads/assets/alipay.jpg" alt="Jacob Zhong Alipay"><p>Alipay</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>Post author: </strong>Jacob Zhong</li><li class="post-copyright-link"><strong>Post link: </strong><a href="http://zyxin.xyz/blog/2020-06/AlgebraBasicsNotes/" title="Notes for Algebra Basics">http://zyxin.xyz/blog/2020-06/AlgebraBasicsNotes/</a></li><li class="post-copyright-license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/blog/tags/Math/" rel="tag"># Math</a> <a href="/blog/tags/Algebra/" rel="tag"># Algebra</a></div><div class="post-nav"><div class="post-nav-item"><a href="/blog/2020-06/ControlSystemNotes/" rel="prev" title="Control System Notes"><i class="fa fa-chevron-left"></i> Control System Notes</a></div><div class="post-nav-item"></div></div></footer></article></div><div class="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">Table of Contents</li><li class="sidebar-nav-overview">Overview</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#null"><span class="nav-number">1.</span> <span class="nav-text">Algebraic Structures</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Operation"><span class="nav-number">1.1.</span> <span class="nav-text">Operation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Group"><span class="nav-number">1.2.</span> <span class="nav-text">Group</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ring"><span class="nav-number">1.3.</span> <span class="nav-text">Ring</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Field"><span class="nav-number">1.4.</span> <span class="nav-text">Field</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Vector-Space"><span class="nav-number">1.5.</span> <span class="nav-text">Vector Space</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Basis-Coordinate"><span class="nav-number">1.5.1.</span> <span class="nav-text">Basis &amp; Coordinate</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Norm-Inner-product"><span class="nav-number">1.5.2.</span> <span class="nav-text">Norm &amp; Inner product</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gramian"><span class="nav-number">1.5.3.</span> <span class="nav-text">Gramian</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#null"><span class="nav-number">2.</span> <span class="nav-text">Linear Algebra</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Linear-Operator"><span class="nav-number">2.1.</span> <span class="nav-text">Linear Operator</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SVD-and-Linear-Equations"><span class="nav-number">2.2.</span> <span class="nav-text">SVD and Linear Equations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Miscellaneous"><span class="nav-number">2.3.</span> <span class="nav-text">Miscellaneous</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#null"><span class="nav-number">3.</span> <span class="nav-text">Real Analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Set-theory"><span class="nav-number">3.1.</span> <span class="nav-text">Set theory</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sequences"><span class="nav-number">3.2.</span> <span class="nav-text">Sequences</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Continuity-and-Compactness"><span class="nav-number">3.3.</span> <span class="nav-text">Continuity and Compactness</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Jacob Zhong" src="/blog/uploads/assets/avatar.png"><p class="site-author-name" itemprop="name">Jacob Zhong</p><div class="site-description" itemprop="description">Blog of Jacob Zhong</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/blog/archives/"><span class="site-state-item-count">50</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/blog/categories/"><span class="site-state-item-count">13</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/blog/tags/"><span class="site-state-item-count">44</span> <span class="site-state-item-name">tags</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/cmpute" title="GitHub → https://github.com/cmpute" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://www.linkedin.com/in/jacobzhong/" title="LinkedIn → https://www.linkedin.com/in/jacobzhong/" rel="noopener" target="_blank"><i class="fa fa-fw fa-linkedin-square"></i>LinkedIn</a> </span><span class="links-of-author-item"><a href="mailto:cmpute@foxmail.com" title="E-Mail → mailto:cmpute@foxmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="https://www.last.fm/user/cmpute" title="Last.fm → https://www.last.fm/user/cmpute" rel="noopener" target="_blank"><i class="fa fa-fw fa-lastfm-square"></i>Last.fm</a></span></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Friend Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="https://hanyuz1996.github.io/" title="https://hanyuz1996.github.io/" rel="noopener" target="_blank">Hanyuz</a></li><li class="links-of-blogroll-item"><a href="https://xingminw.github.io/" title="https://xingminw.github.io/" rel="noopener" target="_blank">Xingminw</a></li></ul></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2017 – <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-fire"></i> </span><span class="author" itemprop="copyrightHolder">Jacob Zhong</span></div><div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="Total Visitors"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="Total Views"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/blog/lib/anime.min.js"></script><script src="/blog/lib/velocity/velocity.min.js"></script><script src="/blog/lib/velocity/velocity.ui.min.js"></script><script src="/blog/js/utils.js"></script><script src="/blog/js/motion.js"></script><script src="/blog/js/schemes/muse.js"></script><script src="/blog/js/next-boot.js"></script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><script>function loadCount(){var o=document,d=o.createElement("script");d.src="https://jacob-zhongs-blog.disqus.com/count.js",d.id="dsq-count-scr",(o.head||o.body).appendChild(d)}window.addEventListener("load",loadCount,!1)</script><script>var disqus_config = function() {
    this.page.url = "http://zyxin.xyz/blog/2020-06/AlgebraBasicsNotes/";
    this.page.identifier = "2020-06/AlgebraBasicsNotes/";
    this.page.title = "Notes for Algebra Basics";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://jacob-zhongs-blog.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });</script></body></html>